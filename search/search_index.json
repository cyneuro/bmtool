{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BMTool","text":"<p>A collection of modules to make developing Neuron and BMTK models easier.</p> <p></p>"},{"location":"#overview","title":"Overview","text":"<p>BMTool provides several modules to simplify the development of computational neuroscience models with NEURON and the Brain Modeling Toolkit (BMTK). It offers functionality for:</p> <ul> <li>Single Cell Modeling: Analyze passive properties, current injection, FI curves, and impedance profiles</li> <li>Synapse Development: Tools for tuning synaptic properties and gap junctions</li> <li>Network Construction: Connectors for building complex network structures</li> <li>Visualization: Plot connection matrices, network positions, and more</li> <li>Simulation Management: Run simulations on SLURM clusters with parameter sweeps</li> <li>Analysis: Process simulation results efficiently</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation\npip install bmtool\n\n# For development installation\ngit clone https://github.com/cyneuro/bmtool.git\ncd bmtool\npython setup.py develop\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<p>BMTool provides multiple modules to assist with different aspects of neural modeling:</p> <ul> <li>Single Cell Module: Analyze and tune biophysical cell models</li> <li>Synapses Module: Configure and tune synaptic connections</li> <li>Connectors Module: Build complex network connectivity patterns</li> <li>BMPlot Module: Visualize network connectivity and simulation results</li> <li>Analysis Module: Process spike and report data from simulations</li> <li>SLURM Module: Manage simulation workflows on HPC clusters</li> <li>Graphs Module: Analyze network properties and connectivity patterns</li> </ul>"},{"location":"#command-line-interface","title":"Command Line Interface","text":"<p>BMTool provides a CLI for accessing functionality directly from the command line:</p> <pre><code>bmtool --help\n</code></pre> <p>See the CLI documentation for more details.</p>"},{"location":"cli/","title":"Command Line Interface","text":"<p>BMTool provides a command-line interface (CLI) that makes many of its features accessible without writing Python code. This page documents the available commands and their usage.</p>"},{"location":"cli/#basic-usage","title":"Basic Usage","text":"<p>To see all available commands:</p> <pre><code>bmtool --help\n</code></pre> <pre><code>Usage: bmtool [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --verbose  Verbose printing\n  --help     Show this message and exit.\n\nCommands:\n  debug\n  plot\n  util\n</code></pre>"},{"location":"cli/#plot-commands","title":"Plot Commands","text":"<p>The <code>plot</code> command provides access to visualization features:</p> <pre><code>bmtool plot --help\n</code></pre> <pre><code>Usage: bmtool plot [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --config PATH  Configuration file to use, default: \"simulation_config.json\"\n  --no-display   When set there will be no plot displayed, useful for saving\n                 plots\n  --help         Show this message and exit.\n\nCommands:\n  connection  Display information related to neuron connections\n  positions   Plot cell positions for a given set of populations\n  raster      Plot the spike raster for a given population\n  report      Plot the specified report using BMTK's default report plotter\n</code></pre>"},{"location":"cli/#examples","title":"Examples","text":"<pre><code># Plot raster from simulation output\nbmtool plot raster\n\n# Plot cell positions\nbmtool plot positions\n\n# Plot connection information\nbmtool plot connection\n</code></pre>"},{"location":"cli/#utility-commands","title":"Utility Commands","text":"<p>The <code>util</code> command provides access to various utilities:</p> <pre><code>bmtool util --help\n</code></pre>"},{"location":"cli/#cell-utilities","title":"Cell Utilities","text":"<pre><code>bmtool util cell --help\n</code></pre>"},{"location":"cli/#cell-tuning","title":"Cell Tuning","text":"<pre><code># For BMTK models with a simulation_config.json file\nbmtool util cell tune --builder\n\n# For non-BMTK cell tuning\nbmtool util cell --template TemplateFile.hoc --mod-folder ./ tune --builder\n</code></pre>"},{"location":"cli/#vhalf-segregation","title":"VHalf Segregation","text":"<pre><code># Interactive wizard mode\nbmtool util cell vhseg\n\n# Command mode\nbmtool util cell --template CA3PyramidalCell vhseg --othersec dend[0],dend[1] \\\n  --infvars inf_im --segvars gbar_im --gleak gl_ichan2CA3 --eleak el_ichan2CA3\n</code></pre> <pre><code># For building simple models\nbmtool util cell --hoc cell_template.hoc vhsegbuild --build\nbmtool util cell --hoc segmented_template.hoc vhsegbuild\n</code></pre>"},{"location":"cli/#debug-commands","title":"Debug Commands","text":"<p>The <code>debug</code> command provides debug utilities:</p> <pre><code>bmtool debug --help\n</code></pre>"},{"location":"contributing/","title":"Contributing to BMTool","text":"<p>Thank you for your interest in contributing to BMTool. This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"contributing/#development-installation","title":"Development Installation","text":"<p>For development, install BMTool in development mode:</p> <pre><code>git clone https://github.com/cyneuro/bmtool.git\ncd bmtool\npython setup.py develop\n</code></pre>"},{"location":"contributing/#package-management","title":"Package Management","text":""},{"location":"contributing/#uploading-to-pypi","title":"Uploading to PyPI","text":"<p>To upload a new version to PyPI, follow these steps:</p> <ol> <li>Install required tools:</li> </ol> <pre><code># Install setuptools and wheel\npython -m pip install --user --upgrade setuptools wheel\n</code></pre> <ol> <li>Build the distribution packages:</li> </ol> <pre><code># Run from setup.py directory\npython setup.py sdist bdist_wheel\n</code></pre> <p>This will generate files in the <code>dist</code> directory: <pre><code>dist/\n  bmtool-X.Y.Z-py3-none-any.whl\n  bmtool-X.Y.Z.tar.gz\n</code></pre></p> <ol> <li>Upload to PyPI:</li> </ol> <pre><code># Install Twine\npython -m pip install --user --upgrade twine\n\n# Upload to Test PyPI (optional)\npython -m twine upload --repository-url https://test.pypi.org/legacy/ dist/*\n\n# Upload to PyPI\npython -m twine upload dist/*\n</code></pre>"},{"location":"contributing/#combined-commands","title":"Combined commands","text":"<p>For convenience, here are all the commands together:</p> <pre><code>python -m pip install --upgrade setuptools wheel\npython setup.py sdist bdist_wheel\npython -m pip install --upgrade twine\npython -m twine upload dist/*\n</code></pre>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Update documentation if needed</li> <li>Ensure all tests pass</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#documentation-contributions","title":"Documentation Contributions","text":"<p>To contribute to the documentation:</p> <ol> <li>Install MkDocs and required extensions:</li> </ol> <pre><code>pip install mkdocs mkdocs-material pymdown-extensions mkdocstrings mkdocstrings-python\n</code></pre> <ol> <li> <p>Make changes to the Markdown files in the <code>docs/</code> directory</p> </li> <li> <p>Preview locally:</p> </li> </ol> <pre><code>mkdocs serve\n</code></pre> <ol> <li>Build the documentation:</li> </ol> <pre><code>mkdocs build\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Tests for BMTool are a work in progress. When contributing, please ensure your changes don't break existing functionality.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>Please follow these guidelines for code style: - Use 4 spaces for indentation (not tabs) - Follow PEP 8 style guidelines where possible - Use meaningful variable and function names - Add docstrings for functions and classes</p>"},{"location":"getting-started/","title":"Getting Started with BMTool","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>BMTool can be installed directly from PyPI:</p> <pre><code>pip install bmtool\n</code></pre>"},{"location":"getting-started/#development-installation","title":"Development Installation","text":"<p>For developers who will be contributing to BMTool or need the latest features:</p> <pre><code>git clone https://github.com/cyneuro/bmtool.git\ncd bmtool\npython setup.py develop\n</code></pre> <p>Update the repository (from the bmtool directory) with:</p> <pre><code>git pull\n</code></pre>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>BMTool requires:</p> <ul> <li>Python 3.6 or later</li> <li>NEURON 7.7 or later (for cell modeling functionality)</li> <li>BMTK (Brain Modeling Toolkit)</li> </ul> <p>Additional dependencies are automatically installed with the package.</p>"},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/#command-line-interface","title":"Command Line Interface","text":"<p>BMTool provides a command-line interface for easy access to many features:</p> <pre><code># View available commands\nbmtool --help\n\n# Access plotting functionality\nbmtool plot --help\n\n# Access utility functions\nbmtool util --help\n</code></pre>"},{"location":"getting-started/#python-module-usage","title":"Python Module Usage","text":"<p>BMTool can be imported as a Python module to access its functionality:</p> <pre><code># Import specific modules\nfrom bmtool.singlecell import Profiler, Passive, CurrentClamp, FI, ZAP\nfrom bmtool.bmplot import total_connection_matrix, plot_3d_positions\nfrom bmtool.connectors import UnidirectionConnector, ReciprocalConnector\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Check out the module documentation for details on specific modules</li> <li>Explore the examples to learn how to use BMTool features</li> <li>Read the API reference for detailed function and class documentation</li> </ul>"},{"location":"api/analysis/","title":"Analysis API Reference","text":"<p>This page provides an overview of the Analysis module, which contains several submodules for different types of neural data analysis.</p> <p>The Analysis module provides tools for processing and analyzing simulation results from BMTK models, including:</p>"},{"location":"api/analysis/#spike-analysis","title":"Spike Analysis","text":"<p>The <code>spikes</code> module provides functions for loading and analyzing spike data from simulations, including: - Loading spike data into pandas DataFrames - Computing firing rate statistics - Calculating population spike rates</p>"},{"location":"api/analysis/#lfpecp-analysis","title":"LFP/ECP Analysis","text":"<p>The <code>lfp</code> module provides tools for analyzing local field potentials (LFP) and extracellular potentials (ECP), including: - Loading and processing ECP/LFP data - Time series analysis and filtering - Spectral analysis and wavelet transforms - Signal-to-noise ratio calculations</p>"},{"location":"api/analysis/#entrainment-analysis","title":"Entrainment Analysis","text":"<p>The <code>entrainment</code> module provides tools for analyzing the relationship between spikes and LFP signals, including: - Phase-locking value (PLV) calculations - Pairwise phase consistency (PPC) analysis - Spike-LFP entrainment metrics - Spike rate and LFP power correlations</p>"},{"location":"api/analysis/#network-connectivity-analysis","title":"Network Connectivity Analysis","text":"<p>The <code>netcon_reports</code> module provides tools for analyzing and reporting network connectivity statistics.</p>"},{"location":"api/bmplot/","title":"BMPlot API Reference","text":"<p>This page provides API reference documentation for the BMPlot module which contains functions for plotting and visualizing BMTK network models and simulation results.</p>"},{"location":"api/bmplot/#connections-module","title":"Connections Module","text":""},{"location":"api/bmplot/#bmtool.bmplot.connections.is_notebook","title":"<code>bmtool.bmplot.connections.is_notebook()</code>","text":"<p>Detect if code is running in a Jupyter notebook environment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if running in a Jupyter notebook, False otherwise.</p> Notes <p>This is used to determine whether to call plt.show() explicitly or rely on Jupyter's automatic display functionality.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; if is_notebook():\n...     plt.show()\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def is_notebook() -&gt; bool:\n    \"\"\"\n    Detect if code is running in a Jupyter notebook environment.\n\n    Returns\n    -------\n    bool\n        True if running in a Jupyter notebook, False otherwise.\n\n    Notes\n    -----\n    This is used to determine whether to call plt.show() explicitly or\n    rely on Jupyter's automatic display functionality.\n\n    Examples\n    --------\n    &gt;&gt;&gt; if is_notebook():\n    ...     plt.show()\n    \"\"\"\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == \"ZMQInteractiveShell\":\n            return True  # Jupyter notebook or qtconsole\n        elif shell == \"TerminalInteractiveShell\":\n            return False  # Terminal running IPython\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False  # Probably standard Python interpreter\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.total_connection_matrix","title":"<code>bmtool.bmplot.connections.total_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, synaptic_info='0', include_gap=True)</code>","text":"<p>Generate a plot displaying total connections or other synaptic statistics.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network names to use as sources.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network names to use as targets.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifiers to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifiers to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, don't display population name before sid or tid in the plot. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>synaptic_info</code> <code>str</code> <p>Type of information to display. Options: - '0': Total connections (default) - '1': Mean and standard deviation of connections - '2': All synapse .mod files used - '3': All synapse .json files used</p> <code>'0'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions and chemical synapses in the analysis. If False, only include chemical synapses. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; total_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     title='PN to LN Connections'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def total_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    synaptic_info: str = \"0\",\n    include_gap: bool = True,\n) -&gt; None:\n    \"\"\"\n    Generate a plot displaying total connections or other synaptic statistics.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network names to use as sources.\n    targets : str, optional\n        Comma-separated string of network names to use as targets.\n    sids : str, optional\n        Comma-separated string of source node identifiers to filter.\n    tids : str, optional\n        Comma-separated string of target node identifiers to filter.\n    no_prepend_pop : bool, optional\n        If True, don't display population name before sid or tid in the plot. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    synaptic_info : str, optional\n        Type of information to display. Options:\n        - '0': Total connections (default)\n        - '1': Mean and standard deviation of connections\n        - '2': All synapse .mod files used\n        - '3': All synapse .json files used\n    include_gap : bool, optional\n        If True, include gap junctions and chemical synapses in the analysis.\n        If False, only include chemical synapses. Default is True.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; total_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     title='PN to LN Connections'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    text, num, source_labels, target_labels = util.connection_totals(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        synaptic_info=synaptic_info,\n        include_gap=include_gap,\n    )\n\n    if title is None or title == \"\":\n        title = \"Total Connections\"\n    if synaptic_info == \"1\":\n        title = \"Mean and Stdev # of Conn on Target\"\n    if synaptic_info == \"2\":\n        title = \"All Synapse .mod Files Used\"\n    if synaptic_info == \"3\":\n        title = \"All Synapse .json Files Used\"\n\n    plot_connection_info(\n        text, num, source_labels, target_labels, title, syn_info=synaptic_info, save_file=save_file\n    )\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.percent_connection_matrix","title":"<code>bmtool.bmplot.connections.percent_connection_matrix(config, nodes=None, edges=None, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, method='total', include_gap=True, return_dict=False)</code>","text":"<p>Generates a plot showing the percent connectivity of a network.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid in the plot. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method for calculating percent connectivity. Options: 'total', 'uni', 'bi'. Default is 'total'.</p> <code>'total'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. If False, only include chemical synapses. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>(dict, optional)</code> <p>Dictionary containing connection information if return_dict=True, None otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = percent_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     return_dict=True\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def percent_connection_matrix(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    method: str = \"total\",\n    include_gap: bool = True,\n    return_dict: bool = False,\n) -&gt; Optional[Dict]:\n    \"\"\"\n    Generates a plot showing the percent connectivity of a network.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid in the plot. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    method : str, optional\n        Method for calculating percent connectivity. Options: 'total', 'uni', 'bi'.\n        Default is 'total'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. If False, only include chemical synapses.\n        Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is False.\n\n    Returns\n    -------\n    dict, optional\n        Dictionary containing connection information if return_dict=True, None otherwise.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = percent_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     return_dict=True\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    text, num, source_labels, target_labels = util.percent_connections(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        method=method,\n        include_gap=include_gap,\n    )\n    if title is None or title == \"\":\n        title = \"Percent Connectivity\"\n\n    if return_dict:\n        result_dict = plot_connection_info(\n            text, num, source_labels, target_labels, title, save_file=save_file, return_dict=return_dict\n        )\n        return result_dict\n    else:\n        plot_connection_info(text, num, source_labels, target_labels, title, save_file=save_file)\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.probability_connection_matrix","title":"<code>bmtool.bmplot.connections.probability_connection_matrix(config, nodes=None, edges=None, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, dist_X=True, dist_Y=True, dist_Z=True, bins=8, line_plot=False, verbose=False, include_gap=True)</code>","text":"<p>Generates probability graphs showing connectivity as a function of distance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>dist_X</code> <code>bool</code> <p>If True, include X distance in calculations. Default is True.</p> <code>True</code> <code>dist_Y</code> <code>bool</code> <p>If True, include Y distance in calculations. Default is True.</p> <code>True</code> <code>dist_Z</code> <code>bool</code> <p>If True, include Z distance in calculations. Default is True.</p> <code>True</code> <code>bins</code> <code>int</code> <p>Number of distance bins for the probability calculation. Default is 8.</p> <code>8</code> <code>line_plot</code> <code>bool</code> <p>If True, plot lines instead of bars. Default is False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, print debugging information. Default is False.</p> <code>False</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> Notes <p>This function needs model_template to be defined to work properly.</p> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def probability_connection_matrix(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    dist_X: bool = True,\n    dist_Y: bool = True,\n    dist_Z: bool = True,\n    bins: int = 8,\n    line_plot: bool = False,\n    verbose: bool = False,\n    include_gap: bool = True,\n) -&gt; None:\n    \"\"\"\n    Generates probability graphs showing connectivity as a function of distance.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    dist_X : bool, optional\n        If True, include X distance in calculations. Default is True.\n    dist_Y : bool, optional\n        If True, include Y distance in calculations. Default is True.\n    dist_Z : bool, optional\n        If True, include Z distance in calculations. Default is True.\n    bins : int, optional\n        Number of distance bins for the probability calculation. Default is 8.\n    line_plot : bool, optional\n        If True, plot lines instead of bars. Default is False.\n    verbose : bool, optional\n        If True, print debugging information. Default is False.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Notes\n    -----\n    This function needs model_template to be defined to work properly.\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    throwaway, data, source_labels, target_labels = util.connection_probabilities(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        dist_X=dist_X,\n        dist_Y=dist_Y,\n        dist_Z=dist_Z,\n        num_bins=bins,\n        include_gap=include_gap,\n    )\n    if not data.any():\n        return\n    if data[0][0] == -1:\n        return\n    # plot_connection_info(data,source_labels,target_labels,title, save_file=save_file)\n\n    # plt.clf()# clears previous plots\n    np.seterr(divide=\"ignore\", invalid=\"ignore\")\n    num_src, num_tar = data.shape\n    fig, axes = plt.subplots(nrows=num_src, ncols=num_tar, figsize=(12, 12))\n    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n\n    for x in range(num_src):\n        for y in range(num_tar):\n            ns = data[x][y][\"ns\"]\n            bins_data = data[x][y][\"bins\"]\n\n            XX = bins_data[:-1]\n            YY = ns[0] / ns[1]\n\n            if line_plot:\n                axes[x, y].plot(XX, YY)\n            else:\n                axes[x, y].bar(XX, YY)\n\n            if x == num_src - 1:\n                axes[x, y].set_xlabel(target_labels[y])\n            if y == 0:\n                axes[x, y].set_ylabel(source_labels[x])\n\n            if verbose:\n                print(\"Source: [\" + source_labels[x] + \"] | Target: [\" + target_labels[y] + \"]\")\n                print(\"X:\")\n                print(XX)\n                print(\"Y:\")\n                print(YY)\n\n    tt = \"Distance Probability Matrix\"\n    if title:\n        tt = title\n    st = fig.suptitle(tt, fontsize=14)\n    fig.text(0.5, 0.04, \"Target\", ha=\"center\")\n    fig.text(0.04, 0.5, \"Source\", va=\"center\", rotation=\"vertical\")\n    notebook = is_notebook()\n    if not notebook:\n        fig.show()\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.convergence_connection_matrix","title":"<code>bmtool.bmplot.connections.convergence_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, convergence=True, method='mean+std', include_gap=True, return_dict=None)</code>","text":"<p>Generates connection plot displaying synaptic convergence data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>convergence</code> <code>bool</code> <p>If True, compute convergence; if False, compute divergence. Default is True.</p> <code>True</code> <code>method</code> <code>str</code> <p>Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'. Default is 'mean+std'.</p> <code>'mean+std'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>(dict, optional)</code> <p>Dictionary containing connection information if return_dict=True, None otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = convergence_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='mean+std'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def convergence_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    convergence: bool = True,\n    method: str = \"mean+std\",\n    include_gap: bool = True,\n    return_dict: Optional[bool] = None,\n) -&gt; Optional[Dict]:\n    \"\"\"\n    Generates connection plot displaying synaptic convergence data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    convergence : bool, optional\n        If True, compute convergence; if False, compute divergence. Default is True.\n    method : str, optional\n        Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'.\n        Default is 'mean+std'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is None.\n\n    Returns\n    -------\n    dict, optional\n        Dictionary containing connection information if return_dict=True, None otherwise.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = convergence_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='mean+std'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    return divergence_connection_matrix(\n        config,\n        title,\n        sources,\n        targets,\n        sids,\n        tids,\n        no_prepend_pop,\n        save_file,\n        convergence,\n        method,\n        include_gap=include_gap,\n        return_dict=return_dict,\n    )\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.divergence_connection_matrix","title":"<code>bmtool.bmplot.connections.divergence_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, convergence=False, method='mean+std', include_gap=True, return_dict=None)</code>","text":"<p>Generates connection plot displaying synaptic divergence data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>convergence</code> <code>bool</code> <p>If True, compute convergence; if False, compute divergence. Default is False.</p> <code>False</code> <code>method</code> <code>str</code> <p>Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'. Default is 'mean+std'.</p> <code>'mean+std'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>(dict, optional)</code> <p>Dictionary containing connection information if return_dict=True, None otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = divergence_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='mean+std'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def divergence_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    convergence: bool = False,\n    method: str = \"mean+std\",\n    include_gap: bool = True,\n    return_dict: Optional[bool] = None,\n) -&gt; Optional[Dict]:\n    \"\"\"\n    Generates connection plot displaying synaptic divergence data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    convergence : bool, optional\n        If True, compute convergence; if False, compute divergence. Default is False.\n    method : str, optional\n        Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'.\n        Default is 'mean+std'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is None.\n\n    Returns\n    -------\n    dict, optional\n        Dictionary containing connection information if return_dict=True, None otherwise.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = divergence_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='mean+std'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    syn_info, data, source_labels, target_labels = util.connection_divergence(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        convergence=convergence,\n        method=method,\n        include_gap=include_gap,\n    )\n\n    # data, labels = util.connection_divergence_average(config=config,nodes=nodes,edges=edges,populations=populations)\n\n    if title is None or title == \"\":\n        if method == \"min\":\n            title = \"Minimum \"\n        elif method == \"max\":\n            title = \"Maximum \"\n        elif method == \"std\":\n            title = \"Standard Deviation \"\n        elif method == \"mean\":\n            title = \"Mean \"\n        else:\n            title = \"Mean + Std \"\n\n        if convergence:\n            title = title + \"Synaptic Convergence\"\n        else:\n            title = title + \"Synaptic Divergence\"\n    if return_dict:\n        result_dict = plot_connection_info(\n            syn_info,\n            data,\n            source_labels,\n            target_labels,\n            title,\n            save_file=save_file,\n            return_dict=return_dict,\n        )\n        return result_dict\n    else:\n        plot_connection_info(\n            syn_info, data, source_labels, target_labels, title, save_file=save_file\n        )\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.gap_junction_matrix","title":"<code>bmtool.bmplot.connections.gap_junction_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, method='convergence')</code>","text":"<p>Generates connection plot displaying gap junction data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method for computing gap junction statistics. Options: 'convergence', 'percent'. Default is 'convergence'.</p> <code>'convergence'</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined, sources/targets are not defined, or method is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gap_junction_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='convergence'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def gap_junction_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    method: str = \"convergence\",\n) -&gt; None:\n    \"\"\"\n    Generates connection plot displaying gap junction data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    method : str, optional\n        Method for computing gap junction statistics. Options: 'convergence', 'percent'.\n        Default is 'convergence'.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If config is not defined, sources/targets are not defined, or method is invalid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; gap_junction_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='convergence'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    if method != \"convergence\" and method != \"percent\":\n        raise Exception(\"type must be 'convergence' or 'percent'\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    syn_info, data, source_labels, target_labels = util.gap_junction_connections(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        method=method,\n    )\n\n    def filter_rows(\n        syn_info: np.ndarray,\n        data: np.ndarray,\n        source_labels: List,\n        target_labels: List,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, List]:\n        \"\"\"\n        Filters out rows in a connectivity matrix that contain only NaN or zero values.\n\n        This function is used to clean up connection matrices by removing rows that have\n        no meaningful data, which helps create more informative visualizations of network connectivity.\n\n        Parameters\n        ----------\n        syn_info : np.ndarray\n            Array containing synaptic information corresponding to the data matrix.\n        data : np.ndarray\n            2D matrix containing connectivity data with rows representing sources\n            and columns representing targets.\n        source_labels : list\n            List of labels for the source populations corresponding to rows in the data matrix.\n        target_labels : list\n            List of labels for the target populations corresponding to columns in the data matrix.\n\n        Returns\n        -------\n        tuple\n            A tuple containing (syn_info, data, source_labels, target_labels) with invalid rows removed.\n        \"\"\"\n        # Identify rows with all NaN or all zeros\n        valid_rows = ~np.all(np.isnan(data), axis=1) &amp; ~np.all(data == 0, axis=1)\n\n        # Filter rows based on valid_rows mask\n        new_syn_info = syn_info[valid_rows]\n        new_data = data[valid_rows]\n        new_source_labels = np.array(source_labels)[valid_rows]\n\n        return new_syn_info, new_data, new_source_labels, target_labels\n\n    def filter_rows_and_columns(\n        syn_info: np.ndarray,\n        data: np.ndarray,\n        source_labels: List,\n        target_labels: List,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, List, List]:\n        \"\"\"\n        Filters out both rows and columns in a connectivity matrix that contain only NaN or zero values.\n\n        This function performs a two-step filtering process: first removing rows with no data,\n        then transposing the matrix and removing columns with no data (by treating them as rows).\n        This creates a cleaner, more informative connectivity matrix visualization.\n\n        Parameters\n        ----------\n        syn_info : np.ndarray\n            Array containing synaptic information corresponding to the data matrix.\n        data : np.ndarray\n            2D matrix containing connectivity data with rows representing sources\n            and columns representing targets.\n        source_labels : list\n            List of labels for the source populations corresponding to rows in the data matrix.\n        target_labels : list\n            List of labels for the target populations corresponding to columns in the data matrix.\n\n        Returns\n        -------\n        tuple\n            A tuple containing (syn_info, data, source_labels, target_labels) with both\n            invalid rows and columns removed.\n        \"\"\"\n        # Filter rows first\n        syn_info, data, source_labels, target_labels = filter_rows(\n            syn_info, data, source_labels, target_labels\n        )\n\n        # Transpose data to filter columns\n        transposed_syn_info = np.transpose(syn_info)\n        transposed_data = np.transpose(data)\n        transposed_source_labels = target_labels\n        transposed_target_labels = source_labels\n\n        # Filter columns (by treating them as rows in transposed data)\n        (\n            transposed_syn_info,\n            transposed_data,\n            transposed_source_labels,\n            transposed_target_labels,\n        ) = filter_rows(\n            transposed_syn_info, transposed_data, transposed_source_labels, transposed_target_labels\n        )\n\n        # Transpose back to original orientation\n        filtered_syn_info = np.transpose(transposed_syn_info)\n        filtered_data = np.transpose(transposed_data)\n        filtered_source_labels = transposed_target_labels  # Back to original source_labels\n        filtered_target_labels = transposed_source_labels  # Back to original target_labels\n\n        return filtered_syn_info, filtered_data, filtered_source_labels, filtered_target_labels\n\n    syn_info, data, source_labels, target_labels = filter_rows_and_columns(\n        syn_info, data, source_labels, target_labels\n    )\n\n    if title is None or title == \"\":\n        title = \"Gap Junction\"\n        if method == \"convergence\":\n            title += \" Syn Convergence\"\n        elif method == \"percent\":\n            title += \" Percent Connectivity\"\n    plot_connection_info(syn_info, data, source_labels, target_labels, title, save_file=save_file)\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.connection_histogram","title":"<code>bmtool.bmplot.connections.connection_histogram(config, nodes=None, edges=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=True, synaptic_info='0', source_cell=None, target_cell=None, include_gap=True)</code>","text":"<p>Generates histogram of the number of connections individual cells receive from another population.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot as sources.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot as targets.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter by.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter by.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not prepended to sid or tid. Default is True.</p> <code>True</code> <code>synaptic_info</code> <code>str</code> <p>Type of synaptic information to display. Default is '0'.</p> <code>'0'</code> <code>source_cell</code> <code>str</code> <p>Specific source cell type to plot connections from.</p> <code>None</code> <code>target_cell</code> <code>str</code> <p>Specific target cell type to plot connections onto.</p> <code>None</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def connection_histogram(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = True,\n    synaptic_info: str = \"0\",\n    source_cell: Optional[str] = None,\n    target_cell: Optional[str] = None,\n    include_gap: bool = True,\n) -&gt; None:\n    \"\"\"\n    Generates histogram of the number of connections individual cells receive from another population.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot as sources.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot as targets.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter by.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter by.\n    no_prepend_pop : bool, optional\n        If True, population name is not prepended to sid or tid. Default is True.\n    synaptic_info : str, optional\n        Type of synaptic information to display. Default is '0'.\n    source_cell : str, optional\n        Specific source cell type to plot connections from.\n    target_cell : str, optional\n        Specific target cell type to plot connections onto.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    sources_list = sources.split(\",\") if sources else []\n    targets_list = targets.split(\",\") if targets else []\n    if sids:\n        sids_list = sids.split(\",\")\n    else:\n        sids_list = []\n    if tids:\n        tids_list = tids.split(\",\")\n    else:\n        tids_list = []\n\n    def connection_pair_histogram(**kwargs: Dict) -&gt; None:\n        \"\"\"\n        Creates a histogram showing the distribution of connection counts between specific cell types.\n\n        This function is designed to be used with the relation_matrix utility and will only\n        create histograms for the specified source and target cell types.\n\n        Parameters\n        ----------\n        kwargs : dict\n            Dictionary containing edge data and filtering information.\n            - edges: DataFrame containing edge information\n            - sid: Column name for source ID type in the edges DataFrame\n            - tid: Column name for target ID type in the edges DataFrame\n            - source_id: Value to filter edges by source ID type\n            - target_id: Value to filter edges by target ID type\n\n        Returns\n        -------\n        None\n        \"\"\"\n        edges_data = kwargs[\"edges\"]\n        source_id_type = kwargs[\"sid\"]\n        target_id_type = kwargs[\"tid\"]\n        source_id = kwargs[\"source_id\"]\n        target_id = kwargs[\"target_id\"]\n        if source_id == source_cell and target_id == target_cell:\n            temp = edges_data[\n                (edges_data[source_id_type] == source_id) &amp; (edges_data[target_id_type] == target_id)\n            ]\n            if not include_gap:\n                gap_col = temp[\"is_gap_junction\"].fillna(False).astype(bool)\n                temp = temp[~gap_col]\n            node_pairs = temp.groupby(\"target_node_id\")[\"source_node_id\"].count()\n            try:\n                conn_mean = statistics.mean(node_pairs.values)\n                conn_std = statistics.stdev(node_pairs.values)\n                conn_median = statistics.median(node_pairs.values)\n                label = \"mean {:.2f} std {:.2f} median {:.2f}\".format(\n                    conn_mean, conn_std, conn_median\n                )\n            except (statistics.StatisticsError, ValueError):  # lazy fix for std not calculated with 1 node\n                conn_mean = statistics.mean(node_pairs.values)\n                conn_median = statistics.median(node_pairs.values)\n                label = \"mean {:.2f} median {:.2f}\".format(conn_mean, conn_median)\n            plt.hist(node_pairs.values, density=False, bins=\"auto\", stacked=True, label=label)\n            plt.legend()\n            plt.xlabel(\"# of conns from {} to {}\".format(source_cell, target_cell))\n            plt.ylabel(\"# of cells\")\n            plt.show()\n        else:  # dont care about other cell pairs so pass\n            pass\n\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    util.relation_matrix(\n        config,\n        nodes,\n        edges,\n        sources_list,\n        targets_list,\n        sids_list,\n        tids_list,\n        not no_prepend_pop,\n        relation_func=connection_pair_histogram,\n        synaptic_info=synaptic_info,\n    )\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.connection_distance","title":"<code>bmtool.bmplot.connections.connection_distance(config, sources, targets, source_cell_id, target_id_type, ignore_z=False)</code>","text":"<p>Plots the 3D spatial distribution of target nodes relative to a source node and a histogram of distances from the source node to each target node.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>sources</code> <code>str</code> <p>Network name(s) to plot as sources.</p> required <code>targets</code> <code>str</code> <p>Network name(s) to plot as targets.</p> required <code>source_cell_id</code> <code>int</code> <p>ID of the source cell for calculating distances to target nodes.</p> required <code>target_id_type</code> <code>str</code> <p>String to filter target nodes based off the target_query.</p> required <code>ignore_z</code> <code>bool</code> <p>If True, ignore Z axis when calculating distance. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; connection_distance(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     source_cell_id=0,\n...     target_id_type='LN',\n...     ignore_z=False\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def connection_distance(\n    config: str,\n    sources: str,\n    targets: str,\n    source_cell_id: int,\n    target_id_type: str,\n    ignore_z: bool = False,\n) -&gt; None:\n    \"\"\"\n    Plots the 3D spatial distribution of target nodes relative to a source node\n    and a histogram of distances from the source node to each target node.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    sources : str\n        Network name(s) to plot as sources.\n    targets : str\n        Network name(s) to plot as targets.\n    source_cell_id : int\n        ID of the source cell for calculating distances to target nodes.\n    target_id_type : str\n        String to filter target nodes based off the target_query.\n    ignore_z : bool, optional\n        If True, ignore Z axis when calculating distance. Default is False.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; connection_distance(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     source_cell_id=0,\n    ...     target_id_type='LN',\n    ...     ignore_z=False\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    # if source != target:\n    # raise Exception(\"Code is setup for source and target to be the same! Look at source code for function to add feature\")\n\n    # Load nodes and edges based on config file\n    nodes, edges = util.load_nodes_edges_from_config(config)\n\n    edge_network = sources + \"_to_\" + targets\n    node_network = sources\n\n    # Filter edges to obtain connections originating from the source node\n    edge = edges[edge_network]\n    edge = edge[edge[\"source_node_id\"] == source_cell_id]\n    if target_id_type:\n        edge = edge[edge[\"target_query\"].str.contains(target_id_type, na=False)]\n\n    target_node_ids = edge[\"target_node_id\"]\n\n    # Filter nodes to obtain only the target and source nodes\n    node = nodes[node_network]\n    target_nodes = node.loc[node.index.isin(target_node_ids)]\n    source_node = node.loc[node.index == source_cell_id]\n\n    # Calculate distances between source node and each target node\n    if ignore_z:\n        target_positions = target_nodes[[\"pos_x\", \"pos_y\"]].values\n        source_position = np.array(\n            [source_node[\"pos_x\"], source_node[\"pos_y\"]]\n        ).ravel()  # Ensure 1D shape\n    else:\n        target_positions = target_nodes[[\"pos_x\", \"pos_y\", \"pos_z\"]].values\n        source_position = np.array(\n            [source_node[\"pos_x\"], source_node[\"pos_y\"], source_node[\"pos_z\"]]\n        ).ravel()  # Ensure 1D shape\n    distances = np.linalg.norm(target_positions - source_position, axis=1)\n\n    # Plot positions of source and target nodes in 3D space or 2D\n    if ignore_z:\n        fig = plt.figure(figsize=(8, 6))\n        ax = fig.add_subplot(111)\n        ax.scatter(target_nodes[\"pos_x\"], target_nodes[\"pos_y\"], c=\"blue\", label=\"target cells\")\n        ax.scatter(source_node[\"pos_x\"], source_node[\"pos_y\"], c=\"red\", label=\"source cell\")\n    else:\n        fig = plt.figure(figsize=(8, 6))\n        ax = fig.add_subplot(111, projection=\"3d\")\n        ax.scatter(\n            target_nodes[\"pos_x\"],\n            target_nodes[\"pos_y\"],\n            target_nodes[\"pos_z\"],\n            c=\"blue\",\n            label=\"target cells\",\n        )\n        ax.scatter(\n            source_node[\"pos_x\"],\n            source_node[\"pos_y\"],\n            source_node[\"pos_z\"],\n            c=\"red\",\n            label=\"source cell\",\n        )\n\n    # Optional: Add text annotations for distances\n    # for i, distance in enumerate(distances):\n    #     ax.text(target_nodes['pos_x'].iloc[i], target_nodes['pos_y'].iloc[i], target_nodes['pos_z'].iloc[i],\n    #             f'{distance:.2f}', color='black', fontsize=8, ha='center')\n\n    plt.legend()\n    plt.show()\n\n    # Plot distances in a separate 2D plot\n    plt.figure(figsize=(8, 6))\n    plt.hist(distances, bins=20, color=\"blue\", edgecolor=\"black\")\n    plt.xlabel(\"Distance\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Distance from Source Node to Each Target Node\")\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.edge_histogram_matrix","title":"<code>bmtool.bmplot.connections.edge_histogram_matrix(config, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=None, edge_property=None, time=None, time_compare=None, report=None, title=None, save_file=None)</code>","text":"<p>Generates a matrix of histograms showing the distribution of edge properties between populations.</p> <p>This function creates a grid of histograms where each cell represents the distribution of a specific edge property between source and target populations.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>sources</code> <code>str</code> <p>Comma-separated list of source network names.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated list of target network names.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated list of source node identifiers to filter by.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated list of target node identifiers to filter by.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population names are not prepended to node identifiers.</p> <code>None</code> <code>edge_property</code> <code>str</code> <p>The edge property to analyze (e.g., 'syn_weight', 'delay').</p> <code>None</code> <code>time</code> <code>int</code> <p>Time point to analyze from a time series report.</p> <code>None</code> <code>time_compare</code> <code>int</code> <p>Second time point for comparison with time.</p> <code>None</code> <code>report</code> <code>str</code> <p>Name of the report to analyze.</p> <code>None</code> <code>title</code> <code>str</code> <p>Custom title for the plot.</p> <code>None</code> <code>save_file</code> <code>str</code> <p>Path to save the generated plot.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; edge_histogram_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     edge_property='syn_weight'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def edge_histogram_matrix(\n    config: str,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: Optional[bool] = None,\n    edge_property: Optional[str] = None,\n    time: Optional[int] = None,\n    time_compare: Optional[int] = None,\n    report: Optional[str] = None,\n    title: Optional[str] = None,\n    save_file: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Generates a matrix of histograms showing the distribution of edge properties between populations.\n\n    This function creates a grid of histograms where each cell represents the distribution\n    of a specific edge property between source and target populations.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    sources : str, optional\n        Comma-separated list of source network names.\n    targets : str, optional\n        Comma-separated list of target network names.\n    sids : str, optional\n        Comma-separated list of source node identifiers to filter by.\n    tids : str, optional\n        Comma-separated list of target node identifiers to filter by.\n    no_prepend_pop : bool, optional\n        If True, population names are not prepended to node identifiers.\n    edge_property : str, optional\n        The edge property to analyze (e.g., 'syn_weight', 'delay').\n    time : int, optional\n        Time point to analyze from a time series report.\n    time_compare : int, optional\n        Second time point for comparison with time.\n    report : str, optional\n        Name of the report to analyze.\n    title : str, optional\n        Custom title for the plot.\n    save_file : str, optional\n        Path to save the generated plot.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; edge_histogram_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     edge_property='syn_weight'\n    ... )\n    \"\"\"\n\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    if time_compare:\n        time_compare = int(time_compare)\n\n    data, source_labels, target_labels = util.edge_property_matrix(\n        edge_property,\n        nodes=None,\n        edges=None,\n        config=config,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        report=report,\n        time=time,\n        time_compare=time_compare,\n    )\n\n    # Fantastic resource\n    # https://stackoverflow.com/questions/7941207/is-there-a-function-to-make-scatterplot-matrices-in-matplotlib\n    num_src, num_tar = data.shape\n    fig, axes = plt.subplots(nrows=num_src, ncols=num_tar, figsize=(12, 12))\n    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n\n    for x in range(num_src):\n        for y in range(num_tar):\n            axes[x, y].hist(data[x][y])\n\n            if x == num_src - 1:\n                axes[x, y].set_xlabel(target_labels[y])\n            if y == 0:\n                axes[x, y].set_ylabel(source_labels[x])\n\n    tt = edge_property + \" Histogram Matrix\"\n    if title:\n        tt = title\n    st = fig.suptitle(tt, fontsize=14)\n    fig.text(0.5, 0.04, \"Target\", ha=\"center\")\n    fig.text(0.04, 0.5, \"Source\", va=\"center\", rotation=\"vertical\")\n    plt.draw()\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.plot_connection_info","title":"<code>bmtool.bmplot.connections.plot_connection_info(text, num, source_labels, target_labels, title, syn_info='0', save_file=None, return_dict=None)</code>","text":"<p>Plot connection information as a heatmap with text annotations.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>ndarray</code> <p>2D array of text annotations for each cell.</p> required <code>num</code> <code>ndarray</code> <p>2D array of numerical values for the heatmap colors.</p> required <code>source_labels</code> <code>list of str</code> <p>Labels for source populations (rows).</p> required <code>target_labels</code> <code>list of str</code> <p>Labels for target populations (columns).</p> required <code>title</code> <code>str</code> <p>Title for the plot.</p> required <code>syn_info</code> <code>str</code> <p>Type of synaptic information being displayed. Options: '0', '1', '2', '3'. Default is '0'.</p> <code>'0'</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple, Dict, None]</code> <p>If return_dict=True, returns a dictionary of connection information. Otherwise, returns a tuple of (Figure, Axes), or None if just displaying.</p> Notes <p>Handles missing source and target values by setting them to 0.</p> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def plot_connection_info(\n    text: np.ndarray,\n    num: np.ndarray,\n    source_labels: List[str],\n    target_labels: List[str],\n    title: str,\n    syn_info: str = \"0\",\n    save_file: Optional[str] = None,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[Tuple, Dict, None]:\n    \"\"\"\n    Plot connection information as a heatmap with text annotations.\n\n    Parameters\n    ----------\n    text : np.ndarray\n        2D array of text annotations for each cell.\n    num : np.ndarray\n        2D array of numerical values for the heatmap colors.\n    source_labels : list of str\n        Labels for source populations (rows).\n    target_labels : list of str\n        Labels for target populations (columns).\n    title : str\n        Title for the plot.\n    syn_info : str, optional\n        Type of synaptic information being displayed. Options: '0', '1', '2', '3'.\n        Default is '0'.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is None.\n\n    Returns\n    -------\n    Union[Tuple, Dict, None]\n        If return_dict=True, returns a dictionary of connection information.\n        Otherwise, returns a tuple of (Figure, Axes), or None if just displaying.\n\n    Notes\n    -----\n    Handles missing source and target values by setting them to 0.\n    \"\"\"\n    # Ensure text dimensions match num dimensions\n    num_source = len(source_labels)\n    num_target = len(target_labels)\n\n    # Set color map\n    matplotlib.rc(\"image\", cmap=\"viridis\")\n\n    # Calculate square cell size to ensure proper aspect ratio\n    base_cell_size = 0.6  # Base size per cell\n\n    # Calculate figure dimensions with proper aspect ratio\n    # Make sure width and height are proportional to the matrix dimensions\n    fig_width = max(8, num_target * base_cell_size + 4)  # Width based on columns\n    fig_height = max(6, num_source * base_cell_size + 3)  # Height based on rows\n\n    # Ensure minimum readable size\n    min_fig_size = 8\n    if fig_width &lt; min_fig_size or fig_height &lt; min_fig_size:\n        scale_factor = min_fig_size / min(fig_width, fig_height)\n        fig_width *= scale_factor\n        fig_height *= scale_factor\n\n    # Create figure and axis\n    fig1, ax1 = plt.subplots(figsize=(fig_width, fig_height))\n\n    # Replace NaN with 0 and create heatmap\n    num_clean = np.nan_to_num(num, nan=0)\n    # if string is nan\\nnan make it 0\n\n    # Use 'auto' aspect ratio to let matplotlib handle it properly\n    # This prevents the stretching issue\n    im1 = ax1.imshow(num_clean, aspect=\"auto\", interpolation=\"nearest\")\n\n    # Set ticks and labels\n    ax1.set_xticks(list(np.arange(len(target_labels))))\n    ax1.set_yticks(list(np.arange(len(source_labels))))\n    ax1.set_xticklabels(target_labels)\n    ax1.set_yticklabels(source_labels)\n\n    # Improved font sizing based on matrix size\n    label_font_size = max(8, min(14, 120 / max(num_source, num_target)))\n\n    # Style the tick labels\n    ax1.tick_params(axis=\"y\", labelsize=label_font_size, pad=5)\n    plt.setp(\n        ax1.get_xticklabels(),\n        rotation=45,\n        ha=\"right\",\n        rotation_mode=\"anchor\",\n        fontsize=label_font_size,\n    )\n\n    # Dictionary to store connection information\n    graph_dict = {}\n\n    # Improved text size calculation - more readable for larger matrices\n    text_size = max(6, min(12, 80 / max(num_source, num_target)))\n\n    # Loop over data dimensions and create text annotations\n    for i in range(num_source):\n        for j in range(num_target):\n            edge_info = text[i, j] if text[i, j] is not None else \"0\\n0\"\n\n            if source_labels[i] not in graph_dict:\n                graph_dict[source_labels[i]] = {}\n            graph_dict[source_labels[i]][target_labels[j]] = edge_info\n\n            # Skip displaying text for NaN values to reduce clutter\n            if edge_info == \"nan\\nnan\":\n                edge_info = \"0\\n\u00b10\"\n\n            # Format the text display\n            if isinstance(edge_info, str) and \"\\n\" in edge_info:\n                # For mean/std format (e.g. \"15.5\\n4.0\")\n                parts = edge_info.split(\"\\n\")\n                if len(parts) == 2:\n                    try:\n                        mean_val = float(parts[0])\n                        std_val = float(parts[1])\n                        display_text = f\"{mean_val:.1f}\\n\u00b1{std_val:.1f}\"\n                    except ValueError:\n                        display_text = edge_info\n                else:\n                    display_text = edge_info\n            else:\n                display_text = str(edge_info)\n\n            # Add text to plot with better contrast\n            text_color = \"white\" if num_clean[i, j] &lt; (np.nanmax(num_clean) * 0.9) else \"black\"\n\n            if syn_info == \"2\" or syn_info == \"3\":\n                ax1.text(\n                    j,\n                    i,\n                    display_text,\n                    ha=\"center\",\n                    va=\"center\",\n                    color=text_color,\n                    rotation=37.5,\n                    fontsize=text_size,\n                    weight=\"bold\",\n                )\n            else:\n                ax1.text(\n                    j,\n                    i,\n                    display_text,\n                    ha=\"center\",\n                    va=\"center\",\n                    color=text_color,\n                    fontsize=text_size,\n                    weight=\"bold\",\n                )\n\n    # Set labels and title\n    title_font_size = max(12, min(18, label_font_size + 4))\n    ax1.set_ylabel(\"Source\", fontsize=title_font_size, weight=\"bold\", labelpad=10)\n    ax1.set_xlabel(\"Target\", fontsize=title_font_size, weight=\"bold\", labelpad=10)\n    ax1.set_title(title, fontsize=title_font_size + 2, weight=\"bold\", pad=20)\n\n    # Add colorbar\n    cbar = plt.colorbar(im1, shrink=0.8)\n    cbar.ax.tick_params(labelsize=label_font_size)\n\n    # Adjust layout to minimize whitespace and prevent stretching\n    plt.tight_layout(pad=1.5)\n\n    # Force square cells by setting equal axis limits if needed\n    ax1.set_xlim(-0.5, num_target - 0.5)\n    ax1.set_ylim(num_source - 0.5, -0.5)  # Inverted for proper matrix orientation\n\n    # Display or save the plot\n    try:\n        # Check if running in notebook\n        from IPython import get_ipython\n\n        notebook = get_ipython() is not None\n    except ImportError:\n        notebook = False\n\n    if not notebook:\n        plt.show()\n\n    if save_file:\n        plt.savefig(save_file, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n\n    if return_dict:\n        return graph_dict\n    else:\n        return fig1, ax1\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.connector_percent_matrix","title":"<code>bmtool.bmplot.connections.connector_percent_matrix(csv_path=None, exclude_strings=None, assemb_key=None, title='Percent connection matrix', pop_order=None)</code>","text":"<p>Generates and plots a connection matrix based on connection probabilities from a CSV file.</p> <p>This function visualizes percent connectivity while factoring in population distance and other parameters. It processes connection data by filtering 'Source' and 'Target' columns in the CSV and displays the percentage of connected pairs for each population combination in a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>str</code> <p>Path to the CSV file containing connection data. The CSV should be an output from the bmtool.connector classes, specifically generated by the <code>save_connection_report()</code> function.</p> <code>None</code> <code>exclude_strings</code> <code>list of str</code> <p>List of strings to exclude rows where 'Source' or 'Target' contain these strings.</p> <code>None</code> <code>assemb_key</code> <code>str</code> <p>Key to identify and process assembly connections.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the generated plot. Default is 'Percent connection matrix'.</p> <code>'Percent connection matrix'</code> <code>pop_order</code> <code>list of str</code> <p>List of population labels to specify the order for x- and y-ticks in the plot.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Displays a heatmap plot of the connection matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; connector_percent_matrix(\n...     csv_path='connections.csv',\n...     exclude_strings=['Gap'],\n...     title='Network Connectivity'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def connector_percent_matrix(\n    csv_path: Optional[str] = None,\n    exclude_strings: Optional[List[str]] = None,\n    assemb_key: Optional[str] = None,\n    title: str = \"Percent connection matrix\",\n    pop_order: Optional[List[str]] = None,\n) -&gt; None:\n    \"\"\"\n    Generates and plots a connection matrix based on connection probabilities from a CSV file.\n\n    This function visualizes percent connectivity while factoring in population distance and other parameters.\n    It processes connection data by filtering 'Source' and 'Target' columns in the CSV and displays the\n    percentage of connected pairs for each population combination in a matrix.\n\n    Parameters\n    ----------\n    csv_path : str, optional\n        Path to the CSV file containing connection data. The CSV should be an output from the\n        bmtool.connector classes, specifically generated by the `save_connection_report()` function.\n    exclude_strings : list of str, optional\n        List of strings to exclude rows where 'Source' or 'Target' contain these strings.\n    assemb_key : str, optional\n        Key to identify and process assembly connections.\n    title : str, optional\n        Title for the generated plot. Default is 'Percent connection matrix'.\n    pop_order : list of str, optional\n        List of population labels to specify the order for x- and y-ticks in the plot.\n\n    Returns\n    -------\n    None\n        Displays a heatmap plot of the connection matrix.\n\n    Examples\n    --------\n    &gt;&gt;&gt; connector_percent_matrix(\n    ...     csv_path='connections.csv',\n    ...     exclude_strings=['Gap'],\n    ...     title='Network Connectivity'\n    ... )\n    \"\"\"\n    # Read the CSV data\n    df = pd.read_csv(csv_path)\n\n    # Choose the column to display\n    selected_column = \"Percent connectionivity within possible connections\"\n\n    # Filter the DataFrame based on exclude_strings\n    def filter_dataframe(df, column_name, exclude_strings):\n        def process_string(string):\n            match = re.search(r\"\\[\\'(.*?)\\'\\]\", string)\n            if exclude_strings and any(ex_string in string for ex_string in exclude_strings):\n                return None\n            elif match:\n                filtered_string = match.group(1)\n                if \"Gap\" in string:\n                    filtered_string = filtered_string + \"-Gap\"\n\n                if assemb_key:\n                    if assemb_key in string:\n                        filtered_string = filtered_string + assemb_key\n\n                return filtered_string  # Return matched string\n\n            return string  # If no match, return the original string\n\n        df[column_name] = df[column_name].apply(process_string)\n        df = df.dropna(subset=[column_name])\n\n        return df\n\n    df = filter_dataframe(df, \"Source\", exclude_strings)\n    df = filter_dataframe(df, \"Target\", exclude_strings)\n\n    # process assem rows and combine them into one prob per assem type\n    if assemb_key:\n        assems = df[df[\"Source\"].str.contains(assemb_key)]\n        unique_sources = assems[\"Source\"].unique()\n\n        for source in unique_sources:\n            source_assems = assems[assems[\"Source\"] == source]\n            unique_targets = source_assems[\n                \"Target\"\n            ].unique()  # Filter targets for the current source\n\n            for target in unique_targets:\n                # Filter the assemblies with the current source and target\n                unique_assems = source_assems[source_assems[\"Target\"] == target]\n\n                # find the prob of a conn\n                forward_probs = []\n                for _, row in unique_assems.iterrows():\n                    selected_percentage = row[selected_column]\n                    selected_percentage = [\n                        float(p) for p in selected_percentage.strip(\"[]\").split()\n                    ]\n                    if len(selected_percentage) == 1 or len(selected_percentage) == 2:\n                        forward_probs.append(selected_percentage[0])\n                    if len(selected_percentage) == 3:\n                        forward_probs.append(selected_percentage[0])\n                        forward_probs.append(selected_percentage[1])\n\n                mean_probs = np.mean(forward_probs)\n                source = source.replace(assemb_key, \"\")\n                target = target.replace(assemb_key, \"\")\n                new_row = pd.DataFrame(\n                    {\n                        \"Source\": [source],\n                        \"Target\": [target],\n                        \"Percent connectionivity within possible connections\": [mean_probs],\n                        \"Percent connectionivity within all connections\": [0],\n                    }\n                )\n\n                df = pd.concat([df, new_row], ignore_index=False)\n\n    # Prepare connection data\n    connection_data = {}\n    for _, row in df.iterrows():\n        source, target, selected_percentage = row[\"Source\"], row[\"Target\"], row[selected_column]\n        if isinstance(selected_percentage, str):\n            selected_percentage = [float(p) for p in selected_percentage.strip(\"[]\").split()]\n        connection_data[(source, target)] = selected_percentage\n\n    # Determine population order\n    populations = sorted(list(set(df[\"Source\"].unique()) | set(df[\"Target\"].unique())))\n    if pop_order:\n        populations = [\n            pop for pop in pop_order if pop in populations\n        ]  # Order according to pop_order, if provided\n    num_populations = len(populations)\n\n    # Create an empty matrix and populate it\n    connection_matrix = np.zeros((num_populations, num_populations), dtype=float)\n    for (source, target), probabilities in connection_data.items():\n        if source in populations and target in populations:\n            source_idx = populations.index(source)\n            target_idx = populations.index(target)\n\n            if isinstance(probabilities, float):\n                connection_matrix[source_idx][target_idx] = probabilities\n            elif len(probabilities) == 1:\n                connection_matrix[source_idx][target_idx] = probabilities[0]\n            elif len(probabilities) == 2:\n                connection_matrix[source_idx][target_idx] = probabilities[0]\n            elif len(probabilities) == 3:\n                connection_matrix[source_idx][target_idx] = probabilities[0]\n                connection_matrix[target_idx][source_idx] = probabilities[1]\n            else:\n                raise Exception(\"unsupported format\")\n\n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 8))\n    im = ax.imshow(connection_matrix, cmap=\"viridis\", interpolation=\"nearest\")\n\n    # Add annotations\n    for i in range(num_populations):\n        for j in range(num_populations):\n            text = ax.text(\n                j,\n                i,\n                f\"{connection_matrix[i, j]:.2f}%\",\n                ha=\"center\",\n                va=\"center\",\n                color=\"w\",\n                size=10,\n                weight=\"semibold\",\n            )\n\n    # Add colorbar\n    plt.colorbar(im, label=f\"{selected_column}\")\n\n    # Set title and axis labels\n    ax.set_title(title)\n    ax.set_xlabel(\"Target Population\")\n    ax.set_ylabel(\"Source Population\")\n\n    # Set ticks and labels based on populations in specified order\n    ax.set_xticks(np.arange(num_populations))\n    ax.set_yticks(np.arange(num_populations))\n    ax.set_xticklabels(populations, rotation=45, ha=\"right\", size=12, weight=\"semibold\")\n    ax.set_yticklabels(populations, size=12, weight=\"semibold\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.plot_3d_positions","title":"<code>bmtool.bmplot.connections.plot_3d_positions(config=None, sources=None, sid=None, title=None, save_file=None, subset=None)</code>","text":"<p>Plots a 3D graph of all cells with x, y, z location.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Which network(s) to plot. If None or 'all', plots all networks.</p> <code>None</code> <code>sid</code> <code>str</code> <p>Column name to group cell types (node grouping criteria).</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title. Default is '3D positions'.</p> <code>None</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>subset</code> <code>int</code> <p>Take every Nth row. This makes plotting large networks easier to visualize.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plot_3d_positions(\n...     config='config.json',\n...     sources='cortex',\n...     sid='node_type_id',\n...     title='3D Neuron Positions'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def plot_3d_positions(\n    config: Optional[str] = None,\n    sources: Optional[str] = None,\n    sid: Optional[str] = None,\n    title: Optional[str] = None,\n    save_file: Optional[str] = None,\n    subset: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Plots a 3D graph of all cells with x, y, z location.\n\n    Parameters\n    ----------\n    config : str, optional\n        Path to a BMTK simulation config file.\n    sources : str, optional\n        Which network(s) to plot. If None or 'all', plots all networks.\n    sid : str, optional\n        Column name to group cell types (node grouping criteria).\n    title : str, optional\n        Plot title. Default is '3D positions'.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    subset : int, optional\n        Take every Nth row. This makes plotting large networks easier to visualize.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; plot_3d_positions(\n    ...     config='config.json',\n    ...     sources='cortex',\n    ...     sid='node_type_id',\n    ...     title='3D Neuron Positions'\n    ... )\n    \"\"\"\n\n    if not config:\n        raise Exception(\"config not defined\")\n\n    if sources is None:\n        sources = \"all\"\n\n    # Set group keys (e.g., node types)\n    group_keys = sid\n    if title is None:\n        title = \"3D positions\"\n\n    # Load nodes from the configuration\n    nodes = util.load_nodes_from_config(config)\n\n    # Get the list of populations to plot\n    if \"all\" in sources:\n        populations = list(nodes)\n    else:\n        populations = sources.split(\",\")\n\n    # Split group_by into list\n    group_keys = group_keys.split(\",\")\n    group_keys += (len(populations) - len(group_keys)) * [\n        \"node_type_id\"\n    ]  # Extend the array to default values if not enough given\n    if len(group_keys) &gt; 1:\n        raise Exception(\"Only one group by is supported currently!\")\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(projection=\"3d\")\n    handles = []\n\n    for pop in list(nodes):\n        if \"all\" not in populations and pop not in populations:\n            continue\n\n        nodes_df = nodes[pop]\n        group_key = group_keys[0]\n\n        # If group_key is provided, ensure the column exists in the dataframe\n        if group_key is not None:\n            if group_key not in nodes_df:\n                raise Exception(f\"Could not find column '{group_key}' in {pop}\")\n\n            groupings = nodes_df.groupby(group_key)\n            n_colors = nodes_df[group_key].nunique()\n            color_norm = colors.Normalize(vmin=0, vmax=(n_colors - 1))\n            scalar_map = cmx.ScalarMappable(norm=color_norm, cmap=\"hsv\")\n            color_map = [scalar_map.to_rgba(i) for i in range(n_colors)]\n        else:\n            groupings = [(None, nodes_df)]\n            color_map = [\"blue\"]\n\n        # Loop over groupings and plot\n        for color, (group_name, group_df) in zip(color_map, groupings):\n            if \"pos_x\" not in group_df or \"pos_y\" not in group_df or \"pos_z\" not in group_df:\n                print(\n                    f\"Warning: Missing position columns in group '{group_name}' for {pop}. Skipping this group.\"\n                )\n                continue  # Skip if position columns are missing\n\n            # Subset the dataframe by taking every Nth row if subset is provided\n            if subset is not None:\n                group_df = group_df.iloc[::subset]\n\n            h = ax.scatter(\n                group_df[\"pos_x\"],\n                group_df[\"pos_y\"],\n                group_df[\"pos_z\"],\n                color=color,\n                label=group_name,\n            )\n            handles.append(h)\n\n    if not handles:\n        print(\"No data to plot.\")\n        return\n\n    # Set plot title and legend\n    plt.title(title)\n    plt.legend(handles=handles)\n\n    # Add axis labels\n    ax.set_xlabel(\"X Position (\u03bcm)\")\n    ax.set_ylabel(\"Y Position (\u03bcm)\")\n    ax.set_zlabel(\"Z Position (\u03bcm)\")\n\n    # Draw the plot\n    plt.draw()\n    plt.tight_layout()\n\n    # Save the plot if save_file is provided\n    if save_file:\n        plt.savefig(save_file)\n\n    # Show if running in notebook\n    if is_notebook:\n        plt.show()\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.plot_3d_cell_rotation","title":"<code>bmtool.bmplot.connections.plot_3d_cell_rotation(config=None, sources=None, sids=None, title=None, save_file=None, quiver_length=None, arrow_length_ratio=None, group=None, subset=None)</code>","text":"<p>Plot 3D visualization of cell rotations with quiver arrows showing rotation orientations.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> <code>None</code> <code>sources</code> <code>list of str</code> <p>Network names to plot. If None or contains 'all', plots all networks.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated column names to group cell types.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title. Default is 'Cell rotations'.</p> <code>None</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>quiver_length</code> <code>float</code> <p>Length of the quiver arrows. If None, use matplotlib default.</p> <code>None</code> <code>arrow_length_ratio</code> <code>float</code> <p>Ratio of arrow head size to quiver length.</p> <code>None</code> <code>group</code> <code>str</code> <p>Comma-separated group names to include. If None, include all groups.</p> <code>None</code> <code>subset</code> <code>int</code> <p>Take every Nth row. Useful for visualizing large networks.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plot_3d_cell_rotation(\n...     config='config.json',\n...     sources=['cortex'],\n...     sids='node_type_id',\n...     title='Cell Rotation Vectors'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def plot_3d_cell_rotation(\n    config: Optional[str] = None,\n    sources: Optional[List[str]] = None,\n    sids: Optional[str] = None,\n    title: Optional[str] = None,\n    save_file: Optional[str] = None,\n    quiver_length: Optional[float] = None,\n    arrow_length_ratio: Optional[float] = None,\n    group: Optional[str] = None,\n    subset: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Plot 3D visualization of cell rotations with quiver arrows showing rotation orientations.\n\n    Parameters\n    ----------\n    config : str, optional\n        Path to a BMTK simulation config file.\n    sources : list of str, optional\n        Network names to plot. If None or contains 'all', plots all networks.\n    sids : str, optional\n        Comma-separated column names to group cell types.\n    title : str, optional\n        Plot title. Default is 'Cell rotations'.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    quiver_length : float, optional\n        Length of the quiver arrows. If None, use matplotlib default.\n    arrow_length_ratio : float, optional\n        Ratio of arrow head size to quiver length.\n    group : str, optional\n        Comma-separated group names to include. If None, include all groups.\n    subset : int, optional\n        Take every Nth row. Useful for visualizing large networks.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; plot_3d_cell_rotation(\n    ...     config='config.json',\n    ...     sources=['cortex'],\n    ...     sids='node_type_id',\n    ...     title='Cell Rotation Vectors'\n    ... )\n    \"\"\"\n    from scipy.spatial.transform import Rotation as R\n\n    if not config:\n        raise Exception(\"config not defined\")\n\n    if sources is None:\n        sources = [\"all\"]\n\n    group_keys = sids.split(\",\") if sids else []\n\n    if title is None:\n        title = \"Cell rotations\"\n\n    nodes = util.load_nodes_from_config(config)\n\n    if \"all\" in sources:\n        populations = list(nodes)\n    else:\n        populations = sources\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111, projection=\"3d\")\n    handles = []\n\n    for nodes_key, group_key in zip(list(nodes), group_keys):\n        if \"all\" not in populations and nodes_key not in populations:\n            continue\n\n        nodes_df = nodes[nodes_key]\n\n        if group_key is not None:\n            if group_key not in nodes_df.columns:\n                raise Exception(f\"Could not find column {group_key}\")\n            groupings = nodes_df.groupby(group_key)\n\n            n_colors = nodes_df[group_key].nunique()\n            color_norm = colors.Normalize(vmin=0, vmax=(n_colors - 1))\n            scalar_map = cmx.ScalarMappable(norm=color_norm, cmap=\"hsv\")\n            color_map = [scalar_map.to_rgba(i) for i in range(n_colors)]\n        else:\n            groupings = [(None, nodes_df)]\n            color_map = [\"blue\"]\n\n        for color, (group_name, group_df) in zip(color_map, groupings):\n            if subset is not None:\n                group_df = group_df.iloc[::subset]\n\n            if group and group_name not in group.split(\",\"):\n                continue\n\n            if \"pos_x\" not in group_df or \"rotation_angle_xaxis\" not in group_df:\n                continue\n\n            X = group_df[\"pos_x\"]\n            Y = group_df[\"pos_y\"]\n            Z = group_df[\"pos_z\"]\n            U = group_df[\"rotation_angle_xaxis\"].values\n            V = group_df[\"rotation_angle_yaxis\"].values\n            W = group_df[\"rotation_angle_zaxis\"].values\n\n            if U is None:\n                U = np.zeros(len(X))\n            if V is None:\n                V = np.zeros(len(Y))\n            if W is None:\n                W = np.zeros(len(Z))\n\n            # Create rotation matrices from Euler angles\n            rotations = R.from_euler(\"xyz\", np.column_stack((U, V, W)), degrees=False)\n\n            # Define initial vectors\n            init_vectors = np.column_stack((np.ones(len(X)), np.zeros(len(Y)), np.zeros(len(Z))))\n\n            # Apply rotations to initial vectors\n            rots = np.dot(rotations.as_matrix(), init_vectors.T).T\n\n            # Extract x, y, and z components of the rotated vectors\n            rot_x = rots[:, 0]\n            rot_y = rots[:, 1]\n            rot_z = rots[:, 2]\n\n            h = ax.quiver(\n                X,\n                Y,\n                Z,\n                rot_x,\n                rot_y,\n                rot_z,\n                color=color,\n                label=group_name,\n                arrow_length_ratio=arrow_length_ratio,\n                length=quiver_length,\n            )\n            ax.scatter(X, Y, Z, color=color, label=group_name)\n            ax.set_xlim([min(X), max(X)])\n            ax.set_ylim([min(Y), max(Y)])\n            ax.set_zlim([min(Z), max(Z)])\n            handles.append(h)\n\n    if not handles:\n        return\n\n    plt.title(title)\n    plt.legend(handles=handles)\n    plt.draw()\n\n    if save_file:\n        plt.savefig(save_file)\n    notebook = is_notebook\n    if not notebook:\n        plt.show()\n</code></pre>"},{"location":"api/bmplot/#spikes-module","title":"Spikes Module","text":""},{"location":"api/bmplot/#bmtool.bmplot.spikes.raster","title":"<code>bmtool.bmplot.spikes.raster(spikes_df=None, config=None, network_name=None, groupby='pop_name', sortby=None, ax=None, tstart=None, tstop=None, color_map=None, dot_size=0.3)</code>","text":"<p>Plots a raster plot of neural spikes, with different colors for each population.</p> Parameters: <p>spikes_df : pd.DataFrame, optional     DataFrame containing spike data with columns 'timestamps', 'node_ids', and optional 'pop_name'. config : str, optional     Path to the configuration file used to load node data. network_name : str, optional     Specific network name to select from the configuration; if not provided, uses the first network. groupby : str, optional     Column name to group spikes by for coloring. Default is 'pop_name'. sortby : str, optional     Column name to sort node_ids within each group. If provided, nodes within each population will be sorted by this column. ax : matplotlib.axes.Axes, optional     Axes on which to plot the raster; if None, a new figure and axes are created. tstart : float, optional     Start time for filtering spikes; only spikes with timestamps greater than <code>tstart</code> will be plotted. tstop : float, optional     Stop time for filtering spikes; only spikes with timestamps less than <code>tstop</code> will be plotted. color_map : dict, optional     Dictionary specifying colors for each population. Keys should be population names, and values should be color values. dot_size: float, optional     Size of the dot to display on the scatterplot</p> Returns: <p>matplotlib.axes.Axes     Axes with the raster plot.</p> Notes: <ul> <li>If <code>config</code> is provided, the function merges population names from the node data with <code>spikes_df</code>.</li> <li>Each unique population from groupby in <code>spikes_df</code> will be represented by a different color if <code>color_map</code> is not specified.</li> <li>If <code>color_map</code> is provided, it should contain colors for all unique <code>pop_name</code> values in <code>spikes_df</code>.</li> </ul> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def raster(\n    spikes_df: Optional[pd.DataFrame] = None,\n    config: Optional[str] = None,\n    network_name: Optional[str] = None,\n    groupby: str = \"pop_name\",\n    sortby: Optional[str] = None,\n    ax: Optional[Axes] = None,\n    tstart: Optional[float] = None,\n    tstop: Optional[float] = None,\n    color_map: Optional[Dict[str, str]] = None,\n    dot_size: float = 0.3,\n) -&gt; Axes:\n    \"\"\"\n    Plots a raster plot of neural spikes, with different colors for each population.\n\n    Parameters:\n    ----------\n    spikes_df : pd.DataFrame, optional\n        DataFrame containing spike data with columns 'timestamps', 'node_ids', and optional 'pop_name'.\n    config : str, optional\n        Path to the configuration file used to load node data.\n    network_name : str, optional\n        Specific network name to select from the configuration; if not provided, uses the first network.\n    groupby : str, optional\n        Column name to group spikes by for coloring. Default is 'pop_name'.\n    sortby : str, optional\n        Column name to sort node_ids within each group. If provided, nodes within each population will be sorted by this column.\n    ax : matplotlib.axes.Axes, optional\n        Axes on which to plot the raster; if None, a new figure and axes are created.\n    tstart : float, optional\n        Start time for filtering spikes; only spikes with timestamps greater than `tstart` will be plotted.\n    tstop : float, optional\n        Stop time for filtering spikes; only spikes with timestamps less than `tstop` will be plotted.\n    color_map : dict, optional\n        Dictionary specifying colors for each population. Keys should be population names, and values should be color values.\n    dot_size: float, optional\n        Size of the dot to display on the scatterplot\n\n    Returns:\n    -------\n    matplotlib.axes.Axes\n        Axes with the raster plot.\n\n    Notes:\n    -----\n    - If `config` is provided, the function merges population names from the node data with `spikes_df`.\n    - Each unique population from groupby in `spikes_df` will be represented by a different color if `color_map` is not specified.\n    - If `color_map` is provided, it should contain colors for all unique `pop_name` values in `spikes_df`.\n    \"\"\"\n    # Initialize axes if none provided\n    sns.set_style(\"whitegrid\")\n    if ax is None:\n        _, ax = plt.subplots(1, 1)\n\n    # Filter spikes by time range if specified\n    if tstart is not None:\n        spikes_df = spikes_df[spikes_df[\"timestamps\"] &gt; tstart]\n    if tstop is not None:\n        spikes_df = spikes_df[spikes_df[\"timestamps\"] &lt; tstop]\n\n    # Load and merge node population data if config is provided\n    if config:\n        nodes = load_nodes_from_config(config)\n        if network_name:\n            nodes = nodes.get(network_name, {})\n        else:\n            nodes = list(nodes.values())[0] if nodes else {}\n            print(\n                \"Grabbing first network; specify a network name to ensure correct node population is selected.\"\n            )\n\n        # Find common columns, but exclude the join key from the list\n        common_columns = spikes_df.columns.intersection(nodes.columns).tolist()\n        common_columns = [\n            col for col in common_columns if col != \"node_ids\"\n        ]  # Remove our join key from the common list\n\n        # Drop all intersecting columns except the join key column from df2\n        spikes_df = spikes_df.drop(columns=common_columns)\n        # merge nodes and spikes df\n        spikes_df = spikes_df.merge(\n            nodes[groupby], left_on=\"node_ids\", right_index=True, how=\"left\"\n        )\n\n    # Get unique population names\n    unique_pop_names = spikes_df[groupby].unique()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"tab10\")  # Default colormap\n        color_map = {\n            pop_name: cmap(i / len(unique_pop_names)) for i, pop_name in enumerate(unique_pop_names)\n        }\n    else:\n        # Ensure color_map contains all population names\n        missing_colors = [pop for pop in unique_pop_names if pop not in color_map]\n        if missing_colors:\n            raise ValueError(f\"color_map is missing colors for populations: {missing_colors}\")\n\n    # Plot each population with its specified or generated color\n    legend_handles = []\n    y_offset = 0  # Track y-position offset for stacking populations\n\n    for pop_name, group in spikes_df.groupby(groupby):\n        if sortby:\n            # Sort by the specified column, putting NaN values at the end\n            group_sorted = group.sort_values(by=sortby, na_position='last')\n            # Create a mapping from node_ids to consecutive y-positions based on sorted order\n            # Use the sorted order to maintain the same sequence for all spikes from same node\n            unique_nodes_sorted = group_sorted['node_ids'].drop_duplicates()\n            node_to_y = {node_id: y_offset + i for i, node_id in enumerate(unique_nodes_sorted)}\n            # Map node_ids to new y-positions for ALL spikes (not just the sorted group)\n            y_positions = group['node_ids'].map(node_to_y)\n            # Verify no data was lost\n            assert len(y_positions) == len(group), f\"Data loss detected in population {pop_name}\"\n            assert y_positions.isna().sum() == 0, f\"Unmapped node_ids found in population {pop_name}\"\n        else:\n            y_positions = group['node_ids']\n\n        ax.scatter(group[\"timestamps\"], y_positions, color=color_map[pop_name], s=dot_size)\n        # Dummy scatter for consistent legend appearance\n        handle = ax.scatter([], [], color=color_map[pop_name], label=pop_name, s=20)\n        legend_handles.append(handle)\n\n        # Update y_offset for next population if sortby is used\n        if sortby:\n            y_offset += len(unique_nodes_sorted)\n\n    # Label axes\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Node ID\")\n    ax.legend(handles=legend_handles, title=\"Population\", loc=\"upper right\", framealpha=0.9)\n\n    return ax\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.spikes.plot_firing_rate_pop_stats","title":"<code>bmtool.bmplot.spikes.plot_firing_rate_pop_stats(firing_stats, groupby, ax=None, color_map=None)</code>","text":"<p>Plots a bar graph of mean firing rates with error bars (standard deviation).</p> Parameters: <p>firing_stats : pd.DataFrame     Dataframe containing 'firing_rate_mean' and 'firing_rate_std'. groupby : str or list of str     Column(s) used for grouping. ax : matplotlib.axes.Axes, optional     Axes on which to plot the bar chart; if None, a new figure and axes are created. color_map : dict, optional     Dictionary specifying colors for each group. Keys should be group names, and values should be color values.</p> Returns: <p>matplotlib.axes.Axes     Axes with the bar plot.</p> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def plot_firing_rate_pop_stats(\n    firing_stats: pd.DataFrame,\n    groupby: Union[str, List[str]],\n    ax: Optional[Axes] = None,\n    color_map: Optional[Dict[str, str]] = None,\n) -&gt; Axes:\n    \"\"\"\n    Plots a bar graph of mean firing rates with error bars (standard deviation).\n\n    Parameters:\n    ----------\n    firing_stats : pd.DataFrame\n        Dataframe containing 'firing_rate_mean' and 'firing_rate_std'.\n    groupby : str or list of str\n        Column(s) used for grouping.\n    ax : matplotlib.axes.Axes, optional\n        Axes on which to plot the bar chart; if None, a new figure and axes are created.\n    color_map : dict, optional\n        Dictionary specifying colors for each group. Keys should be group names, and values should be color values.\n\n    Returns:\n    -------\n    matplotlib.axes.Axes\n        Axes with the bar plot.\n    \"\"\"\n    # Ensure groupby is a list for consistent handling\n    sns.set_style(\"whitegrid\")\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    # Create a categorical column for grouping\n    firing_stats[\"group\"] = firing_stats[groupby].astype(str).agg(\"_\".join, axis=1)\n\n    # Get unique group names\n    unique_groups = firing_stats[\"group\"].unique()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"viridis\")\n        color_map = {group: cmap(i / len(unique_groups)) for i, group in enumerate(unique_groups)}\n    else:\n        # Ensure color_map contains all groups\n        missing_colors = [group for group in unique_groups if group not in color_map]\n        if missing_colors:\n            raise ValueError(f\"color_map is missing colors for groups: {missing_colors}\")\n\n    # Create new figure and axes if ax is not provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Sort data for consistent plotting\n    firing_stats = firing_stats.sort_values(by=\"group\")\n\n    # Extract values for plotting\n    x_labels = firing_stats[\"group\"]\n    means = firing_stats[\"firing_rate_mean\"]\n    std_devs = firing_stats[\"firing_rate_std\"]\n\n    # Get colors for each group\n    colors = [color_map[group] for group in x_labels]\n\n    # Create bar plot\n    bars = ax.bar(x_labels, means, yerr=std_devs, capsize=5, color=colors, edgecolor=\"black\")\n\n    # Add error bars manually with caps\n    _, caps, _ = ax.errorbar(\n        x=np.arange(len(x_labels)),\n        y=means,\n        yerr=std_devs,\n        fmt=\"none\",\n        capsize=5,\n        capthick=2,\n        color=\"black\",\n    )\n\n    # Formatting\n    ax.set_xticks(np.arange(len(x_labels)))\n    ax.set_xticklabels(x_labels, rotation=45, ha=\"right\")\n    ax.set_xlabel(\"Population Group\")\n    ax.set_ylabel(\"Mean Firing Rate (spikes/s)\")\n    ax.set_title(\"Firing Rate Statistics by Population\")\n    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n    return ax\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.spikes.plot_firing_rate_distribution","title":"<code>bmtool.bmplot.spikes.plot_firing_rate_distribution(individual_stats, groupby, ax=None, color_map=None, plot_type='box', swarm_alpha=0.6, logscale=False)</code>","text":"<p>Plots a distribution of individual firing rates using one or more plot types (box plot, violin plot, or swarm plot), overlaying them on top of each other.</p> Parameters: <p>individual_stats : pd.DataFrame     Dataframe containing individual firing rates and corresponding group labels. groupby : str or list of str     Column(s) used for grouping. ax : matplotlib.axes.Axes, optional     Axes on which to plot the graph; if None, a new figure and axes are created. color_map : dict, optional     Dictionary specifying colors for each group. Keys should be group names, and values should be color values. plot_type : str or list of str, optional     List of plot types to generate. Options: \"box\", \"violin\", \"swarm\". Default is \"box\". swarm_alpha : float, optional     Transparency of swarm plot points. Default is 0.6. logscale : bool, optional     If True, use logarithmic scale for the y-axis (default is False).</p> Returns: <p>matplotlib.axes.Axes     Axes with the selected plot type(s) overlayed.</p> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def plot_firing_rate_distribution(\n    individual_stats: pd.DataFrame,\n    groupby: Union[str, List[str]],\n    ax: Optional[Axes] = None,\n    color_map: Optional[Dict[str, str]] = None,\n    plot_type: Union[str, List[str]] = \"box\",\n    swarm_alpha: float = 0.6,\n    logscale: bool = False,\n) -&gt; Axes:\n    \"\"\"\n    Plots a distribution of individual firing rates using one or more plot types\n    (box plot, violin plot, or swarm plot), overlaying them on top of each other.\n\n    Parameters:\n    ----------\n    individual_stats : pd.DataFrame\n        Dataframe containing individual firing rates and corresponding group labels.\n    groupby : str or list of str\n        Column(s) used for grouping.\n    ax : matplotlib.axes.Axes, optional\n        Axes on which to plot the graph; if None, a new figure and axes are created.\n    color_map : dict, optional\n        Dictionary specifying colors for each group. Keys should be group names, and values should be color values.\n    plot_type : str or list of str, optional\n        List of plot types to generate. Options: \"box\", \"violin\", \"swarm\". Default is \"box\".\n    swarm_alpha : float, optional\n        Transparency of swarm plot points. Default is 0.6.\n    logscale : bool, optional\n        If True, use logarithmic scale for the y-axis (default is False).\n\n    Returns:\n    -------\n    matplotlib.axes.Axes\n        Axes with the selected plot type(s) overlayed.\n    \"\"\"\n    sns.set_style(\"whitegrid\")\n    # Ensure groupby is a list for consistent handling\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    # Create a categorical column for grouping\n    individual_stats[\"group\"] = individual_stats[groupby].astype(str).agg(\"_\".join, axis=1)\n\n    # Validate plot_type (it can be a list or a single type)\n    if isinstance(plot_type, str):\n        plot_type = [plot_type]\n\n    for pt in plot_type:\n        if pt not in [\"box\", \"violin\", \"swarm\"]:\n            raise ValueError(\"plot_type must be one of: 'box', 'violin', 'swarm'.\")\n\n    # Get unique groups for coloring\n    unique_groups = individual_stats[\"group\"].unique()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"viridis\")\n        color_map = {group: cmap(i / len(unique_groups)) for i, group in enumerate(unique_groups)}\n\n    # Ensure color_map contains all groups\n    missing_colors = [group for group in unique_groups if group not in color_map]\n    if missing_colors:\n        raise ValueError(f\"color_map is missing colors for groups: {missing_colors}\")\n\n    # Create new figure and axes if ax is not provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Sort data for consistent plotting\n    individual_stats = individual_stats.sort_values(by=\"group\")\n\n    # Loop over each plot type and overlay them\n    for pt in plot_type:\n        if pt == \"box\":\n            sns.boxplot(\n                data=individual_stats,\n                x=\"group\",\n                y=\"firing_rate\",\n                ax=ax,\n                palette=color_map,\n                width=0.5,\n            )\n        elif pt == \"violin\":\n            sns.violinplot(\n                data=individual_stats,\n                x=\"group\",\n                y=\"firing_rate\",\n                ax=ax,\n                palette=color_map,\n                inner=\"box\",\n                alpha=0.4,\n                cut=0,  # This prevents the KDE from extending beyond the data range\n            )\n        elif pt == \"swarm\":\n            sns.swarmplot(\n                data=individual_stats,\n                x=\"group\",\n                y=\"firing_rate\",\n                ax=ax,\n                palette=color_map,\n                alpha=swarm_alpha,\n            )\n\n    # Formatting\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_xlabel(\"Population Group\")\n    ax.set_ylabel(\"Firing Rate (spikes/s)\")\n    ax.set_title(\"Firing Rate Distribution for individual cells\")\n    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n    if logscale:\n        ax.set_yscale('log')\n\n    return ax\n</code></pre>"},{"location":"api/bmplot/#entrainment-module","title":"Entrainment Module","text":""},{"location":"api/bmplot/#bmtool.bmplot.entrainment.plot_spike_power_correlation","title":"<code>bmtool.bmplot.entrainment.plot_spike_power_correlation(spike_df, lfp_data, firing_quantile, fs, pop_names, filter_method='wavelet', bandwidth=2.0, lowcut=None, highcut=None, freq_range=(10, 100), freq_step=5, type_name='raw', time_windows=None, error_type='ci')</code>","text":"<p>Calculate and plot correlation between population spike rates and LFP power across frequencies. Supports both single-signal and trial-based analysis with error bars.</p> <p>Parameters:</p> Name Type Description Default <code>spike_df</code> <code>DataFrame</code> <p>DataFrame containing spike data with columns 'timestamps', 'node_ids', and 'pop_name'.</p> required <code>lfp_data</code> <code>DataArray</code> <p>LFP data</p> required <code>firing_quantile</code> <code>float</code> <p>Upper quantile threshold for selecting high-firing cells (e.g., 0.8 for top 20%)</p> required <code>fs</code> <code>float</code> <p>Sampling frequency</p> required <code>pop_names</code> <code>List[str]</code> <p>List of population names to analyze</p> required <code>filter_method</code> <code>str</code> <p>Filtering method to use, either 'wavelet' or 'butter' (default: 'wavelet')</p> <code>'wavelet'</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2.0</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>freq_range</code> <code>Tuple[float, float]</code> <p>Min and max frequency to analyze (default: (10, 100))</p> <code>(10, 100)</code> <code>freq_step</code> <code>float</code> <p>Step size for frequency analysis (default: 5)</p> <code>5</code> <code>type_name</code> <code>str</code> <p>Which type of spike rate to use if 'type' dimension exists (default: 'raw')</p> <code>'raw'</code> <code>time_windows</code> <code>List[Tuple[float, float]]</code> <p>List of (start, end) time tuples for trial-based analysis. If None, analyze entire signal</p> <code>None</code> <code>error_type</code> <code>str</code> <p>Type of error bars to plot: \"ci\" for 95% confidence interval, \"sem\" for standard error, \"std\" for standard deviation</p> <code>'ci'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The figure containing the correlation plot</p> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def plot_spike_power_correlation(\n    spike_df: pd.DataFrame,\n    lfp_data: xr.DataArray,\n    firing_quantile: float,\n    fs: float,\n    pop_names: List[str],\n    filter_method: str = \"wavelet\",\n    bandwidth: float = 2.0,\n    lowcut: Optional[float] = None,\n    highcut: Optional[float] = None,\n    freq_range: Tuple[float, float] = (10, 100),\n    freq_step: float = 5,\n    type_name: str = \"raw\",\n    time_windows: Optional[List[Tuple[float, float]]] = None,\n    error_type: str = \"ci\",\n):\n    \"\"\"\n    Calculate and plot correlation between population spike rates and LFP power across frequencies.\n    Supports both single-signal and trial-based analysis with error bars.\n\n    Parameters\n    ----------\n    spike_df : pd.DataFrame\n        DataFrame containing spike data with columns 'timestamps', 'node_ids', and 'pop_name'.\n    lfp_data : xr.DataArray\n        LFP data\n    firing_quantile : float\n        Upper quantile threshold for selecting high-firing cells (e.g., 0.8 for top 20%)\n    fs : float\n        Sampling frequency\n    pop_names : List[str]\n        List of population names to analyze\n    filter_method : str, optional\n        Filtering method to use, either 'wavelet' or 'butter' (default: 'wavelet')\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    freq_range : Tuple[float, float], optional\n        Min and max frequency to analyze (default: (10, 100))\n    freq_step : float, optional\n        Step size for frequency analysis (default: 5)\n    type_name : str, optional\n        Which type of spike rate to use if 'type' dimension exists (default: 'raw')\n    time_windows : List[Tuple[float, float]], optional\n        List of (start, end) time tuples for trial-based analysis. If None, analyze entire signal\n    error_type : str, optional\n        Type of error bars to plot: \"ci\" for 95% confidence interval, \"sem\" for standard error, \"std\" for standard deviation\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The figure containing the correlation plot\n    \"\"\"\n\n    if not (0 &lt;= firing_quantile &lt; 1):\n        raise ValueError(\"firing_quantile must be between 0 and 1\")\n\n    if error_type not in [\"ci\", \"sem\", \"std\"]:\n        raise ValueError(\n            \"error_type must be 'ci' for confidence interval, 'sem' for standard error, or 'std' for standard deviation\"\n        )\n\n    # Setup\n    is_trial_based = time_windows is not None\n\n    # Convert spike_df to spike rate with trial-based filtering of high firing cells\n    if is_trial_based:\n        # Initialize storage for trial-based spike rates\n        trial_rates = []\n\n        for start_time, end_time in time_windows:\n            # Get spikes for this trial\n            trial_spikes = spike_df[\n                (spike_df[\"timestamps\"] &gt;= start_time) &amp; (spike_df[\"timestamps\"] &lt;= end_time)\n            ].copy()\n\n            # Filter for high firing cells within this trial\n            trial_spikes = bmspikes.find_highest_firing_cells(\n                trial_spikes, upper_quantile=firing_quantile\n            )\n            # Calculate rate for this trial's filtered spikes\n            trial_rate = bmspikes.get_population_spike_rate(\n                trial_spikes, fs=fs, t_start=start_time, t_stop=end_time\n            )\n            trial_rates.append(trial_rate)\n\n        # Combine all trial rates\n        spike_rate = xr.concat(trial_rates, dim=\"trial\")\n    else:\n        # For non-trial analysis, proceed as before\n        spike_df = bmspikes.find_highest_firing_cells(spike_df, upper_quantile=firing_quantile)\n        spike_rate = bmspikes.get_population_spike_rate(spike_df)\n\n    # Setup frequencies for analysis\n    frequencies = np.arange(freq_range[0], freq_range[1] + 1, freq_step)\n\n    # Pre-calculate LFP power for all frequencies\n    power_by_freq = {}\n    for freq in frequencies:\n        power_by_freq[freq] = get_lfp_power(\n            lfp_data, freq, fs, filter_method, lowcut=lowcut, highcut=highcut, bandwidth=bandwidth\n        )\n\n    # Calculate correlations\n    results = {}\n    for pop in pop_names:\n        pop_spike_rate = spike_rate.sel(population=pop, type=type_name)\n        results[pop] = {}\n\n        for freq in frequencies:\n            lfp_power = power_by_freq[freq]\n\n            if not is_trial_based:\n                # Single signal analysis\n                if len(pop_spike_rate) != len(lfp_power):\n                    print(f\"Warning: Length mismatch for {pop} at {freq} Hz\")\n                    continue\n\n                corr, p_val = stats.spearmanr(pop_spike_rate, lfp_power)\n                results[pop][freq] = {\n                    \"correlation\": corr,\n                    \"p_value\": p_val,\n                }\n            else:\n                # Trial-based analysis using pre-filtered trial rates\n                trial_correlations = []\n\n                for trial_idx in range(len(time_windows)):\n                    # Get time window first\n                    start_time, end_time = time_windows[trial_idx]\n\n                    # Get the pre-filtered spike rate for this trial\n                    trial_spike_rate = pop_spike_rate.sel(trial=trial_idx)\n\n                    # Get corresponding LFP power for this trial window\n                    trial_lfp_power = lfp_power.sel(time=slice(start_time, end_time))\n\n                    # Ensure both signals have same time points\n                    common_times = np.intersect1d(trial_spike_rate.time, trial_lfp_power.time)\n\n                    if len(common_times) &gt; 0:\n                        trial_sr = trial_spike_rate.sel(time=common_times).values\n                        trial_lfp = trial_lfp_power.sel(time=common_times).values\n\n                        if (\n                            len(trial_sr) &gt; 1 and len(trial_lfp) &gt; 1\n                        ):  # Need at least 2 points for correlation\n                            corr, _ = stats.spearmanr(trial_sr, trial_lfp)\n                            if not np.isnan(corr):\n                                trial_correlations.append(corr)\n\n                # Calculate trial statistics\n                if len(trial_correlations) &gt; 0:\n                    trial_correlations = np.array(trial_correlations)\n                    mean_corr = np.mean(trial_correlations)\n\n                    if len(trial_correlations) &gt; 1:\n                        if error_type == \"ci\":\n                            # Calculate 95% confidence interval using t-distribution\n                            df = len(trial_correlations) - 1\n                            sem = stats.sem(trial_correlations)\n                            t_critical = stats.t.ppf(0.975, df)  # 95% CI, two-tailed\n                            error_val = t_critical * sem\n                            error_lower = mean_corr - error_val\n                            error_upper = mean_corr + error_val\n                        elif error_type == \"sem\":\n                            # Calculate standard error of the mean\n                            sem = stats.sem(trial_correlations)\n                            error_lower = mean_corr - sem\n                            error_upper = mean_corr + sem\n                        elif error_type == \"std\":\n                            # Calculate standard deviation\n                            std = np.std(trial_correlations, ddof=1)\n                            error_lower = mean_corr - std\n                            error_upper = mean_corr + std\n                    else:\n                        error_lower = error_upper = mean_corr\n\n                    results[pop][freq] = {\n                        \"correlation\": mean_corr,\n                        \"error_lower\": error_lower,\n                        \"error_upper\": error_upper,\n                        \"n_trials\": len(trial_correlations),\n                        \"trial_correlations\": trial_correlations,\n                    }\n                else:\n                    # No valid trials\n                    results[pop][freq] = {\n                        \"correlation\": np.nan,\n                        \"error_lower\": np.nan,\n                        \"error_upper\": np.nan,\n                        \"n_trials\": 0,\n                        \"trial_correlations\": np.array([]),\n                    }\n\n    # Plotting\n    sns.set_style(\"whitegrid\")\n    fig = plt.figure(figsize=(12, 8))\n\n    for i, pop in enumerate(pop_names):\n        # Extract data for plotting\n        plot_freqs = []\n        plot_corrs = []\n        plot_ci_lower = []\n        plot_ci_upper = []\n\n        for freq in frequencies:\n            if freq in results[pop] and not np.isnan(results[pop][freq][\"correlation\"]):\n                plot_freqs.append(freq)\n                plot_corrs.append(results[pop][freq][\"correlation\"])\n\n                if is_trial_based:\n                    plot_ci_lower.append(results[pop][freq][\"error_lower\"])\n                    plot_ci_upper.append(results[pop][freq][\"error_upper\"])\n\n        if len(plot_freqs) == 0:\n            continue\n\n        # Convert to arrays\n        plot_freqs = np.array(plot_freqs)\n        plot_corrs = np.array(plot_corrs)\n\n        # Get color for this population\n        colors = plt.get_cmap(\"tab10\")\n        color = colors(i)\n\n        # Plot main line\n        plt.plot(\n            plot_freqs, plot_corrs, marker=\"o\", label=pop, linewidth=2, markersize=6, color=color\n        )\n\n        # Plot error bands for trial-based analysis\n        if is_trial_based and len(plot_ci_lower) &gt; 0:\n            plot_ci_lower = np.array(plot_ci_lower)\n            plot_ci_upper = np.array(plot_ci_upper)\n            plt.fill_between(plot_freqs, plot_ci_lower, plot_ci_upper, alpha=0.2, color=color)\n\n    # Formatting\n    plt.xlabel(\"Frequency (Hz)\", fontsize=12)\n    plt.ylabel(\"Spike Rate-Power Correlation\", fontsize=12)\n\n    # Calculate percentage for title\n    firing_percentage = round(float((1 - firing_quantile) * 100), 1)\n    if is_trial_based:\n        title = f\"Trial-averaged Spike Rate-LFP Power Correlation\\nTop {firing_percentage}% Firing Cells (95% CI)\"\n    else:\n        title = f\"Spike Rate-LFP Power Correlation\\nTop {firing_percentage}% Firing Cells\"\n\n    plt.title(title, fontsize=14)\n    plt.grid(True, alpha=0.3)\n    plt.axhline(y=0, color=\"gray\", linestyle=\"-\", alpha=0.5)\n\n    # Legend\n    # Create legend elements for each population\n    from matplotlib.lines import Line2D\n\n    colors = plt.get_cmap(\"tab10\")\n    legend_elements = [\n        Line2D([0], [0], color=colors(i), marker=\"o\", linestyle=\"-\", label=pop)\n        for i, pop in enumerate(pop_names)\n    ]\n\n    # Add error band legend element for trial-based analysis\n    if is_trial_based:\n        # Map error type to legend label\n        error_labels = {\"ci\": \"95% CI\", \"sem\": \"\u00b1SEM\", \"std\": \"\u00b11 SD\"}\n        error_label = error_labels[error_type]\n\n        legend_elements.append(\n            Line2D([0], [0], color=\"gray\", alpha=0.3, linewidth=10, label=error_label)\n        )\n\n    plt.legend(handles=legend_elements, fontsize=10, loc=\"best\")\n\n    # Axis formatting\n    if len(frequencies) &gt; 10:\n        plt.xticks(frequencies[::2])\n    else:\n        plt.xticks(frequencies)\n    plt.xlim(frequencies[0], frequencies[-1])\n\n    y_min, y_max = plt.ylim()\n    plt.ylim(min(y_min, -0.1), max(y_max, 0.1))\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/#lfp-module","title":"LFP Module","text":""},{"location":"api/bmplot/#bmtool.bmplot.lfp.plot_spectrogram","title":"<code>bmtool.bmplot.lfp.plot_spectrogram(sxx_xarray, remove_aperiodic=None, log_power=False, plt_range=None, clr_freq_range=None, pad=0.03, ax=None)</code>","text":"<p>Plot a power spectrogram with optional aperiodic removal and frequency-based coloring.</p> <p>Parameters:</p> Name Type Description Default <code>sxx_xarray</code> <code>array - like</code> <p>Spectrogram data as an xarray DataArray with PSD values.</p> required <code>remove_aperiodic</code> <code>optional</code> <p>FOOOF model object for aperiodic subtraction. If None, raw spectrum is displayed.</p> <code>None</code> <code>log_power</code> <code>bool or str</code> <p>If True or 'dB', convert power to log scale. Default is False.</p> <code>False</code> <code>plt_range</code> <code>tuple of float</code> <p>Frequency range to display as (f_min, f_max). If None, displays full range.</p> <code>None</code> <code>clr_freq_range</code> <code>tuple of float</code> <p>Frequency range to use for determining color limits. If None, uses full range.</p> <code>None</code> <code>pad</code> <code>float</code> <p>Padding for colorbar. Default is 0.03.</p> <code>0.03</code> <code>ax</code> <code>Axes</code> <p>Axes to plot on. If None, creates a new figure and axes.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The figure object containing the spectrogram.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig = plot_spectrogram(\n...     sxx_xarray, log_power='dB',\n...     plt_range=(10, 100), clr_freq_range=(20, 50)\n... )\n</code></pre> Source code in <code>bmtool/bmplot/lfp.py</code> <pre><code>def plot_spectrogram(\n    sxx_xarray: Any,\n    remove_aperiodic: Optional[Any] = None,\n    log_power: bool = False,\n    plt_range: Optional[Tuple[float, float]] = None,\n    clr_freq_range: Optional[Tuple[float, float]] = None,\n    pad: float = 0.03,\n    ax: Optional[plt.Axes] = None,\n) -&gt; Figure:\n    \"\"\"\n    Plot a power spectrogram with optional aperiodic removal and frequency-based coloring.\n\n    Parameters\n    ----------\n    sxx_xarray : array-like\n        Spectrogram data as an xarray DataArray with PSD values.\n    remove_aperiodic : optional\n        FOOOF model object for aperiodic subtraction. If None, raw spectrum is displayed.\n    log_power : bool or str, optional\n        If True or 'dB', convert power to log scale. Default is False.\n    plt_range : tuple of float, optional\n        Frequency range to display as (f_min, f_max). If None, displays full range.\n    clr_freq_range : tuple of float, optional\n        Frequency range to use for determining color limits. If None, uses full range.\n    pad : float, optional\n        Padding for colorbar. Default is 0.03.\n    ax : matplotlib.axes.Axes, optional\n        Axes to plot on. If None, creates a new figure and axes.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The figure object containing the spectrogram.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = plot_spectrogram(\n    ...     sxx_xarray, log_power='dB',\n    ...     plt_range=(10, 100), clr_freq_range=(20, 50)\n    ... )\n    \"\"\"\n    sxx = sxx_xarray.PSD.values.copy()\n    t = sxx_xarray.time.values.copy()\n    f = sxx_xarray.frequency.values.copy()\n\n    cbar_label = \"PSD\" if remove_aperiodic is None else \"PSD Residual\"\n    if log_power:\n        with np.errstate(divide=\"ignore\"):\n            sxx = np.log10(sxx)\n        cbar_label += \" dB\" if log_power == \"dB\" else \" log(power)\"\n\n    if remove_aperiodic is not None:\n        f1_idx = 0 if f[0] else 1\n        ap_fit = gen_aperiodic(f[f1_idx:], remove_aperiodic.aperiodic_params)\n        sxx[f1_idx:, :] -= (ap_fit if log_power else 10**ap_fit)[:, None]\n        sxx[:f1_idx, :] = 0.0\n\n    if log_power == \"dB\":\n        sxx *= 10\n\n    if ax is None:\n        _, ax = plt.subplots(1, 1)\n    plt_range = np.array(f[-1]) if plt_range is None else np.array(plt_range)\n    if plt_range.size == 1:\n        plt_range = [f[0 if f[0] else 1] if log_power else 0.0, plt_range.item()]\n    f_idx = (f &gt;= plt_range[0]) &amp; (f &lt;= plt_range[1])\n    if clr_freq_range is None:\n        vmin, vmax = None, None\n    else:\n        c_idx = (f &gt;= clr_freq_range[0]) &amp; (f &lt;= clr_freq_range[1])\n        vmin, vmax = sxx[c_idx, :].min(), sxx[c_idx, :].max()\n\n    f = f[f_idx]\n    pcm = ax.pcolormesh(t, f, sxx[f_idx, :], shading=\"gouraud\", vmin=vmin, vmax=vmax, rasterized=True)\n    if \"cone_of_influence_frequency\" in sxx_xarray:\n        coif = sxx_xarray.cone_of_influence_frequency\n        ax.plot(t, coif)\n        ax.fill_between(t, coif, step=\"mid\", alpha=0.2)\n    ax.set_xlim(t[0], t[-1])\n    # ax.set_xlim(t[0],0.2)\n    ax.set_ylim(f[0], f[-1])\n    plt.colorbar(mappable=pcm, ax=ax, label=cbar_label, pad=pad)\n    ax.set_xlabel(\"Time (sec)\")\n    ax.set_ylabel(\"Frequency (Hz)\")\n    return ax.figure\n</code></pre>"},{"location":"api/connectors/","title":"Connectors API Reference","text":"<p>This page provides API reference documentation for the Connectors module, which contains classes and functions for creating complex connectivity patterns in BMTK networks.</p>"},{"location":"api/connectors/#utility-functions","title":"Utility Functions","text":""},{"location":"api/connectors/#bmtool.connectors.num_prop","title":"<code>bmtool.connectors.num_prop(ratio, N)</code>","text":"<p>Calculate numbers of total N in proportion to ratio.</p> Parameters: <p>ratio : array-like     Proportions to distribute N across. N : int     Total number to distribute.</p> Returns: <p>numpy.ndarray     Array of integers that sum to N, proportionally distributed according to ratio.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def num_prop(ratio, N):\n    \"\"\"\n    Calculate numbers of total N in proportion to ratio.\n\n    Parameters:\n    -----------\n    ratio : array-like\n        Proportions to distribute N across.\n    N : int\n        Total number to distribute.\n\n    Returns:\n    --------\n    numpy.ndarray\n        Array of integers that sum to N, proportionally distributed according to ratio.\n    \"\"\"\n    ratio = np.asarray(ratio)\n    p = np.cumsum(np.insert(ratio.ravel(), 0, 0))  # cumulative proportion\n    return np.diff(np.round(N / p[-1] * p).astype(int)).reshape(ratio.shape)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.decision","title":"<code>bmtool.connectors.decision(prob, size=None)</code>","text":"<p>Make random decision(s) based on input probability.</p> Parameters: <p>prob : float     Probability threshold between 0 and 1. size : int or tuple, optional     Size of the output array. If None, a single decision is returned.</p> Returns: <p>bool or numpy.ndarray     Boolean result(s) of the random decision(s). True if the random number     is less than prob, False otherwise.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def decision(prob, size=None):\n    \"\"\"\n    Make random decision(s) based on input probability.\n\n    Parameters:\n    -----------\n    prob : float\n        Probability threshold between 0 and 1.\n    size : int or tuple, optional\n        Size of the output array. If None, a single decision is returned.\n\n    Returns:\n    --------\n    bool or numpy.ndarray\n        Boolean result(s) of the random decision(s). True if the random number\n        is less than prob, False otherwise.\n    \"\"\"\n    return rng.random(size) &lt; prob\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.decisions","title":"<code>bmtool.connectors.decisions(prob)</code>","text":"<p>Make multiple random decisions based on input probabilities.</p> Parameters: <p>prob : array-like     Array of probability thresholds between 0 and 1.</p> Returns: <p>numpy.ndarray     Boolean array with the same shape as prob, containing results of     the random decisions.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def decisions(prob):\n    \"\"\"\n    Make multiple random decisions based on input probabilities.\n\n    Parameters:\n    -----------\n    prob : array-like\n        Array of probability thresholds between 0 and 1.\n\n    Returns:\n    --------\n    numpy.ndarray\n        Boolean array with the same shape as prob, containing results of\n        the random decisions.\n    \"\"\"\n    prob = np.asarray(prob)\n    return rng.random(prob.shape) &lt; prob\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.euclid_dist","title":"<code>bmtool.connectors.euclid_dist(p1, p2)</code>","text":"<p>Euclidean distance between two points p1, p2: Coordinates in numpy array</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def euclid_dist(p1, p2):\n    \"\"\"\n    Euclidean distance between two points\n    p1, p2: Coordinates in numpy array\n    \"\"\"\n    dvec = np.asarray(p1) - np.asarray(p2)\n    return (dvec @ dvec) ** 0.5\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.spherical_dist","title":"<code>bmtool.connectors.spherical_dist(node1, node2)</code>","text":"<p>Spherical distance between two input nodes</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def spherical_dist(node1, node2):\n    \"\"\"Spherical distance between two input nodes\"\"\"\n    return euclid_dist(node1[\"positions\"], node2[\"positions\"]).item()\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.cylindrical_dist_z","title":"<code>bmtool.connectors.cylindrical_dist_z(node1, node2)</code>","text":"<p>Cylindircal distance between two input nodes (ignoring z-axis)</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def cylindrical_dist_z(node1, node2):\n    \"\"\"Cylindircal distance between two input nodes (ignoring z-axis)\"\"\"\n    return euclid_dist(node1[\"positions\"][:2], node2[\"positions\"][:2]).item()\n</code></pre>"},{"location":"api/connectors/#probability-functions","title":"Probability Functions","text":""},{"location":"api/connectors/#bmtool.connectors.ProbabilityFunction","title":"<code>bmtool.connectors.ProbabilityFunction</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for connection probability function</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class ProbabilityFunction(ABC):\n    \"\"\"Abstract base class for connection probability function\"\"\"\n\n    @abstractmethod\n    def probability(self, *arg, **kwargs):\n        \"\"\"Allow numpy array input and return probability in numpy array\"\"\"\n        return NotImplemented\n\n    @abstractmethod\n    def __call__(self, *arg, **kwargs):\n        \"\"\"Return probability within [0, 1] for single input\"\"\"\n        return NotImplemented\n\n    @abstractmethod\n    def decisions(self, *arg, **kwargs):\n        \"\"\"Return bool array of decisions according probability\"\"\"\n        return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ProbabilityFunction.__call__","title":"<code>__call__(*arg, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Return probability within [0, 1] for single input</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@abstractmethod\ndef __call__(self, *arg, **kwargs):\n    \"\"\"Return probability within [0, 1] for single input\"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ProbabilityFunction.decisions","title":"<code>decisions(*arg, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Return bool array of decisions according probability</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@abstractmethod\ndef decisions(self, *arg, **kwargs):\n    \"\"\"Return bool array of decisions according probability\"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ProbabilityFunction.probability","title":"<code>probability(*arg, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Allow numpy array input and return probability in numpy array</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@abstractmethod\ndef probability(self, *arg, **kwargs):\n    \"\"\"Allow numpy array input and return probability in numpy array\"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.DistantDependentProbability","title":"<code>bmtool.connectors.DistantDependentProbability</code>","text":"<p>               Bases: <code>ProbabilityFunction</code></p> <p>Base class for distance dependent probability</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class DistantDependentProbability(ProbabilityFunction):\n    \"\"\"Base class for distance dependent probability\"\"\"\n\n    def __init__(self, min_dist=0.0, max_dist=np.inf):\n        assert min_dist &gt;= 0 and min_dist &lt; max_dist\n        self.min_dist, self.max_dist = min_dist, max_dist\n\n    def __call__(self, dist, *arg, **kwargs):\n        \"\"\"Return probability for single distance input\"\"\"\n        if dist &gt;= self.min_dist and dist &lt;= self.max_dist:\n            return self.probability(dist)\n        else:\n            return 0.0\n\n    def decisions(self, dist):\n        \"\"\"Return bool array of decisions given distance array\"\"\"\n        dist = np.asarray(dist)\n        dec = np.zeros(dist.shape, dtype=bool)\n        mask = (dist &gt;= self.min_dist) &amp; (dist &lt;= self.max_dist)\n        dist = dist[mask]\n        prob = np.empty(dist.shape)\n        prob[:] = self.probability(dist)\n        dec[mask] = decisions(prob)\n        return dec\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.DistantDependentProbability.__call__","title":"<code>__call__(dist, *arg, **kwargs)</code>","text":"<p>Return probability for single distance input</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def __call__(self, dist, *arg, **kwargs):\n    \"\"\"Return probability for single distance input\"\"\"\n    if dist &gt;= self.min_dist and dist &lt;= self.max_dist:\n        return self.probability(dist)\n    else:\n        return 0.0\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.DistantDependentProbability.decisions","title":"<code>decisions(dist)</code>","text":"<p>Return bool array of decisions given distance array</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def decisions(self, dist):\n    \"\"\"Return bool array of decisions given distance array\"\"\"\n    dist = np.asarray(dist)\n    dec = np.zeros(dist.shape, dtype=bool)\n    mask = (dist &gt;= self.min_dist) &amp; (dist &lt;= self.max_dist)\n    dist = dist[mask]\n    prob = np.empty(dist.shape)\n    prob[:] = self.probability(dist)\n    dec[mask] = decisions(prob)\n    return dec\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UniformInRange","title":"<code>bmtool.connectors.UniformInRange</code>","text":"<p>               Bases: <code>DistantDependentProbability</code></p> <p>Constant probability within a distance range</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class UniformInRange(DistantDependentProbability):\n    \"\"\"Constant probability within a distance range\"\"\"\n\n    def __init__(self, p=0.0, min_dist=0.0, max_dist=np.inf):\n        super().__init__(min_dist=min_dist, max_dist=max_dist)\n        self.p = np.array(p)\n        assert self.p.size == 1\n        assert p &gt;= 0.0 and p &lt;= 1.0\n\n    def probability(self, dist):\n        return self.p\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.gaussian","title":"<code>bmtool.connectors.gaussian(x, mean=0.0, stdev=1.0, pmax=NORM_COEF)</code>","text":"<p>Gaussian function. Default is the PDF of standard normal distribution</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def gaussian(x, mean=0.0, stdev=1.0, pmax=NORM_COEF):\n    \"\"\"Gaussian function. Default is the PDF of standard normal distribution\"\"\"\n    x = (x - mean) / stdev\n    return pmax * np.exp(-x * x / 2)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GaussianDropoff","title":"<code>bmtool.connectors.GaussianDropoff</code>","text":"<p>               Bases: <code>DistantDependentProbability</code></p> <p>Connection probability class that follows a Gaussian function of distance.</p> <p>This class calculates connection probabilities using a Gaussian function of the distance between cells, with options for spherical or cylindrical metrics.</p> Parameters: <p>mean : float, optional     Mean parameter of the Gaussian function, typically 0 for peak at origin. stdev : float, optional     Standard deviation parameter controlling the width of the Gaussian. min_dist : float, optional     Minimum distance for connections. Below this distance, probability is zero. max_dist : float, optional     Maximum distance for connections. Above this distance, probability is zero. pmax : float, optional     Maximum probability value at the peak of the Gaussian function. ptotal : float, optional     Overall connection probability within the specified distance range.     If provided, pmax is calculated to achieve this overall probability. ptotal_dist_range : tuple, optional     Distance range (min_dist, max_dist) for calculating pmax when ptotal is provided. dist_type : str, optional     Distance metric to use, either 'spherical' (default) or 'cylindrical'.</p> Notes: <p>When ptotal is specified, the maximum probability (pmax) is calculated to achieve the desired overall connection probability within the specified distance range, assuming homogeneous cell density.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class GaussianDropoff(DistantDependentProbability):\n    \"\"\"\n    Connection probability class that follows a Gaussian function of distance.\n\n    This class calculates connection probabilities using a Gaussian function\n    of the distance between cells, with options for spherical or cylindrical metrics.\n\n    Parameters:\n    -----------\n    mean : float, optional\n        Mean parameter of the Gaussian function, typically 0 for peak at origin.\n    stdev : float, optional\n        Standard deviation parameter controlling the width of the Gaussian.\n    min_dist : float, optional\n        Minimum distance for connections. Below this distance, probability is zero.\n    max_dist : float, optional\n        Maximum distance for connections. Above this distance, probability is zero.\n    pmax : float, optional\n        Maximum probability value at the peak of the Gaussian function.\n    ptotal : float, optional\n        Overall connection probability within the specified distance range.\n        If provided, pmax is calculated to achieve this overall probability.\n    ptotal_dist_range : tuple, optional\n        Distance range (min_dist, max_dist) for calculating pmax when ptotal is provided.\n    dist_type : str, optional\n        Distance metric to use, either 'spherical' (default) or 'cylindrical'.\n\n    Notes:\n    ------\n    When ptotal is specified, the maximum probability (pmax) is calculated to achieve\n    the desired overall connection probability within the specified distance range,\n    assuming homogeneous cell density.\n    \"\"\"\n\n    def __init__(\n        self,\n        mean=0.0,\n        stdev=1.0,\n        min_dist=0.0,\n        max_dist=np.inf,\n        pmax=1,\n        ptotal=None,\n        ptotal_dist_range=None,\n        dist_type=\"spherical\",\n    ):\n        super().__init__(min_dist=min_dist, max_dist=max_dist)\n        self.mean, self.stdev = mean, stdev\n        self.ptotal = ptotal\n        self.ptotal_dist_range = (\n            (min_dist, max_dist) if ptotal_dist_range is None else ptotal_dist_range\n        )\n        self.dist_type = dist_type if dist_type in [\"cylindrical\"] else \"spherical\"\n        self.pmax = pmax if ptotal is None else self.calc_pmax_from_ptotal()\n        self.set_probability_func()\n\n    def calc_pmax_from_ptotal(self):\n        \"\"\"\n        Calculate the pmax value such that the expected overall connection\n        probability to all possible targets within the distance range [r1, r2]=\n        `ptotal_dist_range` equals ptotal, assuming homogeneous cell density.\n        That is, integral_r1^r2 {g(r)p(r)dr} = ptotal, where g is the Gaussian\n        function with pmax, p(r) is the cell density per unit distance at r\n        normalized by total cell number within the distance range.\n        For cylindrical distance, p(r) = 2 * r / (r2^2 - r1^2)\n        For spherical distance, p(r) = 3 * r^2 / (r2^3 - r1^3)\n        The solution has a closed form except that te error function erf is in\n        the expression, but only when resulting pmax &lt;= 1.\n\n        Caveat: When the calculated pmax &gt; 1, the actual overall probability\n        will be lower than expected and all cells within certain distance will\n        be always connected. This usually happens when the distance range is\n        set too wide. Because a large population will be included for\n        evaluating ptotal, and there will be a significant drop in the Gaussian\n        function as distance gets further. So, a large pmax will be required to\n        achieve the desired ptotal.\n        \"\"\"\n        mu, sig = self.mean, self.stdev\n        r1, r2 = self.ptotal_dist_range[:2]\n        x1, x2 = (r1 - mu) / sig, (r2 - mu) / sig  # normalized distance\n        if self.dist_type == \"cylindrical\":\n            dr = r2**2 - r1**2\n\n            def F(x):\n                f1 = sig * mu / NORM_COEF * erf(x / 2**0.5)\n                f2 = -2 * sig * sig * gaussian(x, pmax=1.0)\n                return f1 + f2\n        else:\n            dr = r2**3 - r1**3\n\n            def F(x):\n                f1 = 1.5 * sig * (sig**2 + mu**2) / NORM_COEF * erf(x / 2**0.5)\n                f2 = -3 * sig * sig * (2 * mu + sig * x) * gaussian(x, pmax=1.0)\n                return f1 + f2\n\n        return self.ptotal * dr / (F(x2) - F(x1))\n\n    def probability(self):\n        pass  # to be set up in set_probability_func()\n\n    def set_probability_func(self):\n        \"\"\"Set up function for calculating probability\"\"\"\n        keys = [\"mean\", \"stdev\", \"pmax\"]\n        kwargs = {key: getattr(self, key) for key in keys}\n        probability = partial(gaussian, **kwargs)\n\n        # Verify maximum probability\n        # (is not self.pmax if self.mean outside distance range)\n        bounds = (self.min_dist, min(self.max_dist, 1e9))\n        pmax = (\n            self.pmax\n            if self.mean &gt;= bounds[0] and self.mean &lt;= bounds[1]\n            else probability(np.asarray(bounds)).max()\n        )\n        if pmax &gt; 1:\n            d = minimize_scalar(\n                lambda x: (probability(x) - 1) ** 2, method=\"bounded\", bounds=bounds\n            ).x\n            warn = (\n                \"\\nWarning: Maximum probability=%.3f is greater than 1. \"\n                \"Probability crosses 1 at distance %.3g.\\n\"\n            ) % (pmax, d)\n            if self.ptotal is not None:\n                warn += \" ptotal may not be reached.\"\n            print(warn, flush=True)\n            self.probability = lambda dist: np.fmin(probability(dist), 1.0)\n        else:\n            self.probability = probability\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GaussianDropoff.calc_pmax_from_ptotal","title":"<code>calc_pmax_from_ptotal()</code>","text":"<p>Calculate the pmax value such that the expected overall connection probability to all possible targets within the distance range [r1, r2]= <code>ptotal_dist_range</code> equals ptotal, assuming homogeneous cell density. That is, integral_r1^r2 {g(r)p(r)dr} = ptotal, where g is the Gaussian function with pmax, p(r) is the cell density per unit distance at r normalized by total cell number within the distance range. For cylindrical distance, p(r) = 2 * r / (r2^2 - r1^2) For spherical distance, p(r) = 3 * r^2 / (r2^3 - r1^3) The solution has a closed form except that te error function erf is in the expression, but only when resulting pmax &lt;= 1.</p> <p>Caveat: When the calculated pmax &gt; 1, the actual overall probability will be lower than expected and all cells within certain distance will be always connected. This usually happens when the distance range is set too wide. Because a large population will be included for evaluating ptotal, and there will be a significant drop in the Gaussian function as distance gets further. So, a large pmax will be required to achieve the desired ptotal.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def calc_pmax_from_ptotal(self):\n    \"\"\"\n    Calculate the pmax value such that the expected overall connection\n    probability to all possible targets within the distance range [r1, r2]=\n    `ptotal_dist_range` equals ptotal, assuming homogeneous cell density.\n    That is, integral_r1^r2 {g(r)p(r)dr} = ptotal, where g is the Gaussian\n    function with pmax, p(r) is the cell density per unit distance at r\n    normalized by total cell number within the distance range.\n    For cylindrical distance, p(r) = 2 * r / (r2^2 - r1^2)\n    For spherical distance, p(r) = 3 * r^2 / (r2^3 - r1^3)\n    The solution has a closed form except that te error function erf is in\n    the expression, but only when resulting pmax &lt;= 1.\n\n    Caveat: When the calculated pmax &gt; 1, the actual overall probability\n    will be lower than expected and all cells within certain distance will\n    be always connected. This usually happens when the distance range is\n    set too wide. Because a large population will be included for\n    evaluating ptotal, and there will be a significant drop in the Gaussian\n    function as distance gets further. So, a large pmax will be required to\n    achieve the desired ptotal.\n    \"\"\"\n    mu, sig = self.mean, self.stdev\n    r1, r2 = self.ptotal_dist_range[:2]\n    x1, x2 = (r1 - mu) / sig, (r2 - mu) / sig  # normalized distance\n    if self.dist_type == \"cylindrical\":\n        dr = r2**2 - r1**2\n\n        def F(x):\n            f1 = sig * mu / NORM_COEF * erf(x / 2**0.5)\n            f2 = -2 * sig * sig * gaussian(x, pmax=1.0)\n            return f1 + f2\n    else:\n        dr = r2**3 - r1**3\n\n        def F(x):\n            f1 = 1.5 * sig * (sig**2 + mu**2) / NORM_COEF * erf(x / 2**0.5)\n            f2 = -3 * sig * sig * (2 * mu + sig * x) * gaussian(x, pmax=1.0)\n            return f1 + f2\n\n    return self.ptotal * dr / (F(x2) - F(x1))\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GaussianDropoff.set_probability_func","title":"<code>set_probability_func()</code>","text":"<p>Set up function for calculating probability</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def set_probability_func(self):\n    \"\"\"Set up function for calculating probability\"\"\"\n    keys = [\"mean\", \"stdev\", \"pmax\"]\n    kwargs = {key: getattr(self, key) for key in keys}\n    probability = partial(gaussian, **kwargs)\n\n    # Verify maximum probability\n    # (is not self.pmax if self.mean outside distance range)\n    bounds = (self.min_dist, min(self.max_dist, 1e9))\n    pmax = (\n        self.pmax\n        if self.mean &gt;= bounds[0] and self.mean &lt;= bounds[1]\n        else probability(np.asarray(bounds)).max()\n    )\n    if pmax &gt; 1:\n        d = minimize_scalar(\n            lambda x: (probability(x) - 1) ** 2, method=\"bounded\", bounds=bounds\n        ).x\n        warn = (\n            \"\\nWarning: Maximum probability=%.3f is greater than 1. \"\n            \"Probability crosses 1 at distance %.3g.\\n\"\n        ) % (pmax, d)\n        if self.ptotal is not None:\n            warn += \" ptotal may not be reached.\"\n        print(warn, flush=True)\n        self.probability = lambda dist: np.fmin(probability(dist), 1.0)\n    else:\n        self.probability = probability\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.NormalizedReciprocalRate","title":"<code>bmtool.connectors.NormalizedReciprocalRate</code>","text":"<p>               Bases: <code>ProbabilityFunction</code></p> <p>Reciprocal connection probability given normalized reciprocal rate. Normalized reciprocal rate is defined as the ratio between the reciprocal connection probability and the connection probability for a randomly connected network where the two unidirectional connections between any pair of neurons are independent. NRR = pr / (p0 * p1)</p> <p>Parameters:     NRR: a constant or distance dependent function for normalized reciprocal         rate. When being a function, it should be accept vectorized input. Returns:     A callable object that returns the probability value.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class NormalizedReciprocalRate(ProbabilityFunction):\n    \"\"\"Reciprocal connection probability given normalized reciprocal rate.\n    Normalized reciprocal rate is defined as the ratio between the reciprocal\n    connection probability and the connection probability for a randomly\n    connected network where the two unidirectional connections between any pair\n    of neurons are independent. NRR = pr / (p0 * p1)\n\n    Parameters:\n        NRR: a constant or distance dependent function for normalized reciprocal\n            rate. When being a function, it should be accept vectorized input.\n    Returns:\n        A callable object that returns the probability value.\n    \"\"\"\n\n    def __init__(self, NRR=1.0):\n        self.NRR = NRR if callable(NRR) else lambda *x: NRR\n\n    def probability(self, dist, p0, p1):\n        \"\"\"Allow numpy array input and return probability in numpy array\"\"\"\n        return p0 * p1 * self.NRR(dist)\n\n    def __call__(self, dist, p0, p1, *arg, **kwargs):\n        \"\"\"Return probability for single distance input\"\"\"\n        return self.probability(dist, p0, p1)\n\n    def decisions(self, dist, p0, p1, cond=None):\n        \"\"\"Return bool array of decisions\n        dist: distance (scalar or array). Will be ignored if NRR is constant.\n        p0, p1: forward and backward probability (scalar or array)\n        cond: A tuple (direction, array of outcomes) representing the condition.\n            Conditional probability will be returned if specified. The condition\n            event is determined by connection direction (0 for forward, or 1 for\n            backward) and outcomes (bool array of whether connection exists).\n        \"\"\"\n        dist, p0, p1 = map(np.asarray, (dist, p0, p1))\n        pr = np.empty(dist.shape)\n        pr[:] = self.probability(dist, p0, p1)\n        pr = np.clip(pr, a_min=np.fmax(p0 + p1 - 1.0, 0.0), a_max=np.fmin(p0, p1))\n        if cond is not None:\n            mask = np.asarray(cond[1])\n            pr[mask] /= p1 if cond[0] else p0\n            pr[~mask] = 0.0\n        return decisions(pr)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.NormalizedReciprocalRate.__call__","title":"<code>__call__(dist, p0, p1, *arg, **kwargs)</code>","text":"<p>Return probability for single distance input</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def __call__(self, dist, p0, p1, *arg, **kwargs):\n    \"\"\"Return probability for single distance input\"\"\"\n    return self.probability(dist, p0, p1)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.NormalizedReciprocalRate.decisions","title":"<code>decisions(dist, p0, p1, cond=None)</code>","text":"<p>Return bool array of decisions dist: distance (scalar or array). Will be ignored if NRR is constant. p0, p1: forward and backward probability (scalar or array) cond: A tuple (direction, array of outcomes) representing the condition.     Conditional probability will be returned if specified. The condition     event is determined by connection direction (0 for forward, or 1 for     backward) and outcomes (bool array of whether connection exists).</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def decisions(self, dist, p0, p1, cond=None):\n    \"\"\"Return bool array of decisions\n    dist: distance (scalar or array). Will be ignored if NRR is constant.\n    p0, p1: forward and backward probability (scalar or array)\n    cond: A tuple (direction, array of outcomes) representing the condition.\n        Conditional probability will be returned if specified. The condition\n        event is determined by connection direction (0 for forward, or 1 for\n        backward) and outcomes (bool array of whether connection exists).\n    \"\"\"\n    dist, p0, p1 = map(np.asarray, (dist, p0, p1))\n    pr = np.empty(dist.shape)\n    pr[:] = self.probability(dist, p0, p1)\n    pr = np.clip(pr, a_min=np.fmax(p0 + p1 - 1.0, 0.0), a_max=np.fmin(p0, p1))\n    if cond is not None:\n        mask = np.asarray(cond[1])\n        pr[mask] /= p1 if cond[0] else p0\n        pr[~mask] = 0.0\n    return decisions(pr)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.NormalizedReciprocalRate.probability","title":"<code>probability(dist, p0, p1)</code>","text":"<p>Allow numpy array input and return probability in numpy array</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def probability(self, dist, p0, p1):\n    \"\"\"Allow numpy array input and return probability in numpy array\"\"\"\n    return p0 * p1 * self.NRR(dist)\n</code></pre>"},{"location":"api/connectors/#connector-base-classes","title":"Connector Base Classes","text":""},{"location":"api/connectors/#bmtool.connectors.AbstractConnector","title":"<code>bmtool.connectors.AbstractConnector</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for connectors</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class AbstractConnector(ABC):\n    \"\"\"Abstract base class for connectors\"\"\"\n\n    @abstractmethod\n    def setup_nodes(self, source=None, target=None):\n        \"\"\"After network nodes are added to the BMTK network. Pass in the\n        Nodepool objects of source and target nodes using this method.\n        Must run this before building connections.\"\"\"\n        return NotImplemented\n\n    @abstractmethod\n    def edge_params(self, **kwargs):\n        \"\"\"Create the arguments for BMTK add_edges() method including the\n        `connection_rule` method.\"\"\"\n        return NotImplemented\n\n    @staticmethod\n    def constant_function(val):\n        \"\"\"Convert a constant to a constant function\"\"\"\n\n        def constant(*arg):\n            return val\n\n        return constant\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.AbstractConnector.constant_function","title":"<code>constant_function(val)</code>  <code>staticmethod</code>","text":"<p>Convert a constant to a constant function</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@staticmethod\ndef constant_function(val):\n    \"\"\"Convert a constant to a constant function\"\"\"\n\n    def constant(*arg):\n        return val\n\n    return constant\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.AbstractConnector.edge_params","title":"<code>edge_params(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create the arguments for BMTK add_edges() method including the <code>connection_rule</code> method.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@abstractmethod\ndef edge_params(self, **kwargs):\n    \"\"\"Create the arguments for BMTK add_edges() method including the\n    `connection_rule` method.\"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.AbstractConnector.setup_nodes","title":"<code>setup_nodes(source=None, target=None)</code>  <code>abstractmethod</code>","text":"<p>After network nodes are added to the BMTK network. Pass in the Nodepool objects of source and target nodes using this method. Must run this before building connections.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@abstractmethod\ndef setup_nodes(self, source=None, target=None):\n    \"\"\"After network nodes are added to the BMTK network. Pass in the\n    Nodepool objects of source and target nodes using this method.\n    Must run this before building connections.\"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.is_same_pop","title":"<code>bmtool.connectors.is_same_pop(source, target, quick=False)</code>","text":"<p>Check whether two NodePool objects direct to the same population</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def is_same_pop(source, target, quick=False):\n    \"\"\"Check whether two NodePool objects direct to the same population\"\"\"\n    if quick:\n        # Quick check (compare filter conditions)\n        same = (\n            source.network_name == target.network_name\n            and source._NodePool__properties == target._NodePool__properties\n        )\n    else:\n        # Strict check (compare all nodes)\n        same = (\n            source.network_name == target.network_name\n            and len(source) == len(target)\n            and all([s.node_id == t.node_id for s, t in zip(source, target)])\n        )\n    return same\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.Timer","title":"<code>bmtool.connectors.Timer</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>bmtool/connectors.py</code> <pre><code>class Timer(object):\n    def __init__(self, unit=\"sec\"):\n        if unit == \"ms\":\n            self.scale = 1e3\n        elif unit == \"us\":\n            self.scale = 1e6\n        elif unit == \"min\":\n            self.scale = 1 / 60\n        else:\n            self.scale = 1\n            unit = \"sec\"\n        self.unit = unit\n        self.start()\n\n    def start(self):\n        self._start = time.perf_counter()\n\n    def end(self):\n        return (time.perf_counter() - self._start) * self.scale\n\n    def report(self, msg=\"Run time\"):\n        print((msg + \": %.3f \" + self.unit) % self.end(), flush=True)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.pr_2_rho","title":"<code>bmtool.connectors.pr_2_rho(p0, p1, pr)</code>","text":"<p>Calculate correlation coefficient rho given reciprocal probability pr</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def pr_2_rho(p0, p1, pr):\n    \"\"\"Calculate correlation coefficient rho given reciprocal probability pr\"\"\"\n    for p in (p0, p1):\n        assert p &gt; 0 and p &lt; 1\n    assert pr &gt;= 0 and pr &lt;= p0 and pr &lt;= p1 and pr &gt;= p0 + p1 - 1\n    return (pr - p0 * p1) / (p0 * (1 - p0) * p1 * (1 - p1)) ** 0.5\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.rho_2_pr","title":"<code>bmtool.connectors.rho_2_pr(p0, p1, rho)</code>","text":"<p>Calculate reciprocal probability pr given correlation coefficient rho</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def rho_2_pr(p0, p1, rho):\n    \"\"\"Calculate reciprocal probability pr given correlation coefficient rho\"\"\"\n    for p in (p0, p1):\n        assert p &gt; 0 and p &lt; 1\n    pr = p0 * p1 + rho * (p0 * (1 - p0) * p1 * (1 - p1)) ** 0.5\n    if not (pr &gt;= 0 and pr &lt;= p0 and pr &lt;= p1 and pr &gt;= p0 + p1 - 1):\n        pr0, pr = pr, np.max((0.0, p0 + p1 - 1, np.min((p0, p1, pr))))\n        rho0, rho = rho, (pr - p0 * p1) / (p0 * (1 - p0) * p1 * (1 - p1)) ** 0.5\n        print(\n            \"rho changed from %.3f to %.3f; pr changed from %.3f to %.3f\" % (rho0, rho, pr0, pr),\n            flush=True,\n        )\n    return pr\n</code></pre>"},{"location":"api/connectors/#connector-implementations","title":"Connector Implementations","text":""},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector","title":"<code>bmtool.connectors.ReciprocalConnector</code>","text":"<p>               Bases: <code>AbstractConnector</code></p> <p>Object for buiilding connections in bmtk network model with reciprocal probability within a single population (or between two populations).</p> <p>Algorithm:     Create random connection for every pair of cells independently,     following a bivariate Bernoulli distribution. Each variable is 0 or 1,     whether a connection exists in a forward or backward direction. There     are four possible outcomes for each pair, no connection, unidirectional     connection in two ways, and reciprocal connection. The probability of     each outcome forms a contingency table.         b a c k w a r d     f   ---------------     o  |   |  0  |  1  |  The total forward connection probability is     r  |---|-----|-----|  p0 = p10 + p11     w  | 0 | p00 | p01 |  The total backward connection probability is     a  |---|-----|-----|  p1 = p01 + p11     r  | 1 | p10 | p11 |  The reciprocal connection probability is     d   ---------------   pr = p11     The distribution can be characterized by three parameters, p0, p1, pr.     pr = p0 * p1 when two directions are independent. The correlation     coefficient rho between the two has a relation with pr as follow.     rho = (pr-p0p1) / (p0(1-p0)p1(1-p1))^(1/2)     Generating random outcome consists of two steps. First draw random     outcome for forward connection with probability p0. Then draw backward     outcome following a conditional probability given the forward outcome,     represented by p0, p1, and either pr or rho.</p> <p>Use with BMTK:     1. Create this object with parameters.</p> <pre><code>    connector = ReciprocalConnector(**parameters)\n\n2. After network nodes are added to the BMTK network. Pass in the\nNodepool objects of source and target nodes using setup_nodes() method.\n\n    source = net.nodes(**source_filter)\n    target = net.nodes(**target_filter)\n    connector.setup_nodes(source, target)\n\n3. Use edge_params() method to get the arguments for BMTK add_edges()\nmethod including the `connection_rule` method.\n\n    net.add_edges(**connector.edge_params(),\n                  **other_source_to_target_edge_properties)\n\nIf the source and target are two different populations, do this again\nfor the backward connections (from target to source population).\n\n    net.add_edges(**connector.edge_params(),\n                  **other_target_to_source_edge_properties)\n\n4. When executing net.build(), BMTK uses built-in `one_to_all` iterator\nthat calls the make_forward_connection() method to build connections\nfrom source to target. If the two are different populations,\n`all_to_one` iterator that calls the make_backward_connection() method\nis then used to build connections from target to source.\nDuring the initial iteration when make_forward_connection() is called,\nthe algorithm is run to generate a connection matrix for both forward\nand backward connections. In the iterations afterward, it's only\nassigning the generated connections in BMTK.\n</code></pre> <p>Parameters:     p0, p1: Probability of forward and backward connection. It can be a         constant or a deterministic function whose value must be within         range [0, 1], otherwise incorrect value may occur in the algorithm.         When p0, p1 are constant, the connection is homogenous.     symmetric_p1: Whether p0 and p1 are identical. When the probabilities         are equal for forward and backward connections, set this to True,         Argument p1 will be ignored. This is forced to be True when the         population is recurrent, i.e., the source and target are the same.         This is forced to be False if symmetric_p1_arg is False.     p0_arg, p1_arg: Input argument(s) for p0 and p1 function, e.g.,         p0(p0_arg). It can be a constant or a deterministic function whose         input arguments are two node objects in BMTK, e.g.,         p0_arg(src_node,trg_node), p1_arg(trg_node,src_node). The latter         has reversed order since it's for backward connection. They are         usually distance between two nodes which is used for distance         dependent connection probability, where the order does not matter.         When p0 and p1 does not need inputs arguments, set p0_arg and         p1_arg to None as so by default. Functions p0 and p1 need to accept         one unused positional argument as placeholder, e.g., p0(args), so         it does not raise an error when p0(None) is called.     symmetric_p1_arg: Whether p0_arg and p1_arg are identical. If this is         set to True, argument p1_arg will be ignored. This is forced to be         True when the population is recurrent.     pr, pr_arg: Probability of reciprocal connection and its first input         argument when it is a function, similar to p0, p0_arg, p1, p1_arg.         It can be a function when it has an explicit relation with some node         properties such as distance. A function pr requires two additional         positional arguments p0 and p1 even if they are not used, i.e.,         pr(pr_arg, p0, p1), just in case pr is dependent on p0 and p1, e.g.,         when normalized reciprocal rate NRR = pr/(p0p1) is given.         When pr_arg is a string, the same value as p1_arg will be used for         pr_arg if the string contains '1', e.g., '1', 'p1'. Otherwise, e.g.,         '', '0', 'p0', p0_arg will be used for pr_arg. Specifying this can         avoid recomputing pr_arg when it's given by p0_arg or p1_arg.     estimate_rho: Whether estimate rho that result in an overall pr. This         is forced to be False if pr is a function or if rho is specified.         To estimate rho, all the pairs with possible connections, meaning         p0 and p1 are both non-zero for these pairs, are used to estimate         a value of rho that will result in an expected number of reciprocal         connections with the given pr. Note that pr is not over all pairs         of source and target cells but only those has a chance to connect,         e.g., for only pair of cells within some distance range. The         estimation is done before generating random connections. The values         of p0, p0_arg, p1, p1_arg can be cached during estimation of rho         and retrieved when generating random connections for performance.     dist_range_forward: If specified, when estimating rho, consider only         cell pairs whose distance (p0_arg) is within the specified range.     dist_range_backward: Similar to dist_range_forward but consider         backward distance range (p1_arg) instead. If both are specified,         consider only cell pairs whose both distances are within range. If         neither is specified, infer valid pairs by non-zero connection         probability.     rho: The correlation coefficient rho. When specified, do not estimate         it but instead use the given value throughout, pr will not be used.         In cases where both p0 and p1 are simple functions, i.e., are         constant on their support, e.g., function UniformInRange(), the         estimation of rho will be equal to pr_2_rho(p0, p1, pr) where p0,         p1 are non-zero. Estimation is not necessary. Directly set rho.     n_syn0, n_syn1: Number of synapses in the forward and backward         connection if connected. It can be a constant or a (deterministic         or random) function whose input arguments are two node objects in         BMTK like p0_arg, p1_arg. n_syn1 is force to be the same as n_syn0         when the population is recurrent. Warning: The number must not be         greater than 255 since it will be converted to uint8 when written         into the connection matrix to reduce memory consumption.     autapses: Whether to allow connecting a cell to itself. Default: False.         This is ignored when the population is not recurrent.     quick_pop_check: Whether to use quick method to check if source and         target populations are the same. Default: False.         Quick method checks only whether filter conditions match.         Strict method checks whether all node id's match considering order.     cache_data: Whether to cache the values of p0, p0_arg, p1, p1_arg         during estimation of rho. This improves performance when         estimate_rho is True while not creating a significant overhead in         the opposite case. However, it requires large memory allocation         as the population size grows. Set it to False if there is a memory         issue.     verbose: Whether show verbose information in console.</p> <p>Returns:     An object that works with BMTK to build edges in a network.</p> <p>Important attributes:     vars: Dictionary that stores part of the original input parameters.     source, target: NodePool objects for the source and target populations.     recurrent: Whether the source and target populations are the same.     callable_set: Set of arguments that are functions but not constants.     cache: ConnectorCache object for caching data.     conn_mat: Connection matrix     stage: Indicator of stage. 0 for forward and 1 for backward connection.     conn_prop: List of two dictionaries that stores properties of connected         pairs, for forward and backward connections respectively. In each         dictionary, each key is the source node id and the value is a         dictionary, where each key is the target node id that the source         node connects to, and the value is the value of p0_arg or p1_arg.         Example: [{sid0: {tid0: p0_arg0, tid1: p0_arg1, ...},                    sid1: {...}, sid2: {...}, ... },                   {tid2: {sid3: p1_arg0, sid4: p1_arg1, ...},                    tid3: {...}, tid4: {...}, ... }]         This is useful when properties of edges such as distance is used to         determine other edge properties such as delay. So the distance does         not need to be calculated repeatedly. The connector can be passed         as an argument for the functions that generates additional edge         properties, so that they can access the information here.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class ReciprocalConnector(AbstractConnector):\n    \"\"\"\n    Object for buiilding connections in bmtk network model with reciprocal\n    probability within a single population (or between two populations).\n\n    Algorithm:\n        Create random connection for every pair of cells independently,\n        following a bivariate Bernoulli distribution. Each variable is 0 or 1,\n        whether a connection exists in a forward or backward direction. There\n        are four possible outcomes for each pair, no connection, unidirectional\n        connection in two ways, and reciprocal connection. The probability of\n        each outcome forms a contingency table.\n            b a c k w a r d\n        f   ---------------\n        o  |   |  0  |  1  |  The total forward connection probability is\n        r  |---|-----|-----|  p0 = p10 + p11\n        w  | 0 | p00 | p01 |  The total backward connection probability is\n        a  |---|-----|-----|  p1 = p01 + p11\n        r  | 1 | p10 | p11 |  The reciprocal connection probability is\n        d   ---------------   pr = p11\n        The distribution can be characterized by three parameters, p0, p1, pr.\n        pr = p0 * p1 when two directions are independent. The correlation\n        coefficient rho between the two has a relation with pr as follow.\n        rho = (pr-p0*p1) / (p0*(1-p0)*p1*(1-p1))^(1/2)\n        Generating random outcome consists of two steps. First draw random\n        outcome for forward connection with probability p0. Then draw backward\n        outcome following a conditional probability given the forward outcome,\n        represented by p0, p1, and either pr or rho.\n\n    Use with BMTK:\n        1. Create this object with parameters.\n\n            connector = ReciprocalConnector(**parameters)\n\n        2. After network nodes are added to the BMTK network. Pass in the\n        Nodepool objects of source and target nodes using setup_nodes() method.\n\n            source = net.nodes(**source_filter)\n            target = net.nodes(**target_filter)\n            connector.setup_nodes(source, target)\n\n        3. Use edge_params() method to get the arguments for BMTK add_edges()\n        method including the `connection_rule` method.\n\n            net.add_edges(**connector.edge_params(),\n                          **other_source_to_target_edge_properties)\n\n        If the source and target are two different populations, do this again\n        for the backward connections (from target to source population).\n\n            net.add_edges(**connector.edge_params(),\n                          **other_target_to_source_edge_properties)\n\n        4. When executing net.build(), BMTK uses built-in `one_to_all` iterator\n        that calls the make_forward_connection() method to build connections\n        from source to target. If the two are different populations,\n        `all_to_one` iterator that calls the make_backward_connection() method\n        is then used to build connections from target to source.\n        During the initial iteration when make_forward_connection() is called,\n        the algorithm is run to generate a connection matrix for both forward\n        and backward connections. In the iterations afterward, it's only\n        assigning the generated connections in BMTK.\n\n    Parameters:\n        p0, p1: Probability of forward and backward connection. It can be a\n            constant or a deterministic function whose value must be within\n            range [0, 1], otherwise incorrect value may occur in the algorithm.\n            When p0, p1 are constant, the connection is homogenous.\n        symmetric_p1: Whether p0 and p1 are identical. When the probabilities\n            are equal for forward and backward connections, set this to True,\n            Argument p1 will be ignored. This is forced to be True when the\n            population is recurrent, i.e., the source and target are the same.\n            This is forced to be False if symmetric_p1_arg is False.\n        p0_arg, p1_arg: Input argument(s) for p0 and p1 function, e.g.,\n            p0(p0_arg). It can be a constant or a deterministic function whose\n            input arguments are two node objects in BMTK, e.g.,\n            p0_arg(src_node,trg_node), p1_arg(trg_node,src_node). The latter\n            has reversed order since it's for backward connection. They are\n            usually distance between two nodes which is used for distance\n            dependent connection probability, where the order does not matter.\n            When p0 and p1 does not need inputs arguments, set p0_arg and\n            p1_arg to None as so by default. Functions p0 and p1 need to accept\n            one unused positional argument as placeholder, e.g., p0(*args), so\n            it does not raise an error when p0(None) is called.\n        symmetric_p1_arg: Whether p0_arg and p1_arg are identical. If this is\n            set to True, argument p1_arg will be ignored. This is forced to be\n            True when the population is recurrent.\n        pr, pr_arg: Probability of reciprocal connection and its first input\n            argument when it is a function, similar to p0, p0_arg, p1, p1_arg.\n            It can be a function when it has an explicit relation with some node\n            properties such as distance. A function pr requires two additional\n            positional arguments p0 and p1 even if they are not used, i.e.,\n            pr(pr_arg, p0, p1), just in case pr is dependent on p0 and p1, e.g.,\n            when normalized reciprocal rate NRR = pr/(p0*p1) is given.\n            When pr_arg is a string, the same value as p1_arg will be used for\n            pr_arg if the string contains '1', e.g., '1', 'p1'. Otherwise, e.g.,\n            '', '0', 'p0', p0_arg will be used for pr_arg. Specifying this can\n            avoid recomputing pr_arg when it's given by p0_arg or p1_arg.\n        estimate_rho: Whether estimate rho that result in an overall pr. This\n            is forced to be False if pr is a function or if rho is specified.\n            To estimate rho, all the pairs with possible connections, meaning\n            p0 and p1 are both non-zero for these pairs, are used to estimate\n            a value of rho that will result in an expected number of reciprocal\n            connections with the given pr. Note that pr is not over all pairs\n            of source and target cells but only those has a chance to connect,\n            e.g., for only pair of cells within some distance range. The\n            estimation is done before generating random connections. The values\n            of p0, p0_arg, p1, p1_arg can be cached during estimation of rho\n            and retrieved when generating random connections for performance.\n        dist_range_forward: If specified, when estimating rho, consider only\n            cell pairs whose distance (p0_arg) is within the specified range.\n        dist_range_backward: Similar to dist_range_forward but consider\n            backward distance range (p1_arg) instead. If both are specified,\n            consider only cell pairs whose both distances are within range. If\n            neither is specified, infer valid pairs by non-zero connection\n            probability.\n        rho: The correlation coefficient rho. When specified, do not estimate\n            it but instead use the given value throughout, pr will not be used.\n            In cases where both p0 and p1 are simple functions, i.e., are\n            constant on their support, e.g., function UniformInRange(), the\n            estimation of rho will be equal to pr_2_rho(p0, p1, pr) where p0,\n            p1 are non-zero. Estimation is not necessary. Directly set rho.\n        n_syn0, n_syn1: Number of synapses in the forward and backward\n            connection if connected. It can be a constant or a (deterministic\n            or random) function whose input arguments are two node objects in\n            BMTK like p0_arg, p1_arg. n_syn1 is force to be the same as n_syn0\n            when the population is recurrent. Warning: The number must not be\n            greater than 255 since it will be converted to uint8 when written\n            into the connection matrix to reduce memory consumption.\n        autapses: Whether to allow connecting a cell to itself. Default: False.\n            This is ignored when the population is not recurrent.\n        quick_pop_check: Whether to use quick method to check if source and\n            target populations are the same. Default: False.\n            Quick method checks only whether filter conditions match.\n            Strict method checks whether all node id's match considering order.\n        cache_data: Whether to cache the values of p0, p0_arg, p1, p1_arg\n            during estimation of rho. This improves performance when\n            estimate_rho is True while not creating a significant overhead in\n            the opposite case. However, it requires large memory allocation\n            as the population size grows. Set it to False if there is a memory\n            issue.\n        verbose: Whether show verbose information in console.\n\n    Returns:\n        An object that works with BMTK to build edges in a network.\n\n    Important attributes:\n        vars: Dictionary that stores part of the original input parameters.\n        source, target: NodePool objects for the source and target populations.\n        recurrent: Whether the source and target populations are the same.\n        callable_set: Set of arguments that are functions but not constants.\n        cache: ConnectorCache object for caching data.\n        conn_mat: Connection matrix\n        stage: Indicator of stage. 0 for forward and 1 for backward connection.\n        conn_prop: List of two dictionaries that stores properties of connected\n            pairs, for forward and backward connections respectively. In each\n            dictionary, each key is the source node id and the value is a\n            dictionary, where each key is the target node id that the source\n            node connects to, and the value is the value of p0_arg or p1_arg.\n            Example: [{sid0: {tid0: p0_arg0, tid1: p0_arg1, ...},\n                       sid1: {...}, sid2: {...}, ... },\n                      {tid2: {sid3: p1_arg0, sid4: p1_arg1, ...},\n                       tid3: {...}, tid4: {...}, ... }]\n            This is useful when properties of edges such as distance is used to\n            determine other edge properties such as delay. So the distance does\n            not need to be calculated repeatedly. The connector can be passed\n            as an argument for the functions that generates additional edge\n            properties, so that they can access the information here.\n    \"\"\"\n\n    def __init__(\n        self,\n        p0=1.0,\n        p1=1.0,\n        symmetric_p1=False,\n        p0_arg=None,\n        p1_arg=None,\n        symmetric_p1_arg=False,\n        pr=0.0,\n        pr_arg=None,\n        estimate_rho=True,\n        rho=None,\n        dist_range_forward=None,\n        dist_range_backward=None,\n        n_syn0=1,\n        n_syn1=1,\n        autapses=False,\n        quick_pop_check=False,\n        cache_data=True,\n        verbose=True,\n        save_report=True,\n        report_name=None,\n    ):\n        args = locals()\n        var_set = (\"p0\", \"p0_arg\", \"p1\", \"p1_arg\", \"pr\", \"pr_arg\", \"n_syn0\", \"n_syn1\")\n        self.vars = {key: args[key] for key in var_set}\n\n        self.symmetric_p1 = symmetric_p1 and symmetric_p1_arg\n        self.symmetric_p1_arg = symmetric_p1_arg\n\n        self.estimate_rho = estimate_rho and not callable(pr) and rho is None\n        self.dist_range_forward = dist_range_forward\n        self.dist_range_backward = dist_range_backward\n        self.rho = rho\n\n        self.autapses = autapses\n        self.quick = quick_pop_check\n        self.cache = self.ConnectorCache(cache_data and self.estimate_rho)\n        self.verbose = verbose\n        self.save_report = save_report\n\n        if report_name is None:\n            report_name = globals().get(\"report_name\", \"default_report.csv\")\n        self.report_name = report_name\n\n        self.conn_prop = [{}, {}]\n        self.stage = 0\n        self.iter_count = 0\n\n    # *** Two methods executed during bmtk edge creation net.add_edges() ***\n    def setup_nodes(self, source=None, target=None):\n        \"\"\"Must run this before building connections\"\"\"\n        if self.stage:\n            # check whether the correct populations\n            if (\n                source is None\n                or target is None\n                or not is_same_pop(source, self.target, quick=self.quick)\n                or not is_same_pop(target, self.source, quick=self.quick)\n            ):\n                raise ValueError(\"Source or target population not consistent.\")\n            # Skip adding nodes for the backward stage.\n            return\n\n        # Update node pools\n        self.source = source\n        self.target = target\n        if self.source is None or len(self.source) == 0:\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(f\"{src_str} nodes do not exists\")\n        if self.target is None or len(self.target) == 0:\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(f\"{trg_str} nodes do not exists\")\n\n        # Setup nodes\n        self.recurrent = is_same_pop(self.source, self.target, quick=self.quick)\n        self.source_ids = [s.node_id for s in self.source]\n        self.n_source = len(self.source_ids)\n        self.source_list = list(self.source)\n        if self.recurrent:\n            self.target_ids = self.source_ids\n            self.n_target = self.n_source\n            self.target_list = self.source_list\n        else:\n            self.target_ids = [t.node_id for t in self.target]\n            self.n_target = len(self.target_ids)\n            self.target_list = list(self.target)\n\n        # Setup for recurrent connection\n        if self.recurrent:\n            self.symmetric_p1_arg = True\n            self.symmetric_p1 = True\n            self.vars[\"n_syn1\"] = self.vars[\"n_syn0\"]\n        if self.symmetric_p1_arg:\n            self.vars[\"p1_arg\"] = self.vars[\"p0_arg\"]\n        if self.symmetric_p1:\n            self.vars[\"p1\"] = self.vars[\"p0\"]\n\n    def edge_params(self):\n        \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n        if self.stage == 0:\n            params = {\n                \"source\": self.source,\n                \"target\": self.target,\n                \"iterator\": \"one_to_all\",\n                \"connection_rule\": self.make_forward_connection,\n            }\n        else:\n            params = {\n                \"source\": self.target,\n                \"target\": self.source,\n                \"iterator\": \"all_to_one\",\n                \"connection_rule\": self.make_backward_connection,\n            }\n        self.stage += 1\n        return params\n\n    # *** Methods executed during bmtk network.build() ***\n    # *** Helper functions ***\n    class ConnectorCache(object):\n        def __init__(self, enable=True):\n            self.enable = enable\n            self._output = {}\n            self.cache_dict = {}\n            self.set_next_it()\n            self.write_mode()\n\n        def cache_output(self, func, func_name, cache=True):\n            if self.enable and cache:\n                self.cache_dict[func_name] = func\n                self._output[func_name] = []\n                output = self._output[func_name]\n\n                def writer(*args):\n                    val = func(*args)\n                    output.append(val)\n                    return val\n\n                setattr(self, func_name, writer)\n            else:\n                setattr(self, func_name, func)\n\n        def write_mode(self):\n            for val in self._output.values():\n                val.clear()\n            self.mode = \"write\"\n            self.iter_count = 0\n\n        def fetch_output(self, func_name, fetch=True):\n            output = self._output[func_name]\n\n            if fetch:\n\n                def reader(*args):\n                    return output[self.iter_count]\n\n                setattr(self, func_name, reader)\n            else:\n                setattr(self, func_name, self.cache_dict[func_name])\n\n        def read_mode(self):\n            if self.enable and len(self.cache_dict):\n                # check whether outputs were written correctly\n                output_len = [len(val) for val in self._output.values()]\n                # whether any stored and have the same length\n                valid = [n for n in output_len if n]\n                flag = len(valid) &gt; 0 and all(n == valid[0] for n in valid[1:])\n                if flag:\n                    for func_name, out_len in zip(self._output, output_len):\n                        fetch = out_len &gt; 0\n                        if not fetch:\n                            print(\n                                \"\\nWarning: Cache did not work properly for \" + func_name + \"\\n\",\n                                flush=True,\n                            )\n                        self.fetch_output(func_name, fetch)\n                    self.iter_count = 0\n                else:\n                    # if output not correct, disable and use original function\n                    print(\"\\nWarning: Cache did not work properly.\\n\", flush=True)\n                    for func_name in self.cache_dict:\n                        self.fetch_output(func_name, False)\n                    self.enable = False\n            self.mode = \"read\"\n\n        def set_next_it(self):\n            if self.enable:\n\n                def next_it():\n                    self.iter_count += 1\n            else:\n\n                def next_it():\n                    pass\n\n            self.next_it = next_it\n\n    def node_2_idx_input(self, var_func, reverse=False):\n        \"\"\"Convert a function that accept nodes as input\n        to accept indices as input\"\"\"\n        if reverse:\n\n            def idx_2_var(j, i):\n                return var_func(self.target_list[j], self.source_list[i])\n        else:\n\n            def idx_2_var(i, j):\n                return var_func(self.source_list[i], self.target_list[j])\n\n        return idx_2_var\n\n    def iterate_pairs(self):\n        \"\"\"Generate indices of source and target for each case\"\"\"\n        if self.recurrent:\n            if self.autapses:\n                for i in range(self.n_source):\n                    for j in range(i, self.n_target):\n                        yield i, j\n            else:\n                for i in range(self.n_source - 1):\n                    for j in range(i + 1, self.n_target):\n                        yield i, j\n        else:\n            for i in range(self.n_source):\n                for j in range(self.n_target):\n                    yield i, j\n\n    def calc_pair(self, i, j):\n        \"\"\"Calculate intermediate data that can be cached\"\"\"\n        cache = self.cache\n        # cache = self  # test performance for not using cache\n        p0_arg = cache.p0_arg(i, j)\n        p1_arg = p0_arg if self.symmetric_p1_arg else cache.p1_arg(j, i)\n        p0 = cache.p0(p0_arg)\n        p1 = p0 if self.symmetric_p1 else cache.p1(p1_arg)\n        return p0_arg, p1_arg, p0, p1\n\n    def setup_conditional_backward_probability(self):\n        \"\"\"Create a function that calculates the conditional probability of\n        backward connection given the forward connection outcome 'cond'\"\"\"\n        # For all cases, assume p0, p1, pr are all within [0, 1] already.\n        self.wrong_pr = False\n        if self.rho is None:\n            # Determine by pr for each pair\n            if self.verbose:\n\n                def cond_backward(cond, p0, p1, pr):\n                    if p0 &gt; 0:\n                        pr_bound = (p0 + p1 - 1, min(p0, p1))\n                        # check whether pr within bounds\n                        if pr &lt; pr_bound[0] or pr &gt; pr_bound[1]:\n                            self.wrong_pr = True\n                            pr = min(max(pr, pr_bound[0]), pr_bound[1])\n                        return pr / p0 if cond else (p1 - pr) / (1 - p0)\n                    else:\n                        return p1\n            else:\n\n                def cond_backward(cond, p0, p1, pr):\n                    if p0 &gt; 0:\n                        pr_bound = (p0 + p1 - 1, min(p0, p1))\n                        pr = min(max(pr, pr_bound[0]), pr_bound[1])\n                        return pr / p0 if cond else (p1 - pr) / (1 - p0)\n                    else:\n                        return p1\n        elif self.rho == 0:\n            # Independent case\n            def cond_backward(cond, p0, p1, pr):\n                return p1\n        else:\n            # Dependent with fixed correlation coefficient rho\n            def cond_backward(cond, p0, p1, pr):\n                # Standard deviation of r.v. for p1\n                sd = ((1 - p1) * p1) ** 0.5\n                # Z-score of random variable for p0\n                zs = ((1 - p0) / p0) ** 0.5 if cond else -((p0 / (1 - p0)) ** 0.5)\n                return p1 + self.rho * sd * zs\n\n        self.cond_backward = cond_backward\n\n    def add_conn_prop(self, src, trg, prop, stage=0):\n        \"\"\"Store p0_arg and p1_arg for a connected pair\"\"\"\n        sid = self.source_ids[src]\n        tid = self.target_ids[trg]\n        conn_dict = self.conn_prop[stage]\n        if stage:\n            sid, tid = tid, sid  # during backward, from target to source\n        trg_dict = conn_dict.setdefault(sid, {})\n        trg_dict[tid] = prop\n\n    def get_conn_prop(self, sid, tid):\n        \"\"\"Get stored value given node ids in a connection\"\"\"\n        return self.conn_prop[self.stage][sid][tid]\n\n    # *** A sequence of major methods executed during build ***\n    def setup_variables(self):\n        # If pr_arg is string, use the same value as p0_arg or p1_arg\n        if isinstance(self.vars[\"pr_arg\"], str):\n            pr_arg_func = \"p1_arg\" if \"1\" in self.vars[\"pr_arg\"] else \"p0_arg\"\n            self.vars[\"pr_arg\"] = self.vars[pr_arg_func]\n        else:\n            pr_arg_func = None\n\n        callable_set = set()\n        # Make constant variables constant functions\n        for name, var in self.vars.items():\n            if callable(var):\n                callable_set.add(name)  # record callable variables\n                setattr(self, name, var)\n            else:\n                setattr(self, name, self.constant_function(var))\n        self.callable_set = callable_set\n\n        # Make callable variables except a few, accept index input instead\n        for name in callable_set - {\"p0\", \"p1\", \"pr\"}:\n            var = self.vars[name]\n            setattr(self, name, self.node_2_idx_input(var, \"1\" in name))\n\n        # Set up function for pr_arg if use value from p0_arg or p1_arg\n        if pr_arg_func is None:\n            self._pr_arg = self.pr_arg  # use specified pr_arg\n        else:\n            self._pr_arg_val = 0.0  # storing current value from p_arg\n            p_arg = getattr(self, pr_arg_func)\n\n            def p_arg_4_pr(*args, **kwargs):\n                val = p_arg(*args, **kwargs)\n                self._pr_arg_val = val\n                return val\n\n            setattr(self, pr_arg_func, p_arg_4_pr)\n\n            def pr_arg(self, *arg):\n                return self._pr_arg_val\n\n            self._pr_arg = types.MethodType(pr_arg, self)\n\n    def cache_variables(self):\n        # Select cacheable attrilbutes\n        cache_set = {\"p0\", \"p0_arg\", \"p1\", \"p1_arg\"}\n        if self.symmetric_p1:\n            cache_set.remove(\"p1\")\n        if self.symmetric_p1_arg:\n            cache_set.remove(\"p1_arg\")\n        # Output of callable variables will be cached\n        # Constant functions will be called from cache but output not cached\n        for name in cache_set:\n            var = getattr(self, name)\n            self.cache.cache_output(var, name, name in self.callable_set)\n        if self.verbose and len(self.cache.cache_dict):\n            print(\"Output of %s will be cached.\" % \", \".join(self.cache.cache_dict), flush=True)\n\n    def setup_dist_range_checker(self):\n        # Checker that determines whether to consider a pair for rho estimation\n        if self.dist_range_forward is None and self.dist_range_backward is None:\n\n            def checker(var):\n                p0, p1 = var[2:]\n                return p0 &gt; 0 and p1 &gt; 0\n        else:\n\n            def in_range(p_arg, dist_range):\n                return p_arg &gt;= dist_range[0] and p_arg &lt;= dist_range[1]\n\n            r0, r1 = self.dist_range_forward, self.dist_range_backward\n            if r1 is None:\n\n                def checker(var):\n                    return in_range(var[0], r0)\n            elif r0 is None:\n\n                def checker(var):\n                    return in_range(var[1], r1)\n            else:\n\n                def checker(var):\n                    return in_range(var[0], r0) and in_range(var[1], r1)\n\n        return checker\n\n    def initialize(self):\n        self.setup_variables()\n        self.cache_variables()\n        # Intialize connection matrix and get nubmer of pairs\n        self.end_stage = 0 if self.recurrent else 1\n        shape = (self.end_stage + 1, self.n_source, self.n_target)\n        self.conn_mat = np.zeros(shape, dtype=np.uint8)  # 1 byte per entry\n\n    def initial_all_to_all(self):\n        \"\"\"The major part of the algorithm run at beginning of BMTK iterator\"\"\"\n        if self.verbose:\n            src_str, trg_str = self.get_nodes_info()\n            print(\n                \"\\nStart building connection between: \\n  \" + src_str + \"\\n  \" + trg_str, flush=True\n            )\n        self.initialize()\n        cache = self.cache  # write mode\n\n        # Estimate pr\n        if self.verbose:\n            self.timer = Timer()\n        if self.estimate_rho:\n            dist_range_checker = self.setup_dist_range_checker()\n            p0p1_sum = 0.0\n            norm_fac_sum = 0.0\n            n = 0\n            # Make sure each cacheable function runs excatly once per iteration\n            for i, j in self.iterate_pairs():\n                var = self.calc_pair(i, j)\n                valid = dist_range_checker(var)\n                if valid:\n                    n += 1\n                    p0, p1 = var[2:]\n                    p0p1_sum += p0 * p1\n                    norm_fac_sum += (p0 * (1 - p0) * p1 * (1 - p1)) ** 0.5\n            if norm_fac_sum &gt; 0:\n                rho = (self.pr() * n - p0p1_sum) / norm_fac_sum\n                if abs(rho) &gt; 1:\n                    print(\n                        \"\\nWarning: Estimated value of rho=%.3f \"\n                        \"outside the range [-1, 1].\" % rho,\n                        flush=True,\n                    )\n                    rho = np.clip(rho, -1, 1).item()\n                    print(\"Force rho to be %.0f.\\n\" % rho, flush=True)\n                elif self.verbose:\n                    print(\"Estimated value of rho=%.3f\" % rho, flush=True)\n                self.rho = rho\n            else:\n                self.rho = 0\n\n            if self.verbose:\n                self.timer.report(\"Time for estimating rho\")\n\n        # Setup function for calculating conditional backward probability\n        self.setup_conditional_backward_probability()\n\n        # Make random connections\n        cache.read_mode()\n        possible_count = 0 if self.recurrent else np.zeros(3)\n        for i, j in self.iterate_pairs():\n            p0_arg, p1_arg, p0, p1 = self.calc_pair(i, j)\n            # Check whether at all possible and count\n            forward = p0 &gt; 0\n            backward = p1 &gt; 0\n            if self.recurrent:\n                possible_count += forward\n            else:\n                possible_count += [forward, backward, forward and backward]\n\n            # Make random decision\n            if forward:\n                forward = decision(p0)\n            if backward:\n                pr = self.pr(self._pr_arg(i, j), p0, p1)\n                backward = decision(self.cond_backward(forward, p0, p1, pr))\n\n            # Make connection\n            if forward:\n                n_forward = self.n_syn0(i, j)\n                self.add_conn_prop(i, j, p0_arg, 0)\n                self.conn_mat[0, i, j] = n_forward\n            if backward:\n                n_backward = self.n_syn1(j, i)\n                if self.recurrent:\n                    if i != j:\n                        self.conn_mat[0, j, i] = n_backward\n                        self.add_conn_prop(j, i, p1_arg, 0)\n                else:\n                    self.conn_mat[1, i, j] = n_backward\n                    self.add_conn_prop(i, j, p1_arg, 1)\n            self.cache.next_it()\n        self.cache.write_mode()  # clear memory\n        self.possible_count = possible_count\n\n        if self.verbose:\n            self.timer.report(\"Total time for creating connection matrix\")\n            if self.wrong_pr:\n                print(\"Warning: Value of 'pr' outside the bounds occurred.\\n\", flush=True)\n            self.connection_number_info()\n        if self.save_report:\n            self.save_connection_report()\n\n    def make_connection(self):\n        \"\"\"Assign number of synapses per iteration.\n        Use iterator one_to_all for forward and all_to_one for backward.\n        \"\"\"\n        nsyns = self.conn_mat[self.stage, self.iter_count, :]\n        self.iter_count += 1\n\n        # Detect end of iteration\n        if self.iter_count == self.n_source:\n            self.iter_count = 0\n            if self.stage == self.end_stage:\n                if self.verbose:\n                    self.timer.report(\"Done! \\nTime for building connections\")\n                self.free_memory()\n        return nsyns\n\n    def make_forward_connection(self, source, targets, *args, **kwargs):\n        \"\"\"Function to be called by BMTK iterator for forward connection\"\"\"\n        # Initialize in the first iteration\n        if self.iter_count == 0:\n            self.stage = 0\n            self.initial_all_to_all()\n            if self.verbose:\n                print(\"Assigning forward connections.\", flush=True)\n                self.timer.start()\n        return self.make_connection()\n\n    def make_backward_connection(self, targets, source, *args, **kwargs):\n        \"\"\"Function to be called by BMTK iterator for backward connection\"\"\"\n        if self.iter_count == 0:\n            self.stage = 1\n            if self.verbose:\n                print(\"Assigning backward connections.\", flush=True)\n        return self.make_connection()\n\n    def free_memory(self):\n        \"\"\"Free up memory after connections are built\"\"\"\n        # Do not clear self.conn_prop if it will be used by conn.add_properties\n        variables = (\"conn_mat\", \"source_list\", \"target_list\", \"source_ids\", \"target_ids\")\n        for var in variables:\n            setattr(self, var, None)\n\n    # *** Helper functions for verbose ***\n    def get_nodes_info(self):\n        \"\"\"Get strings with source and target population information\"\"\"\n        source_str = self.source.network_name + \": \" + self.source.filter_str\n        target_str = self.target.network_name + \": \" + self.target.filter_str\n        return source_str, target_str\n\n    def connection_number(self):\n        \"\"\"\n        Return the number of the following:\n        n_conn: connected pairs [forward, (backward,) reciprocal]\n        n_poss: possible connections (prob&gt;0) [forward, (backward, reciprocal)]\n        n_pair: pairs of cells\n        proportion: of connections in possible and total pairs\n        \"\"\"\n        conn_mat = self.conn_mat.astype(bool)\n        n_conn = np.count_nonzero(conn_mat, axis=(1, 2))\n        n_poss = np.array(self.possible_count)\n        n_pair = conn_mat.size / 2\n        if self.recurrent:\n            n_recp = np.count_nonzero(conn_mat[0] &amp; conn_mat[0].T)\n            if self.autapses:\n                n_recp -= np.count_nonzero(np.diag(conn_mat[0]))\n            n_recp //= 2\n            n_conn -= n_recp\n            n_poss = n_poss[None]\n            n_pair += (1 if self.autapses else -1) * self.n_source / 2\n        else:\n            n_recp = np.count_nonzero(conn_mat[0] &amp; conn_mat[1])\n        n_conn = np.append(n_conn, n_recp)\n        n_pair = int(n_pair)\n        fraction = np.array([n_conn / n_poss, n_conn / n_pair])\n        fraction[np.isnan(fraction)] = 0.0\n        return n_conn, n_poss, n_pair, fraction\n\n    def connection_number_info(self):\n        \"\"\"Print connection numbers after connections built\"\"\"\n\n        def arr2str(a, f):\n            return \", \".join([f] * a.size) % tuple(a.tolist())\n\n        n_conn, n_poss, n_pair, fraction = self.connection_number()\n        conn_type = \"(all, reciprocal)\" if self.recurrent else \"(forward, backward, reciprocal)\"\n        print(\"Numbers of \" + conn_type + \" connections:\", flush=True)\n        print(\"Number of connected pairs: (%s)\" % arr2str(n_conn, \"%d\"), flush=True)\n        print(\"Number of possible connections: (%s)\" % arr2str(n_poss, \"%d\"), flush=True)\n        print(\n            \"Fraction of connected pairs in possible ones: (%s)\"\n            % arr2str(100 * fraction[0], \"%.2f%%\"),\n            flush=True,\n        )\n        print(\"Number of total pairs: %d\" % n_pair, flush=True)\n        print(\n            \"Fraction of connected pairs in all pairs: (%s)\\n\"\n            % arr2str(100 * fraction[1], \"%.2f%%\"),\n            flush=True,\n        )\n\n    def save_connection_report(self):\n        \"\"\"Save connections into a CSV file to be read from later\"\"\"\n        src_str, trg_str = self.get_nodes_info()\n        n_conn, n_poss, n_pair, fraction = self.connection_number()\n\n        # Extract the population name from source_str and target_str\n        data = {\n            \"Source\": [src_str],\n            \"Target\": [trg_str],\n            \"Percent connectionivity within possible connections\": [fraction[0] * 100],\n            \"Percent connectionivity within all connections\": [fraction[1] * 100],\n        }\n        df = pd.DataFrame(data)\n\n        # Append the data to the CSV file\n        try:\n            # Check if the file exists by trying to read it\n            existing_df = pd.read_csv(self.report_name)\n            # If no exception is raised, append without header\n            df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n        except FileNotFoundError:\n            # If the file does not exist, write with header\n            df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.add_conn_prop","title":"<code>add_conn_prop(src, trg, prop, stage=0)</code>","text":"<p>Store p0_arg and p1_arg for a connected pair</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def add_conn_prop(self, src, trg, prop, stage=0):\n    \"\"\"Store p0_arg and p1_arg for a connected pair\"\"\"\n    sid = self.source_ids[src]\n    tid = self.target_ids[trg]\n    conn_dict = self.conn_prop[stage]\n    if stage:\n        sid, tid = tid, sid  # during backward, from target to source\n    trg_dict = conn_dict.setdefault(sid, {})\n    trg_dict[tid] = prop\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.calc_pair","title":"<code>calc_pair(i, j)</code>","text":"<p>Calculate intermediate data that can be cached</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def calc_pair(self, i, j):\n    \"\"\"Calculate intermediate data that can be cached\"\"\"\n    cache = self.cache\n    # cache = self  # test performance for not using cache\n    p0_arg = cache.p0_arg(i, j)\n    p1_arg = p0_arg if self.symmetric_p1_arg else cache.p1_arg(j, i)\n    p0 = cache.p0(p0_arg)\n    p1 = p0 if self.symmetric_p1 else cache.p1(p1_arg)\n    return p0_arg, p1_arg, p0, p1\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.connection_number","title":"<code>connection_number()</code>","text":"<p>Return the number of the following: n_conn: connected pairs [forward, (backward,) reciprocal] n_poss: possible connections (prob&gt;0) [forward, (backward, reciprocal)] n_pair: pairs of cells proportion: of connections in possible and total pairs</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def connection_number(self):\n    \"\"\"\n    Return the number of the following:\n    n_conn: connected pairs [forward, (backward,) reciprocal]\n    n_poss: possible connections (prob&gt;0) [forward, (backward, reciprocal)]\n    n_pair: pairs of cells\n    proportion: of connections in possible and total pairs\n    \"\"\"\n    conn_mat = self.conn_mat.astype(bool)\n    n_conn = np.count_nonzero(conn_mat, axis=(1, 2))\n    n_poss = np.array(self.possible_count)\n    n_pair = conn_mat.size / 2\n    if self.recurrent:\n        n_recp = np.count_nonzero(conn_mat[0] &amp; conn_mat[0].T)\n        if self.autapses:\n            n_recp -= np.count_nonzero(np.diag(conn_mat[0]))\n        n_recp //= 2\n        n_conn -= n_recp\n        n_poss = n_poss[None]\n        n_pair += (1 if self.autapses else -1) * self.n_source / 2\n    else:\n        n_recp = np.count_nonzero(conn_mat[0] &amp; conn_mat[1])\n    n_conn = np.append(n_conn, n_recp)\n    n_pair = int(n_pair)\n    fraction = np.array([n_conn / n_poss, n_conn / n_pair])\n    fraction[np.isnan(fraction)] = 0.0\n    return n_conn, n_poss, n_pair, fraction\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.connection_number_info","title":"<code>connection_number_info()</code>","text":"<p>Print connection numbers after connections built</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def connection_number_info(self):\n    \"\"\"Print connection numbers after connections built\"\"\"\n\n    def arr2str(a, f):\n        return \", \".join([f] * a.size) % tuple(a.tolist())\n\n    n_conn, n_poss, n_pair, fraction = self.connection_number()\n    conn_type = \"(all, reciprocal)\" if self.recurrent else \"(forward, backward, reciprocal)\"\n    print(\"Numbers of \" + conn_type + \" connections:\", flush=True)\n    print(\"Number of connected pairs: (%s)\" % arr2str(n_conn, \"%d\"), flush=True)\n    print(\"Number of possible connections: (%s)\" % arr2str(n_poss, \"%d\"), flush=True)\n    print(\n        \"Fraction of connected pairs in possible ones: (%s)\"\n        % arr2str(100 * fraction[0], \"%.2f%%\"),\n        flush=True,\n    )\n    print(\"Number of total pairs: %d\" % n_pair, flush=True)\n    print(\n        \"Fraction of connected pairs in all pairs: (%s)\\n\"\n        % arr2str(100 * fraction[1], \"%.2f%%\"),\n        flush=True,\n    )\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.edge_params","title":"<code>edge_params()</code>","text":"<p>Create the arguments for BMTK add_edges() method</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def edge_params(self):\n    \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n    if self.stage == 0:\n        params = {\n            \"source\": self.source,\n            \"target\": self.target,\n            \"iterator\": \"one_to_all\",\n            \"connection_rule\": self.make_forward_connection,\n        }\n    else:\n        params = {\n            \"source\": self.target,\n            \"target\": self.source,\n            \"iterator\": \"all_to_one\",\n            \"connection_rule\": self.make_backward_connection,\n        }\n    self.stage += 1\n    return params\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.free_memory","title":"<code>free_memory()</code>","text":"<p>Free up memory after connections are built</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def free_memory(self):\n    \"\"\"Free up memory after connections are built\"\"\"\n    # Do not clear self.conn_prop if it will be used by conn.add_properties\n    variables = (\"conn_mat\", \"source_list\", \"target_list\", \"source_ids\", \"target_ids\")\n    for var in variables:\n        setattr(self, var, None)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.get_conn_prop","title":"<code>get_conn_prop(sid, tid)</code>","text":"<p>Get stored value given node ids in a connection</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def get_conn_prop(self, sid, tid):\n    \"\"\"Get stored value given node ids in a connection\"\"\"\n    return self.conn_prop[self.stage][sid][tid]\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.get_nodes_info","title":"<code>get_nodes_info()</code>","text":"<p>Get strings with source and target population information</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def get_nodes_info(self):\n    \"\"\"Get strings with source and target population information\"\"\"\n    source_str = self.source.network_name + \": \" + self.source.filter_str\n    target_str = self.target.network_name + \": \" + self.target.filter_str\n    return source_str, target_str\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.initial_all_to_all","title":"<code>initial_all_to_all()</code>","text":"<p>The major part of the algorithm run at beginning of BMTK iterator</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def initial_all_to_all(self):\n    \"\"\"The major part of the algorithm run at beginning of BMTK iterator\"\"\"\n    if self.verbose:\n        src_str, trg_str = self.get_nodes_info()\n        print(\n            \"\\nStart building connection between: \\n  \" + src_str + \"\\n  \" + trg_str, flush=True\n        )\n    self.initialize()\n    cache = self.cache  # write mode\n\n    # Estimate pr\n    if self.verbose:\n        self.timer = Timer()\n    if self.estimate_rho:\n        dist_range_checker = self.setup_dist_range_checker()\n        p0p1_sum = 0.0\n        norm_fac_sum = 0.0\n        n = 0\n        # Make sure each cacheable function runs excatly once per iteration\n        for i, j in self.iterate_pairs():\n            var = self.calc_pair(i, j)\n            valid = dist_range_checker(var)\n            if valid:\n                n += 1\n                p0, p1 = var[2:]\n                p0p1_sum += p0 * p1\n                norm_fac_sum += (p0 * (1 - p0) * p1 * (1 - p1)) ** 0.5\n        if norm_fac_sum &gt; 0:\n            rho = (self.pr() * n - p0p1_sum) / norm_fac_sum\n            if abs(rho) &gt; 1:\n                print(\n                    \"\\nWarning: Estimated value of rho=%.3f \"\n                    \"outside the range [-1, 1].\" % rho,\n                    flush=True,\n                )\n                rho = np.clip(rho, -1, 1).item()\n                print(\"Force rho to be %.0f.\\n\" % rho, flush=True)\n            elif self.verbose:\n                print(\"Estimated value of rho=%.3f\" % rho, flush=True)\n            self.rho = rho\n        else:\n            self.rho = 0\n\n        if self.verbose:\n            self.timer.report(\"Time for estimating rho\")\n\n    # Setup function for calculating conditional backward probability\n    self.setup_conditional_backward_probability()\n\n    # Make random connections\n    cache.read_mode()\n    possible_count = 0 if self.recurrent else np.zeros(3)\n    for i, j in self.iterate_pairs():\n        p0_arg, p1_arg, p0, p1 = self.calc_pair(i, j)\n        # Check whether at all possible and count\n        forward = p0 &gt; 0\n        backward = p1 &gt; 0\n        if self.recurrent:\n            possible_count += forward\n        else:\n            possible_count += [forward, backward, forward and backward]\n\n        # Make random decision\n        if forward:\n            forward = decision(p0)\n        if backward:\n            pr = self.pr(self._pr_arg(i, j), p0, p1)\n            backward = decision(self.cond_backward(forward, p0, p1, pr))\n\n        # Make connection\n        if forward:\n            n_forward = self.n_syn0(i, j)\n            self.add_conn_prop(i, j, p0_arg, 0)\n            self.conn_mat[0, i, j] = n_forward\n        if backward:\n            n_backward = self.n_syn1(j, i)\n            if self.recurrent:\n                if i != j:\n                    self.conn_mat[0, j, i] = n_backward\n                    self.add_conn_prop(j, i, p1_arg, 0)\n            else:\n                self.conn_mat[1, i, j] = n_backward\n                self.add_conn_prop(i, j, p1_arg, 1)\n        self.cache.next_it()\n    self.cache.write_mode()  # clear memory\n    self.possible_count = possible_count\n\n    if self.verbose:\n        self.timer.report(\"Total time for creating connection matrix\")\n        if self.wrong_pr:\n            print(\"Warning: Value of 'pr' outside the bounds occurred.\\n\", flush=True)\n        self.connection_number_info()\n    if self.save_report:\n        self.save_connection_report()\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.iterate_pairs","title":"<code>iterate_pairs()</code>","text":"<p>Generate indices of source and target for each case</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def iterate_pairs(self):\n    \"\"\"Generate indices of source and target for each case\"\"\"\n    if self.recurrent:\n        if self.autapses:\n            for i in range(self.n_source):\n                for j in range(i, self.n_target):\n                    yield i, j\n        else:\n            for i in range(self.n_source - 1):\n                for j in range(i + 1, self.n_target):\n                    yield i, j\n    else:\n        for i in range(self.n_source):\n            for j in range(self.n_target):\n                yield i, j\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.make_backward_connection","title":"<code>make_backward_connection(targets, source, *args, **kwargs)</code>","text":"<p>Function to be called by BMTK iterator for backward connection</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_backward_connection(self, targets, source, *args, **kwargs):\n    \"\"\"Function to be called by BMTK iterator for backward connection\"\"\"\n    if self.iter_count == 0:\n        self.stage = 1\n        if self.verbose:\n            print(\"Assigning backward connections.\", flush=True)\n    return self.make_connection()\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.make_connection","title":"<code>make_connection()</code>","text":"<p>Assign number of synapses per iteration. Use iterator one_to_all for forward and all_to_one for backward.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_connection(self):\n    \"\"\"Assign number of synapses per iteration.\n    Use iterator one_to_all for forward and all_to_one for backward.\n    \"\"\"\n    nsyns = self.conn_mat[self.stage, self.iter_count, :]\n    self.iter_count += 1\n\n    # Detect end of iteration\n    if self.iter_count == self.n_source:\n        self.iter_count = 0\n        if self.stage == self.end_stage:\n            if self.verbose:\n                self.timer.report(\"Done! \\nTime for building connections\")\n            self.free_memory()\n    return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.make_forward_connection","title":"<code>make_forward_connection(source, targets, *args, **kwargs)</code>","text":"<p>Function to be called by BMTK iterator for forward connection</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_forward_connection(self, source, targets, *args, **kwargs):\n    \"\"\"Function to be called by BMTK iterator for forward connection\"\"\"\n    # Initialize in the first iteration\n    if self.iter_count == 0:\n        self.stage = 0\n        self.initial_all_to_all()\n        if self.verbose:\n            print(\"Assigning forward connections.\", flush=True)\n            self.timer.start()\n    return self.make_connection()\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.node_2_idx_input","title":"<code>node_2_idx_input(var_func, reverse=False)</code>","text":"<p>Convert a function that accept nodes as input to accept indices as input</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def node_2_idx_input(self, var_func, reverse=False):\n    \"\"\"Convert a function that accept nodes as input\n    to accept indices as input\"\"\"\n    if reverse:\n\n        def idx_2_var(j, i):\n            return var_func(self.target_list[j], self.source_list[i])\n    else:\n\n        def idx_2_var(i, j):\n            return var_func(self.source_list[i], self.target_list[j])\n\n    return idx_2_var\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.save_connection_report","title":"<code>save_connection_report()</code>","text":"<p>Save connections into a CSV file to be read from later</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def save_connection_report(self):\n    \"\"\"Save connections into a CSV file to be read from later\"\"\"\n    src_str, trg_str = self.get_nodes_info()\n    n_conn, n_poss, n_pair, fraction = self.connection_number()\n\n    # Extract the population name from source_str and target_str\n    data = {\n        \"Source\": [src_str],\n        \"Target\": [trg_str],\n        \"Percent connectionivity within possible connections\": [fraction[0] * 100],\n        \"Percent connectionivity within all connections\": [fraction[1] * 100],\n    }\n    df = pd.DataFrame(data)\n\n    # Append the data to the CSV file\n    try:\n        # Check if the file exists by trying to read it\n        existing_df = pd.read_csv(self.report_name)\n        # If no exception is raised, append without header\n        df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n    except FileNotFoundError:\n        # If the file does not exist, write with header\n        df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.setup_conditional_backward_probability","title":"<code>setup_conditional_backward_probability()</code>","text":"<p>Create a function that calculates the conditional probability of backward connection given the forward connection outcome 'cond'</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def setup_conditional_backward_probability(self):\n    \"\"\"Create a function that calculates the conditional probability of\n    backward connection given the forward connection outcome 'cond'\"\"\"\n    # For all cases, assume p0, p1, pr are all within [0, 1] already.\n    self.wrong_pr = False\n    if self.rho is None:\n        # Determine by pr for each pair\n        if self.verbose:\n\n            def cond_backward(cond, p0, p1, pr):\n                if p0 &gt; 0:\n                    pr_bound = (p0 + p1 - 1, min(p0, p1))\n                    # check whether pr within bounds\n                    if pr &lt; pr_bound[0] or pr &gt; pr_bound[1]:\n                        self.wrong_pr = True\n                        pr = min(max(pr, pr_bound[0]), pr_bound[1])\n                    return pr / p0 if cond else (p1 - pr) / (1 - p0)\n                else:\n                    return p1\n        else:\n\n            def cond_backward(cond, p0, p1, pr):\n                if p0 &gt; 0:\n                    pr_bound = (p0 + p1 - 1, min(p0, p1))\n                    pr = min(max(pr, pr_bound[0]), pr_bound[1])\n                    return pr / p0 if cond else (p1 - pr) / (1 - p0)\n                else:\n                    return p1\n    elif self.rho == 0:\n        # Independent case\n        def cond_backward(cond, p0, p1, pr):\n            return p1\n    else:\n        # Dependent with fixed correlation coefficient rho\n        def cond_backward(cond, p0, p1, pr):\n            # Standard deviation of r.v. for p1\n            sd = ((1 - p1) * p1) ** 0.5\n            # Z-score of random variable for p0\n            zs = ((1 - p0) / p0) ** 0.5 if cond else -((p0 / (1 - p0)) ** 0.5)\n            return p1 + self.rho * sd * zs\n\n    self.cond_backward = cond_backward\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.setup_nodes","title":"<code>setup_nodes(source=None, target=None)</code>","text":"<p>Must run this before building connections</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def setup_nodes(self, source=None, target=None):\n    \"\"\"Must run this before building connections\"\"\"\n    if self.stage:\n        # check whether the correct populations\n        if (\n            source is None\n            or target is None\n            or not is_same_pop(source, self.target, quick=self.quick)\n            or not is_same_pop(target, self.source, quick=self.quick)\n        ):\n            raise ValueError(\"Source or target population not consistent.\")\n        # Skip adding nodes for the backward stage.\n        return\n\n    # Update node pools\n    self.source = source\n    self.target = target\n    if self.source is None or len(self.source) == 0:\n        src_str, trg_str = self.get_nodes_info()\n        raise ValueError(f\"{src_str} nodes do not exists\")\n    if self.target is None or len(self.target) == 0:\n        src_str, trg_str = self.get_nodes_info()\n        raise ValueError(f\"{trg_str} nodes do not exists\")\n\n    # Setup nodes\n    self.recurrent = is_same_pop(self.source, self.target, quick=self.quick)\n    self.source_ids = [s.node_id for s in self.source]\n    self.n_source = len(self.source_ids)\n    self.source_list = list(self.source)\n    if self.recurrent:\n        self.target_ids = self.source_ids\n        self.n_target = self.n_source\n        self.target_list = self.source_list\n    else:\n        self.target_ids = [t.node_id for t in self.target]\n        self.n_target = len(self.target_ids)\n        self.target_list = list(self.target)\n\n    # Setup for recurrent connection\n    if self.recurrent:\n        self.symmetric_p1_arg = True\n        self.symmetric_p1 = True\n        self.vars[\"n_syn1\"] = self.vars[\"n_syn0\"]\n    if self.symmetric_p1_arg:\n        self.vars[\"p1_arg\"] = self.vars[\"p0_arg\"]\n    if self.symmetric_p1:\n        self.vars[\"p1\"] = self.vars[\"p0\"]\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector","title":"<code>bmtool.connectors.UnidirectionConnector</code>","text":"<p>               Bases: <code>AbstractConnector</code></p> <p>Object for building unidirectional connections in bmtk network model with given probability within a single population (or between two populations).</p> <p>Parameters:     p, p_arg: Probability of forward connection and its input argument when         it is a function, similar to p0, p0_arg in ReciprocalConnector. It         can be a constant or a deterministic function whose value must be         within range [0, 1]. When p is constant, the connection is         homogenous.     n_syn: Number of synapses in the forward connection if connected. It         can be a constant or a (deterministic or random) function whose         input arguments are two node objects in BMTK like p_arg.     verbose: Whether show verbose information in console.</p> <p>Returns:     An object that works with BMTK to build edges in a network.</p> <p>Important attributes:     vars: Dictionary that stores part of the original input parameters.     source, target: NodePool objects for the source and target populations.     conn_prop: A dictionaries that stores properties of connected pairs.     Each key is the source node id and the value is a dictionary, where     each key is the target node id that the source node connects to, and     the value is the value of p_arg.         Example: {sid0: {tid0: p_arg0, tid1: p_arg1, ...},                   sid1: {...}, sid2: {...}, ... }         This is useful in similar manner as in ReciprocalConnector.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class UnidirectionConnector(AbstractConnector):\n    \"\"\"\n    Object for building unidirectional connections in bmtk network model with\n    given probability within a single population (or between two populations).\n\n    Parameters:\n        p, p_arg: Probability of forward connection and its input argument when\n            it is a function, similar to p0, p0_arg in ReciprocalConnector. It\n            can be a constant or a deterministic function whose value must be\n            within range [0, 1]. When p is constant, the connection is\n            homogenous.\n        n_syn: Number of synapses in the forward connection if connected. It\n            can be a constant or a (deterministic or random) function whose\n            input arguments are two node objects in BMTK like p_arg.\n        verbose: Whether show verbose information in console.\n\n    Returns:\n        An object that works with BMTK to build edges in a network.\n\n    Important attributes:\n        vars: Dictionary that stores part of the original input parameters.\n        source, target: NodePool objects for the source and target populations.\n        conn_prop: A dictionaries that stores properties of connected pairs.\n        Each key is the source node id and the value is a dictionary, where\n        each key is the target node id that the source node connects to, and\n        the value is the value of p_arg.\n            Example: {sid0: {tid0: p_arg0, tid1: p_arg1, ...},\n                      sid1: {...}, sid2: {...}, ... }\n            This is useful in similar manner as in ReciprocalConnector.\n    \"\"\"\n\n    def __init__(\n        self, p=1.0, p_arg=None, n_syn=1, verbose=True, save_report=True, report_name=None\n    ):\n        args = locals()\n        var_set = (\"p\", \"p_arg\", \"n_syn\")\n        self.vars = {key: args[key] for key in var_set}\n\n        self.verbose = verbose\n        self.save_report = save_report\n        if report_name is None:\n            report_name = globals().get(\"report_name\", \"default_report.csv\")\n        self.report_name = report_name\n\n        self.conn_prop = {}\n        self.iter_count = 0\n\n    # *** Two methods executed during bmtk edge creation net.add_edges() ***\n    def setup_nodes(self, source=None, target=None):\n        \"\"\"Must run this before building connections\"\"\"\n        # Update node pools\n        self.source = source\n        self.target = target\n        if self.source is None or len(self.source) == 0:\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(f\"{src_str} nodes do not exists\")\n        if self.target is None or len(self.target) == 0:\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(f\"{trg_str} nodes do not exists\")\n        self.n_pair = len(self.source) * len(self.target)\n\n    def edge_params(self):\n        \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n        params = {\n            \"source\": self.source,\n            \"target\": self.target,\n            \"iterator\": \"one_to_one\",\n            \"connection_rule\": self.make_connection,\n        }\n        return params\n\n    # *** Methods executed during bmtk network.build() ***\n    # *** Helper functions ***\n    def add_conn_prop(self, sid, tid, prop):\n        \"\"\"Store p0_arg and p1_arg for a connected pair\"\"\"\n        trg_dict = self.conn_prop.setdefault(sid, {})\n        trg_dict[tid] = prop\n\n    def get_conn_prop(self, sid, tid):\n        \"\"\"Get stored value given node ids in a connection\"\"\"\n        return self.conn_prop[sid][tid]\n\n    def setup_variables(self):\n        \"\"\"Make constant variables constant functions\"\"\"\n        for name, var in self.vars.items():\n            if callable(var):\n                setattr(self, name, var)\n            else:\n                setattr(self, name, self.constant_function(var))\n\n    def initialize(self):\n        self.setup_variables()\n        self.n_conn = 0\n        self.n_poss = 0\n        if self.verbose:\n            self.timer = Timer()\n\n    def make_connection(self, source, target, *args, **kwargs):\n        \"\"\"Assign number of synapses per iteration using one_to_one iterator\"\"\"\n        # Initialize in the first iteration\n        if self.iter_count == 0:\n            self.initialize()\n            if self.verbose:\n                src_str, trg_str = self.get_nodes_info()\n                print(\n                    \"\\nStart building connection \\n  from \" + src_str + \"\\n  to \" + trg_str,\n                    flush=True,\n                )\n\n        # Make random connections\n\n        p_arg = self.p_arg(source, target)\n        p = self.p(p_arg)\n        possible = p &gt; 0\n        self.n_poss += possible\n        if possible and decision(p):\n            nsyns = self.n_syn(source, target)\n            self.add_conn_prop(source.node_id, target.node_id, p_arg)\n            self.n_conn += 1\n        else:\n            nsyns = 0\n\n        self.iter_count += 1\n\n        # Detect end of iteration\n        if self.iter_count == self.n_pair:\n            if self.verbose:\n                self.connection_number_info()\n                self.timer.report(\"Done! \\nTime for building connections\")\n            if self.save_report:\n                self.save_connection_report()\n\n        return nsyns\n\n    # *** Helper functions for verbose ***\n    def get_nodes_info(self):\n        \"\"\"Get strings with source and target population information\"\"\"\n        source_str = self.source.network_name + \": \" + self.source.filter_str\n        target_str = self.target.network_name + \": \" + self.target.filter_str\n        return source_str, target_str\n\n    def connection_number_info(self):\n        \"\"\"Print connection numbers after connections built\"\"\"\n        print(\"Number of connected pairs: %d\" % self.n_conn, flush=True)\n        print(\"Number of possible connections: %d\" % self.n_poss, flush=True)\n        print(\n            \"Fraction of connected pairs in possible ones: %.2f%%\"\n            % (100.0 * self.n_conn / self.n_poss)\n            if self.n_poss\n            else 0.0\n        )\n        print(\"Number of total pairs: %d\" % self.n_pair, flush=True)\n        print(\n            \"Fraction of connected pairs in all pairs: %.2f%%\\n\"\n            % (100.0 * self.n_conn / self.n_pair),\n            flush=True,\n        )\n\n    def save_connection_report(self):\n        \"\"\"Save connections into a CSV file to be read from later\"\"\"\n        src_str, trg_str = self.get_nodes_info()\n\n        possible_fraction = 100.0 * self.n_conn / self.n_poss\n        all_fraction = 100.0 * self.n_conn / self.n_pair\n\n        # Extract the population name from source_str and target_str\n        data = {\n            \"Source\": [src_str],\n            \"Target\": [trg_str],\n            \"Percent connectionivity within possible connections\": [possible_fraction],\n            \"Percent connectionivity within all connections\": [all_fraction],\n        }\n        df = pd.DataFrame(data)\n\n        # Append the data to the CSV file\n        try:\n            # Check if the file exists by trying to read it\n            existing_df = pd.read_csv(self.report_name)\n            # If no exception is raised, append without header\n            df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n        except FileNotFoundError:\n            # If the file does not exist, write with header\n            df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.add_conn_prop","title":"<code>add_conn_prop(sid, tid, prop)</code>","text":"<p>Store p0_arg and p1_arg for a connected pair</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def add_conn_prop(self, sid, tid, prop):\n    \"\"\"Store p0_arg and p1_arg for a connected pair\"\"\"\n    trg_dict = self.conn_prop.setdefault(sid, {})\n    trg_dict[tid] = prop\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.connection_number_info","title":"<code>connection_number_info()</code>","text":"<p>Print connection numbers after connections built</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def connection_number_info(self):\n    \"\"\"Print connection numbers after connections built\"\"\"\n    print(\"Number of connected pairs: %d\" % self.n_conn, flush=True)\n    print(\"Number of possible connections: %d\" % self.n_poss, flush=True)\n    print(\n        \"Fraction of connected pairs in possible ones: %.2f%%\"\n        % (100.0 * self.n_conn / self.n_poss)\n        if self.n_poss\n        else 0.0\n    )\n    print(\"Number of total pairs: %d\" % self.n_pair, flush=True)\n    print(\n        \"Fraction of connected pairs in all pairs: %.2f%%\\n\"\n        % (100.0 * self.n_conn / self.n_pair),\n        flush=True,\n    )\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.edge_params","title":"<code>edge_params()</code>","text":"<p>Create the arguments for BMTK add_edges() method</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def edge_params(self):\n    \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n    params = {\n        \"source\": self.source,\n        \"target\": self.target,\n        \"iterator\": \"one_to_one\",\n        \"connection_rule\": self.make_connection,\n    }\n    return params\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.get_conn_prop","title":"<code>get_conn_prop(sid, tid)</code>","text":"<p>Get stored value given node ids in a connection</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def get_conn_prop(self, sid, tid):\n    \"\"\"Get stored value given node ids in a connection\"\"\"\n    return self.conn_prop[sid][tid]\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.get_nodes_info","title":"<code>get_nodes_info()</code>","text":"<p>Get strings with source and target population information</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def get_nodes_info(self):\n    \"\"\"Get strings with source and target population information\"\"\"\n    source_str = self.source.network_name + \": \" + self.source.filter_str\n    target_str = self.target.network_name + \": \" + self.target.filter_str\n    return source_str, target_str\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.make_connection","title":"<code>make_connection(source, target, *args, **kwargs)</code>","text":"<p>Assign number of synapses per iteration using one_to_one iterator</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_connection(self, source, target, *args, **kwargs):\n    \"\"\"Assign number of synapses per iteration using one_to_one iterator\"\"\"\n    # Initialize in the first iteration\n    if self.iter_count == 0:\n        self.initialize()\n        if self.verbose:\n            src_str, trg_str = self.get_nodes_info()\n            print(\n                \"\\nStart building connection \\n  from \" + src_str + \"\\n  to \" + trg_str,\n                flush=True,\n            )\n\n    # Make random connections\n\n    p_arg = self.p_arg(source, target)\n    p = self.p(p_arg)\n    possible = p &gt; 0\n    self.n_poss += possible\n    if possible and decision(p):\n        nsyns = self.n_syn(source, target)\n        self.add_conn_prop(source.node_id, target.node_id, p_arg)\n        self.n_conn += 1\n    else:\n        nsyns = 0\n\n    self.iter_count += 1\n\n    # Detect end of iteration\n    if self.iter_count == self.n_pair:\n        if self.verbose:\n            self.connection_number_info()\n            self.timer.report(\"Done! \\nTime for building connections\")\n        if self.save_report:\n            self.save_connection_report()\n\n    return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.save_connection_report","title":"<code>save_connection_report()</code>","text":"<p>Save connections into a CSV file to be read from later</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def save_connection_report(self):\n    \"\"\"Save connections into a CSV file to be read from later\"\"\"\n    src_str, trg_str = self.get_nodes_info()\n\n    possible_fraction = 100.0 * self.n_conn / self.n_poss\n    all_fraction = 100.0 * self.n_conn / self.n_pair\n\n    # Extract the population name from source_str and target_str\n    data = {\n        \"Source\": [src_str],\n        \"Target\": [trg_str],\n        \"Percent connectionivity within possible connections\": [possible_fraction],\n        \"Percent connectionivity within all connections\": [all_fraction],\n    }\n    df = pd.DataFrame(data)\n\n    # Append the data to the CSV file\n    try:\n        # Check if the file exists by trying to read it\n        existing_df = pd.read_csv(self.report_name)\n        # If no exception is raised, append without header\n        df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n    except FileNotFoundError:\n        # If the file does not exist, write with header\n        df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.setup_nodes","title":"<code>setup_nodes(source=None, target=None)</code>","text":"<p>Must run this before building connections</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def setup_nodes(self, source=None, target=None):\n    \"\"\"Must run this before building connections\"\"\"\n    # Update node pools\n    self.source = source\n    self.target = target\n    if self.source is None or len(self.source) == 0:\n        src_str, trg_str = self.get_nodes_info()\n        raise ValueError(f\"{src_str} nodes do not exists\")\n    if self.target is None or len(self.target) == 0:\n        src_str, trg_str = self.get_nodes_info()\n        raise ValueError(f\"{trg_str} nodes do not exists\")\n    self.n_pair = len(self.source) * len(self.target)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.setup_variables","title":"<code>setup_variables()</code>","text":"<p>Make constant variables constant functions</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def setup_variables(self):\n    \"\"\"Make constant variables constant functions\"\"\"\n    for name, var in self.vars.items():\n        if callable(var):\n            setattr(self, name, var)\n        else:\n            setattr(self, name, self.constant_function(var))\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GapJunction","title":"<code>bmtool.connectors.GapJunction</code>","text":"<p>               Bases: <code>UnidirectionConnector</code></p> <p>Object for buiilding gap junction connections in bmtk network model with given probabilities within a single population which is uncorrelated with the recurrent chemical synapses in this population.</p> <p>Parameters:     p, p_arg: Probability of forward connection and its input argument when         it is a function, similar to p0, p0_arg in ReciprocalConnector. It         can be a constant or a deterministic function whose value must be         within range [0, 1]. When p is constant, the connection is         homogenous.     verbose: Whether show verbose information in console.</p> <p>Returns:     An object that works with BMTK to build edges in a network.</p> <p>Important attributes:     Similar to <code>UnidirectionConnector</code>.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class GapJunction(UnidirectionConnector):\n    \"\"\"\n    Object for buiilding gap junction connections in bmtk network model with\n    given probabilities within a single population which is uncorrelated with\n    the recurrent chemical synapses in this population.\n\n    Parameters:\n        p, p_arg: Probability of forward connection and its input argument when\n            it is a function, similar to p0, p0_arg in ReciprocalConnector. It\n            can be a constant or a deterministic function whose value must be\n            within range [0, 1]. When p is constant, the connection is\n            homogenous.\n        verbose: Whether show verbose information in console.\n\n    Returns:\n        An object that works with BMTK to build edges in a network.\n\n    Important attributes:\n        Similar to `UnidirectionConnector`.\n    \"\"\"\n\n    def __init__(self, p=1.0, p_arg=None, verbose=True, save_report=True, report_name=None):\n        super().__init__(\n            p=p, p_arg=p_arg, verbose=verbose, save_report=save_report, report_name=None\n        )\n\n    def setup_nodes(self, source=None, target=None):\n        super().setup_nodes(source=source, target=target)\n        if len(self.source) != len(self.target):\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(\n                f\"Source and target must be the same for \"\n                f\"gap junction. Nodes are {src_str} and {trg_str}\"\n            )\n        self.n_source = len(self.source)\n\n    def make_connection(self, source, target, *args, **kwargs):\n        \"\"\"Assign gap junction per iteration using one_to_one iterator\"\"\"\n        # Initialize in the first iteration\n        if self.iter_count == 0:\n            self.initialize()\n            if self.verbose:\n                src_str, _ = self.get_nodes_info()\n                print(\"\\nStart building gap junction \\n  in \" + src_str, flush=True)\n\n        # Consider each pair only once\n        nsyns = 0\n        i, j = divmod(self.iter_count, self.n_source)\n        if i &lt; j:\n            p_arg = self.p_arg(source, target)\n            p = self.p(p_arg)\n            possible = p &gt; 0\n            self.n_poss += possible\n            if possible and decision(p):\n                nsyns = 1\n                sid, tid = source.node_id, target.node_id\n                self.add_conn_prop(sid, tid, p_arg)\n                self.add_conn_prop(tid, sid, p_arg)\n                self.n_conn += 1\n\n        self.iter_count += 1\n\n        # Detect end of iteration\n        if self.iter_count == self.n_pair:\n            if self.verbose:\n                self.connection_number_info()\n                self.timer.report(\"Done! \\nTime for building connections\")\n            if self.save_report:\n                self.save_connection_report()\n        return nsyns\n\n    def connection_number_info(self):\n        n_pair = self.n_pair\n        self.n_pair = (n_pair - len(self.source)) // 2\n        super().connection_number_info()\n        self.n_pair = n_pair\n\n    def save_connection_report(self):\n        \"\"\"Save connections into a CSV file to be read from later\"\"\"\n        src_str, trg_str = self.get_nodes_info()\n        n_pair = self.n_pair\n        fraction_0 = self.n_conn / self.n_poss if self.n_poss else 0.0\n        fraction_1 = self.n_conn / self.n_pair\n\n        # Convert fraction to percentage and prepare data for the DataFrame\n        data = {\n            \"Source\": [src_str + \"Gap\"],\n            \"Target\": [trg_str + \"Gap\"],\n            \"Percent connectionivity within possible connections\": [fraction_0 * 100],\n            \"Percent connectionivity within all connections\": [fraction_1 * 100],\n        }\n        df = pd.DataFrame(data)\n\n        # Append the data to the CSV file\n        try:\n            # Check if the file exists by trying to read it\n            existing_df = pd.read_csv(self.report_name)\n            # If no exception is raised, append without header\n            df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n        except FileNotFoundError:\n            # If the file does not exist, write with header\n            df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GapJunction.make_connection","title":"<code>make_connection(source, target, *args, **kwargs)</code>","text":"<p>Assign gap junction per iteration using one_to_one iterator</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_connection(self, source, target, *args, **kwargs):\n    \"\"\"Assign gap junction per iteration using one_to_one iterator\"\"\"\n    # Initialize in the first iteration\n    if self.iter_count == 0:\n        self.initialize()\n        if self.verbose:\n            src_str, _ = self.get_nodes_info()\n            print(\"\\nStart building gap junction \\n  in \" + src_str, flush=True)\n\n    # Consider each pair only once\n    nsyns = 0\n    i, j = divmod(self.iter_count, self.n_source)\n    if i &lt; j:\n        p_arg = self.p_arg(source, target)\n        p = self.p(p_arg)\n        possible = p &gt; 0\n        self.n_poss += possible\n        if possible and decision(p):\n            nsyns = 1\n            sid, tid = source.node_id, target.node_id\n            self.add_conn_prop(sid, tid, p_arg)\n            self.add_conn_prop(tid, sid, p_arg)\n            self.n_conn += 1\n\n    self.iter_count += 1\n\n    # Detect end of iteration\n    if self.iter_count == self.n_pair:\n        if self.verbose:\n            self.connection_number_info()\n            self.timer.report(\"Done! \\nTime for building connections\")\n        if self.save_report:\n            self.save_connection_report()\n    return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GapJunction.save_connection_report","title":"<code>save_connection_report()</code>","text":"<p>Save connections into a CSV file to be read from later</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def save_connection_report(self):\n    \"\"\"Save connections into a CSV file to be read from later\"\"\"\n    src_str, trg_str = self.get_nodes_info()\n    n_pair = self.n_pair\n    fraction_0 = self.n_conn / self.n_poss if self.n_poss else 0.0\n    fraction_1 = self.n_conn / self.n_pair\n\n    # Convert fraction to percentage and prepare data for the DataFrame\n    data = {\n        \"Source\": [src_str + \"Gap\"],\n        \"Target\": [trg_str + \"Gap\"],\n        \"Percent connectionivity within possible connections\": [fraction_0 * 100],\n        \"Percent connectionivity within all connections\": [fraction_1 * 100],\n    }\n    df = pd.DataFrame(data)\n\n    # Append the data to the CSV file\n    try:\n        # Check if the file exists by trying to read it\n        existing_df = pd.read_csv(self.report_name)\n        # If no exception is raised, append without header\n        df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n    except FileNotFoundError:\n        # If the file does not exist, write with header\n        df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.CorrelatedGapJunction","title":"<code>bmtool.connectors.CorrelatedGapJunction</code>","text":"<p>               Bases: <code>GapJunction</code></p> <p>Object for buiilding gap junction connections in bmtk network model with given probabilities within a single population which could be correlated with the recurrent chemical synapses in this population.</p> <p>Parameters:     p_non, p_uni, p_rec: Probabilities of gap junction connection for each         pair of cells given the following three conditions of chemical         synaptic connections between them, no connection, unidirectional,         and reciprocal, respectively. It can be a constant or a         deterministic function whose value must be within range [0, 1].     p_arg: Input argument for p_non, p_uni, or p_rec, when any of them is a         function, similar to p0_arg, p1_arg in ReciprocalConnector.     connector: Connector object used to generate the chemical synapses of         within this population, which contains the connection information         in its attribute <code>conn_prop</code>. So this connector should have         generated the chemical synapses before generating the gap junction.     verbose: Whether show verbose information in console.</p> <p>Returns:     An object that works with BMTK to build edges in a network.</p> <p>Important attributes:     Similar to <code>UnidirectionConnector</code>.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class CorrelatedGapJunction(GapJunction):\n    \"\"\"\n    Object for buiilding gap junction connections in bmtk network model with\n    given probabilities within a single population which could be correlated\n    with the recurrent chemical synapses in this population.\n\n    Parameters:\n        p_non, p_uni, p_rec: Probabilities of gap junction connection for each\n            pair of cells given the following three conditions of chemical\n            synaptic connections between them, no connection, unidirectional,\n            and reciprocal, respectively. It can be a constant or a\n            deterministic function whose value must be within range [0, 1].\n        p_arg: Input argument for p_non, p_uni, or p_rec, when any of them is a\n            function, similar to p0_arg, p1_arg in ReciprocalConnector.\n        connector: Connector object used to generate the chemical synapses of\n            within this population, which contains the connection information\n            in its attribute `conn_prop`. So this connector should have\n            generated the chemical synapses before generating the gap junction.\n        verbose: Whether show verbose information in console.\n\n    Returns:\n        An object that works with BMTK to build edges in a network.\n\n    Important attributes:\n        Similar to `UnidirectionConnector`.\n    \"\"\"\n\n    def __init__(\n        self,\n        p_non=1.0,\n        p_uni=1.0,\n        p_rec=1.0,\n        p_arg=None,\n        connector=None,\n        verbose=True,\n        save_report=True,\n        report_name=None,\n    ):\n        super().__init__(\n            p=p_non, p_arg=p_arg, verbose=verbose, save_report=save_report, report_name=None\n        )\n        self.vars[\"p_non\"] = self.vars.pop(\"p\")\n        self.vars[\"p_uni\"] = p_uni\n        self.vars[\"p_rec\"] = p_rec\n        self.connector = connector\n        conn_prop = connector.conn_prop\n        if isinstance(conn_prop, list):\n            conn_prop = conn_prop[0]\n        self.ref_conn_prop = conn_prop\n\n    def conn_exist(self, sid, tid):\n        trg_dict = self.ref_conn_prop.get(sid)\n        if trg_dict is not None and tid in trg_dict:\n            return True, trg_dict[tid]\n        else:\n            return False, None\n\n    def connection_type(self, sid, tid):\n        conn0, prop0 = self.conn_exist(sid, tid)\n        conn1, prop1 = self.conn_exist(tid, sid)\n        return conn0 + conn1, prop0 if conn0 else prop1\n\n    def initialize(self):\n        self.has_p_arg = self.vars[\"p_arg\"] is not None\n        if not self.has_p_arg:\n            var = self.connector.vars\n            self.vars[\"p_arg\"] = var.get(\"p_arg\", var.get(\"p0_arg\", None))\n        super().initialize()\n        self.ps = [self.p_non, self.p_uni, self.p_rec]\n\n    def make_connection(self, source, target, *args, **kwargs):\n        \"\"\"Assign gap junction per iteration using one_to_one iterator\"\"\"\n        # Initialize in the first iteration\n        if self.iter_count == 0:\n            self.initialize()\n            if self.verbose:\n                src_str, _ = self.get_nodes_info()\n                print(\"\\nStart building gap junction \\n  in \" + src_str, flush=True)\n\n        # Consider each pair only once\n        nsyns = 0\n        i, j = divmod(self.iter_count, self.n_source)\n        if i &lt; j:\n            sid, tid = source.node_id, target.node_id\n            conn_type, p_arg = self.connection_type(sid, tid)\n            if self.has_p_arg or not conn_type:\n                p_arg = self.p_arg(source, target)\n            p = self.ps[conn_type](p_arg)\n            possible = p &gt; 0\n            self.n_poss += possible\n            if possible and decision(p):\n                nsyns = 1\n                self.add_conn_prop(sid, tid, p_arg)\n                self.add_conn_prop(tid, sid, p_arg)\n                self.n_conn += 1\n\n        self.iter_count += 1\n\n        # Detect end of iteration\n        if self.iter_count == self.n_pair:\n            if self.verbose:\n                self.connection_number_info()\n                self.timer.report(\"Done! \\nTime for building connections\")\n            if self.save_report:\n                self.save_connection_report()\n        return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.CorrelatedGapJunction.make_connection","title":"<code>make_connection(source, target, *args, **kwargs)</code>","text":"<p>Assign gap junction per iteration using one_to_one iterator</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_connection(self, source, target, *args, **kwargs):\n    \"\"\"Assign gap junction per iteration using one_to_one iterator\"\"\"\n    # Initialize in the first iteration\n    if self.iter_count == 0:\n        self.initialize()\n        if self.verbose:\n            src_str, _ = self.get_nodes_info()\n            print(\"\\nStart building gap junction \\n  in \" + src_str, flush=True)\n\n    # Consider each pair only once\n    nsyns = 0\n    i, j = divmod(self.iter_count, self.n_source)\n    if i &lt; j:\n        sid, tid = source.node_id, target.node_id\n        conn_type, p_arg = self.connection_type(sid, tid)\n        if self.has_p_arg or not conn_type:\n            p_arg = self.p_arg(source, target)\n        p = self.ps[conn_type](p_arg)\n        possible = p &gt; 0\n        self.n_poss += possible\n        if possible and decision(p):\n            nsyns = 1\n            self.add_conn_prop(sid, tid, p_arg)\n            self.add_conn_prop(tid, sid, p_arg)\n            self.n_conn += 1\n\n    self.iter_count += 1\n\n    # Detect end of iteration\n    if self.iter_count == self.n_pair:\n        if self.verbose:\n            self.connection_number_info()\n            self.timer.report(\"Done! \\nTime for building connections\")\n        if self.save_report:\n            self.save_connection_report()\n    return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.OneToOneSequentialConnector","title":"<code>bmtool.connectors.OneToOneSequentialConnector</code>","text":"<p>               Bases: <code>AbstractConnector</code></p> <p>Object for buiilding one to one correspondence connections in bmtk network model with between two populations. One of the population can consist of multiple sub-populations. These sub-populations need to be added sequentially using setup_nodes() and edge_params() methods followed by BMTK add_edges() method. For example, to connect 30 nodes in population A to 30 nodes in populations B1, B2, B3, each with 10 nodes, set up as follows.     connector = OneToOneSequentialConnector(parameters)     connector.setup_nodes(source=A, target=B1)     net.add_edges(connector.edge_params())     connector.setup_nodes(target=B2)     net.add_edges(connector.edge_params())     connector.setup_nodes(target=B3)     net.add_edges(connector.edge_params()) After BMTK executes net.build(), the first 10 nodes in A will connect one- to-one to the 10 nodes in B1, then the 11 to 20-th nodes to those in B2, finally the 21 to 30-th nodes to those in B3. This connector is useful for creating input drives to a population. Each node in it receives an independent drive from a unique source node.</p> <p>Parameters:     n_syn: Number of synapses in each connection. It accepts only constant         for now.     partition_source: Whether the source population consists of multiple         sub-populations. By default, the source has one population, and the         target can have multiple sub-populations. If set to true, the         source can have multiple sub-populations and the target has only         one population.     verbose: Whether show verbose information in console.</p> <p>Returns:     An object that works with BMTK to build edges in a network.</p> <p>Important attributes:     source: NodePool object for the single population.     targets: List of NodePool objects for the multiple sub-populations.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class OneToOneSequentialConnector(AbstractConnector):\n    \"\"\"Object for buiilding one to one correspondence connections in bmtk\n    network model with between two populations. One of the population can\n    consist of multiple sub-populations. These sub-populations need to be added\n    sequentially using setup_nodes() and edge_params() methods followed by BMTK\n    add_edges() method. For example, to connect 30 nodes in population A to 30\n    nodes in populations B1, B2, B3, each with 10 nodes, set up as follows.\n        connector = OneToOneSequentialConnector(**parameters)\n        connector.setup_nodes(source=A, target=B1)\n        net.add_edges(**connector.edge_params())\n        connector.setup_nodes(target=B2)\n        net.add_edges(**connector.edge_params())\n        connector.setup_nodes(target=B3)\n        net.add_edges(**connector.edge_params())\n    After BMTK executes net.build(), the first 10 nodes in A will connect one-\n    to-one to the 10 nodes in B1, then the 11 to 20-th nodes to those in B2,\n    finally the 21 to 30-th nodes to those in B3.\n    This connector is useful for creating input drives to a population. Each\n    node in it receives an independent drive from a unique source node.\n\n    Parameters:\n        n_syn: Number of synapses in each connection. It accepts only constant\n            for now.\n        partition_source: Whether the source population consists of multiple\n            sub-populations. By default, the source has one population, and the\n            target can have multiple sub-populations. If set to true, the\n            source can have multiple sub-populations and the target has only\n            one population.\n        verbose: Whether show verbose information in console.\n\n    Returns:\n        An object that works with BMTK to build edges in a network.\n\n    Important attributes:\n        source: NodePool object for the single population.\n        targets: List of NodePool objects for the multiple sub-populations.\n    \"\"\"\n\n    def __init__(self, n_syn=1, partition_source=False, verbose=True):\n        self.n_syn = int(n_syn)\n        self.partition_source = partition_source\n        self.verbose = verbose\n\n        self.targets = []\n        self.n_source = 0\n        self.idx_range = [0]\n        self.target_count = 0\n        self.iter_count = 0\n\n    # *** Two methods executed during bmtk edge creation net.add_edges() ***\n    def setup_nodes(self, source=None, target=None):\n        \"\"\"Must run this before building connections\"\"\"\n        # Update node pools\n        if self.partition_source:\n            source, target = target, source\n        if self.target_count == 0:\n            if source is None or len(source) == 0:\n                src_str, trg_str = self.get_nodes_info()\n                raise ValueError(\n                    (f\"{trg_str}\" if self.partition_source else f\"{src_str}\")\n                    + \" nodes do not exists\"\n                )\n            self.source = source\n            self.n_source = len(source)\n        if target is None or len(target) == 0:\n            raise ValueError(\n                (\"Source\" if self.partition_source else \"Target\") + \" nodes do not exists\"\n            )\n\n        self.targets.append(target)\n        self.idx_range.append(self.idx_range[-1] + len(target))\n        self.target_count += 1\n\n        if self.idx_range[-1] &gt; self.n_source:\n            if self.partition_source:\n                raise ValueError(\n                    \"Total target populations exceed the source population.\"\n                    if self.partition_source\n                    else \"Total source populations exceed the target population.\"\n                )\n\n        if self.verbose and self.idx_range[-1] == self.n_source:\n            print(\n                \"All \"\n                + (\"source\" if self.partition_source else \"target\")\n                + \" population partitions are filled.\",\n                flush=True,\n            )\n\n    def edge_params(self, target_pop_idx=-1):\n        \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n        if self.partition_source:\n            params = {\n                \"source\": self.targets[target_pop_idx],\n                \"target\": self.source,\n                \"iterator\": \"one_to_all\",\n            }\n        else:\n            params = {\n                \"source\": self.source,\n                \"target\": self.targets[target_pop_idx],\n                \"iterator\": \"all_to_one\",\n            }\n        params[\"connection_rule\"] = self.make_connection\n        return params\n\n    # *** Methods executed during bmtk network.build() ***\n    def make_connection(self, source, targets, *args, **kwargs):\n        \"\"\"Assign one connection per iteration using all_to_one iterator\"\"\"\n        # Initialize in the first iteration\n        if self.verbose:\n            if self.iter_count == 0:\n                # Very beginning\n                self.target_count = 0\n                src_str, trg_str = self.get_nodes_info()\n                print(\n                    \"\\nStart building connection \"\n                    + (\"to \" if self.partition_source else \"from \")\n                    + src_str,\n                    flush=True,\n                )\n                self.timer = Timer()\n\n            if self.iter_count == self.idx_range[self.target_count]:\n                # Beginning of each target population\n                src_str, trg_str = self.get_nodes_info(self.target_count)\n                print(\n                    (\"  %d. \" % self.target_count)\n                    + (\"from \" if self.partition_source else \"to \")\n                    + trg_str,\n                    flush=True,\n                )\n                self.target_count += 1\n                self.timer_part = Timer()\n\n        # Make connection\n        nsyns = np.zeros(self.n_source, dtype=int)\n        nsyns[self.iter_count] = self.n_syn\n        self.iter_count += 1\n\n        # Detect end of iteration\n        if self.verbose:\n            if self.iter_count == self.idx_range[self.target_count]:\n                # End of each target population\n                self.timer_part.report(\"    Time for this partition\")\n            if self.iter_count == self.n_source:\n                # Very end\n                self.timer.report(\"Done! \\nTime for building connections\")\n        return nsyns\n\n    # *** Helper functions for verbose ***\n    def get_nodes_info(self, target_pop_idx=-1):\n        \"\"\"Get strings with source and target population information\"\"\"\n        target = self.targets[target_pop_idx]\n        source_str = self.source.network_name + \": \" + self.source.filter_str\n        target_str = target.network_name + \": \" + target.filter_str\n        return source_str, target_str\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.OneToOneSequentialConnector.edge_params","title":"<code>edge_params(target_pop_idx=-1)</code>","text":"<p>Create the arguments for BMTK add_edges() method</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def edge_params(self, target_pop_idx=-1):\n    \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n    if self.partition_source:\n        params = {\n            \"source\": self.targets[target_pop_idx],\n            \"target\": self.source,\n            \"iterator\": \"one_to_all\",\n        }\n    else:\n        params = {\n            \"source\": self.source,\n            \"target\": self.targets[target_pop_idx],\n            \"iterator\": \"all_to_one\",\n        }\n    params[\"connection_rule\"] = self.make_connection\n    return params\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.OneToOneSequentialConnector.get_nodes_info","title":"<code>get_nodes_info(target_pop_idx=-1)</code>","text":"<p>Get strings with source and target population information</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def get_nodes_info(self, target_pop_idx=-1):\n    \"\"\"Get strings with source and target population information\"\"\"\n    target = self.targets[target_pop_idx]\n    source_str = self.source.network_name + \": \" + self.source.filter_str\n    target_str = target.network_name + \": \" + target.filter_str\n    return source_str, target_str\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.OneToOneSequentialConnector.make_connection","title":"<code>make_connection(source, targets, *args, **kwargs)</code>","text":"<p>Assign one connection per iteration using all_to_one iterator</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_connection(self, source, targets, *args, **kwargs):\n    \"\"\"Assign one connection per iteration using all_to_one iterator\"\"\"\n    # Initialize in the first iteration\n    if self.verbose:\n        if self.iter_count == 0:\n            # Very beginning\n            self.target_count = 0\n            src_str, trg_str = self.get_nodes_info()\n            print(\n                \"\\nStart building connection \"\n                + (\"to \" if self.partition_source else \"from \")\n                + src_str,\n                flush=True,\n            )\n            self.timer = Timer()\n\n        if self.iter_count == self.idx_range[self.target_count]:\n            # Beginning of each target population\n            src_str, trg_str = self.get_nodes_info(self.target_count)\n            print(\n                (\"  %d. \" % self.target_count)\n                + (\"from \" if self.partition_source else \"to \")\n                + trg_str,\n                flush=True,\n            )\n            self.target_count += 1\n            self.timer_part = Timer()\n\n    # Make connection\n    nsyns = np.zeros(self.n_source, dtype=int)\n    nsyns[self.iter_count] = self.n_syn\n    self.iter_count += 1\n\n    # Detect end of iteration\n    if self.verbose:\n        if self.iter_count == self.idx_range[self.target_count]:\n            # End of each target population\n            self.timer_part.report(\"    Time for this partition\")\n        if self.iter_count == self.n_source:\n            # Very end\n            self.timer.report(\"Done! \\nTime for building connections\")\n    return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.OneToOneSequentialConnector.setup_nodes","title":"<code>setup_nodes(source=None, target=None)</code>","text":"<p>Must run this before building connections</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def setup_nodes(self, source=None, target=None):\n    \"\"\"Must run this before building connections\"\"\"\n    # Update node pools\n    if self.partition_source:\n        source, target = target, source\n    if self.target_count == 0:\n        if source is None or len(source) == 0:\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(\n                (f\"{trg_str}\" if self.partition_source else f\"{src_str}\")\n                + \" nodes do not exists\"\n            )\n        self.source = source\n        self.n_source = len(source)\n    if target is None or len(target) == 0:\n        raise ValueError(\n            (\"Source\" if self.partition_source else \"Target\") + \" nodes do not exists\"\n        )\n\n    self.targets.append(target)\n    self.idx_range.append(self.idx_range[-1] + len(target))\n    self.target_count += 1\n\n    if self.idx_range[-1] &gt; self.n_source:\n        if self.partition_source:\n            raise ValueError(\n                \"Total target populations exceed the source population.\"\n                if self.partition_source\n                else \"Total source populations exceed the target population.\"\n            )\n\n    if self.verbose and self.idx_range[-1] == self.n_source:\n        print(\n            \"All \"\n            + (\"source\" if self.partition_source else \"target\")\n            + \" population partitions are filled.\",\n            flush=True,\n        )\n</code></pre>"},{"location":"api/connectors/#synapse-helper-functions","title":"Synapse Helper Functions","text":""},{"location":"api/connectors/#bmtool.connectors.syn_const_delay","title":"<code>bmtool.connectors.syn_const_delay(source=None, target=None, dist=100, min_delay=SYN_MIN_DELAY, velocity=SYN_VELOCITY, fluc_stdev=FLUC_STDEV, delay_bound=(DELAY_LOWBOUND, DELAY_UPBOUND), connector=None)</code>","text":"<p>Synapse delay constant with some random fluctuation.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_const_delay(\n    source=None,\n    target=None,\n    dist=100,\n    min_delay=SYN_MIN_DELAY,\n    velocity=SYN_VELOCITY,\n    fluc_stdev=FLUC_STDEV,\n    delay_bound=(DELAY_LOWBOUND, DELAY_UPBOUND),\n    connector=None,\n):\n    \"\"\"Synapse delay constant with some random fluctuation.\"\"\"\n    del_fluc = fluc_stdev * rng.normal()\n    delay = dist / SYN_VELOCITY + SYN_MIN_DELAY + del_fluc\n    delay = min(max(delay, DELAY_LOWBOUND), DELAY_UPBOUND)\n    return delay\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.syn_dist_delay_feng","title":"<code>bmtool.connectors.syn_dist_delay_feng(source, target, min_delay=SYN_MIN_DELAY, velocity=SYN_VELOCITY, fluc_stdev=FLUC_STDEV, delay_bound=(DELAY_LOWBOUND, DELAY_UPBOUND), connector=None)</code>","text":"<p>Synpase delay linearly dependent on distance. min_delay: minimum delay (ms) velocity: synapse conduction velocity (micron/ms) fluc_stdev: standard deviation of random Gaussian fluctuation (ms) delay_bound: (lower, upper) bounds of delay (ms) connector: connector object from which to read distance</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_dist_delay_feng(\n    source,\n    target,\n    min_delay=SYN_MIN_DELAY,\n    velocity=SYN_VELOCITY,\n    fluc_stdev=FLUC_STDEV,\n    delay_bound=(DELAY_LOWBOUND, DELAY_UPBOUND),\n    connector=None,\n):\n    \"\"\"Synpase delay linearly dependent on distance.\n    min_delay: minimum delay (ms)\n    velocity: synapse conduction velocity (micron/ms)\n    fluc_stdev: standard deviation of random Gaussian fluctuation (ms)\n    delay_bound: (lower, upper) bounds of delay (ms)\n    connector: connector object from which to read distance\n    \"\"\"\n    if connector is None:\n        dist = euclid_dist(target[\"positions\"], source[\"positions\"])\n    else:\n        dist = connector.get_conn_prop(source.node_id, target.node_id)\n    del_fluc = fluc_stdev * rng.normal()\n    delay = dist / velocity + min_delay + del_fluc\n    delay = min(max(delay, delay_bound[0]), delay_bound[1])\n    return delay\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.syn_section_PN","title":"<code>bmtool.connectors.syn_section_PN(source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs)</code>","text":"<p>Synapse location follows a Bernoulli distribution, with probability p to obtain the former in sec_id and sec_x</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_section_PN(source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs):\n    \"\"\"Synapse location follows a Bernoulli distribution, with probability p\n    to obtain the former in sec_id and sec_x\"\"\"\n    syn_loc = int(not decision(p))\n    return sec_id[syn_loc], sec_x[syn_loc]\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.syn_const_delay_feng_section_PN","title":"<code>bmtool.connectors.syn_const_delay_feng_section_PN(source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs)</code>","text":"<p>Assign both synapse delay and location with constant distance assumed</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_const_delay_feng_section_PN(\n    source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs\n):\n    \"\"\"Assign both synapse delay and location with constant distance assumed\"\"\"\n    delay = syn_const_delay(source, target, **kwargs)\n    s_id, s_x = syn_section_PN(source, target, p=p, sec_id=sec_id, sec_x=sec_x)\n    return delay, s_id, s_x\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.syn_dist_delay_feng_section_PN","title":"<code>bmtool.connectors.syn_dist_delay_feng_section_PN(source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs)</code>","text":"<p>Assign both synapse delay and location</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_dist_delay_feng_section_PN(\n    source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs\n):\n    \"\"\"Assign both synapse delay and location\"\"\"\n    delay = syn_dist_delay_feng(source, target, **kwargs)\n    s_id, s_x = syn_section_PN(source, target, p=p, sec_id=sec_id, sec_x=sec_x)\n    return delay, s_id, s_x\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.syn_uniform_delay_section","title":"<code>bmtool.connectors.syn_uniform_delay_section(source, target, low=DELAY_LOWBOUND, high=DELAY_UPBOUND, **kwargs)</code>","text":"Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_uniform_delay_section(source, target, low=DELAY_LOWBOUND, high=DELAY_UPBOUND, **kwargs):\n    return rng.uniform(low, high)\n</code></pre>"},{"location":"api/graphs/","title":"Graphs API Reference","text":"<p>This page provides API reference documentation for the Graphs module, which contains functions for network graph analysis and visualization.</p>"},{"location":"api/graphs/#graph-generation-functions","title":"Graph Generation Functions","text":""},{"location":"api/graphs/#bmtool.graphs.generate_graph","title":"<code>bmtool.graphs.generate_graph(config, source, target)</code>","text":"<p>Generate a NetworkX graph from BMTK network configuration.</p> Parameters: <p>config : str     Path to a BMTK simulation config file. source : str     Network name for source nodes. target : str     Network name for target nodes.</p> Returns: <p>nx.DiGraph     A directed graph representing the network with nodes containing     position and population information.</p> Source code in <code>bmtool/graphs.py</code> <pre><code>def generate_graph(config, source, target):\n    \"\"\"\n    Generate a NetworkX graph from BMTK network configuration.\n\n    Parameters:\n    -----------\n    config : str\n        Path to a BMTK simulation config file.\n    source : str\n        Network name for source nodes.\n    target : str\n        Network name for target nodes.\n\n    Returns:\n    --------\n    nx.DiGraph\n        A directed graph representing the network with nodes containing\n        position and population information.\n    \"\"\"\n    nodes, edges = u.load_nodes_edges_from_config(config)\n    nodes_source = nodes[source]\n    nodes_target = nodes[target]\n    if source != target:\n        # Concatenate the DataFrames if source and target are different nodes\n        nodes = pd.concat([nodes_source, nodes_target])\n    else:\n        nodes = nodes[source]\n    edge_to_grap = source + \"_to_\" + target\n    edges = edges[edge_to_grap]\n\n    # Create an empty graph\n    G = nx.DiGraph()\n\n    # Add nodes to the graph with their positions and labels\n    for index, node_data in nodes.iterrows():\n        G.add_node(\n            index,\n            pos=(node_data[\"pos_x\"], node_data[\"pos_y\"], node_data[\"pos_z\"]),\n            label=node_data[\"pop_name\"],\n        )\n\n    # Add edges to the graph\n    for _, row in edges.iterrows():\n        G.add_edge(row[\"source_node_id\"], row[\"target_node_id\"])\n\n    return G\n</code></pre>"},{"location":"api/graphs/#graph-export-functions","title":"Graph Export Functions","text":""},{"location":"api/graphs/#bmtool.graphs.export_node_connections_to_csv","title":"<code>bmtool.graphs.export_node_connections_to_csv(Graph, filename)</code>","text":"<p>Generate a CSV file with node type and all incoming connections that node has.</p> Parameters: <p>Graph : nx.DiGraph     A directed graph object from NetworkX. filename : str     Path and filename for the output CSV file (must end in .csv).</p> Returns: <p>None     The function saves the results to the specified CSV file.</p> Notes: <p>The resulting CSV file will have the node label as the first column, followed by columns for each type of incoming connection.</p> Source code in <code>bmtool/graphs.py</code> <pre><code>def export_node_connections_to_csv(Graph, filename):\n    \"\"\"\n    Generate a CSV file with node type and all incoming connections that node has.\n\n    Parameters:\n    -----------\n    Graph : nx.DiGraph\n        A directed graph object from NetworkX.\n    filename : str\n        Path and filename for the output CSV file (must end in .csv).\n\n    Returns:\n    --------\n    None\n        The function saves the results to the specified CSV file.\n\n    Notes:\n    ------\n    The resulting CSV file will have the node label as the first column,\n    followed by columns for each type of incoming connection.\n    \"\"\"\n    # Create an empty dictionary to store the connections for each node\n    node_connections = {}\n\n    # Iterate over each node in the graph\n    for node in Graph.nodes():\n        # Initialize a dictionary to store the outgoing connections for the current node\n        connections = {}\n        node_label = Graph.nodes[node][\"label\"]\n\n        # Iterate over each presuccessor (ingoing neighbor) of the current node\n        for successor in Graph.predecessors(node):\n            # Get the label of the successor node\n            successor_label = Graph.nodes[successor][\"label\"]\n\n            # Increment the connection count for the current node and successor label\n            connections[f\"{successor_label} incoming Connections\"] = (\n                connections.get(f\"{successor_label} incoming Connections\", 0) + 1\n            )\n\n        # Add the connections information for the current node to the dictionary\n        connections[\"Node Label\"] = node_label\n        node_connections[node] = connections\n\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(node_connections).fillna(0).T\n\n    # Reorder columns so that 'Node Label' is the leftmost column\n    cols = df.columns.tolist()\n    cols = [\"Node Label\"] + [col for col in cols if col != \"Node Label\"]\n    df = df[cols]\n\n    # Write the DataFrame to a CSV file\n    df.to_csv(filename)\n</code></pre>"},{"location":"api/singlecell/","title":"Single Cell API Reference","text":"<p>This page provides API reference documentation for the Single Cell module, which contains functions and classes for working with individual neuron models.</p>"},{"location":"api/singlecell/#utility-functions","title":"Utility Functions","text":""},{"location":"api/singlecell/#bmtool.singlecell.load_biophys1","title":"<code>bmtool.singlecell.load_biophys1()</code>","text":"<p>Load the Biophys1 template from BMTK if it hasn't been loaded yet.</p> <p>This function checks if the Biophys1 object exists in NEURON's h namespace. If not, it loads the necessary HOC files for Allen Cell Types Database models.</p> Notes: <p>This is primarily used for working with cell models from the Allen Cell Types Database.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def load_biophys1():\n    \"\"\"\n    Load the Biophys1 template from BMTK if it hasn't been loaded yet.\n\n    This function checks if the Biophys1 object exists in NEURON's h namespace.\n    If not, it loads the necessary HOC files for Allen Cell Types Database models.\n\n    Notes:\n    ------\n    This is primarily used for working with cell models from the Allen Cell Types Database.\n    \"\"\"\n    if not hasattr(h, \"Biophys1\"):\n        from bmtk import utils\n\n        module_dir = os.path.dirname(os.path.abspath(utils.__file__))\n        hoc_file = os.path.join(module_dir, \"scripts\", \"bionet\", \"templates\", \"Biophys1.hoc\")\n        h.load_file(\"import3d.hoc\")\n        h.load_file(hoc_file)\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.load_allen_database_cells","title":"<code>bmtool.singlecell.load_allen_database_cells(morphology, dynamic_params, model_processing='aibs_perisomatic')</code>","text":"<p>Create a cell model from the Allen Cell Types Database.</p> Parameters: <p>morphology : str     Path to the morphology file (SWC or ASC format). dynamic_params : str     Path to the JSON file containing biophysical parameters. model_processing : str, optional     Model processing type from the AllenCellType database.     Default is 'aibs_perisomatic'.</p> Returns: <p>callable     A function that, when called, creates and returns a NEURON cell object     with the specified morphology and biophysical properties.</p> Notes: <p>This function creates a closure that loads and returns a cell when called. The cell is created using the Allen Institute's modeling framework.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def load_allen_database_cells(morphology, dynamic_params, model_processing=\"aibs_perisomatic\"):\n    \"\"\"\n    Create a cell model from the Allen Cell Types Database.\n\n    Parameters:\n    -----------\n    morphology : str\n        Path to the morphology file (SWC or ASC format).\n    dynamic_params : str\n        Path to the JSON file containing biophysical parameters.\n    model_processing : str, optional\n        Model processing type from the AllenCellType database.\n        Default is 'aibs_perisomatic'.\n\n    Returns:\n    --------\n    callable\n        A function that, when called, creates and returns a NEURON cell object\n        with the specified morphology and biophysical properties.\n\n    Notes:\n    ------\n    This function creates a closure that loads and returns a cell when called.\n    The cell is created using the Allen Institute's modeling framework.\n    \"\"\"\n    from bmtk.simulator.bionet.default_setters import cell_models\n\n    load_biophys1()\n    model_processing = getattr(cell_models, model_processing)\n    with open(dynamic_params) as f:\n        dynamics_params = json.load(f)\n\n    def create_cell():\n        hobj = h.Biophys1(morphology)\n        hobj = model_processing(hobj, cell=None, dynamics_params=dynamics_params)\n        return hobj\n\n    return create_cell\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.get_target_site","title":"<code>bmtool.singlecell.get_target_site(cell, sec=('soma', 0), loc=0.5, site='')</code>","text":"<p>Get a segment and its section from a cell model using flexible section specification.</p> Parameters: <p>cell : NEURON cell object     The cell object to access sections from. sec : str, int, or tuple, optional     Section specification, which can be:     - str: Section name (defaults to index 0 if multiple sections)     - int: Index into the 'all' section list     - tuple: (section_name, index) for accessing indexed sections     Default is ('soma', 0). loc : float, optional     Location along the section (0-1), default is 0.5 (middle of section). site : str, optional     Name of the site for error messages (e.g., 'injection', 'recording').</p> Returns: <p>tuple     (segment, section) at the specified location</p> Raises: <p>ValueError     If the section cannot be found or accessed.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def get_target_site(cell, sec=(\"soma\", 0), loc=0.5, site=\"\"):\n    \"\"\"\n    Get a segment and its section from a cell model using flexible section specification.\n\n    Parameters:\n    -----------\n    cell : NEURON cell object\n        The cell object to access sections from.\n    sec : str, int, or tuple, optional\n        Section specification, which can be:\n        - str: Section name (defaults to index 0 if multiple sections)\n        - int: Index into the 'all' section list\n        - tuple: (section_name, index) for accessing indexed sections\n        Default is ('soma', 0).\n    loc : float, optional\n        Location along the section (0-1), default is 0.5 (middle of section).\n    site : str, optional\n        Name of the site for error messages (e.g., 'injection', 'recording').\n\n    Returns:\n    --------\n    tuple\n        (segment, section) at the specified location\n\n    Raises:\n    -------\n    ValueError\n        If the section cannot be found or accessed.\n    \"\"\"\n    if isinstance(sec, str):\n        sec = (sec, 0)\n    elif isinstance(sec, int):\n        if not hasattr(cell, \"all\"):\n            raise ValueError(\"Section list named 'all' does not exist in the template.\")\n        sec = (\"all\", sec)\n    loc = float(loc)\n    try:\n        section = next(s for i, s in enumerate(getattr(cell, sec[0])) if i == sec[1])\n        seg = section(loc)\n    except Exception as e0:\n        try:\n            section = eval(\"cell.\" + sec[0])\n            seg = section(loc)\n        except Exception as e:\n            print(e0)\n            print(e)\n            raise ValueError(\"Hint: Are you selecting the correct \" + site + \" location?\")\n    return seg, section\n</code></pre>"},{"location":"api/singlecell/#current-clamp","title":"Current Clamp","text":""},{"location":"api/singlecell/#bmtool.singlecell.CurrentClamp","title":"<code>bmtool.singlecell.CurrentClamp</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>bmtool/singlecell.py</code> <pre><code>class CurrentClamp(object):\n    def __init__(\n        self,\n        template_name,\n        post_init_function=None,\n        record_sec=\"soma\",\n        record_loc=0.5,\n        threshold=None,\n        inj_sec=\"soma\",\n        inj_loc=0.5,\n        inj_amp=100.0,\n        inj_delay=100.0,\n        inj_dur=1000.0,\n        tstop=1000.0,\n    ):\n        \"\"\"\n        Initialize a current clamp simulation environment.\n\n        Parameters:\n        -----------\n        template_name : str or callable\n            Either the name of the cell template located in HOC or\n            a function that creates and returns a cell object.\n        post_init_function : str, optional\n            Function of the cell to be called after initialization.\n        record_sec : str, int, or tuple, optional\n            Section to record from. Can be:\n            - str: Section name (defaults to index 0 if multiple sections)\n            - int: Index into the 'all' section list\n            - tuple: (section_name, index) for accessing indexed sections\n            Default is 'soma'.\n        record_loc : float, optional\n            Location (0-1) within section to record from. Default is 0.5.\n        threshold : float, optional\n            Spike threshold (mV). If specified, spikes are detected and counted.\n        inj_sec : str, int, or tuple, optional\n            Section for current injection. Same format as record_sec. Default is 'soma'.\n        inj_loc : float, optional\n            Location (0-1) within section for current injection. Default is 0.5.\n        inj_amp : float, optional\n            Current injection amplitude (pA). Default is 100.0.\n        inj_delay : float, optional\n            Start time for current injection (ms). Default is 100.0.\n        inj_dur : float, optional\n            Duration of current injection (ms). Default is 1000.0.\n        tstop : float, optional\n            Total simulation time (ms). Default is 1000.0.\n            Will be extended if necessary to include the full current injection.\n        \"\"\"\n        self.create_cell = (\n            getattr(h, template_name) if isinstance(template_name, str) else template_name\n        )\n        self.record_sec = record_sec\n        self.record_loc = record_loc\n        self.inj_sec = inj_sec\n        self.inj_loc = inj_loc\n        self.threshold = threshold\n\n        self.tstop = max(tstop, inj_delay + inj_dur)\n        self.inj_delay = inj_delay  # use x ms after start of inj to calculate r_in, etc\n        self.inj_dur = inj_dur\n        self.inj_amp = inj_amp * 1e-3  # pA to nA\n\n        # sometimes people may put a hoc object in for the template name\n        if callable(template_name):\n            self.cell = template_name()\n        else:\n            self.cell = self.create_cell()\n        if post_init_function:\n            eval(f\"self.cell.{post_init_function}\")\n\n        self.setup()\n\n    def setup(self):\n        \"\"\"\n        Set up the simulation environment for current clamp experiments.\n\n        This method:\n        1. Creates the current clamp stimulus at the specified injection site\n        2. Sets up voltage recording at the specified recording site\n        3. Creates vectors to store time and voltage data\n\n        Notes:\n        ------\n        Sets self.cell_src as the current clamp object that can be accessed later.\n        \"\"\"\n        inj_seg, _ = get_target_site(self.cell, self.inj_sec, self.inj_loc, \"injection\")\n        self.cell_src = h.IClamp(inj_seg)\n        self.cell_src.delay = self.inj_delay\n        self.cell_src.dur = self.inj_dur\n        self.cell_src.amp = self.inj_amp\n\n        rec_seg, rec_sec = get_target_site(self.cell, self.record_sec, self.record_loc, \"recording\")\n        self.v_vec = h.Vector()\n        self.v_vec.record(rec_seg._ref_v)\n\n        self.t_vec = h.Vector()\n        self.t_vec.record(h._ref_t)\n\n        if self.threshold is not None:\n            self.nc = h.NetCon(rec_seg._ref_v, None, sec=rec_sec)\n            self.nc.threshold = self.threshold\n            self.tspk_vec = h.Vector()\n            self.nc.record(self.tspk_vec)\n\n        print(f\"Injection location: {inj_seg}\")\n        print(f\"Recording: {rec_seg}._ref_v\")\n\n    def execute(self) -&gt; Tuple[list, list]:\n        \"\"\"\n        Run the current clamp simulation and return recorded data.\n\n        This method:\n        1. Sets up the simulation duration\n        2. Initializes and runs the NEURON simulation\n        3. Converts recorded vectors to Python lists\n\n        Returns:\n        --------\n        tuple\n            (time_vector, voltage_vector) where:\n            - time_vector: List of time points (ms)\n            - voltage_vector: List of membrane potentials (mV) at those time points\n        \"\"\"\n        print(\"Current clamp simulation running...\")\n        h.tstop = self.tstop\n        h.stdinit()\n        h.run()\n\n        if self.threshold is not None:\n            self.nspks = len(self.tspk_vec)\n            print()\n            print(f\"Number of spikes: {self.nspks:d}\")\n            print()\n        return self.t_vec.to_python(), self.v_vec.to_python()\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.CurrentClamp.__init__","title":"<code>__init__(template_name, post_init_function=None, record_sec='soma', record_loc=0.5, threshold=None, inj_sec='soma', inj_loc=0.5, inj_amp=100.0, inj_delay=100.0, inj_dur=1000.0, tstop=1000.0)</code>","text":"<p>Initialize a current clamp simulation environment.</p> Parameters: <p>template_name : str or callable     Either the name of the cell template located in HOC or     a function that creates and returns a cell object. post_init_function : str, optional     Function of the cell to be called after initialization. record_sec : str, int, or tuple, optional     Section to record from. Can be:     - str: Section name (defaults to index 0 if multiple sections)     - int: Index into the 'all' section list     - tuple: (section_name, index) for accessing indexed sections     Default is 'soma'. record_loc : float, optional     Location (0-1) within section to record from. Default is 0.5. threshold : float, optional     Spike threshold (mV). If specified, spikes are detected and counted. inj_sec : str, int, or tuple, optional     Section for current injection. Same format as record_sec. Default is 'soma'. inj_loc : float, optional     Location (0-1) within section for current injection. Default is 0.5. inj_amp : float, optional     Current injection amplitude (pA). Default is 100.0. inj_delay : float, optional     Start time for current injection (ms). Default is 100.0. inj_dur : float, optional     Duration of current injection (ms). Default is 1000.0. tstop : float, optional     Total simulation time (ms). Default is 1000.0.     Will be extended if necessary to include the full current injection.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def __init__(\n    self,\n    template_name,\n    post_init_function=None,\n    record_sec=\"soma\",\n    record_loc=0.5,\n    threshold=None,\n    inj_sec=\"soma\",\n    inj_loc=0.5,\n    inj_amp=100.0,\n    inj_delay=100.0,\n    inj_dur=1000.0,\n    tstop=1000.0,\n):\n    \"\"\"\n    Initialize a current clamp simulation environment.\n\n    Parameters:\n    -----------\n    template_name : str or callable\n        Either the name of the cell template located in HOC or\n        a function that creates and returns a cell object.\n    post_init_function : str, optional\n        Function of the cell to be called after initialization.\n    record_sec : str, int, or tuple, optional\n        Section to record from. Can be:\n        - str: Section name (defaults to index 0 if multiple sections)\n        - int: Index into the 'all' section list\n        - tuple: (section_name, index) for accessing indexed sections\n        Default is 'soma'.\n    record_loc : float, optional\n        Location (0-1) within section to record from. Default is 0.5.\n    threshold : float, optional\n        Spike threshold (mV). If specified, spikes are detected and counted.\n    inj_sec : str, int, or tuple, optional\n        Section for current injection. Same format as record_sec. Default is 'soma'.\n    inj_loc : float, optional\n        Location (0-1) within section for current injection. Default is 0.5.\n    inj_amp : float, optional\n        Current injection amplitude (pA). Default is 100.0.\n    inj_delay : float, optional\n        Start time for current injection (ms). Default is 100.0.\n    inj_dur : float, optional\n        Duration of current injection (ms). Default is 1000.0.\n    tstop : float, optional\n        Total simulation time (ms). Default is 1000.0.\n        Will be extended if necessary to include the full current injection.\n    \"\"\"\n    self.create_cell = (\n        getattr(h, template_name) if isinstance(template_name, str) else template_name\n    )\n    self.record_sec = record_sec\n    self.record_loc = record_loc\n    self.inj_sec = inj_sec\n    self.inj_loc = inj_loc\n    self.threshold = threshold\n\n    self.tstop = max(tstop, inj_delay + inj_dur)\n    self.inj_delay = inj_delay  # use x ms after start of inj to calculate r_in, etc\n    self.inj_dur = inj_dur\n    self.inj_amp = inj_amp * 1e-3  # pA to nA\n\n    # sometimes people may put a hoc object in for the template name\n    if callable(template_name):\n        self.cell = template_name()\n    else:\n        self.cell = self.create_cell()\n    if post_init_function:\n        eval(f\"self.cell.{post_init_function}\")\n\n    self.setup()\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.CurrentClamp.setup","title":"<code>setup()</code>","text":"<p>Set up the simulation environment for current clamp experiments.</p> <p>This method: 1. Creates the current clamp stimulus at the specified injection site 2. Sets up voltage recording at the specified recording site 3. Creates vectors to store time and voltage data</p> Notes: <p>Sets self.cell_src as the current clamp object that can be accessed later.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def setup(self):\n    \"\"\"\n    Set up the simulation environment for current clamp experiments.\n\n    This method:\n    1. Creates the current clamp stimulus at the specified injection site\n    2. Sets up voltage recording at the specified recording site\n    3. Creates vectors to store time and voltage data\n\n    Notes:\n    ------\n    Sets self.cell_src as the current clamp object that can be accessed later.\n    \"\"\"\n    inj_seg, _ = get_target_site(self.cell, self.inj_sec, self.inj_loc, \"injection\")\n    self.cell_src = h.IClamp(inj_seg)\n    self.cell_src.delay = self.inj_delay\n    self.cell_src.dur = self.inj_dur\n    self.cell_src.amp = self.inj_amp\n\n    rec_seg, rec_sec = get_target_site(self.cell, self.record_sec, self.record_loc, \"recording\")\n    self.v_vec = h.Vector()\n    self.v_vec.record(rec_seg._ref_v)\n\n    self.t_vec = h.Vector()\n    self.t_vec.record(h._ref_t)\n\n    if self.threshold is not None:\n        self.nc = h.NetCon(rec_seg._ref_v, None, sec=rec_sec)\n        self.nc.threshold = self.threshold\n        self.tspk_vec = h.Vector()\n        self.nc.record(self.tspk_vec)\n\n    print(f\"Injection location: {inj_seg}\")\n    print(f\"Recording: {rec_seg}._ref_v\")\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.CurrentClamp.execute","title":"<code>execute()</code>","text":"<p>Run the current clamp simulation and return recorded data.</p> <p>This method: 1. Sets up the simulation duration 2. Initializes and runs the NEURON simulation 3. Converts recorded vectors to Python lists</p> Returns: <p>tuple     (time_vector, voltage_vector) where:     - time_vector: List of time points (ms)     - voltage_vector: List of membrane potentials (mV) at those time points</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def execute(self) -&gt; Tuple[list, list]:\n    \"\"\"\n    Run the current clamp simulation and return recorded data.\n\n    This method:\n    1. Sets up the simulation duration\n    2. Initializes and runs the NEURON simulation\n    3. Converts recorded vectors to Python lists\n\n    Returns:\n    --------\n    tuple\n        (time_vector, voltage_vector) where:\n        - time_vector: List of time points (ms)\n        - voltage_vector: List of membrane potentials (mV) at those time points\n    \"\"\"\n    print(\"Current clamp simulation running...\")\n    h.tstop = self.tstop\n    h.stdinit()\n    h.run()\n\n    if self.threshold is not None:\n        self.nspks = len(self.tspk_vec)\n        print()\n        print(f\"Number of spikes: {self.nspks:d}\")\n        print()\n    return self.t_vec.to_python(), self.v_vec.to_python()\n</code></pre>"},{"location":"api/singlecell/#passive-properties","title":"Passive Properties","text":""},{"location":"api/singlecell/#bmtool.singlecell.Passive","title":"<code>bmtool.singlecell.Passive</code>","text":"<p>               Bases: <code>CurrentClamp</code></p> Source code in <code>bmtool/singlecell.py</code> <pre><code>class Passive(CurrentClamp):\n    def __init__(\n        self,\n        template_name,\n        inj_amp=-100.0,\n        inj_delay=200.0,\n        inj_dur=1000.0,\n        tstop=1200.0,\n        method=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize a passive membrane property simulation environment.\n\n        Parameters:\n        -----------\n        template_name : str or callable\n            Either the name of the cell template located in HOC or\n            a function that creates and returns a cell object.\n        inj_amp : float, optional\n            Current injection amplitude (pA). Default is -100.0 (negative to measure passive properties).\n        inj_delay : float, optional\n            Start time for current injection (ms). Default is 200.0.\n        inj_dur : float, optional\n            Duration of current injection (ms). Default is 1000.0.\n        tstop : float, optional\n            Total simulation time (ms). Default is 1200.0.\n        method : str, optional\n            Method to estimate membrane time constant:\n            - 'simple': Find the time to reach 0.632 of voltage change\n            - 'exp': Fit a single exponential curve\n            - 'exp2': Fit a double exponential curve\n            Default is None, which uses 'simple' when calculations are performed.\n        **kwargs :\n            Additional keyword arguments to pass to the parent CurrentClamp constructor.\n\n        Notes:\n        ------\n        This class is designed for measuring passive membrane properties including\n        input resistance and membrane time constant.\n\n        Raises:\n        -------\n        AssertionError\n            If inj_amp is zero (must be non-zero to measure passive properties).\n        \"\"\"\n        assert inj_amp != 0\n        super().__init__(\n            template_name=template_name,\n            tstop=tstop,\n            inj_amp=inj_amp,\n            inj_delay=inj_delay,\n            inj_dur=inj_dur,\n            **kwargs,\n        )\n        self.inj_stop = inj_delay + inj_dur\n        self.method = method\n        self.tau_methods = {\n            \"simple\": self.tau_simple,\n            \"exp2\": self.tau_double_exponential,\n            \"exp\": self.tau_single_exponential,\n        }\n\n    def tau_simple(self):\n        \"\"\"\n        Calculate membrane time constant using the simple 0.632 criterion method.\n\n        This method calculates the membrane time constant by finding the time it takes\n        for the membrane potential to reach 63.2% (1-1/e) of its final value after\n        a step current injection.\n\n        Returns:\n        --------\n        callable\n            A function that prints the calculation details when called.\n\n        Notes:\n        ------\n        Sets the following attributes:\n        - tau: The calculated membrane time constant in ms\n        \"\"\"\n        v_t_const = self.cell_v_final - self.v_diff / np.e\n        index_v_tau = next(x for x, val in enumerate(self.v_vec_inj) if val &lt;= v_t_const)\n        self.tau = self.t_vec[self.index_v_rest + index_v_tau] - self.v_rest_time  # ms\n\n        def print_calc():\n            print()\n            print(\"Tau Calculation: time until 63.2% of dV\")\n            print(\"v_rest + 0.632*(v_final-v_rest)\")\n            print(\n                f\"{self.v_rest:.2f} + 0.632*({self.cell_v_final:.2f}-({self.v_rest:.2f})) = {v_t_const:.2f} (mV)\"\n            )\n            print(f\"Time where V = {v_t_const:.2f} (mV) is {self.v_rest_time + self.tau:.2f} (ms)\")\n            print(f\"{self.v_rest_time + self.tau:.2f} - {self.v_rest_time:g} = {self.tau:.2f} (ms)\")\n            print()\n\n        return print_calc\n\n    @staticmethod\n    def single_exponential(t, a0, a, tau):\n        \"\"\"\n        Single exponential function for fitting membrane potential response.\n\n        Parameters:\n        -----------\n        t : array-like\n            Time values\n        a0 : float\n            Offset (steady-state) value\n        a : float\n            Amplitude of the exponential component\n        tau : float\n            Time constant of the exponential decay\n\n        Returns:\n        --------\n        array-like\n            Function values at the given time points\n        \"\"\"\n        return a0 + a * np.exp(-t / tau)\n\n    def tau_single_exponential(self):\n        \"\"\"\n        Calculate membrane time constant by fitting a single exponential curve.\n\n        This method:\n        1. Identifies the peak response (for sag characterization)\n        2. Falls back to simple method for initial estimate\n        3. Fits a single exponential function to the membrane potential response\n        4. Sets tau to the exponential time constant\n\n        Returns:\n        --------\n        callable\n            A function that prints the calculation details when called.\n\n        Notes:\n        ------\n        Sets the following attributes:\n        - tau: The calculated membrane time constant in ms\n        - t_peak, v_peak: Time and voltage of peak response\n        - v_sag: Sag potential (difference between peak and steady-state)\n        - v_max_diff: Maximum potential difference from rest\n        - sag_norm: Normalized sag ratio\n        - popt: Optimized parameters from curve fitting\n        - pcov: Covariance matrix of the optimization\n        \"\"\"\n        index_v_peak = (np.sign(self.inj_amp) * self.v_vec_inj).argmax()\n        self.t_peak = self.t_vec_inj[index_v_peak]\n        self.v_peak = self.v_vec_inj[index_v_peak]\n        self.v_sag = self.v_peak - self.cell_v_final\n        self.v_max_diff = self.v_diff + self.v_sag\n        self.sag_norm = self.v_sag / self.v_max_diff\n\n        self.tau_simple()\n\n        p0 = (self.v_diff, self.tau)  # initial estimate\n        v0 = self.v_rest\n\n        def fit_func(t, a, tau):\n            return self.single_exponential(t, a0=v0 - a, a=a, tau=tau)\n\n        bounds = ((-np.inf, 1e-3), np.inf)\n        popt, self.pcov = curve_fit(\n            fit_func, self.t_vec_inj, self.v_vec_inj, p0=p0, bounds=bounds, maxfev=10000\n        )\n        self.popt = np.insert(popt, 0, v0 - popt[0])\n        self.tau = self.popt[2]\n\n        def print_calc():\n            print()\n            print(\n                \"Tau Calculation: Fit a single exponential curve to the membrane potential response\"\n            )\n            print(\"f(t) = a0 + a*exp(-t/tau)\")\n            print(\n                f\"Fit parameters: (a0, a, tau) = ({self.popt[0]:.2f}, {self.popt[1]:.2f}, {self.popt[2]:.2f})\"\n            )\n            print(\n                f\"Membrane time constant is determined from the exponential term: {self.tau:.2f} (ms)\"\n            )\n            print()\n            print(\"Sag potential: v_sag = v_peak - v_final = %.2f (mV)\" % self.v_sag)\n            print(\"Normalized sag potential: v_sag / (v_peak - v_rest) = %.3f\" % self.sag_norm)\n            print()\n\n        return print_calc\n\n    @staticmethod\n    def double_exponential(t, a0, a1, a2, tau1, tau2):\n        \"\"\"\n        Double exponential function for fitting membrane potential response.\n\n        This function is particularly useful for modeling cells with sag responses,\n        where the membrane potential shows two distinct time constants.\n\n        Parameters:\n        -----------\n        t : array-like\n            Time values\n        a0 : float\n            Offset (steady-state) value\n        a1 : float\n            Amplitude of the first exponential component\n        a2 : float\n            Amplitude of the second exponential component\n        tau1 : float\n            Time constant of the first exponential component\n        tau2 : float\n            Time constant of the second exponential component\n\n        Returns:\n        --------\n        array-like\n            Function values at the given time points\n        \"\"\"\n        return a0 + a1 * np.exp(-t / tau1) + a2 * np.exp(-t / tau2)\n\n    def tau_double_exponential(self):\n        \"\"\"\n        Calculate membrane time constant by fitting a double exponential curve.\n\n        This method is useful for cells with sag responses that cannot be\n        fitted well with a single exponential.\n\n        Returns:\n        --------\n        callable\n            A function that prints the calculation details when called.\n\n        Notes:\n        ------\n        Sets the following attributes:\n        - tau: The calculated membrane time constant (the slower of the two time constants)\n        - t_peak, v_peak: Time and voltage of peak response\n        - v_sag: Sag potential (difference between peak and steady-state)\n        - v_max_diff: Maximum potential difference from rest\n        - sag_norm: Normalized sag ratio\n        - popt: Optimized parameters from curve fitting\n        - pcov: Covariance matrix of the optimization\n        \"\"\"\n        index_v_peak = (np.sign(self.inj_amp) * self.v_vec_inj).argmax()\n        self.t_peak = self.t_vec_inj[index_v_peak]\n        self.v_peak = self.v_vec_inj[index_v_peak]\n        self.v_sag = self.v_peak - self.cell_v_final\n        self.v_max_diff = self.v_diff + self.v_sag\n        self.sag_norm = self.v_sag / self.v_max_diff\n\n        self.tau_simple()\n        p0 = (self.v_sag, -self.v_max_diff, self.t_peak, self.tau)  # initial estimate\n        v0 = self.v_rest\n\n        def fit_func(t, a1, a2, tau1, tau2):\n            return self.double_exponential(t, v0 - a1 - a2, a1, a2, tau1, tau2)\n\n        bounds = ((-np.inf, -np.inf, 1e-3, 1e-3), np.inf)\n        popt, self.pcov = curve_fit(\n            fit_func, self.t_vec_inj, self.v_vec_inj, p0=p0, bounds=bounds, maxfev=10000\n        )\n        self.popt = np.insert(popt, 0, v0 - sum(popt[:2]))\n        self.tau = max(self.popt[-2:])\n\n        def print_calc():\n            print()\n            print(\n                \"Tau Calculation: Fit a double exponential curve to the membrane potential response\"\n            )\n            print(\"f(t) = a0 + a1*exp(-t/tau1) + a2*exp(-t/tau2)\")\n            print(\"Constrained by initial value: f(0) = a0 + a1 + a2 = v_rest\")\n            print(\n                \"Fit parameters: (a0, a1, a2, tau1, tau2) = (\"\n                + \", \".join(f\"{x:.2f}\" for x in self.popt)\n                + \")\"\n            )\n            print(\n                f\"Membrane time constant is determined from the slowest exponential term: {self.tau:.2f} (ms)\"\n            )\n            print()\n            print(\"Sag potential: v_sag = v_peak - v_final = %.2f (mV)\" % self.v_sag)\n            print(\"Normalized sag potential: v_sag / (v_peak - v_rest) = %.3f\" % self.sag_norm)\n            print()\n\n        return print_calc\n\n    def double_exponential_fit(self):\n        \"\"\"\n        Get the double exponential fit values for plotting.\n\n        Returns:\n        --------\n        tuple\n            (time_vector, fitted_values) where:\n            - time_vector: Time points starting from rest time\n            - fitted_values: Membrane potential values predicted by the double exponential function\n        \"\"\"\n        t_vec = self.v_rest_time + self.t_vec_inj\n        v_fit = self.double_exponential(self.t_vec_inj, *self.popt)\n        return t_vec, v_fit\n\n    def single_exponential_fit(self):\n        \"\"\"\n        Get the single exponential fit values for plotting.\n\n        Returns:\n        --------\n        tuple\n            (time_vector, fitted_values) where:\n            - time_vector: Time points starting from rest time\n            - fitted_values: Membrane potential values predicted by the single exponential function\n        \"\"\"\n        t_vec = self.v_rest_time + self.t_vec_inj\n        v_fit = self.single_exponential(self.t_vec_inj, *self.popt)\n        return t_vec, v_fit\n\n    def execute(self):\n        \"\"\"\n        Run the simulation and calculate passive membrane properties.\n\n        This method:\n        1. Runs the NEURON simulation\n        2. Extracts membrane potential at rest and steady-state\n        3. Calculates input resistance from the step response\n        4. Calculates membrane time constant using the specified method\n        5. Prints detailed calculations for educational purposes\n\n        Returns:\n        --------\n        tuple\n            (time_vector, voltage_vector) from the simulation\n\n        Notes:\n        ------\n        Sets several attributes including:\n        - v_rest: Resting membrane potential\n        - r_in: Input resistance in MOhms\n        - tau: Membrane time constant in ms\n        \"\"\"\n        print(\"Running simulation for passive properties...\")\n        h.tstop = self.tstop\n        h.stdinit()\n        h.run()\n\n        self.index_v_rest = int(self.inj_delay / h.dt)\n        self.index_v_final = int(self.inj_stop / h.dt)\n        self.v_rest = self.v_vec[self.index_v_rest]\n        self.v_rest_time = self.t_vec[self.index_v_rest]\n        self.cell_v_final = self.v_vec[self.index_v_final]\n        self.v_final_time = self.t_vec[self.index_v_final]\n\n        t_idx = slice(self.index_v_rest, self.index_v_final + 1)\n        self.v_vec_inj = np.array(self.v_vec)[t_idx]\n        self.t_vec_inj = np.array(self.t_vec)[t_idx] - self.v_rest_time\n\n        self.v_diff = self.cell_v_final - self.v_rest\n        self.r_in = self.v_diff / self.inj_amp  # MegaOhms\n\n        print_calc = self.tau_methods.get(self.method, self.tau_simple)()\n\n        print()\n        print(f\"V Rest: {self.v_rest:.2f} (mV)\")\n        print(f\"Resistance: {self.r_in:.2f} (MOhms)\")\n        print(f\"Membrane time constant: {self.tau:.2f} (ms)\")\n        print()\n        print(f\"V_rest Calculation: Voltage taken at time {self.v_rest_time:.1f} (ms) is\")\n        print(f\"{self.v_rest:.2f} (mV)\")\n        print()\n        print(\"R_in Calculation: dV/dI = (v_final-v_rest)/(i_final-i_start)\")\n        print(f\"({self.cell_v_final:.2f} - ({self.v_rest:.2f})) / ({self.inj_amp:g} - 0)\")\n        print(\n            f\"{np.sign(self.inj_amp) * self.v_diff:.2f} (mV) / {np.abs(self.inj_amp)} (nA) = {self.r_in:.2f} (MOhms)\"\n        )\n        print_calc()\n\n        return self.t_vec.to_python(), self.v_vec.to_python()\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.__init__","title":"<code>__init__(template_name, inj_amp=-100.0, inj_delay=200.0, inj_dur=1000.0, tstop=1200.0, method=None, **kwargs)</code>","text":"<p>Initialize a passive membrane property simulation environment.</p> Parameters: <p>template_name : str or callable     Either the name of the cell template located in HOC or     a function that creates and returns a cell object. inj_amp : float, optional     Current injection amplitude (pA). Default is -100.0 (negative to measure passive properties). inj_delay : float, optional     Start time for current injection (ms). Default is 200.0. inj_dur : float, optional     Duration of current injection (ms). Default is 1000.0. tstop : float, optional     Total simulation time (ms). Default is 1200.0. method : str, optional     Method to estimate membrane time constant:     - 'simple': Find the time to reach 0.632 of voltage change     - 'exp': Fit a single exponential curve     - 'exp2': Fit a double exponential curve     Default is None, which uses 'simple' when calculations are performed. **kwargs :     Additional keyword arguments to pass to the parent CurrentClamp constructor.</p> Notes: <p>This class is designed for measuring passive membrane properties including input resistance and membrane time constant.</p> Raises: <p>AssertionError     If inj_amp is zero (must be non-zero to measure passive properties).</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def __init__(\n    self,\n    template_name,\n    inj_amp=-100.0,\n    inj_delay=200.0,\n    inj_dur=1000.0,\n    tstop=1200.0,\n    method=None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize a passive membrane property simulation environment.\n\n    Parameters:\n    -----------\n    template_name : str or callable\n        Either the name of the cell template located in HOC or\n        a function that creates and returns a cell object.\n    inj_amp : float, optional\n        Current injection amplitude (pA). Default is -100.0 (negative to measure passive properties).\n    inj_delay : float, optional\n        Start time for current injection (ms). Default is 200.0.\n    inj_dur : float, optional\n        Duration of current injection (ms). Default is 1000.0.\n    tstop : float, optional\n        Total simulation time (ms). Default is 1200.0.\n    method : str, optional\n        Method to estimate membrane time constant:\n        - 'simple': Find the time to reach 0.632 of voltage change\n        - 'exp': Fit a single exponential curve\n        - 'exp2': Fit a double exponential curve\n        Default is None, which uses 'simple' when calculations are performed.\n    **kwargs :\n        Additional keyword arguments to pass to the parent CurrentClamp constructor.\n\n    Notes:\n    ------\n    This class is designed for measuring passive membrane properties including\n    input resistance and membrane time constant.\n\n    Raises:\n    -------\n    AssertionError\n        If inj_amp is zero (must be non-zero to measure passive properties).\n    \"\"\"\n    assert inj_amp != 0\n    super().__init__(\n        template_name=template_name,\n        tstop=tstop,\n        inj_amp=inj_amp,\n        inj_delay=inj_delay,\n        inj_dur=inj_dur,\n        **kwargs,\n    )\n    self.inj_stop = inj_delay + inj_dur\n    self.method = method\n    self.tau_methods = {\n        \"simple\": self.tau_simple,\n        \"exp2\": self.tau_double_exponential,\n        \"exp\": self.tau_single_exponential,\n    }\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.tau_simple","title":"<code>tau_simple()</code>","text":"<p>Calculate membrane time constant using the simple 0.632 criterion method.</p> <p>This method calculates the membrane time constant by finding the time it takes for the membrane potential to reach 63.2% (1-1/e) of its final value after a step current injection.</p> Returns: <p>callable     A function that prints the calculation details when called.</p> Notes: <p>Sets the following attributes: - tau: The calculated membrane time constant in ms</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def tau_simple(self):\n    \"\"\"\n    Calculate membrane time constant using the simple 0.632 criterion method.\n\n    This method calculates the membrane time constant by finding the time it takes\n    for the membrane potential to reach 63.2% (1-1/e) of its final value after\n    a step current injection.\n\n    Returns:\n    --------\n    callable\n        A function that prints the calculation details when called.\n\n    Notes:\n    ------\n    Sets the following attributes:\n    - tau: The calculated membrane time constant in ms\n    \"\"\"\n    v_t_const = self.cell_v_final - self.v_diff / np.e\n    index_v_tau = next(x for x, val in enumerate(self.v_vec_inj) if val &lt;= v_t_const)\n    self.tau = self.t_vec[self.index_v_rest + index_v_tau] - self.v_rest_time  # ms\n\n    def print_calc():\n        print()\n        print(\"Tau Calculation: time until 63.2% of dV\")\n        print(\"v_rest + 0.632*(v_final-v_rest)\")\n        print(\n            f\"{self.v_rest:.2f} + 0.632*({self.cell_v_final:.2f}-({self.v_rest:.2f})) = {v_t_const:.2f} (mV)\"\n        )\n        print(f\"Time where V = {v_t_const:.2f} (mV) is {self.v_rest_time + self.tau:.2f} (ms)\")\n        print(f\"{self.v_rest_time + self.tau:.2f} - {self.v_rest_time:g} = {self.tau:.2f} (ms)\")\n        print()\n\n    return print_calc\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.tau_single_exponential","title":"<code>tau_single_exponential()</code>","text":"<p>Calculate membrane time constant by fitting a single exponential curve.</p> <p>This method: 1. Identifies the peak response (for sag characterization) 2. Falls back to simple method for initial estimate 3. Fits a single exponential function to the membrane potential response 4. Sets tau to the exponential time constant</p> Returns: <p>callable     A function that prints the calculation details when called.</p> Notes: <p>Sets the following attributes: - tau: The calculated membrane time constant in ms - t_peak, v_peak: Time and voltage of peak response - v_sag: Sag potential (difference between peak and steady-state) - v_max_diff: Maximum potential difference from rest - sag_norm: Normalized sag ratio - popt: Optimized parameters from curve fitting - pcov: Covariance matrix of the optimization</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def tau_single_exponential(self):\n    \"\"\"\n    Calculate membrane time constant by fitting a single exponential curve.\n\n    This method:\n    1. Identifies the peak response (for sag characterization)\n    2. Falls back to simple method for initial estimate\n    3. Fits a single exponential function to the membrane potential response\n    4. Sets tau to the exponential time constant\n\n    Returns:\n    --------\n    callable\n        A function that prints the calculation details when called.\n\n    Notes:\n    ------\n    Sets the following attributes:\n    - tau: The calculated membrane time constant in ms\n    - t_peak, v_peak: Time and voltage of peak response\n    - v_sag: Sag potential (difference between peak and steady-state)\n    - v_max_diff: Maximum potential difference from rest\n    - sag_norm: Normalized sag ratio\n    - popt: Optimized parameters from curve fitting\n    - pcov: Covariance matrix of the optimization\n    \"\"\"\n    index_v_peak = (np.sign(self.inj_amp) * self.v_vec_inj).argmax()\n    self.t_peak = self.t_vec_inj[index_v_peak]\n    self.v_peak = self.v_vec_inj[index_v_peak]\n    self.v_sag = self.v_peak - self.cell_v_final\n    self.v_max_diff = self.v_diff + self.v_sag\n    self.sag_norm = self.v_sag / self.v_max_diff\n\n    self.tau_simple()\n\n    p0 = (self.v_diff, self.tau)  # initial estimate\n    v0 = self.v_rest\n\n    def fit_func(t, a, tau):\n        return self.single_exponential(t, a0=v0 - a, a=a, tau=tau)\n\n    bounds = ((-np.inf, 1e-3), np.inf)\n    popt, self.pcov = curve_fit(\n        fit_func, self.t_vec_inj, self.v_vec_inj, p0=p0, bounds=bounds, maxfev=10000\n    )\n    self.popt = np.insert(popt, 0, v0 - popt[0])\n    self.tau = self.popt[2]\n\n    def print_calc():\n        print()\n        print(\n            \"Tau Calculation: Fit a single exponential curve to the membrane potential response\"\n        )\n        print(\"f(t) = a0 + a*exp(-t/tau)\")\n        print(\n            f\"Fit parameters: (a0, a, tau) = ({self.popt[0]:.2f}, {self.popt[1]:.2f}, {self.popt[2]:.2f})\"\n        )\n        print(\n            f\"Membrane time constant is determined from the exponential term: {self.tau:.2f} (ms)\"\n        )\n        print()\n        print(\"Sag potential: v_sag = v_peak - v_final = %.2f (mV)\" % self.v_sag)\n        print(\"Normalized sag potential: v_sag / (v_peak - v_rest) = %.3f\" % self.sag_norm)\n        print()\n\n    return print_calc\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.tau_double_exponential","title":"<code>tau_double_exponential()</code>","text":"<p>Calculate membrane time constant by fitting a double exponential curve.</p> <p>This method is useful for cells with sag responses that cannot be fitted well with a single exponential.</p> Returns: <p>callable     A function that prints the calculation details when called.</p> Notes: <p>Sets the following attributes: - tau: The calculated membrane time constant (the slower of the two time constants) - t_peak, v_peak: Time and voltage of peak response - v_sag: Sag potential (difference between peak and steady-state) - v_max_diff: Maximum potential difference from rest - sag_norm: Normalized sag ratio - popt: Optimized parameters from curve fitting - pcov: Covariance matrix of the optimization</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def tau_double_exponential(self):\n    \"\"\"\n    Calculate membrane time constant by fitting a double exponential curve.\n\n    This method is useful for cells with sag responses that cannot be\n    fitted well with a single exponential.\n\n    Returns:\n    --------\n    callable\n        A function that prints the calculation details when called.\n\n    Notes:\n    ------\n    Sets the following attributes:\n    - tau: The calculated membrane time constant (the slower of the two time constants)\n    - t_peak, v_peak: Time and voltage of peak response\n    - v_sag: Sag potential (difference between peak and steady-state)\n    - v_max_diff: Maximum potential difference from rest\n    - sag_norm: Normalized sag ratio\n    - popt: Optimized parameters from curve fitting\n    - pcov: Covariance matrix of the optimization\n    \"\"\"\n    index_v_peak = (np.sign(self.inj_amp) * self.v_vec_inj).argmax()\n    self.t_peak = self.t_vec_inj[index_v_peak]\n    self.v_peak = self.v_vec_inj[index_v_peak]\n    self.v_sag = self.v_peak - self.cell_v_final\n    self.v_max_diff = self.v_diff + self.v_sag\n    self.sag_norm = self.v_sag / self.v_max_diff\n\n    self.tau_simple()\n    p0 = (self.v_sag, -self.v_max_diff, self.t_peak, self.tau)  # initial estimate\n    v0 = self.v_rest\n\n    def fit_func(t, a1, a2, tau1, tau2):\n        return self.double_exponential(t, v0 - a1 - a2, a1, a2, tau1, tau2)\n\n    bounds = ((-np.inf, -np.inf, 1e-3, 1e-3), np.inf)\n    popt, self.pcov = curve_fit(\n        fit_func, self.t_vec_inj, self.v_vec_inj, p0=p0, bounds=bounds, maxfev=10000\n    )\n    self.popt = np.insert(popt, 0, v0 - sum(popt[:2]))\n    self.tau = max(self.popt[-2:])\n\n    def print_calc():\n        print()\n        print(\n            \"Tau Calculation: Fit a double exponential curve to the membrane potential response\"\n        )\n        print(\"f(t) = a0 + a1*exp(-t/tau1) + a2*exp(-t/tau2)\")\n        print(\"Constrained by initial value: f(0) = a0 + a1 + a2 = v_rest\")\n        print(\n            \"Fit parameters: (a0, a1, a2, tau1, tau2) = (\"\n            + \", \".join(f\"{x:.2f}\" for x in self.popt)\n            + \")\"\n        )\n        print(\n            f\"Membrane time constant is determined from the slowest exponential term: {self.tau:.2f} (ms)\"\n        )\n        print()\n        print(\"Sag potential: v_sag = v_peak - v_final = %.2f (mV)\" % self.v_sag)\n        print(\"Normalized sag potential: v_sag / (v_peak - v_rest) = %.3f\" % self.sag_norm)\n        print()\n\n    return print_calc\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.double_exponential_fit","title":"<code>double_exponential_fit()</code>","text":"<p>Get the double exponential fit values for plotting.</p> Returns: <p>tuple     (time_vector, fitted_values) where:     - time_vector: Time points starting from rest time     - fitted_values: Membrane potential values predicted by the double exponential function</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def double_exponential_fit(self):\n    \"\"\"\n    Get the double exponential fit values for plotting.\n\n    Returns:\n    --------\n    tuple\n        (time_vector, fitted_values) where:\n        - time_vector: Time points starting from rest time\n        - fitted_values: Membrane potential values predicted by the double exponential function\n    \"\"\"\n    t_vec = self.v_rest_time + self.t_vec_inj\n    v_fit = self.double_exponential(self.t_vec_inj, *self.popt)\n    return t_vec, v_fit\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.single_exponential_fit","title":"<code>single_exponential_fit()</code>","text":"<p>Get the single exponential fit values for plotting.</p> Returns: <p>tuple     (time_vector, fitted_values) where:     - time_vector: Time points starting from rest time     - fitted_values: Membrane potential values predicted by the single exponential function</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def single_exponential_fit(self):\n    \"\"\"\n    Get the single exponential fit values for plotting.\n\n    Returns:\n    --------\n    tuple\n        (time_vector, fitted_values) where:\n        - time_vector: Time points starting from rest time\n        - fitted_values: Membrane potential values predicted by the single exponential function\n    \"\"\"\n    t_vec = self.v_rest_time + self.t_vec_inj\n    v_fit = self.single_exponential(self.t_vec_inj, *self.popt)\n    return t_vec, v_fit\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.execute","title":"<code>execute()</code>","text":"<p>Run the simulation and calculate passive membrane properties.</p> <p>This method: 1. Runs the NEURON simulation 2. Extracts membrane potential at rest and steady-state 3. Calculates input resistance from the step response 4. Calculates membrane time constant using the specified method 5. Prints detailed calculations for educational purposes</p> Returns: <p>tuple     (time_vector, voltage_vector) from the simulation</p> Notes: <p>Sets several attributes including: - v_rest: Resting membrane potential - r_in: Input resistance in MOhms - tau: Membrane time constant in ms</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def execute(self):\n    \"\"\"\n    Run the simulation and calculate passive membrane properties.\n\n    This method:\n    1. Runs the NEURON simulation\n    2. Extracts membrane potential at rest and steady-state\n    3. Calculates input resistance from the step response\n    4. Calculates membrane time constant using the specified method\n    5. Prints detailed calculations for educational purposes\n\n    Returns:\n    --------\n    tuple\n        (time_vector, voltage_vector) from the simulation\n\n    Notes:\n    ------\n    Sets several attributes including:\n    - v_rest: Resting membrane potential\n    - r_in: Input resistance in MOhms\n    - tau: Membrane time constant in ms\n    \"\"\"\n    print(\"Running simulation for passive properties...\")\n    h.tstop = self.tstop\n    h.stdinit()\n    h.run()\n\n    self.index_v_rest = int(self.inj_delay / h.dt)\n    self.index_v_final = int(self.inj_stop / h.dt)\n    self.v_rest = self.v_vec[self.index_v_rest]\n    self.v_rest_time = self.t_vec[self.index_v_rest]\n    self.cell_v_final = self.v_vec[self.index_v_final]\n    self.v_final_time = self.t_vec[self.index_v_final]\n\n    t_idx = slice(self.index_v_rest, self.index_v_final + 1)\n    self.v_vec_inj = np.array(self.v_vec)[t_idx]\n    self.t_vec_inj = np.array(self.t_vec)[t_idx] - self.v_rest_time\n\n    self.v_diff = self.cell_v_final - self.v_rest\n    self.r_in = self.v_diff / self.inj_amp  # MegaOhms\n\n    print_calc = self.tau_methods.get(self.method, self.tau_simple)()\n\n    print()\n    print(f\"V Rest: {self.v_rest:.2f} (mV)\")\n    print(f\"Resistance: {self.r_in:.2f} (MOhms)\")\n    print(f\"Membrane time constant: {self.tau:.2f} (ms)\")\n    print()\n    print(f\"V_rest Calculation: Voltage taken at time {self.v_rest_time:.1f} (ms) is\")\n    print(f\"{self.v_rest:.2f} (mV)\")\n    print()\n    print(\"R_in Calculation: dV/dI = (v_final-v_rest)/(i_final-i_start)\")\n    print(f\"({self.cell_v_final:.2f} - ({self.v_rest:.2f})) / ({self.inj_amp:g} - 0)\")\n    print(\n        f\"{np.sign(self.inj_amp) * self.v_diff:.2f} (mV) / {np.abs(self.inj_amp)} (nA) = {self.r_in:.2f} (MOhms)\"\n    )\n    print_calc()\n\n    return self.t_vec.to_python(), self.v_vec.to_python()\n</code></pre>"},{"location":"api/singlecell/#frequency-current-f-i-analysis","title":"Frequency-Current (F-I) Analysis","text":""},{"location":"api/singlecell/#bmtool.singlecell.FI","title":"<code>bmtool.singlecell.FI</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>bmtool/singlecell.py</code> <pre><code>class FI(object):\n    def __init__(\n        self,\n        template_name,\n        post_init_function=None,\n        i_start=0.0,\n        i_stop=1050.0,\n        i_increment=100.0,\n        tstart=50.0,\n        tdur=1000.0,\n        threshold=0.0,\n        record_sec=\"soma\",\n        record_loc=0.5,\n        inj_sec=\"soma\",\n        inj_loc=0.5,\n    ):\n        \"\"\"\n        Initialize a frequency-current (F-I) curve simulation environment.\n\n        Parameters:\n        -----------\n        template_name : str or callable\n            Either the name of the cell template located in HOC or\n            a function that creates and returns a cell object.\n        post_init_function : str, optional\n            Function of the cell to be called after initialization.\n        i_start : float, optional\n            Initial current injection amplitude (pA). Default is 0.0.\n        i_stop : float, optional\n            Maximum current injection amplitude (pA). Default is 1050.0.\n        i_increment : float, optional\n            Amplitude increment between trials (pA). Default is 100.0.\n        tstart : float, optional\n            Current injection start time (ms). Default is 50.0.\n        tdur : float, optional\n            Current injection duration (ms). Default is 1000.0.\n        threshold : float, optional\n            Spike threshold (mV). Default is 0.0.\n        record_sec : str, int, or tuple, optional\n            Section to record from. Same format as in CurrentClamp. Default is 'soma'.\n        record_loc : float, optional\n            Location (0-1) within section to record from. Default is 0.5.\n        inj_sec : str, int, or tuple, optional\n            Section for current injection. Same format as record_sec. Default is 'soma'.\n        inj_loc : float, optional\n            Location (0-1) within section for current injection. Default is 0.5.\n\n        Notes:\n        ------\n        This class creates multiple instances of the cell model, one for each\n        current amplitude to be tested, allowing all simulations to be run\n        in a single call to NEURON's run() function.\n        \"\"\"\n        self.create_cell = (\n            getattr(h, template_name) if isinstance(template_name, str) else template_name\n        )\n        self.post_init_function = post_init_function\n        self.i_start = i_start * 1e-3  # pA to nA\n        self.i_stop = i_stop * 1e-3\n        self.i_increment = i_increment * 1e-3\n        self.tstart = tstart\n        self.tdur = tdur\n        self.tstop = tstart + tdur\n        self.threshold = threshold\n\n        self.record_sec = record_sec\n        self.record_loc = record_loc\n        self.inj_sec = inj_sec\n        self.inj_loc = inj_loc\n\n        self.cells = []\n        self.sources = []\n        self.ncs = []\n        self.tspk_vecs = []\n        self.nspks = []\n\n        self.ntrials = int((self.i_stop - self.i_start) // self.i_increment + 1)\n        self.amps = (self.i_start + np.arange(self.ntrials) * self.i_increment).tolist()\n        for _ in range(self.ntrials):\n            # Cell definition\n            cell = self.create_cell()\n            if post_init_function:\n                eval(f\"cell.{post_init_function}\")\n            self.cells.append(cell)\n\n        self.setup()\n\n    def setup(self):\n        \"\"\"\n        Set up the simulation environment for frequency-current (F-I) analysis.\n\n        For each current amplitude to be tested, this method:\n        1. Creates a current source at the injection site\n        2. Sets up spike detection at the recording site\n        3. Creates vectors to record spike times\n\n        Notes:\n        ------\n        This preparation allows multiple simulations to be run with different\n        current amplitudes in a single call to h.run().\n        \"\"\"\n        for cell, amp in zip(self.cells, self.amps):\n            inj_seg, _ = get_target_site(cell, self.inj_sec, self.inj_loc, \"injection\")\n            src = h.IClamp(inj_seg)\n            src.delay = self.tstart\n            src.dur = self.tdur\n            src.amp = amp\n            self.sources.append(src)\n\n            rec_seg, rec_sec = get_target_site(cell, self.record_sec, self.record_loc, \"recording\")\n            nc = h.NetCon(rec_seg._ref_v, None, sec=rec_sec)\n            nc.threshold = self.threshold\n            spvec = h.Vector()\n            nc.record(spvec)\n            self.ncs.append(nc)\n            self.tspk_vecs.append(spvec)\n\n        print(f\"Injection location: {inj_seg}\")\n        print(f\"Recording: {rec_seg}._ref_v\")\n\n    def execute(self):\n        \"\"\"\n        Run the simulation and count spikes for each current amplitude.\n\n        This method:\n        1. Initializes and runs a single NEURON simulation that evaluates all current amplitudes\n        2. Counts spikes for each current amplitude\n        3. Prints a summary of results in tabular format\n\n        Returns:\n        --------\n        tuple\n            (current_amplitudes, spike_counts) where:\n            - current_amplitudes: List of current injection amplitudes (pA)\n            - spike_counts: List of spike counts corresponding to each amplitude\n        \"\"\"\n        print(\"Running simulations for FI curve...\")\n        h.tstop = self.tstop\n        h.stdinit()\n        h.run()\n\n        self.nspks = [len(v) for v in self.tspk_vecs]\n        print()\n        print(\"Results\")\n        # lets make a df so the results line up nice\n        data = {\"Injection (pA):\": [amp * 1000 for amp in self.amps], \"number of spikes\": self.nspks}\n        df = pd.DataFrame(data)\n        print(df)\n        # print(f'Injection (pA): ' + ', '.join(f'{x:g}' for x in self.amps))\n        # print(f'Number of spikes: ' + ', '.join(f'{x:d}' for x in self.nspks))\n        print()\n\n        return [amp * 1000 for amp in self.amps], self.nspks\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.FI.__init__","title":"<code>__init__(template_name, post_init_function=None, i_start=0.0, i_stop=1050.0, i_increment=100.0, tstart=50.0, tdur=1000.0, threshold=0.0, record_sec='soma', record_loc=0.5, inj_sec='soma', inj_loc=0.5)</code>","text":"<p>Initialize a frequency-current (F-I) curve simulation environment.</p> Parameters: <p>template_name : str or callable     Either the name of the cell template located in HOC or     a function that creates and returns a cell object. post_init_function : str, optional     Function of the cell to be called after initialization. i_start : float, optional     Initial current injection amplitude (pA). Default is 0.0. i_stop : float, optional     Maximum current injection amplitude (pA). Default is 1050.0. i_increment : float, optional     Amplitude increment between trials (pA). Default is 100.0. tstart : float, optional     Current injection start time (ms). Default is 50.0. tdur : float, optional     Current injection duration (ms). Default is 1000.0. threshold : float, optional     Spike threshold (mV). Default is 0.0. record_sec : str, int, or tuple, optional     Section to record from. Same format as in CurrentClamp. Default is 'soma'. record_loc : float, optional     Location (0-1) within section to record from. Default is 0.5. inj_sec : str, int, or tuple, optional     Section for current injection. Same format as record_sec. Default is 'soma'. inj_loc : float, optional     Location (0-1) within section for current injection. Default is 0.5.</p> Notes: <p>This class creates multiple instances of the cell model, one for each current amplitude to be tested, allowing all simulations to be run in a single call to NEURON's run() function.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def __init__(\n    self,\n    template_name,\n    post_init_function=None,\n    i_start=0.0,\n    i_stop=1050.0,\n    i_increment=100.0,\n    tstart=50.0,\n    tdur=1000.0,\n    threshold=0.0,\n    record_sec=\"soma\",\n    record_loc=0.5,\n    inj_sec=\"soma\",\n    inj_loc=0.5,\n):\n    \"\"\"\n    Initialize a frequency-current (F-I) curve simulation environment.\n\n    Parameters:\n    -----------\n    template_name : str or callable\n        Either the name of the cell template located in HOC or\n        a function that creates and returns a cell object.\n    post_init_function : str, optional\n        Function of the cell to be called after initialization.\n    i_start : float, optional\n        Initial current injection amplitude (pA). Default is 0.0.\n    i_stop : float, optional\n        Maximum current injection amplitude (pA). Default is 1050.0.\n    i_increment : float, optional\n        Amplitude increment between trials (pA). Default is 100.0.\n    tstart : float, optional\n        Current injection start time (ms). Default is 50.0.\n    tdur : float, optional\n        Current injection duration (ms). Default is 1000.0.\n    threshold : float, optional\n        Spike threshold (mV). Default is 0.0.\n    record_sec : str, int, or tuple, optional\n        Section to record from. Same format as in CurrentClamp. Default is 'soma'.\n    record_loc : float, optional\n        Location (0-1) within section to record from. Default is 0.5.\n    inj_sec : str, int, or tuple, optional\n        Section for current injection. Same format as record_sec. Default is 'soma'.\n    inj_loc : float, optional\n        Location (0-1) within section for current injection. Default is 0.5.\n\n    Notes:\n    ------\n    This class creates multiple instances of the cell model, one for each\n    current amplitude to be tested, allowing all simulations to be run\n    in a single call to NEURON's run() function.\n    \"\"\"\n    self.create_cell = (\n        getattr(h, template_name) if isinstance(template_name, str) else template_name\n    )\n    self.post_init_function = post_init_function\n    self.i_start = i_start * 1e-3  # pA to nA\n    self.i_stop = i_stop * 1e-3\n    self.i_increment = i_increment * 1e-3\n    self.tstart = tstart\n    self.tdur = tdur\n    self.tstop = tstart + tdur\n    self.threshold = threshold\n\n    self.record_sec = record_sec\n    self.record_loc = record_loc\n    self.inj_sec = inj_sec\n    self.inj_loc = inj_loc\n\n    self.cells = []\n    self.sources = []\n    self.ncs = []\n    self.tspk_vecs = []\n    self.nspks = []\n\n    self.ntrials = int((self.i_stop - self.i_start) // self.i_increment + 1)\n    self.amps = (self.i_start + np.arange(self.ntrials) * self.i_increment).tolist()\n    for _ in range(self.ntrials):\n        # Cell definition\n        cell = self.create_cell()\n        if post_init_function:\n            eval(f\"cell.{post_init_function}\")\n        self.cells.append(cell)\n\n    self.setup()\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.FI.setup","title":"<code>setup()</code>","text":"<p>Set up the simulation environment for frequency-current (F-I) analysis.</p> <p>For each current amplitude to be tested, this method: 1. Creates a current source at the injection site 2. Sets up spike detection at the recording site 3. Creates vectors to record spike times</p> Notes: <p>This preparation allows multiple simulations to be run with different current amplitudes in a single call to h.run().</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def setup(self):\n    \"\"\"\n    Set up the simulation environment for frequency-current (F-I) analysis.\n\n    For each current amplitude to be tested, this method:\n    1. Creates a current source at the injection site\n    2. Sets up spike detection at the recording site\n    3. Creates vectors to record spike times\n\n    Notes:\n    ------\n    This preparation allows multiple simulations to be run with different\n    current amplitudes in a single call to h.run().\n    \"\"\"\n    for cell, amp in zip(self.cells, self.amps):\n        inj_seg, _ = get_target_site(cell, self.inj_sec, self.inj_loc, \"injection\")\n        src = h.IClamp(inj_seg)\n        src.delay = self.tstart\n        src.dur = self.tdur\n        src.amp = amp\n        self.sources.append(src)\n\n        rec_seg, rec_sec = get_target_site(cell, self.record_sec, self.record_loc, \"recording\")\n        nc = h.NetCon(rec_seg._ref_v, None, sec=rec_sec)\n        nc.threshold = self.threshold\n        spvec = h.Vector()\n        nc.record(spvec)\n        self.ncs.append(nc)\n        self.tspk_vecs.append(spvec)\n\n    print(f\"Injection location: {inj_seg}\")\n    print(f\"Recording: {rec_seg}._ref_v\")\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.FI.execute","title":"<code>execute()</code>","text":"<p>Run the simulation and count spikes for each current amplitude.</p> <p>This method: 1. Initializes and runs a single NEURON simulation that evaluates all current amplitudes 2. Counts spikes for each current amplitude 3. Prints a summary of results in tabular format</p> Returns: <p>tuple     (current_amplitudes, spike_counts) where:     - current_amplitudes: List of current injection amplitudes (pA)     - spike_counts: List of spike counts corresponding to each amplitude</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def execute(self):\n    \"\"\"\n    Run the simulation and count spikes for each current amplitude.\n\n    This method:\n    1. Initializes and runs a single NEURON simulation that evaluates all current amplitudes\n    2. Counts spikes for each current amplitude\n    3. Prints a summary of results in tabular format\n\n    Returns:\n    --------\n    tuple\n        (current_amplitudes, spike_counts) where:\n        - current_amplitudes: List of current injection amplitudes (pA)\n        - spike_counts: List of spike counts corresponding to each amplitude\n    \"\"\"\n    print(\"Running simulations for FI curve...\")\n    h.tstop = self.tstop\n    h.stdinit()\n    h.run()\n\n    self.nspks = [len(v) for v in self.tspk_vecs]\n    print()\n    print(\"Results\")\n    # lets make a df so the results line up nice\n    data = {\"Injection (pA):\": [amp * 1000 for amp in self.amps], \"number of spikes\": self.nspks}\n    df = pd.DataFrame(data)\n    print(df)\n    # print(f'Injection (pA): ' + ', '.join(f'{x:g}' for x in self.amps))\n    # print(f'Number of spikes: ' + ', '.join(f'{x:d}' for x in self.nspks))\n    print()\n\n    return [amp * 1000 for amp in self.amps], self.nspks\n</code></pre>"},{"location":"api/singlecell/#impedance-analysis","title":"Impedance Analysis","text":""},{"location":"api/singlecell/#bmtool.singlecell.ZAP","title":"<code>bmtool.singlecell.ZAP</code>","text":"<p>               Bases: <code>CurrentClamp</code></p> Source code in <code>bmtool/singlecell.py</code> <pre><code>class ZAP(CurrentClamp):\n    def __init__(\n        self,\n        template_name,\n        inj_amp=100.0,\n        inj_delay=200.0,\n        inj_dur=15000.0,\n        tstop=15500.0,\n        fstart=0.0,\n        fend=15.0,\n        chirp_type=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize a ZAP (impedance amplitude profile) simulation environment.\n\n        Parameters:\n        -----------\n        template_name : str or callable\n            Either the name of the cell template located in HOC or\n            a function that creates and returns a cell object.\n        inj_amp : float, optional\n            Current injection amplitude (pA). Default is 100.0.\n        inj_delay : float, optional\n            Start time for current injection (ms). Default is 200.0.\n        inj_dur : float, optional\n            Duration of current injection (ms). Default is 15000.0.\n        tstop : float, optional\n            Total simulation time (ms). Default is 15500.0.\n        fstart : float, optional\n            Starting frequency of the chirp current (Hz). Default is 0.0.\n        fend : float, optional\n            Ending frequency of the chirp current (Hz). Default is 15.0.\n        chirp_type : str, optional\n            Type of chirp current determining how frequency increases over time:\n            - 'linear': Linear increase in frequency (default if None)\n            - 'exponential': Exponential increase in frequency\n        **kwargs :\n            Additional keyword arguments to pass to the parent CurrentClamp constructor.\n\n        Notes:\n        ------\n        This class is designed for measuring the frequency-dependent impedance profile\n        of a neuron using a chirp current that sweeps through frequencies.\n\n        Raises:\n        -------\n        AssertionError\n            - If inj_amp is zero\n            - If chirp_type is 'exponential' and either fstart or fend is &lt;= 0\n        \"\"\"\n        assert inj_amp != 0\n        super().__init__(\n            template_name=template_name,\n            tstop=tstop,\n            inj_amp=inj_amp,\n            inj_delay=inj_delay,\n            inj_dur=inj_dur,\n            **kwargs,\n        )\n        self.inj_stop = inj_delay + inj_dur\n        self.fstart = fstart\n        self.fend = fend\n        self.chirp_type = chirp_type\n        self.chirp_func = {\"linear\": self.linear_chirp, \"exponential\": self.exponential_chirp}\n        if chirp_type == \"exponential\":\n            assert fstart &gt; 0 and fend &gt; 0\n\n    def linear_chirp(self, t, f0, f1):\n        \"\"\"\n        Generate a chirp current with linearly increasing frequency.\n\n        Parameters:\n        -----------\n        t : ndarray\n            Time vector (ms)\n        f0 : float\n            Start frequency (kHz)\n        f1 : float\n            End frequency (kHz)\n\n        Returns:\n        --------\n        ndarray\n            Current values with amplitude self.inj_amp and frequency\n            increasing linearly from f0 to f1 Hz over time t\n        \"\"\"\n        return self.inj_amp * np.sin(np.pi * (2 * f0 + (f1 - f0) / t[-1] * t) * t)\n\n    def exponential_chirp(self, t, f0, f1):\n        \"\"\"\n        Generate a chirp current with exponentially increasing frequency.\n\n        Parameters:\n        -----------\n        t : ndarray\n            Time vector (ms)\n        f0 : float\n            Start frequency (kHz), must be &gt; 0\n        f1 : float\n            End frequency (kHz), must be &gt; 0\n\n        Returns:\n        --------\n        ndarray\n            Current values with amplitude self.inj_amp and frequency\n            increasing exponentially from f0 to f1 Hz over time t\n\n        Notes:\n        ------\n        For exponential chirp, both f0 and f1 must be positive.\n        \"\"\"\n        L = np.log(f1 / f0) / t[-1]\n        return self.inj_amp * np.sin(np.pi * 2 * f0 / L * (np.exp(L * t) - 1))\n\n    def zap_current(self):\n        \"\"\"\n        Create a frequency-modulated (chirp) current for probing impedance.\n\n        This method:\n        1. Sets up time vectors for the simulation and current injection\n        2. Creates a chirp current based on the specified parameters (linear or exponential)\n        3. Prepares the current vector for NEURON playback\n\n        Notes:\n        ------\n        The chirp current increases in frequency from fstart to fend Hz over the duration\n        of the injection. This allows frequency-dependent impedance to be measured in\n        a single simulation.\n        \"\"\"\n        self.dt = dt = h.dt\n        self.index_v_rest = int(self.inj_delay / dt)\n        self.index_v_final = int(self.inj_stop / dt)\n\n        t = np.arange(int(self.tstop / dt) + 1) * dt\n        t_inj = t[: self.index_v_final - self.index_v_rest + 1]\n        f0 = self.fstart * 1e-3  # Hz to 1/ms\n        f1 = self.fend * 1e-3\n        chirp_func = self.chirp_func.get(self.chirp_type, self.linear_chirp)\n        self.zap_vec_inj = chirp_func(t_inj, f0, f1)\n        i_inj = np.zeros_like(t)\n        i_inj[self.index_v_rest : self.index_v_final + 1] = self.zap_vec_inj\n\n        self.zap_vec = h.Vector()\n        self.zap_vec.from_python(i_inj)\n        self.zap_vec.play(self.cell_src._ref_amp, dt)\n\n    def get_impedance(self, smooth=1):\n        \"\"\"\n        Calculate and extract the frequency-dependent impedance profile.\n\n        This method:\n        1. Filters the impedance to the frequency range of interest\n        2. Optionally applies smoothing to reduce noise\n        3. Identifies the resonant frequency (peak impedance)\n\n        Parameters:\n        -----------\n        smooth : int, optional\n            Window size for smoothing the impedance. Default is 1 (no smoothing).\n\n        Returns:\n        --------\n        tuple\n            (frequencies, impedance_values) in the range of interest\n\n        Notes:\n        ------\n        Sets self.peak_freq to the resonant frequency (frequency of maximum impedance).\n        \"\"\"\n        f_idx = (self.freq &gt; min(self.fstart, self.fend)) &amp; (\n            self.freq &lt; max(self.fstart, self.fend)\n        )\n        impedance = self.impedance\n        if smooth &gt; 1:\n            impedance = np.convolve(impedance, np.ones(smooth) / smooth, mode=\"same\")\n        freq, impedance = self.freq[f_idx], impedance[f_idx]\n        self.peak_freq = freq[np.argmax(impedance)]\n        print(f\"Resonant Peak Frequency: {self.peak_freq:.3g} (Hz)\")\n        return freq, impedance\n\n    def execute(self) -&gt; Tuple[list, list]:\n        \"\"\"\n        Run the ZAP simulation and calculate the impedance profile.\n\n        This method:\n        1. Sets up the chirp current\n        2. Runs the NEURON simulation\n        3. Calculates the impedance using FFT\n        4. Prints a summary of the frequency range and analysis method\n\n        Returns:\n        --------\n        tuple\n            (time_vector, voltage_vector) from the simulation\n\n        Notes:\n        ------\n        Sets several attributes including:\n        - Z: Complex impedance values (from FFT)\n        - freq: Frequency values for the impedance profile\n        - impedance: Absolute impedance values\n        \"\"\"\n        print(\"ZAP current simulation running...\")\n        self.zap_current()\n        h.tstop = self.tstop\n        h.stdinit()\n        h.run()\n\n        self.zap_vec.resize(self.t_vec.size())\n        self.v_rest = self.v_vec[self.index_v_rest]\n        self.v_rest_time = self.t_vec[self.index_v_rest]\n\n        t_idx = slice(self.index_v_rest, self.index_v_final + 1)\n        self.v_vec_inj = np.array(self.v_vec)[t_idx] - self.v_rest\n        self.t_vec_inj = np.array(self.t_vec)[t_idx] - self.v_rest_time\n\n        self.cell_v_amp_max = np.abs(self.v_vec_inj).max()\n        self.Z = np.fft.rfft(self.v_vec_inj) / np.fft.rfft(self.zap_vec_inj)  # MOhms\n        self.freq = np.fft.rfftfreq(self.zap_vec_inj.size, d=self.dt * 1e-3)  # ms to sec\n        self.impedance = np.abs(self.Z)\n\n        print()\n        print(\n            \"Chirp current injection with frequency changing from \"\n            f\"{self.fstart:g} to {self.fend:g} Hz over {self.inj_dur * 1e-3:g} seconds\"\n        )\n        print(\n            \"Impedance is calculated as the ratio of FFT amplitude \"\n            \"of membrane voltage to FFT amplitude of chirp current\"\n        )\n        print()\n        return self.t_vec.to_python(), self.v_vec.to_python()\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.ZAP.__init__","title":"<code>__init__(template_name, inj_amp=100.0, inj_delay=200.0, inj_dur=15000.0, tstop=15500.0, fstart=0.0, fend=15.0, chirp_type=None, **kwargs)</code>","text":"<p>Initialize a ZAP (impedance amplitude profile) simulation environment.</p> Parameters: <p>template_name : str or callable     Either the name of the cell template located in HOC or     a function that creates and returns a cell object. inj_amp : float, optional     Current injection amplitude (pA). Default is 100.0. inj_delay : float, optional     Start time for current injection (ms). Default is 200.0. inj_dur : float, optional     Duration of current injection (ms). Default is 15000.0. tstop : float, optional     Total simulation time (ms). Default is 15500.0. fstart : float, optional     Starting frequency of the chirp current (Hz). Default is 0.0. fend : float, optional     Ending frequency of the chirp current (Hz). Default is 15.0. chirp_type : str, optional     Type of chirp current determining how frequency increases over time:     - 'linear': Linear increase in frequency (default if None)     - 'exponential': Exponential increase in frequency **kwargs :     Additional keyword arguments to pass to the parent CurrentClamp constructor.</p> Notes: <p>This class is designed for measuring the frequency-dependent impedance profile of a neuron using a chirp current that sweeps through frequencies.</p> Raises: <p>AssertionError     - If inj_amp is zero     - If chirp_type is 'exponential' and either fstart or fend is &lt;= 0</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def __init__(\n    self,\n    template_name,\n    inj_amp=100.0,\n    inj_delay=200.0,\n    inj_dur=15000.0,\n    tstop=15500.0,\n    fstart=0.0,\n    fend=15.0,\n    chirp_type=None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize a ZAP (impedance amplitude profile) simulation environment.\n\n    Parameters:\n    -----------\n    template_name : str or callable\n        Either the name of the cell template located in HOC or\n        a function that creates and returns a cell object.\n    inj_amp : float, optional\n        Current injection amplitude (pA). Default is 100.0.\n    inj_delay : float, optional\n        Start time for current injection (ms). Default is 200.0.\n    inj_dur : float, optional\n        Duration of current injection (ms). Default is 15000.0.\n    tstop : float, optional\n        Total simulation time (ms). Default is 15500.0.\n    fstart : float, optional\n        Starting frequency of the chirp current (Hz). Default is 0.0.\n    fend : float, optional\n        Ending frequency of the chirp current (Hz). Default is 15.0.\n    chirp_type : str, optional\n        Type of chirp current determining how frequency increases over time:\n        - 'linear': Linear increase in frequency (default if None)\n        - 'exponential': Exponential increase in frequency\n    **kwargs :\n        Additional keyword arguments to pass to the parent CurrentClamp constructor.\n\n    Notes:\n    ------\n    This class is designed for measuring the frequency-dependent impedance profile\n    of a neuron using a chirp current that sweeps through frequencies.\n\n    Raises:\n    -------\n    AssertionError\n        - If inj_amp is zero\n        - If chirp_type is 'exponential' and either fstart or fend is &lt;= 0\n    \"\"\"\n    assert inj_amp != 0\n    super().__init__(\n        template_name=template_name,\n        tstop=tstop,\n        inj_amp=inj_amp,\n        inj_delay=inj_delay,\n        inj_dur=inj_dur,\n        **kwargs,\n    )\n    self.inj_stop = inj_delay + inj_dur\n    self.fstart = fstart\n    self.fend = fend\n    self.chirp_type = chirp_type\n    self.chirp_func = {\"linear\": self.linear_chirp, \"exponential\": self.exponential_chirp}\n    if chirp_type == \"exponential\":\n        assert fstart &gt; 0 and fend &gt; 0\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.ZAP.linear_chirp","title":"<code>linear_chirp(t, f0, f1)</code>","text":"<p>Generate a chirp current with linearly increasing frequency.</p> Parameters: <p>t : ndarray     Time vector (ms) f0 : float     Start frequency (kHz) f1 : float     End frequency (kHz)</p> Returns: <p>ndarray     Current values with amplitude self.inj_amp and frequency     increasing linearly from f0 to f1 Hz over time t</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def linear_chirp(self, t, f0, f1):\n    \"\"\"\n    Generate a chirp current with linearly increasing frequency.\n\n    Parameters:\n    -----------\n    t : ndarray\n        Time vector (ms)\n    f0 : float\n        Start frequency (kHz)\n    f1 : float\n        End frequency (kHz)\n\n    Returns:\n    --------\n    ndarray\n        Current values with amplitude self.inj_amp and frequency\n        increasing linearly from f0 to f1 Hz over time t\n    \"\"\"\n    return self.inj_amp * np.sin(np.pi * (2 * f0 + (f1 - f0) / t[-1] * t) * t)\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.ZAP.exponential_chirp","title":"<code>exponential_chirp(t, f0, f1)</code>","text":"<p>Generate a chirp current with exponentially increasing frequency.</p> Parameters: <p>t : ndarray     Time vector (ms) f0 : float     Start frequency (kHz), must be &gt; 0 f1 : float     End frequency (kHz), must be &gt; 0</p> Returns: <p>ndarray     Current values with amplitude self.inj_amp and frequency     increasing exponentially from f0 to f1 Hz over time t</p> Notes: <p>For exponential chirp, both f0 and f1 must be positive.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def exponential_chirp(self, t, f0, f1):\n    \"\"\"\n    Generate a chirp current with exponentially increasing frequency.\n\n    Parameters:\n    -----------\n    t : ndarray\n        Time vector (ms)\n    f0 : float\n        Start frequency (kHz), must be &gt; 0\n    f1 : float\n        End frequency (kHz), must be &gt; 0\n\n    Returns:\n    --------\n    ndarray\n        Current values with amplitude self.inj_amp and frequency\n        increasing exponentially from f0 to f1 Hz over time t\n\n    Notes:\n    ------\n    For exponential chirp, both f0 and f1 must be positive.\n    \"\"\"\n    L = np.log(f1 / f0) / t[-1]\n    return self.inj_amp * np.sin(np.pi * 2 * f0 / L * (np.exp(L * t) - 1))\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.ZAP.zap_current","title":"<code>zap_current()</code>","text":"<p>Create a frequency-modulated (chirp) current for probing impedance.</p> <p>This method: 1. Sets up time vectors for the simulation and current injection 2. Creates a chirp current based on the specified parameters (linear or exponential) 3. Prepares the current vector for NEURON playback</p> Notes: <p>The chirp current increases in frequency from fstart to fend Hz over the duration of the injection. This allows frequency-dependent impedance to be measured in a single simulation.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def zap_current(self):\n    \"\"\"\n    Create a frequency-modulated (chirp) current for probing impedance.\n\n    This method:\n    1. Sets up time vectors for the simulation and current injection\n    2. Creates a chirp current based on the specified parameters (linear or exponential)\n    3. Prepares the current vector for NEURON playback\n\n    Notes:\n    ------\n    The chirp current increases in frequency from fstart to fend Hz over the duration\n    of the injection. This allows frequency-dependent impedance to be measured in\n    a single simulation.\n    \"\"\"\n    self.dt = dt = h.dt\n    self.index_v_rest = int(self.inj_delay / dt)\n    self.index_v_final = int(self.inj_stop / dt)\n\n    t = np.arange(int(self.tstop / dt) + 1) * dt\n    t_inj = t[: self.index_v_final - self.index_v_rest + 1]\n    f0 = self.fstart * 1e-3  # Hz to 1/ms\n    f1 = self.fend * 1e-3\n    chirp_func = self.chirp_func.get(self.chirp_type, self.linear_chirp)\n    self.zap_vec_inj = chirp_func(t_inj, f0, f1)\n    i_inj = np.zeros_like(t)\n    i_inj[self.index_v_rest : self.index_v_final + 1] = self.zap_vec_inj\n\n    self.zap_vec = h.Vector()\n    self.zap_vec.from_python(i_inj)\n    self.zap_vec.play(self.cell_src._ref_amp, dt)\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.ZAP.get_impedance","title":"<code>get_impedance(smooth=1)</code>","text":"<p>Calculate and extract the frequency-dependent impedance profile.</p> <p>This method: 1. Filters the impedance to the frequency range of interest 2. Optionally applies smoothing to reduce noise 3. Identifies the resonant frequency (peak impedance)</p> Parameters: <p>smooth : int, optional     Window size for smoothing the impedance. Default is 1 (no smoothing).</p> Returns: <p>tuple     (frequencies, impedance_values) in the range of interest</p> Notes: <p>Sets self.peak_freq to the resonant frequency (frequency of maximum impedance).</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def get_impedance(self, smooth=1):\n    \"\"\"\n    Calculate and extract the frequency-dependent impedance profile.\n\n    This method:\n    1. Filters the impedance to the frequency range of interest\n    2. Optionally applies smoothing to reduce noise\n    3. Identifies the resonant frequency (peak impedance)\n\n    Parameters:\n    -----------\n    smooth : int, optional\n        Window size for smoothing the impedance. Default is 1 (no smoothing).\n\n    Returns:\n    --------\n    tuple\n        (frequencies, impedance_values) in the range of interest\n\n    Notes:\n    ------\n    Sets self.peak_freq to the resonant frequency (frequency of maximum impedance).\n    \"\"\"\n    f_idx = (self.freq &gt; min(self.fstart, self.fend)) &amp; (\n        self.freq &lt; max(self.fstart, self.fend)\n    )\n    impedance = self.impedance\n    if smooth &gt; 1:\n        impedance = np.convolve(impedance, np.ones(smooth) / smooth, mode=\"same\")\n    freq, impedance = self.freq[f_idx], impedance[f_idx]\n    self.peak_freq = freq[np.argmax(impedance)]\n    print(f\"Resonant Peak Frequency: {self.peak_freq:.3g} (Hz)\")\n    return freq, impedance\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.ZAP.execute","title":"<code>execute()</code>","text":"<p>Run the ZAP simulation and calculate the impedance profile.</p> <p>This method: 1. Sets up the chirp current 2. Runs the NEURON simulation 3. Calculates the impedance using FFT 4. Prints a summary of the frequency range and analysis method</p> Returns: <p>tuple     (time_vector, voltage_vector) from the simulation</p> Notes: <p>Sets several attributes including: - Z: Complex impedance values (from FFT) - freq: Frequency values for the impedance profile - impedance: Absolute impedance values</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def execute(self) -&gt; Tuple[list, list]:\n    \"\"\"\n    Run the ZAP simulation and calculate the impedance profile.\n\n    This method:\n    1. Sets up the chirp current\n    2. Runs the NEURON simulation\n    3. Calculates the impedance using FFT\n    4. Prints a summary of the frequency range and analysis method\n\n    Returns:\n    --------\n    tuple\n        (time_vector, voltage_vector) from the simulation\n\n    Notes:\n    ------\n    Sets several attributes including:\n    - Z: Complex impedance values (from FFT)\n    - freq: Frequency values for the impedance profile\n    - impedance: Absolute impedance values\n    \"\"\"\n    print(\"ZAP current simulation running...\")\n    self.zap_current()\n    h.tstop = self.tstop\n    h.stdinit()\n    h.run()\n\n    self.zap_vec.resize(self.t_vec.size())\n    self.v_rest = self.v_vec[self.index_v_rest]\n    self.v_rest_time = self.t_vec[self.index_v_rest]\n\n    t_idx = slice(self.index_v_rest, self.index_v_final + 1)\n    self.v_vec_inj = np.array(self.v_vec)[t_idx] - self.v_rest\n    self.t_vec_inj = np.array(self.t_vec)[t_idx] - self.v_rest_time\n\n    self.cell_v_amp_max = np.abs(self.v_vec_inj).max()\n    self.Z = np.fft.rfft(self.v_vec_inj) / np.fft.rfft(self.zap_vec_inj)  # MOhms\n    self.freq = np.fft.rfftfreq(self.zap_vec_inj.size, d=self.dt * 1e-3)  # ms to sec\n    self.impedance = np.abs(self.Z)\n\n    print()\n    print(\n        \"Chirp current injection with frequency changing from \"\n        f\"{self.fstart:g} to {self.fend:g} Hz over {self.inj_dur * 1e-3:g} seconds\"\n    )\n    print(\n        \"Impedance is calculated as the ratio of FFT amplitude \"\n        \"of membrane voltage to FFT amplitude of chirp current\"\n    )\n    print()\n    return self.t_vec.to_python(), self.v_vec.to_python()\n</code></pre>"},{"location":"api/singlecell/#cell-profiler","title":"Cell Profiler","text":""},{"location":"api/singlecell/#bmtool.singlecell.Profiler","title":"<code>bmtool.singlecell.Profiler</code>","text":"<p>All in one single cell profiler</p> <p>This Profiler now supports being initialized with either explicit <code>template_dir</code> and <code>mechanism_dir</code> paths or with a BMTK <code>config</code> file (which should contain <code>components.templates_dir</code> and <code>components.mechanisms_dir</code>). When <code>config</code> is provided it will be used to load mechanisms and templates via the utility helpers.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>class Profiler:\n    \"\"\"All in one single cell profiler\n\n    This Profiler now supports being initialized with either explicit\n    `template_dir` and `mechanism_dir` paths or with a BMTK `config` file\n    (which should contain `components.templates_dir` and\n    `components.mechanisms_dir`). When `config` is provided it will be used\n    to load mechanisms and templates via the utility helpers.\n    \"\"\"\n\n    def __init__(self, template_dir: str = None, mechanism_dir: str = None, dt=None, config: str = None):\n        # initialize to None and then prefer config-derived paths if provided\n        self.template_dir = None\n        self.mechanism_dir = None\n        self.templates = None  # Initialize templates attribute\n        self.config = config  # Store config path\n        self.last_figure = None  # Store reference to last generated figure\n\n        # If a BMTK config is provided, load mechanisms/templates from it\n        if config is not None:\n            try:\n                # load and apply the config values for directories\n                conf = load_config(config)\n                # conf behaves like a dict returned by bmtk Config.from_json\n                try:\n                    comps = conf[\"components\"]\n                except Exception:\n                    comps = getattr(conf, \"components\", None)\n\n                if comps is not None:\n                    # support dict-like and object-like components\n                    try:\n                        self.template_dir = comps.get(\"templates_dir\")\n                    except Exception:\n                        self.template_dir = getattr(comps, \"templates_dir\", None)\n                    try:\n                        self.mechanism_dir = comps.get(\"mechanisms_dir\")\n                    except Exception:\n                        self.mechanism_dir = getattr(comps, \"mechanisms_dir\", None)\n\n                # actually load mechanisms and templates using the helper\n                load_templates_from_config(config)\n            except Exception:\n                # fall back to explicit dirs if config parsing/loading fails\n                print('failed')\n\n        else:\n            # fall back to explicit args if not set by config\n            if not self.template_dir:\n                self.template_dir = template_dir\n            if not self.mechanism_dir:\n                self.mechanism_dir = mechanism_dir\n\n            # template_dir is required for loading templates later\n            if self.template_dir is None:\n                raise ValueError(\"Profiler requires either 'template_dir' or a 'config' containing components.templates_dir\")\n\n            self.templates = None\n\n            self.load_templates()\n\n        h.load_file(\"stdrun.hoc\")\n        if dt is not None:\n            h.dt = dt\n            h.steps_per_ms = 1 / h.dt\n\n    def load_templates(self, hoc_template_file=None):\n        if self.templates is None:  # Can really only do this once\n            # Check if we have a config file - if so, extract templates from node configs\n            if hasattr(self, 'config') and self.config is not None:\n                try:\n                    from bmtool.util.util import load_nodes_from_config\n                    nodes_networks = load_nodes_from_config(config=self.config)\n                    template_names = set()\n                    for nodes in nodes_networks:\n                        try:\n                            cell_template_names = nodes_networks[nodes]['model_template'].unique()\n                            # Clean up template names (remove 'hoc:' prefix if present)\n                            for template in cell_template_names:\n                                if isinstance(template, str):\n                                    # Remove 'hoc:' prefix if present\n                                    clean_name = template.replace('hoc:', '') if template.startswith('hoc:') else template\n                                    template_names.add(clean_name)\n                        except:\n                            # If fails, means no model_templates in that network\n                            pass\n\n                    self.templates = sorted(list(template_names))\n                    self.hoc_templates = []  # Templates loaded via config, not hoc files\n\n                except Exception as e:\n                    print(f\"Failed to load templates from config: {e}\")\n            else:\n                # Traditional loading with template_dir and mechanism_dir\n                if (\n                    self.mechanism_dir != \"./\"\n                    and self.mechanism_dir != \".\"\n                    and self.mechanism_dir != \"././\"\n                ):\n                    neuron.load_mechanisms(self.mechanism_dir)\n                h_base = set(dir(h))\n\n                cwd = os.getcwd()\n                os.chdir(self.template_dir)\n                if not hoc_template_file:\n                    self.hoc_templates = glob.glob(\"*.hoc\")\n                    for hoc_template in self.hoc_templates:\n                        h.load_file(str(hoc_template))\n                else:\n                    self.hoc_templates = [hoc_template_file]\n                    h.load_file(hoc_template_file)\n\n                os.chdir(cwd)\n\n                h_loaded = dir(h)\n                self.templates = [x for x in h_loaded if x not in h_base]\n\n        return self.templates\n\n    def passive_properties(\n        self,\n        template_name: str,\n        post_init_function: str = None,\n        record_sec: str = \"soma\",\n        inj_sec: str = \"soma\",\n        plot: bool = True,\n        method=None,\n        **kwargs,\n    ) -&gt; Tuple[list, list]:\n        \"\"\"\n        Calculates passive properties for the specified cell template_name\n\n        Parameters\n        ==========\n        template_name: str or callable\n            name of the cell template located in hoc\n            or a function that creates and returns a cell object\n        post_init_function: str\n            function of the cell to be called after the cell has been initialized (like insert_mechs(123))\n        record_sec: str\n            section of the cell you want to record spikes from (default: soma)\n        inj_sec: str\n            section of the cell you want to inject current to (default: soma)\n        plot: bool\n            automatically plot the cell profile\n        method: str\n            method to estimate membrane time constant (see Passive)\n        **kwargs:\n            extra key word arguments for Passive()\n\n        Returns time (ms), membrane voltage (mV)\n        \"\"\"\n        passive = Passive(\n            template_name,\n            post_init_function=post_init_function,\n            record_sec=record_sec,\n            inj_sec=inj_sec,\n            method=method,\n            **kwargs,\n        )\n        time, amp = passive.execute()\n\n        if plot:\n            plt.figure()\n            t_array = np.array(time)\n            amp_array = np.array(amp)\n            t_idx = (t_array &gt;= passive.inj_delay) &amp; (t_array &lt;= passive.inj_delay + passive.inj_dur)\n            plt.plot(t_array[t_idx], amp_array[t_idx])\n            if passive.method == \"exp2\":\n                plt.plot(*passive.double_exponential_fit(), \"r:\", label=\"double exponential fit\")\n                plt.legend()\n            elif passive.method == \"exp\":\n                plt.plot(*passive.single_exponential_fit(), \"r:\", label=\"single exponential fit\")\n                plt.legend()\n            plt.title(\"Passive Cell Current Injection\")\n            plt.xlabel(\"Time (ms)\")\n            plt.ylabel(\"Membrane Potential (mV)\")\n            self.last_figure = plt.gcf()\n            plt.show()\n\n        return time, amp\n\n    def current_injection(\n        self,\n        template_name: str,\n        post_init_function: str = None,\n        record_sec: str = \"soma\",\n        inj_sec: str = \"soma\",\n        plot: bool = True,\n        **kwargs,\n    ) -&gt; Tuple[list, list]:\n        ccl = CurrentClamp(\n            template_name,\n            post_init_function=post_init_function,\n            record_sec=record_sec,\n            inj_sec=inj_sec,\n            **kwargs,\n        )\n        time, amp = ccl.execute()\n\n        if plot:\n            plt.figure()\n            plt.plot(time, amp)\n            plt.title(\"Current Injection\")\n            plt.xlabel(\"Time (ms)\")\n            plt.ylabel(\"Membrane Potential (mV)\")\n            plt.xlim(ccl.inj_delay - 10, ccl.inj_delay + ccl.inj_dur + 10)\n            self.last_figure = plt.gcf()\n            plt.show()\n\n        return time, amp\n\n    def fi_curve(\n        self,\n        template_name: str,\n        post_init_function: str = None,\n        record_sec: str = \"soma\",\n        inj_sec: str = \"soma\",\n        plot: bool = True,\n        **kwargs,\n    ) -&gt; Tuple[list, list]:\n        \"\"\"\n        Calculates an FI curve for the specified cell template_name\n\n        Parameters\n        ==========\n        template_name: str or callable\n            name of the cell template located in hoc\n            or a function that creates and returns a cell object\n        post_init_function: str\n            function of the cell to be called after the cell has been initialized (like insert_mechs(123))\n        record_sec: str\n            section of the cell you want to record spikes from (default: soma)\n        inj_sec: str\n            section of the cell you want to inject current to (default: soma)\n        plot: bool\n            automatically plot an fi curve\n\n        Returns the injection amplitudes (pA) used, number of spikes per amplitude supplied\n            list(amps), list(# of spikes)\n        \"\"\"\n        fi = FI(\n            template_name,\n            post_init_function=post_init_function,\n            record_sec=record_sec,\n            inj_sec=inj_sec,\n            **kwargs,\n        )\n        amp, nspk = fi.execute()\n\n        if plot:\n            plt.figure()\n            plt.plot(amp, nspk)\n            plt.title(\"FI Curve\")\n            plt.xlabel(\"Injection (pA)\")\n            plt.ylabel(\"# Spikes\")\n            self.last_figure = plt.gcf()\n            plt.show()\n\n        return amp, nspk\n\n    def impedance_amplitude_profile(\n        self,\n        template_name: str,\n        post_init_function: str = None,\n        record_sec: str = \"soma\",\n        inj_sec: str = \"soma\",\n        plot: bool = True,\n        chirp_type=None,\n        smooth: int = 9,\n        **kwargs,\n    ) -&gt; Tuple[list, list]:\n        \"\"\"\n        chirp_type: str\n            Type of chirp current (see ZAP)\n        smooth: int\n            Window size for smoothing the impedance in frequency domain\n        **kwargs:\n            extra key word arguments for ZAP()\n        \"\"\"\n        zap = ZAP(\n            template_name,\n            post_init_function=post_init_function,\n            record_sec=record_sec,\n            inj_sec=inj_sec,\n            chirp_type=chirp_type,\n            **kwargs,\n        )\n        time, amp = zap.execute()\n\n        if plot:\n            plt.figure()\n            plt.plot(time, amp)\n            plt.title(\"ZAP Response\")\n            plt.xlabel(\"Time (ms)\")\n            plt.ylabel(\"Membrane Potential (mV)\")\n            self.last_figure = plt.gcf()\n\n            plt.figure()\n            plt.plot(time, zap.zap_vec)\n            plt.title(\"ZAP Current\")\n            plt.xlabel(\"Time (ms)\")\n            plt.ylabel(\"Current Injection (nA)\")\n            # Note: This will overwrite last_figure with the current plot\n            self.last_figure = plt.gcf()\n\n            plt.figure()\n            plt.plot(*zap.get_impedance(smooth=smooth))\n            plt.title(\"Impedance Amplitude Profile\")\n            plt.xlabel(\"Frequency (Hz)\")\n            plt.ylabel(\"Impedance (MOhms)\")\n            self.last_figure = plt.gcf()\n            plt.show()\n\n        return time, amp\n\n    def interactive_runner(self):\n        \"\"\"Interactive runner for single cell profiling with GUI widgets.\n\n        This method creates an interactive interface using ipywidgets that allows\n        users to select templates and analysis methods, adjust parameters, and run\n        simulations with real-time plotting.\n        \"\"\"\n        try:\n            import ipywidgets as widgets\n            from IPython.display import display, clear_output\n            import matplotlib.pyplot as plt\n        except ImportError:\n            raise ImportError(\"ipywidgets and matplotlib are required for interactive mode. Install with: pip install ipywidgets matplotlib\")\n\n        # Get available templates\n        available_templates = self.load_templates()\n\n        # Check what NEURON objects are available\n        import neuron\n        h = neuron.h\n\n        # Create widgets\n        template_dropdown = widgets.Dropdown(\n            options=available_templates,\n            value=available_templates[0] if available_templates else None,\n            description='Template:',\n            style={'description_width': '80px'},\n            layout=widgets.Layout(width='300px')\n        )\n\n        method_dropdown = widgets.Dropdown(\n            options=['passive_properties', 'current_injection', 'fi_curve', 'impedance_amplitude_profile'],\n            value='passive_properties',\n            description='Method:',\n            style={'description_width': '80px'},\n            layout=widgets.Layout(width='300px')\n        )\n\n        # Default values based on method - from basic_settings in single_cell_tuning.ipynb\n        method_defaults = {\n            'passive_properties': {\n                'inj_amp': -20.0,\n                'inj_delay': 1500.0,\n                'inj_dur': 1000.0,\n                'tstop': 2500.0,\n                'tau_method': 'exp2'\n            },\n            'current_injection': {\n                'inj_amp': 50.0,\n                'inj_delay': 1500.0,\n                'inj_dur': 1000.0,\n                'tstop': 3000.0\n            },\n            'fi_curve': {\n                'i_start': -100.0,\n                'i_stop': 800.0,\n                'i_increment': 20.0,\n                'inj_delay': 1500.0,\n                'inj_dur': 1000.0\n            },\n            'impedance_amplitude_profile': {\n                'inj_amp': 100.0,\n                'inj_delay': 1000.0,\n                'inj_dur': 15000.0,\n                'tstop': 15500.0,\n                'fstart': 0.0,\n                'fend': 15.0,\n                'chirp_type': 'linear'\n            }\n        }\n\n        # Common parameters - always plot results, no need for toggle\n\n        # Method-specific parameters - styled like synapses.py sliders\n        slider_style = {'description_width': 'initial'}\n        slider_layout = None  # Use default width for longer sliders\n        text_style = {'description_width': 'initial'}\n        text_layout = widgets.Layout(width='200px')\n\n        # Initialize sliders with default values for passive_properties (initial method)\n        defaults = method_defaults['passive_properties']\n\n        inj_amp_slider = widgets.FloatSlider(value=defaults['inj_amp'], min=-500.0, max=1000.0, step=10.0, description='Injection Amp (pA):', style=slider_style)\n        inj_delay_slider = widgets.FloatSlider(value=defaults['inj_delay'], min=0.0, max=3000.0, step=10.0, description='Injection Delay (ms):', style=slider_style)\n        inj_dur_slider = widgets.FloatSlider(value=defaults['inj_dur'], min=100.0, max=20000.0, step=100.0, description='Injection Duration (ms):', style=slider_style)\n        tstop_slider = widgets.FloatSlider(value=defaults['tstop'], min=500.0, max=25000.0, step=100.0, description='Total Time (ms):', style=slider_style)\n\n        # FI curve specific\n        fi_defaults = method_defaults['fi_curve']\n        i_start_slider = widgets.FloatSlider(value=fi_defaults['i_start'], min=-500.0, max=500.0, step=10.0, description='I Start (pA):', style=slider_style)\n        i_stop_slider = widgets.FloatSlider(value=fi_defaults['i_stop'], min=0.0, max=2000.0, step=50.0, description='I Stop (pA):', style=slider_style)\n        i_increment_slider = widgets.FloatSlider(value=fi_defaults['i_increment'], min=10.0, max=500.0, step=10.0, description='I Increment (pA):', style=slider_style)\n\n        # ZAP specific\n        zap_defaults = method_defaults['impedance_amplitude_profile']\n        fstart_slider = widgets.FloatSlider(value=zap_defaults['fstart'], min=0.0, max=50.0, step=1.0, description='Start Freq (Hz):', style=slider_style)\n        fend_slider = widgets.FloatSlider(value=zap_defaults['fend'], min=1.0, max=100.0, step=1.0, description='End Freq (Hz):', style=slider_style)\n        chirp_dropdown = widgets.Dropdown(options=['linear', 'exponential'], value=zap_defaults['chirp_type'], description='Chirp Type:', style=slider_style)\n\n        # Passive properties specific\n        tau_method_dropdown = widgets.Dropdown(\n            options=['simple', 'exp', 'exp2'], \n            value=defaults['tau_method'], \n            description='Tau Method:', \n            style=slider_style\n        )\n\n        # Sections\n        record_sec_text = widgets.Text(value='soma', description='Record Section:', style=text_style, layout=text_layout)\n        inj_sec_text = widgets.Text(value='soma', description='Injection Section:', style=text_style, layout=text_layout)\n\n        # Post init function\n        post_init_text = widgets.Text(value='', description='Post Init Function:', placeholder='e.g., insert_mechs(123)', style={'description_width': 'initial'}, layout=widgets.Layout(width='300px'))\n\n        run_button = widgets.Button(\n            description='Run Analysis', \n            button_style='primary', \n            icon='play',\n            layout=widgets.Layout(width='140px')\n        )\n\n        reset_button = widgets.Button(\n            description='Reset to Defaults',\n            button_style='warning',\n            icon='refresh',\n            layout=widgets.Layout(width='150px')\n        )\n\n        save_path_text = widgets.Text(value='', description='Save Path:', placeholder='e.g., plot.png', style={'description_width': 'initial'}, layout=widgets.Layout(width='300px'))\n\n        save_button = widgets.Button(\n            description='Save Plot', \n            button_style='success', \n            icon='save',\n            layout=widgets.Layout(width='120px')\n        )\n\n        output_area = widgets.Output(\n            layout=widgets.Layout(border='1px solid #ccc', padding='10px', margin='10px 0 0 0')\n        )\n\n        # Layout containers - organized like synapse tuner\n        # Top row - template and method selection\n        selection_row = widgets.HBox([\n            template_dropdown,\n            method_dropdown\n        ], layout=widgets.Layout(margin='0 0 10px 0'))\n\n        # Button row - main controls\n        button_row = widgets.HBox([\n            run_button,\n            reset_button,\n            save_button\n        ], layout=widgets.Layout(margin='0 0 10px 0'))\n\n        # Section row - recording and injection sections  \n        section_row = widgets.HBox([\n            record_sec_text,\n            inj_sec_text,\n            post_init_text\n        ], layout=widgets.Layout(margin='0 0 10px 0'))\n\n        # Save row\n        save_row = widgets.HBox([\n            save_path_text\n        ], layout=widgets.Layout(margin='0 0 10px 0'))\n\n        # Parameter columns - organized in columns like synapse tuner\n        injection_params_col1 = widgets.VBox([\n            inj_amp_slider,\n            inj_delay_slider\n        ], layout=widgets.Layout(margin='0 10px 0 0'))\n\n        injection_params_col2 = widgets.VBox([\n            inj_dur_slider,\n            tstop_slider\n        ], layout=widgets.Layout(margin='0 0 0 10px'))\n\n        # Passive properties specific columns\n        passive_params_col1 = widgets.VBox([\n            inj_amp_slider,\n            inj_delay_slider\n        ], layout=widgets.Layout(margin='0 10px 0 0'))\n\n        passive_params_col2 = widgets.VBox([\n            inj_dur_slider,\n            tstop_slider,\n            tau_method_dropdown\n        ], layout=widgets.Layout(margin='0 0 0 10px'))\n\n        fi_params_col1 = widgets.VBox([\n            i_start_slider,\n            i_stop_slider\n        ], layout=widgets.Layout(margin='0 10px 0 0'))\n\n        fi_params_col2 = widgets.VBox([\n            i_increment_slider,\n            inj_dur_slider  # Use duration for FI curve too\n        ], layout=widgets.Layout(margin='0 0 0 10px'))\n\n        zap_params_col1 = widgets.VBox([\n            inj_amp_slider,\n            inj_delay_slider,\n            inj_dur_slider\n        ], layout=widgets.Layout(margin='0 10px 0 0'))\n\n        zap_params_col2 = widgets.VBox([\n            tstop_slider,\n            fstart_slider,\n            fend_slider,\n            chirp_dropdown\n        ], layout=widgets.Layout(margin='0 0 0 10px'))\n\n        # Function to update slider values based on method defaults\n        def update_slider_values(method):\n            \"\"\"Update slider values to match the defaults for the selected method\"\"\"\n            if method in method_defaults:\n                defaults = method_defaults[method]\n\n                # Update common sliders if they exist in defaults\n                if 'inj_amp' in defaults:\n                    inj_amp_slider.value = defaults['inj_amp']\n                if 'inj_delay' in defaults:\n                    inj_delay_slider.value = defaults['inj_delay']\n                if 'inj_dur' in defaults:\n                    inj_dur_slider.value = defaults['inj_dur']\n                if 'tstop' in defaults:\n                    tstop_slider.value = defaults['tstop']\n\n                # Update method-specific sliders\n                if method == 'fi_curve':\n                    if 'i_start' in defaults:\n                        i_start_slider.value = defaults['i_start']\n                    if 'i_stop' in defaults:\n                        i_stop_slider.value = defaults['i_stop']\n                    if 'i_increment' in defaults:\n                        i_increment_slider.value = defaults['i_increment']\n\n                elif method == 'impedance_amplitude_profile':\n                    if 'fstart' in defaults:\n                        fstart_slider.value = defaults['fstart']\n                    if 'fend' in defaults:\n                        fend_slider.value = defaults['fend']\n                    if 'chirp_type' in defaults:\n                        chirp_dropdown.value = defaults['chirp_type']\n\n                elif method == 'passive_properties':\n                    if 'tau_method' in defaults:\n                        tau_method_dropdown.value = defaults['tau_method']\n\n\n        # Function to update parameter visibility based on selected method\n        def update_params(*args):\n            method = method_dropdown.value\n\n            # Update slider values to defaults for the selected method\n            update_slider_values(method)\n\n            # Update parameter column visibility\n            if method == 'passive_properties':\n                param_columns.children = [widgets.HBox([passive_params_col1, passive_params_col2])]\n            elif method == 'current_injection':\n                param_columns.children = [widgets.HBox([injection_params_col1, injection_params_col2])]\n            elif method == 'fi_curve':\n                param_columns.children = [widgets.HBox([fi_params_col1, fi_params_col2])]\n            elif method == 'impedance_amplitude_profile':\n                param_columns.children = [widgets.HBox([zap_params_col1, zap_params_col2])]\n\n        method_dropdown.observe(update_params, 'value')\n\n        # Initialize parameter columns container\n        param_columns = widgets.VBox([widgets.HBox([passive_params_col1, passive_params_col2])])\n\n        # Run function\n        def run_analysis(b):\n            output_area.clear_output()  # Clear immediately on click\n            with output_area:\n\n                template = template_dropdown.value\n                method = method_dropdown.value\n                record_sec = record_sec_text.value\n                inj_sec = inj_sec_text.value\n                post_init = post_init_text.value if post_init_text.value else None\n\n                kwargs = {\n                    'record_sec': record_sec,\n                    'inj_sec': inj_sec,\n                    'plot': True  # Always plot results\n                }\n\n                if post_init:\n                    kwargs['post_init_function'] = post_init\n\n                # Add method-specific parameters\n                if method == 'passive_properties':\n                    kwargs.update({\n                        'inj_amp': inj_amp_slider.value,\n                        'inj_delay': inj_delay_slider.value,\n                        'inj_dur': inj_dur_slider.value,\n                        'tstop': tstop_slider.value,\n                        'method': tau_method_dropdown.value\n                    })\n                elif method == 'current_injection':\n                    kwargs.update({\n                        'inj_amp': inj_amp_slider.value,\n                        'inj_delay': inj_delay_slider.value,\n                        'inj_dur': inj_dur_slider.value,\n                        'tstop': tstop_slider.value\n                    })\n                elif method == 'fi_curve':\n                    kwargs.update({\n                        'i_start': i_start_slider.value,\n                        'i_stop': i_stop_slider.value,\n                        'i_increment': i_increment_slider.value,\n                        'tstart': inj_delay_slider.value,\n                        'tdur': inj_dur_slider.value\n                    })\n                elif method == 'impedance_amplitude_profile':\n                    kwargs.update({\n                        'inj_amp': inj_amp_slider.value,\n                        'inj_delay': inj_delay_slider.value,\n                        'inj_dur': inj_dur_slider.value,\n                        'tstop': tstop_slider.value,\n                        'fstart': fstart_slider.value,\n                        'fend': fend_slider.value,\n                        'chirp_type': chirp_dropdown.value\n                    })\n\n                print(\"=\"*60)\n                print(f\"Running {method} for template: {template}\")\n                print(\"=\"*60)\n                print(\"Parameters:\")\n                for key, value in kwargs.items():\n                    print(f\"  {key}: {value}\")\n                print(\"-\"*60)\n\n                try:\n                    if method == 'passive_properties':\n                        result = self.passive_properties(template, **kwargs)\n\n                    elif method == 'current_injection':\n                        result = self.current_injection(template, **kwargs)\n                    elif method == 'fi_curve':\n                        result = self.fi_curve(template, **kwargs)\n                    elif method == 'impedance_amplitude_profile':\n                        result = self.impedance_amplitude_profile(template, **kwargs)\n\n\n                except Exception as e:\n                    print(\"=\"*60)\n                    print(f\"\u2717 Error running analysis: {e}\")\n                    print(\"=\"*60)\n                    import traceback\n                    traceback.print_exc()\n\n        # Reset function\n        def reset_to_defaults(b):\n            output_area.clear_output()  # Clear immediately on click\n            with output_area:\n                method = method_dropdown.value\n                update_slider_values(method)\n                print(f\"Reset all parameters to defaults for {method}\")\n\n        # Save function\n        def save_plot(b):\n            path = save_path_text.value\n            if not path:\n                with output_area:\n                    print(\"Please enter a save path\")\n                return\n            if self.last_figure is None:\n                with output_area:\n                    print(\"No plot to save. Run an analysis first.\")\n                return\n            try:\n                self.last_figure.savefig(path)\n                with output_area:\n                    print(f\"Plot saved to {path}\")\n            except Exception as e:\n                with output_area:\n                    print(f\"Error saving plot: {e}\")\n\n        run_button.on_click(run_analysis)\n        reset_button.on_click(reset_to_defaults)\n        save_button.on_click(save_plot)\n\n        # Create main UI layout - matching synapse tuner structure\n        ui = widgets.VBox([\n            selection_row,\n            button_row, \n            section_row,\n            param_columns,\n            save_row\n        ], layout=widgets.Layout(padding='10px'))\n\n        # Display the interface - UI on top, output below (like synapse tuner)\n        display(ui)\n        display(output_area)\n\n        # Initial update\n        update_params()\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.__init__","title":"<code>__init__(template_dir=None, mechanism_dir=None, dt=None, config=None)</code>","text":"Source code in <code>bmtool/singlecell.py</code> <pre><code>def __init__(self, template_dir: str = None, mechanism_dir: str = None, dt=None, config: str = None):\n    # initialize to None and then prefer config-derived paths if provided\n    self.template_dir = None\n    self.mechanism_dir = None\n    self.templates = None  # Initialize templates attribute\n    self.config = config  # Store config path\n    self.last_figure = None  # Store reference to last generated figure\n\n    # If a BMTK config is provided, load mechanisms/templates from it\n    if config is not None:\n        try:\n            # load and apply the config values for directories\n            conf = load_config(config)\n            # conf behaves like a dict returned by bmtk Config.from_json\n            try:\n                comps = conf[\"components\"]\n            except Exception:\n                comps = getattr(conf, \"components\", None)\n\n            if comps is not None:\n                # support dict-like and object-like components\n                try:\n                    self.template_dir = comps.get(\"templates_dir\")\n                except Exception:\n                    self.template_dir = getattr(comps, \"templates_dir\", None)\n                try:\n                    self.mechanism_dir = comps.get(\"mechanisms_dir\")\n                except Exception:\n                    self.mechanism_dir = getattr(comps, \"mechanisms_dir\", None)\n\n            # actually load mechanisms and templates using the helper\n            load_templates_from_config(config)\n        except Exception:\n            # fall back to explicit dirs if config parsing/loading fails\n            print('failed')\n\n    else:\n        # fall back to explicit args if not set by config\n        if not self.template_dir:\n            self.template_dir = template_dir\n        if not self.mechanism_dir:\n            self.mechanism_dir = mechanism_dir\n\n        # template_dir is required for loading templates later\n        if self.template_dir is None:\n            raise ValueError(\"Profiler requires either 'template_dir' or a 'config' containing components.templates_dir\")\n\n        self.templates = None\n\n        self.load_templates()\n\n    h.load_file(\"stdrun.hoc\")\n    if dt is not None:\n        h.dt = dt\n        h.steps_per_ms = 1 / h.dt\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.load_templates","title":"<code>load_templates(hoc_template_file=None)</code>","text":"Source code in <code>bmtool/singlecell.py</code> <pre><code>def load_templates(self, hoc_template_file=None):\n    if self.templates is None:  # Can really only do this once\n        # Check if we have a config file - if so, extract templates from node configs\n        if hasattr(self, 'config') and self.config is not None:\n            try:\n                from bmtool.util.util import load_nodes_from_config\n                nodes_networks = load_nodes_from_config(config=self.config)\n                template_names = set()\n                for nodes in nodes_networks:\n                    try:\n                        cell_template_names = nodes_networks[nodes]['model_template'].unique()\n                        # Clean up template names (remove 'hoc:' prefix if present)\n                        for template in cell_template_names:\n                            if isinstance(template, str):\n                                # Remove 'hoc:' prefix if present\n                                clean_name = template.replace('hoc:', '') if template.startswith('hoc:') else template\n                                template_names.add(clean_name)\n                    except:\n                        # If fails, means no model_templates in that network\n                        pass\n\n                self.templates = sorted(list(template_names))\n                self.hoc_templates = []  # Templates loaded via config, not hoc files\n\n            except Exception as e:\n                print(f\"Failed to load templates from config: {e}\")\n        else:\n            # Traditional loading with template_dir and mechanism_dir\n            if (\n                self.mechanism_dir != \"./\"\n                and self.mechanism_dir != \".\"\n                and self.mechanism_dir != \"././\"\n            ):\n                neuron.load_mechanisms(self.mechanism_dir)\n            h_base = set(dir(h))\n\n            cwd = os.getcwd()\n            os.chdir(self.template_dir)\n            if not hoc_template_file:\n                self.hoc_templates = glob.glob(\"*.hoc\")\n                for hoc_template in self.hoc_templates:\n                    h.load_file(str(hoc_template))\n            else:\n                self.hoc_templates = [hoc_template_file]\n                h.load_file(hoc_template_file)\n\n            os.chdir(cwd)\n\n            h_loaded = dir(h)\n            self.templates = [x for x in h_loaded if x not in h_base]\n\n    return self.templates\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.passive_properties","title":"<code>passive_properties(template_name, post_init_function=None, record_sec='soma', inj_sec='soma', plot=True, method=None, **kwargs)</code>","text":"<p>Calculates passive properties for the specified cell template_name</p>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.passive_properties--parameters","title":"Parameters","text":"<p>template_name: str or callable     name of the cell template located in hoc     or a function that creates and returns a cell object post_init_function: str     function of the cell to be called after the cell has been initialized (like insert_mechs(123)) record_sec: str     section of the cell you want to record spikes from (default: soma) inj_sec: str     section of the cell you want to inject current to (default: soma) plot: bool     automatically plot the cell profile method: str     method to estimate membrane time constant (see Passive) **kwargs:     extra key word arguments for Passive()</p> <p>Returns time (ms), membrane voltage (mV)</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def passive_properties(\n    self,\n    template_name: str,\n    post_init_function: str = None,\n    record_sec: str = \"soma\",\n    inj_sec: str = \"soma\",\n    plot: bool = True,\n    method=None,\n    **kwargs,\n) -&gt; Tuple[list, list]:\n    \"\"\"\n    Calculates passive properties for the specified cell template_name\n\n    Parameters\n    ==========\n    template_name: str or callable\n        name of the cell template located in hoc\n        or a function that creates and returns a cell object\n    post_init_function: str\n        function of the cell to be called after the cell has been initialized (like insert_mechs(123))\n    record_sec: str\n        section of the cell you want to record spikes from (default: soma)\n    inj_sec: str\n        section of the cell you want to inject current to (default: soma)\n    plot: bool\n        automatically plot the cell profile\n    method: str\n        method to estimate membrane time constant (see Passive)\n    **kwargs:\n        extra key word arguments for Passive()\n\n    Returns time (ms), membrane voltage (mV)\n    \"\"\"\n    passive = Passive(\n        template_name,\n        post_init_function=post_init_function,\n        record_sec=record_sec,\n        inj_sec=inj_sec,\n        method=method,\n        **kwargs,\n    )\n    time, amp = passive.execute()\n\n    if plot:\n        plt.figure()\n        t_array = np.array(time)\n        amp_array = np.array(amp)\n        t_idx = (t_array &gt;= passive.inj_delay) &amp; (t_array &lt;= passive.inj_delay + passive.inj_dur)\n        plt.plot(t_array[t_idx], amp_array[t_idx])\n        if passive.method == \"exp2\":\n            plt.plot(*passive.double_exponential_fit(), \"r:\", label=\"double exponential fit\")\n            plt.legend()\n        elif passive.method == \"exp\":\n            plt.plot(*passive.single_exponential_fit(), \"r:\", label=\"single exponential fit\")\n            plt.legend()\n        plt.title(\"Passive Cell Current Injection\")\n        plt.xlabel(\"Time (ms)\")\n        plt.ylabel(\"Membrane Potential (mV)\")\n        self.last_figure = plt.gcf()\n        plt.show()\n\n    return time, amp\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.current_injection","title":"<code>current_injection(template_name, post_init_function=None, record_sec='soma', inj_sec='soma', plot=True, **kwargs)</code>","text":"Source code in <code>bmtool/singlecell.py</code> <pre><code>def current_injection(\n    self,\n    template_name: str,\n    post_init_function: str = None,\n    record_sec: str = \"soma\",\n    inj_sec: str = \"soma\",\n    plot: bool = True,\n    **kwargs,\n) -&gt; Tuple[list, list]:\n    ccl = CurrentClamp(\n        template_name,\n        post_init_function=post_init_function,\n        record_sec=record_sec,\n        inj_sec=inj_sec,\n        **kwargs,\n    )\n    time, amp = ccl.execute()\n\n    if plot:\n        plt.figure()\n        plt.plot(time, amp)\n        plt.title(\"Current Injection\")\n        plt.xlabel(\"Time (ms)\")\n        plt.ylabel(\"Membrane Potential (mV)\")\n        plt.xlim(ccl.inj_delay - 10, ccl.inj_delay + ccl.inj_dur + 10)\n        self.last_figure = plt.gcf()\n        plt.show()\n\n    return time, amp\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.fi_curve","title":"<code>fi_curve(template_name, post_init_function=None, record_sec='soma', inj_sec='soma', plot=True, **kwargs)</code>","text":"<p>Calculates an FI curve for the specified cell template_name</p>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.fi_curve--parameters","title":"Parameters","text":"<p>template_name: str or callable     name of the cell template located in hoc     or a function that creates and returns a cell object post_init_function: str     function of the cell to be called after the cell has been initialized (like insert_mechs(123)) record_sec: str     section of the cell you want to record spikes from (default: soma) inj_sec: str     section of the cell you want to inject current to (default: soma) plot: bool     automatically plot an fi curve</p> <p>Returns the injection amplitudes (pA) used, number of spikes per amplitude supplied     list(amps), list(# of spikes)</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def fi_curve(\n    self,\n    template_name: str,\n    post_init_function: str = None,\n    record_sec: str = \"soma\",\n    inj_sec: str = \"soma\",\n    plot: bool = True,\n    **kwargs,\n) -&gt; Tuple[list, list]:\n    \"\"\"\n    Calculates an FI curve for the specified cell template_name\n\n    Parameters\n    ==========\n    template_name: str or callable\n        name of the cell template located in hoc\n        or a function that creates and returns a cell object\n    post_init_function: str\n        function of the cell to be called after the cell has been initialized (like insert_mechs(123))\n    record_sec: str\n        section of the cell you want to record spikes from (default: soma)\n    inj_sec: str\n        section of the cell you want to inject current to (default: soma)\n    plot: bool\n        automatically plot an fi curve\n\n    Returns the injection amplitudes (pA) used, number of spikes per amplitude supplied\n        list(amps), list(# of spikes)\n    \"\"\"\n    fi = FI(\n        template_name,\n        post_init_function=post_init_function,\n        record_sec=record_sec,\n        inj_sec=inj_sec,\n        **kwargs,\n    )\n    amp, nspk = fi.execute()\n\n    if plot:\n        plt.figure()\n        plt.plot(amp, nspk)\n        plt.title(\"FI Curve\")\n        plt.xlabel(\"Injection (pA)\")\n        plt.ylabel(\"# Spikes\")\n        self.last_figure = plt.gcf()\n        plt.show()\n\n    return amp, nspk\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.impedance_amplitude_profile","title":"<code>impedance_amplitude_profile(template_name, post_init_function=None, record_sec='soma', inj_sec='soma', plot=True, chirp_type=None, smooth=9, **kwargs)</code>","text":"<p>chirp_type: str     Type of chirp current (see ZAP) smooth: int     Window size for smoothing the impedance in frequency domain **kwargs:     extra key word arguments for ZAP()</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def impedance_amplitude_profile(\n    self,\n    template_name: str,\n    post_init_function: str = None,\n    record_sec: str = \"soma\",\n    inj_sec: str = \"soma\",\n    plot: bool = True,\n    chirp_type=None,\n    smooth: int = 9,\n    **kwargs,\n) -&gt; Tuple[list, list]:\n    \"\"\"\n    chirp_type: str\n        Type of chirp current (see ZAP)\n    smooth: int\n        Window size for smoothing the impedance in frequency domain\n    **kwargs:\n        extra key word arguments for ZAP()\n    \"\"\"\n    zap = ZAP(\n        template_name,\n        post_init_function=post_init_function,\n        record_sec=record_sec,\n        inj_sec=inj_sec,\n        chirp_type=chirp_type,\n        **kwargs,\n    )\n    time, amp = zap.execute()\n\n    if plot:\n        plt.figure()\n        plt.plot(time, amp)\n        plt.title(\"ZAP Response\")\n        plt.xlabel(\"Time (ms)\")\n        plt.ylabel(\"Membrane Potential (mV)\")\n        self.last_figure = plt.gcf()\n\n        plt.figure()\n        plt.plot(time, zap.zap_vec)\n        plt.title(\"ZAP Current\")\n        plt.xlabel(\"Time (ms)\")\n        plt.ylabel(\"Current Injection (nA)\")\n        # Note: This will overwrite last_figure with the current plot\n        self.last_figure = plt.gcf()\n\n        plt.figure()\n        plt.plot(*zap.get_impedance(smooth=smooth))\n        plt.title(\"Impedance Amplitude Profile\")\n        plt.xlabel(\"Frequency (Hz)\")\n        plt.ylabel(\"Impedance (MOhms)\")\n        self.last_figure = plt.gcf()\n        plt.show()\n\n    return time, amp\n</code></pre>"},{"location":"api/singlecell/#helper-functions","title":"Helper Functions","text":""},{"location":"api/singlecell/#bmtool.singlecell.run_and_plot","title":"<code>bmtool.singlecell.run_and_plot(sim, title=None, xlabel='Time (ms)', ylabel='Membrane Potential (mV)', plot=True, plot_injection_only=False)</code>","text":"<p>Helper function for running simulation and plot sim: instance of the simulation class in this module title, xlabel, ylabel: plot labels plot: whether or not to plot plot_injection_only: plot only the injection duration Return: outputs by sim.execute()</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def run_and_plot(\n    sim,\n    title=None,\n    xlabel=\"Time (ms)\",\n    ylabel=\"Membrane Potential (mV)\",\n    plot=True,\n    plot_injection_only=False,\n):\n    \"\"\"Helper function for running simulation and plot\n    sim: instance of the simulation class in this module\n    title, xlabel, ylabel: plot labels\n    plot: whether or not to plot\n    plot_injection_only: plot only the injection duration\n    Return: outputs by sim.execute()\n    \"\"\"\n    X, Y = sim.execute()\n    X = np.array(X)\n    Y = np.array(Y)\n    if plot:\n        plt.figure()\n        if plot_injection_only:\n            t_idx = (X &gt;= sim.inj_delay) &amp; (X &lt;= sim.inj_delay + sim.inj_dur)\n            plt.plot(X[t_idx], Y[t_idx])\n        else:\n            plt.plot(X, Y)\n        if title is None:\n            title = type(sim).__name__\n        plt.title(title)\n        plt.xlabel(xlabel)\n        plt.ylabel(ylabel)\n    return X, Y\n</code></pre>"},{"location":"api/slurm/","title":"SLURM API Reference","text":"<p>This page provides API reference documentation for the SLURM module, which contains functions and classes for managing batch simulations on SLURM-based HPC clusters.</p>"},{"location":"api/slurm/#utility-functions","title":"Utility Functions","text":""},{"location":"api/slurm/#bmtool.SLURM.check_job_status","title":"<code>bmtool.SLURM.check_job_status(job_id)</code>","text":"<p>Checks the status of a SLURM job using scontrol.</p> <p>Args:     job_id (str): The SLURM job ID.</p> <p>Returns:     str: The state of the job.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def check_job_status(job_id):\n    \"\"\"\n    Checks the status of a SLURM job using scontrol.\n\n    Args:\n        job_id (str): The SLURM job ID.\n\n    Returns:\n        str: The state of the job.\n    \"\"\"\n    try:\n        result = subprocess.run([\"scontrol\", \"show\", \"job\", job_id], capture_output=True, text=True)\n        if result.returncode != 0:\n            # this check is not needed if check_interval is less than 5 min (~300 seconds)\n            if \"slurm_load_jobs error: Invalid job id specified\" in result.stderr:\n                return \"COMPLETED\"  # Treat invalid job ID as completed because scontrol expires and removed job info when done.\n            # raise Exception(f\"Error checking job status: {result.stderr}\")\n\n        job_state = None\n        for line in result.stdout.split(\"\\n\"):\n            if \"JobState=\" in line:\n                job_state = line.strip().split(\"JobState=\")[1].split()[0]\n                break\n\n        if job_state is None:\n            raise Exception(f\"Failed to retrieve job status for job ID: {job_id}\")\n\n        return job_state\n    except Exception as e:\n        print(f\"Exception while checking job status: {e}\", flush=True)\n        return \"UNKNOWN\"\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.submit_job","title":"<code>bmtool.SLURM.submit_job(script_path)</code>","text":"<p>Submits a SLURM job script.</p> <p>Args:     script_path (str): The path to the SLURM job script.</p> <p>Returns:     str: The job ID of the submitted job.</p> <p>Raises:     Exception: If there is an error in submitting the job.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def submit_job(script_path):\n    \"\"\"\n    Submits a SLURM job script.\n\n    Args:\n        script_path (str): The path to the SLURM job script.\n\n    Returns:\n        str: The job ID of the submitted job.\n\n    Raises:\n        Exception: If there is an error in submitting the job.\n    \"\"\"\n    result = subprocess.run([\"sbatch\", script_path], capture_output=True, text=True)\n    if result.returncode != 0:\n        raise Exception(f\"Error submitting job: {result.stderr}\")\n    job_id = result.stdout.strip().split()[-1]\n    return job_id\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.send_teams_message","title":"<code>bmtool.SLURM.send_teams_message(webhook, message)</code>","text":"<p>Sends a message to a teams channel or chat</p> <p>Args:     webhook (str): A microsoft teams webhook     message (str): A message to send in the chat/channel</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def send_teams_message(webhook, message):\n    \"\"\"Sends a message to a teams channel or chat\n\n    Args:\n        webhook (str): A microsoft teams webhook\n        message (str): A message to send in the chat/channel\n    \"\"\"\n    message = {\"text\": f\"{message}\"}\n\n    # Send POST request to trigger the flow\n    response = requests.post(\n        webhook,\n        json=message,  # Using 'json' instead of 'data' for automatic serialization\n        headers={\"Content-Type\": \"application/json\"},\n    )\n</code></pre>"},{"location":"api/slurm/#parameter-sweep-classes","title":"Parameter Sweep Classes","text":""},{"location":"api/slurm/#bmtool.SLURM.seedSweep","title":"<code>bmtool.SLURM.seedSweep</code>","text":"Source code in <code>bmtool/SLURM.py</code> <pre><code>class seedSweep:\n    def __init__(self, json_file_path, param_name):\n        \"\"\"\n        Initializes the seedSweep instance.\n\n        Args:\n            json_file_path (str): Path to the JSON file to be updated.\n            param_name (str): The name of the parameter to be modified.\n        \"\"\"\n        self.json_file_path = json_file_path\n        self.param_name = param_name\n\n    def edit_json(self, new_value):\n        \"\"\"\n        Updates the JSON file with a new parameter value.\n\n        Args:\n            new_value: The new value for the parameter.\n        \"\"\"\n        with open(self.json_file_path, \"r\") as f:\n            data = json.load(f)\n\n        data[self.param_name] = new_value\n\n        with open(self.json_file_path, \"w\") as f:\n            json.dump(data, f, indent=4)\n\n        print(\n            f\"JSON file '{self.json_file_path}' modified successfully with {self.param_name}={new_value}.\",\n            flush=True,\n        )\n\n    def change_json_file_path(self, new_json_file_path):\n        self.json_file_path = new_json_file_path\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.seedSweep.__init__","title":"<code>__init__(json_file_path, param_name)</code>","text":"<p>Initializes the seedSweep instance.</p> <p>Args:     json_file_path (str): Path to the JSON file to be updated.     param_name (str): The name of the parameter to be modified.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def __init__(self, json_file_path, param_name):\n    \"\"\"\n    Initializes the seedSweep instance.\n\n    Args:\n        json_file_path (str): Path to the JSON file to be updated.\n        param_name (str): The name of the parameter to be modified.\n    \"\"\"\n    self.json_file_path = json_file_path\n    self.param_name = param_name\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.seedSweep.edit_json","title":"<code>edit_json(new_value)</code>","text":"<p>Updates the JSON file with a new parameter value.</p> <p>Args:     new_value: The new value for the parameter.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def edit_json(self, new_value):\n    \"\"\"\n    Updates the JSON file with a new parameter value.\n\n    Args:\n        new_value: The new value for the parameter.\n    \"\"\"\n    with open(self.json_file_path, \"r\") as f:\n        data = json.load(f)\n\n    data[self.param_name] = new_value\n\n    with open(self.json_file_path, \"w\") as f:\n        json.dump(data, f, indent=4)\n\n    print(\n        f\"JSON file '{self.json_file_path}' modified successfully with {self.param_name}={new_value}.\",\n        flush=True,\n    )\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.seedSweep.change_json_file_path","title":"<code>change_json_file_path(new_json_file_path)</code>","text":"Source code in <code>bmtool/SLURM.py</code> <pre><code>def change_json_file_path(self, new_json_file_path):\n    self.json_file_path = new_json_file_path\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.multiSeedSweep","title":"<code>bmtool.SLURM.multiSeedSweep</code>","text":"<p>               Bases: <code>seedSweep</code></p> <p>MultSeedSweeps are centered around some base JSON cell file. When that base JSON is updated, the other JSONs change according to their ratio with the base JSON.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>class multiSeedSweep(seedSweep):\n    \"\"\"\n    MultSeedSweeps are centered around some base JSON cell file. When that base JSON is updated, the other JSONs\n    change according to their ratio with the base JSON.\n    \"\"\"\n\n    def __init__(self, base_json_file_path, param_name, syn_dict, base_ratio=1):\n        \"\"\"\n        Initializes the multipleSeedSweep instance.\n\n        Args:\n            base_json_file_path (str): File path for the base JSON file.\n            param_name (str): The name of the parameter to be modified.\n            syn_dict_list (list): A list containing dictionaries with the 'json_file_path' and 'ratio' (in comparison to the base_json) for each JSON file.\n            base_ratio (float): The ratio between the other JSONs; usually the current value for the parameter.\n        \"\"\"\n        super().__init__(base_json_file_path, param_name)\n        self.syn_dict_for_multi = syn_dict\n        self.base_ratio = base_ratio\n\n    def edit_all_jsons(self, new_value):\n        \"\"\"\n        Updates the base JSON file with a new parameter value and then updates the other JSON files based on the ratio.\n\n        Args:\n            new_value: The new value for the parameter in the base JSON.\n        \"\"\"\n        self.edit_json(new_value)\n        base_ratio = self.base_ratio\n\n        json_file_path = self.syn_dict_for_multi[\"json_file_path\"]\n        new_ratio = self.syn_dict_for_multi[\"ratio\"] / base_ratio\n\n        with open(json_file_path, \"r\") as f:\n            data = json.load(f)\n        altered_value = new_ratio * new_value\n        data[self.param_name] = altered_value\n\n        with open(json_file_path, \"w\") as f:\n            json.dump(data, f, indent=4)\n\n        print(\n            f\"JSON file '{json_file_path}' modified successfully with {self.param_name}={altered_value}.\",\n            flush=True,\n        )\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.multiSeedSweep.__init__","title":"<code>__init__(base_json_file_path, param_name, syn_dict, base_ratio=1)</code>","text":"<p>Initializes the multipleSeedSweep instance.</p> <p>Args:     base_json_file_path (str): File path for the base JSON file.     param_name (str): The name of the parameter to be modified.     syn_dict_list (list): A list containing dictionaries with the 'json_file_path' and 'ratio' (in comparison to the base_json) for each JSON file.     base_ratio (float): The ratio between the other JSONs; usually the current value for the parameter.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def __init__(self, base_json_file_path, param_name, syn_dict, base_ratio=1):\n    \"\"\"\n    Initializes the multipleSeedSweep instance.\n\n    Args:\n        base_json_file_path (str): File path for the base JSON file.\n        param_name (str): The name of the parameter to be modified.\n        syn_dict_list (list): A list containing dictionaries with the 'json_file_path' and 'ratio' (in comparison to the base_json) for each JSON file.\n        base_ratio (float): The ratio between the other JSONs; usually the current value for the parameter.\n    \"\"\"\n    super().__init__(base_json_file_path, param_name)\n    self.syn_dict_for_multi = syn_dict\n    self.base_ratio = base_ratio\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.multiSeedSweep.edit_all_jsons","title":"<code>edit_all_jsons(new_value)</code>","text":"<p>Updates the base JSON file with a new parameter value and then updates the other JSON files based on the ratio.</p> <p>Args:     new_value: The new value for the parameter in the base JSON.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def edit_all_jsons(self, new_value):\n    \"\"\"\n    Updates the base JSON file with a new parameter value and then updates the other JSON files based on the ratio.\n\n    Args:\n        new_value: The new value for the parameter in the base JSON.\n    \"\"\"\n    self.edit_json(new_value)\n    base_ratio = self.base_ratio\n\n    json_file_path = self.syn_dict_for_multi[\"json_file_path\"]\n    new_ratio = self.syn_dict_for_multi[\"ratio\"] / base_ratio\n\n    with open(json_file_path, \"r\") as f:\n        data = json.load(f)\n    altered_value = new_ratio * new_value\n    data[self.param_name] = altered_value\n\n    with open(json_file_path, \"w\") as f:\n        json.dump(data, f, indent=4)\n\n    print(\n        f\"JSON file '{json_file_path}' modified successfully with {self.param_name}={altered_value}.\",\n        flush=True,\n    )\n</code></pre>"},{"location":"api/slurm/#simulation-block-management","title":"Simulation Block Management","text":""},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock","title":"<code>bmtool.SLURM.SimulationBlock</code>","text":"Source code in <code>bmtool/SLURM.py</code> <pre><code>class SimulationBlock:\n    def __init__(\n        self,\n        block_name,\n        time,\n        partition,\n        nodes,\n        ntasks,\n        mem,\n        simulation_cases,\n        output_base_dir,\n        account=None,\n        additional_commands=None,\n        status_list=[\"COMPLETED\", \"FAILED\", \"CANCELLED\"],\n        component_path=None,\n    ):\n        \"\"\"\n        Initializes the SimulationBlock instance.\n\n        Args:\n            block_name (str): Name of the block.\n            time (str): Time limit for the job.\n            partition (str): Partition to submit the job to.\n            nodes (int): Number of nodes to request.\n            ntasks (int): Number of tasks.\n            mem (int) : Number of gigabytes (per node)\n            simulation_cases (dict): Dictionary of simulation cases with their commands.\n            output_base_dir (str): Base directory for the output files.\n            account (str) : account to charge on HPC\n            additional commands (list): commands to run before bmtk model starts useful for loading modules\n            status_list (list): List of things to check before running next block.\n                Adding RUNNING runs blocks faster but uses MUCH more resources and is only recommended on large HPC\n        \"\"\"\n        self.block_name = block_name\n        self.time = time\n        self.partition = partition\n        self.nodes = nodes\n        self.ntasks = ntasks\n        self.mem = mem\n        self.simulation_cases = simulation_cases\n        self.output_base_dir = output_base_dir\n        self.account = account\n        self.additional_commands = additional_commands if additional_commands is not None else []\n        self.status_list = status_list\n        self.job_ids = []\n        self.component_path = component_path\n\n    def create_batch_script(self, case_name, command):\n        \"\"\"\n        Creates a SLURM batch script for the given simulation case.\n\n        Args:\n            case_name (str): Name of the simulation case.\n            command (str): Command to run the simulation.\n\n        Returns:\n            str: Path to the batch script file.\n        \"\"\"\n        block_output_dir = os.path.join(\n            self.output_base_dir, self.block_name\n        )  # Create block-specific output folder\n        case_output_dir = os.path.join(\n            block_output_dir, case_name\n        )  # Create case-specific output folder\n        os.makedirs(case_output_dir, exist_ok=True)\n\n        batch_script_path = os.path.join(block_output_dir, f\"{case_name}_script.sh\")\n        additional_commands_str = \"\\n\".join(self.additional_commands)\n        # Conditional account linegit\n        account_line = f\"#SBATCH --account={self.account}\\n\" if self.account else \"\"\n        env_var_component_path = (\n            f\"export COMPONENT_PATH={self.component_path}\" if self.component_path else \"\"\n        )\n        mem_per_cpu = int(\n            np.ceil(int(self.mem) / int(self.ntasks))\n        )  # do ceil cause more mem is always better then less\n\n        # Write the batch script to the file\n        with open(batch_script_path, \"w\") as script_file:\n            script_file.write(\n                f\"\"\"#!/bin/bash\n#SBATCH --job-name={self.block_name}_{case_name}\n#SBATCH --output={block_output_dir}/%x_%j.out\n#SBATCH --error={block_output_dir}/%x_%j.err\n#SBATCH --time={self.time}\n#SBATCH --partition={self.partition}\n#SBATCH --nodes={self.nodes}\n#SBATCH --ntasks={self.ntasks}\n#SBATCH --mem-per-cpu={mem_per_cpu}G\n{account_line}\n\n# Additional user-defined commands\n{additional_commands_str}\n\n#enviroment vars\n{env_var_component_path}\n\nexport OUTPUT_DIR={case_output_dir}\n\n{command}\n\"\"\"\n            )\n\n        # print(f\"Batch script created: {batch_script_path}\", flush=True)\n\n        return batch_script_path\n\n    def submit_block(self):\n        \"\"\"\n        Submits all simulation cases in the block as separate SLURM jobs.\n        \"\"\"\n        for case_name, command in self.simulation_cases.items():\n            script_path = self.create_batch_script(case_name, command)\n            result = subprocess.run([\"sbatch\", script_path], capture_output=True, text=True)\n            if result.returncode == 0:\n                job_id = result.stdout.strip().split()[-1]\n                self.job_ids.append(job_id)\n                print(f\"Submitted {case_name} with job ID {job_id}\", flush=True)\n            else:\n                print(f\"Failed to submit {case_name}: {result.stderr}\", flush=True)\n\n    def check_block_status(self):\n        \"\"\"\n        Checks the status of all jobs in the block.\n\n        Returns:\n            bool: True if all jobs in the block are completed, False otherwise.\n        \"\"\"\n        for job_id in self.job_ids:\n            status = check_job_status(job_id)\n            if status not in self.status_list:\n                return False\n        return True\n\n    def check_block_completed(self):\n        \"\"\"checks if all the jobs in the block have been completed by slurm\n\n        Returns:\n            bool: True if all block jobs have been ran, false if job is still running\n        \"\"\"\n        for job_id in self.job_ids:\n            status = check_job_status(job_id)\n            # print(f\"status of job is {status}\")\n            if (\n                status != \"COMPLETED\"\n            ):  # can add PENDING here for debugging NOT FOR ACTUALLY USING IT\n                return False\n        return True\n\n    def check_block_running(self):\n        \"\"\"checks if a job is running\n\n        Returns:\n            bool: True if jobs are RUNNING false if anything else\n        \"\"\"\n        for job_id in self.job_ids:\n            status = check_job_status(job_id)\n            if status != \"RUNNING\":  #\n                return False\n        return True\n\n    def check_block_submited(self):\n        \"\"\"checks if a job is running\n\n        Returns:\n            bool: True if jobs are RUNNING false if anything else\n        \"\"\"\n        for job_id in self.job_ids:\n            status = check_job_status(job_id)\n            if status != \"PENDING\":  #\n                return False\n        return True\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.__init__","title":"<code>__init__(block_name, time, partition, nodes, ntasks, mem, simulation_cases, output_base_dir, account=None, additional_commands=None, status_list=['COMPLETED', 'FAILED', 'CANCELLED'], component_path=None)</code>","text":"<p>Initializes the SimulationBlock instance.</p> <p>Args:     block_name (str): Name of the block.     time (str): Time limit for the job.     partition (str): Partition to submit the job to.     nodes (int): Number of nodes to request.     ntasks (int): Number of tasks.     mem (int) : Number of gigabytes (per node)     simulation_cases (dict): Dictionary of simulation cases with their commands.     output_base_dir (str): Base directory for the output files.     account (str) : account to charge on HPC     additional commands (list): commands to run before bmtk model starts useful for loading modules     status_list (list): List of things to check before running next block.         Adding RUNNING runs blocks faster but uses MUCH more resources and is only recommended on large HPC</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def __init__(\n    self,\n    block_name,\n    time,\n    partition,\n    nodes,\n    ntasks,\n    mem,\n    simulation_cases,\n    output_base_dir,\n    account=None,\n    additional_commands=None,\n    status_list=[\"COMPLETED\", \"FAILED\", \"CANCELLED\"],\n    component_path=None,\n):\n    \"\"\"\n    Initializes the SimulationBlock instance.\n\n    Args:\n        block_name (str): Name of the block.\n        time (str): Time limit for the job.\n        partition (str): Partition to submit the job to.\n        nodes (int): Number of nodes to request.\n        ntasks (int): Number of tasks.\n        mem (int) : Number of gigabytes (per node)\n        simulation_cases (dict): Dictionary of simulation cases with their commands.\n        output_base_dir (str): Base directory for the output files.\n        account (str) : account to charge on HPC\n        additional commands (list): commands to run before bmtk model starts useful for loading modules\n        status_list (list): List of things to check before running next block.\n            Adding RUNNING runs blocks faster but uses MUCH more resources and is only recommended on large HPC\n    \"\"\"\n    self.block_name = block_name\n    self.time = time\n    self.partition = partition\n    self.nodes = nodes\n    self.ntasks = ntasks\n    self.mem = mem\n    self.simulation_cases = simulation_cases\n    self.output_base_dir = output_base_dir\n    self.account = account\n    self.additional_commands = additional_commands if additional_commands is not None else []\n    self.status_list = status_list\n    self.job_ids = []\n    self.component_path = component_path\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.create_batch_script","title":"<code>create_batch_script(case_name, command)</code>","text":"<p>Creates a SLURM batch script for the given simulation case.</p> <p>Args:     case_name (str): Name of the simulation case.     command (str): Command to run the simulation.</p> <p>Returns:     str: Path to the batch script file.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>    def create_batch_script(self, case_name, command):\n        \"\"\"\n        Creates a SLURM batch script for the given simulation case.\n\n        Args:\n            case_name (str): Name of the simulation case.\n            command (str): Command to run the simulation.\n\n        Returns:\n            str: Path to the batch script file.\n        \"\"\"\n        block_output_dir = os.path.join(\n            self.output_base_dir, self.block_name\n        )  # Create block-specific output folder\n        case_output_dir = os.path.join(\n            block_output_dir, case_name\n        )  # Create case-specific output folder\n        os.makedirs(case_output_dir, exist_ok=True)\n\n        batch_script_path = os.path.join(block_output_dir, f\"{case_name}_script.sh\")\n        additional_commands_str = \"\\n\".join(self.additional_commands)\n        # Conditional account linegit\n        account_line = f\"#SBATCH --account={self.account}\\n\" if self.account else \"\"\n        env_var_component_path = (\n            f\"export COMPONENT_PATH={self.component_path}\" if self.component_path else \"\"\n        )\n        mem_per_cpu = int(\n            np.ceil(int(self.mem) / int(self.ntasks))\n        )  # do ceil cause more mem is always better then less\n\n        # Write the batch script to the file\n        with open(batch_script_path, \"w\") as script_file:\n            script_file.write(\n                f\"\"\"#!/bin/bash\n#SBATCH --job-name={self.block_name}_{case_name}\n#SBATCH --output={block_output_dir}/%x_%j.out\n#SBATCH --error={block_output_dir}/%x_%j.err\n#SBATCH --time={self.time}\n#SBATCH --partition={self.partition}\n#SBATCH --nodes={self.nodes}\n#SBATCH --ntasks={self.ntasks}\n#SBATCH --mem-per-cpu={mem_per_cpu}G\n{account_line}\n\n# Additional user-defined commands\n{additional_commands_str}\n\n#enviroment vars\n{env_var_component_path}\n\nexport OUTPUT_DIR={case_output_dir}\n\n{command}\n\"\"\"\n            )\n\n        # print(f\"Batch script created: {batch_script_path}\", flush=True)\n\n        return batch_script_path\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.submit_block","title":"<code>submit_block()</code>","text":"<p>Submits all simulation cases in the block as separate SLURM jobs.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def submit_block(self):\n    \"\"\"\n    Submits all simulation cases in the block as separate SLURM jobs.\n    \"\"\"\n    for case_name, command in self.simulation_cases.items():\n        script_path = self.create_batch_script(case_name, command)\n        result = subprocess.run([\"sbatch\", script_path], capture_output=True, text=True)\n        if result.returncode == 0:\n            job_id = result.stdout.strip().split()[-1]\n            self.job_ids.append(job_id)\n            print(f\"Submitted {case_name} with job ID {job_id}\", flush=True)\n        else:\n            print(f\"Failed to submit {case_name}: {result.stderr}\", flush=True)\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.check_block_status","title":"<code>check_block_status()</code>","text":"<p>Checks the status of all jobs in the block.</p> <p>Returns:     bool: True if all jobs in the block are completed, False otherwise.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def check_block_status(self):\n    \"\"\"\n    Checks the status of all jobs in the block.\n\n    Returns:\n        bool: True if all jobs in the block are completed, False otherwise.\n    \"\"\"\n    for job_id in self.job_ids:\n        status = check_job_status(job_id)\n        if status not in self.status_list:\n            return False\n    return True\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.check_block_completed","title":"<code>check_block_completed()</code>","text":"<p>checks if all the jobs in the block have been completed by slurm</p> <p>Returns:     bool: True if all block jobs have been ran, false if job is still running</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def check_block_completed(self):\n    \"\"\"checks if all the jobs in the block have been completed by slurm\n\n    Returns:\n        bool: True if all block jobs have been ran, false if job is still running\n    \"\"\"\n    for job_id in self.job_ids:\n        status = check_job_status(job_id)\n        # print(f\"status of job is {status}\")\n        if (\n            status != \"COMPLETED\"\n        ):  # can add PENDING here for debugging NOT FOR ACTUALLY USING IT\n            return False\n    return True\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.check_block_running","title":"<code>check_block_running()</code>","text":"<p>checks if a job is running</p> <p>Returns:     bool: True if jobs are RUNNING false if anything else</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def check_block_running(self):\n    \"\"\"checks if a job is running\n\n    Returns:\n        bool: True if jobs are RUNNING false if anything else\n    \"\"\"\n    for job_id in self.job_ids:\n        status = check_job_status(job_id)\n        if status != \"RUNNING\":  #\n            return False\n    return True\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.check_block_submited","title":"<code>check_block_submited()</code>","text":"<p>checks if a job is running</p> <p>Returns:     bool: True if jobs are RUNNING false if anything else</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def check_block_submited(self):\n    \"\"\"checks if a job is running\n\n    Returns:\n        bool: True if jobs are RUNNING false if anything else\n    \"\"\"\n    for job_id in self.job_ids:\n        status = check_job_status(job_id)\n        if status != \"PENDING\":  #\n            return False\n    return True\n</code></pre>"},{"location":"api/slurm/#file-transfer","title":"File Transfer","text":""},{"location":"api/slurm/#bmtool.SLURM.get_relative_path","title":"<code>bmtool.SLURM.get_relative_path(endpoint, absolute_path)</code>","text":"<p>Convert absolute path to relative path for Globus transfer.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def get_relative_path(endpoint, absolute_path):\n    \"\"\"Convert absolute path to relative path for Globus transfer.\"\"\"\n    try:\n        # Get the directories at the mount point\n        result = subprocess.run(\n            [\"globus\", \"ls\", f\"{endpoint}:/\"], capture_output=True, text=True, check=True\n        )\n        dirs = set(result.stdout.splitlines())  # Convert to a set for quicker lookup\n\n        # Split the absolute path into parts\n        path_parts = absolute_path.strip(\"/\").split(\"/\")\n\n        # Find the first matching directory in the list\n        for i, part in enumerate(path_parts):\n            if part + \"/\" in dirs:\n                # The mount point is everything up to and including this directory\n                mount_point = \"/\" + \"/\".join(path_parts[:i])\n                relative_path = absolute_path.replace(mount_point, \"\", 1).lstrip(\"/\")\n                return relative_path\n\n        print(\"Error: Could not determine relative path.\")\n        return None\n    except subprocess.CalledProcessError as e:\n        print(f\"Error retrieving directories from Globus: {e}\")\n        return None\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.globus_transfer","title":"<code>bmtool.SLURM.globus_transfer(source_endpoint, dest_endpoint, source_path, dest_path)</code>","text":"<p>Transfers file using custom globus transfer function. For more info see https://github.com/GregGlickert/transfer-files/blob/main/globus_transfer.sh work in progress still... kinda forgot about this</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def globus_transfer(source_endpoint, dest_endpoint, source_path, dest_path):\n    \"\"\"\n    Transfers file using custom globus transfer function.\n    For more info see https://github.com/GregGlickert/transfer-files/blob/main/globus_transfer.sh\n    work in progress still... kinda forgot about this\n    \"\"\"\n    relative_source_path = get_relative_path(source_endpoint, source_path)\n    if relative_source_path is None:\n        print(\"Transfer aborted: Could not determine relative source path.\")\n        return\n\n    command = f\"globus transfer {source_endpoint}:{relative_source_path} {dest_endpoint}:{dest_path} --label 'bmtool slurm transfer'\"\n    os.system(command)\n</code></pre>"},{"location":"api/slurm/#blockrunner","title":"BlockRunner","text":""},{"location":"api/slurm/#bmtool.SLURM.BlockRunner","title":"<code>bmtool.SLURM.BlockRunner</code>","text":"<p>Class to handle submitting multiple blocks sequentially.</p> <p>Attributes:     blocks (list): List of SimulationBlock instances to be run.     json_editor (seedSweep or multiSweep): Instance of seedSweep to edit JSON file.     param_values (list): List of values for the parameter to be modified.     webhook (str): a microsoft webhook for teams. When used will send teams messages to the hook!</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>class BlockRunner:\n    \"\"\"\n    Class to handle submitting multiple blocks sequentially.\n\n    Attributes:\n        blocks (list): List of SimulationBlock instances to be run.\n        json_editor (seedSweep or multiSweep): Instance of seedSweep to edit JSON file.\n        param_values (list): List of values for the parameter to be modified.\n        webhook (str): a microsoft webhook for teams. When used will send teams messages to the hook!\n    \"\"\"\n\n    def __init__(\n        self,\n        blocks,\n        json_editor=None,\n        json_file_path=None,\n        param_name=None,\n        param_values=None,\n        check_interval=60,\n        syn_dict=None,\n        webhook=None,\n    ):\n        self.blocks = blocks\n        self.json_editor = json_editor\n        self.param_values = param_values\n        self.check_interval = check_interval\n        self.webhook = webhook\n        self.param_name = param_name\n        self.json_file_path = json_file_path\n        self.syn_dict = syn_dict\n        # Store original component paths to restore later\n        self.original_component_paths = [block.component_path for block in self.blocks]\n\n    def restore_component_paths(self):\n        \"\"\"\n        Restores all blocks' component_path to their original values.\n        \"\"\"\n        for i, block in enumerate(self.blocks):\n            block.component_path = self.original_component_paths[i]\n        print(\"Component paths restored to original values.\", flush=True)\n\n    def submit_blocks_sequentially(self):\n        \"\"\"\n        Submits all blocks sequentially, ensuring each block starts only after the previous block has completed or is running.\n        Updates the JSON file with new parameters before each block run.\n        \"\"\"\n        for i, block in enumerate(self.blocks):\n            print(block.output_base_dir)\n            # Update JSON file with new parameter value\n            if self.json_file_path is None and self.param_values is None:\n                source_dir = block.component_path\n                destination_dir = f\"{source_dir}{i+1}\"\n                block.component_path = destination_dir\n                shutil.copytree(\n                    source_dir, destination_dir, dirs_exist_ok=True\n                )  # create new components folder\n                print(f\"skipping json editing for block {block.block_name}\", flush=True)\n            else:\n                if len(self.blocks) != len(self.param_values):\n                    raise Exception(\"Number of blocks needs to each number of params given\")\n                new_value = self.param_values[i]\n                # hope this path is correct\n                source_dir = block.component_path\n                destination_dir = f\"{source_dir}{i+1}\"\n                block.component_path = destination_dir\n\n                shutil.copytree(\n                    source_dir, destination_dir, dirs_exist_ok=True\n                )  # create new components folder\n                json_file_path = os.path.join(destination_dir, self.json_file_path)\n\n                if self.syn_dict is None:\n                    json_editor = seedSweep(json_file_path, self.param_name)\n                    json_editor.edit_json(new_value)\n                else:\n                    # need to keep the orignal around\n                    syn_dict_temp = copy.deepcopy(self.syn_dict)\n                    json_to_be_ratioed = syn_dict_temp[\"json_file_path\"]\n                    corrected_ratio_path = os.path.join(destination_dir, json_to_be_ratioed)\n                    syn_dict_temp[\"json_file_path\"] = corrected_ratio_path\n                    json_editor = multiSeedSweep(\n                        json_file_path, self.param_name, syn_dict=syn_dict_temp, base_ratio=1\n                    )\n                    json_editor.edit_all_jsons(new_value)\n\n            # Submit the block\n            print(f\"Submitting block: {block.block_name}\", flush=True)\n            block.submit_block()\n            if self.webhook:\n                message = f\"SIMULATION UPDATE: Block {i} has been submitted! There are {(len(self.blocks)-1)-i} left to be submitted\"\n                send_teams_message(self.webhook, message)\n\n            # Wait for the block to complete\n            if i == len(self.blocks) - 1:\n                while not block.check_block_status():\n                    print(f\"Waiting for the last block {i} to complete...\")\n                    time.sleep(self.check_interval)\n            else:  # Not the last block so if job is running lets start a new one (checks status list)\n                while not block.check_block_status():\n                    print(f\"Waiting for block {i} to complete...\")\n                    time.sleep(self.check_interval)\n\n            print(f\"Block {block.block_name} completed.\", flush=True)\n        print(\"All blocks are done!\", flush=True)\n        # Restore component paths to their original values\n        self.restore_component_paths()\n        if self.webhook:\n            message = \"SIMULATION UPDATE: Simulation are Done!\"\n            send_teams_message(self.webhook, message)\n\n    def submit_blocks_parallel(self):\n        \"\"\"\n        submits all the blocks at once onto the queue. To do this the components dir will be cloned and each block will have its own.\n        Also the json_file_path should be the path after the components dir\n        \"\"\"\n        for i, block in enumerate(self.blocks):\n            if self.param_values is None:\n                source_dir = block.component_path\n                destination_dir = f\"{source_dir}{i+1}\"\n                block.component_path = destination_dir\n                shutil.copytree(\n                    source_dir, destination_dir, dirs_exist_ok=True\n                )  # create new components folder\n                print(f\"skipping json editing for block {block.block_name}\", flush=True)\n            else:\n                if block.component_path is None:\n                    raise Exception(\n                        \"Unable to use parallel submitter without defining the component path\"\n                    )\n                new_value = self.param_values[i]\n\n                source_dir = block.component_path\n                destination_dir = f\"{source_dir}{i+1}\"\n                block.component_path = destination_dir\n\n                shutil.copytree(\n                    source_dir, destination_dir, dirs_exist_ok=True\n                )  # create new components folder\n                json_file_path = os.path.join(destination_dir, self.json_file_path)\n\n                if self.syn_dict is None:\n                    json_editor = seedSweep(json_file_path, self.param_name)\n                    json_editor.edit_json(new_value)\n                else:\n                    # need to keep the orignal around\n                    syn_dict_temp = copy.deepcopy(self.syn_dict)\n                    json_to_be_ratioed = syn_dict_temp[\"json_file_path\"]\n                    corrected_ratio_path = os.path.join(destination_dir, json_to_be_ratioed)\n                    syn_dict_temp[\"json_file_path\"] = corrected_ratio_path\n                    json_editor = multiSeedSweep(\n                        json_file_path, self.param_name, syn_dict_temp, base_ratio=1\n                    )\n                    json_editor.edit_all_jsons(new_value)\n                # submit block with new component path\n            print(f\"Submitting block: {block.block_name}\", flush=True)\n            block.submit_block()\n            if i == len(self.blocks) - 1:\n                print(\n                    \"\\nEverything has been submitted. You can close out of this or keep this script running to get a message when everything is finished\\n\"\n                )\n                while not block.check_block_status():\n                    print(f\"Waiting for the last block {i} to complete...\")\n                    time.sleep(self.check_interval)\n\n        print(\"All blocks are done!\", flush=True)\n        # Restore component paths to their original values\n        self.restore_component_paths()\n        if self.webhook:\n            message = \"SIMULATION UPDATE: Simulations are Done!\"\n            send_teams_message(self.webhook, message)\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.BlockRunner.__init__","title":"<code>__init__(blocks, json_editor=None, json_file_path=None, param_name=None, param_values=None, check_interval=60, syn_dict=None, webhook=None)</code>","text":"Source code in <code>bmtool/SLURM.py</code> <pre><code>def __init__(\n    self,\n    blocks,\n    json_editor=None,\n    json_file_path=None,\n    param_name=None,\n    param_values=None,\n    check_interval=60,\n    syn_dict=None,\n    webhook=None,\n):\n    self.blocks = blocks\n    self.json_editor = json_editor\n    self.param_values = param_values\n    self.check_interval = check_interval\n    self.webhook = webhook\n    self.param_name = param_name\n    self.json_file_path = json_file_path\n    self.syn_dict = syn_dict\n    # Store original component paths to restore later\n    self.original_component_paths = [block.component_path for block in self.blocks]\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.BlockRunner.submit_blocks_sequentially","title":"<code>submit_blocks_sequentially()</code>","text":"<p>Submits all blocks sequentially, ensuring each block starts only after the previous block has completed or is running. Updates the JSON file with new parameters before each block run.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def submit_blocks_sequentially(self):\n    \"\"\"\n    Submits all blocks sequentially, ensuring each block starts only after the previous block has completed or is running.\n    Updates the JSON file with new parameters before each block run.\n    \"\"\"\n    for i, block in enumerate(self.blocks):\n        print(block.output_base_dir)\n        # Update JSON file with new parameter value\n        if self.json_file_path is None and self.param_values is None:\n            source_dir = block.component_path\n            destination_dir = f\"{source_dir}{i+1}\"\n            block.component_path = destination_dir\n            shutil.copytree(\n                source_dir, destination_dir, dirs_exist_ok=True\n            )  # create new components folder\n            print(f\"skipping json editing for block {block.block_name}\", flush=True)\n        else:\n            if len(self.blocks) != len(self.param_values):\n                raise Exception(\"Number of blocks needs to each number of params given\")\n            new_value = self.param_values[i]\n            # hope this path is correct\n            source_dir = block.component_path\n            destination_dir = f\"{source_dir}{i+1}\"\n            block.component_path = destination_dir\n\n            shutil.copytree(\n                source_dir, destination_dir, dirs_exist_ok=True\n            )  # create new components folder\n            json_file_path = os.path.join(destination_dir, self.json_file_path)\n\n            if self.syn_dict is None:\n                json_editor = seedSweep(json_file_path, self.param_name)\n                json_editor.edit_json(new_value)\n            else:\n                # need to keep the orignal around\n                syn_dict_temp = copy.deepcopy(self.syn_dict)\n                json_to_be_ratioed = syn_dict_temp[\"json_file_path\"]\n                corrected_ratio_path = os.path.join(destination_dir, json_to_be_ratioed)\n                syn_dict_temp[\"json_file_path\"] = corrected_ratio_path\n                json_editor = multiSeedSweep(\n                    json_file_path, self.param_name, syn_dict=syn_dict_temp, base_ratio=1\n                )\n                json_editor.edit_all_jsons(new_value)\n\n        # Submit the block\n        print(f\"Submitting block: {block.block_name}\", flush=True)\n        block.submit_block()\n        if self.webhook:\n            message = f\"SIMULATION UPDATE: Block {i} has been submitted! There are {(len(self.blocks)-1)-i} left to be submitted\"\n            send_teams_message(self.webhook, message)\n\n        # Wait for the block to complete\n        if i == len(self.blocks) - 1:\n            while not block.check_block_status():\n                print(f\"Waiting for the last block {i} to complete...\")\n                time.sleep(self.check_interval)\n        else:  # Not the last block so if job is running lets start a new one (checks status list)\n            while not block.check_block_status():\n                print(f\"Waiting for block {i} to complete...\")\n                time.sleep(self.check_interval)\n\n        print(f\"Block {block.block_name} completed.\", flush=True)\n    print(\"All blocks are done!\", flush=True)\n    # Restore component paths to their original values\n    self.restore_component_paths()\n    if self.webhook:\n        message = \"SIMULATION UPDATE: Simulation are Done!\"\n        send_teams_message(self.webhook, message)\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.BlockRunner.submit_blocks_parallel","title":"<code>submit_blocks_parallel()</code>","text":"<p>submits all the blocks at once onto the queue. To do this the components dir will be cloned and each block will have its own. Also the json_file_path should be the path after the components dir</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def submit_blocks_parallel(self):\n    \"\"\"\n    submits all the blocks at once onto the queue. To do this the components dir will be cloned and each block will have its own.\n    Also the json_file_path should be the path after the components dir\n    \"\"\"\n    for i, block in enumerate(self.blocks):\n        if self.param_values is None:\n            source_dir = block.component_path\n            destination_dir = f\"{source_dir}{i+1}\"\n            block.component_path = destination_dir\n            shutil.copytree(\n                source_dir, destination_dir, dirs_exist_ok=True\n            )  # create new components folder\n            print(f\"skipping json editing for block {block.block_name}\", flush=True)\n        else:\n            if block.component_path is None:\n                raise Exception(\n                    \"Unable to use parallel submitter without defining the component path\"\n                )\n            new_value = self.param_values[i]\n\n            source_dir = block.component_path\n            destination_dir = f\"{source_dir}{i+1}\"\n            block.component_path = destination_dir\n\n            shutil.copytree(\n                source_dir, destination_dir, dirs_exist_ok=True\n            )  # create new components folder\n            json_file_path = os.path.join(destination_dir, self.json_file_path)\n\n            if self.syn_dict is None:\n                json_editor = seedSweep(json_file_path, self.param_name)\n                json_editor.edit_json(new_value)\n            else:\n                # need to keep the orignal around\n                syn_dict_temp = copy.deepcopy(self.syn_dict)\n                json_to_be_ratioed = syn_dict_temp[\"json_file_path\"]\n                corrected_ratio_path = os.path.join(destination_dir, json_to_be_ratioed)\n                syn_dict_temp[\"json_file_path\"] = corrected_ratio_path\n                json_editor = multiSeedSweep(\n                    json_file_path, self.param_name, syn_dict_temp, base_ratio=1\n                )\n                json_editor.edit_all_jsons(new_value)\n            # submit block with new component path\n        print(f\"Submitting block: {block.block_name}\", flush=True)\n        block.submit_block()\n        if i == len(self.blocks) - 1:\n            print(\n                \"\\nEverything has been submitted. You can close out of this or keep this script running to get a message when everything is finished\\n\"\n            )\n            while not block.check_block_status():\n                print(f\"Waiting for the last block {i} to complete...\")\n                time.sleep(self.check_interval)\n\n    print(\"All blocks are done!\", flush=True)\n    # Restore component paths to their original values\n    self.restore_component_paths()\n    if self.webhook:\n        message = \"SIMULATION UPDATE: Simulations are Done!\"\n        send_teams_message(self.webhook, message)\n</code></pre>"},{"location":"api/synapses/","title":"Synapses API Reference","text":"<p>This page provides API reference documentation for the Synapses module, which contains tools for creating, tuning and optimizing synaptic connections in NEURON models.</p>"},{"location":"api/synapses/#synapse-tuning","title":"Synapse Tuning","text":""},{"location":"api/synapses/#bmtool.synapses.SynapseTuner","title":"<code>bmtool.synapses.SynapseTuner</code>","text":"Source code in <code>bmtool/synapses.py</code> <pre><code>class SynapseTuner:\n    def __init__(\n        self,\n        conn_type_settings: Optional[Dict[str, dict]] = None,\n        connection: Optional[str] = None,\n        current_name: str = \"i\",\n        mechanisms_dir: Optional[str] = None,\n        templates_dir: Optional[str] = None,\n        config: Optional[str] = None,\n        general_settings: Optional[dict] = None,\n        json_folder_path: Optional[str] = None,\n        other_vars_to_record: Optional[List[str]] = None,\n        slider_vars: Optional[List[str]] = None,\n        hoc_cell: Optional[object] = None,\n        network: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the SynapseTuner class with connection type settings, mechanisms, and template directories.\n\n        Parameters:\n        -----------\n        mechanisms_dir : Optional[str]\n            Directory path containing the compiled mod files needed for NEURON mechanisms.\n        templates_dir : Optional[str]\n            Directory path containing cell template files (.hoc or .py) loaded into NEURON.\n        conn_type_settings : Optional[dict]\n            A dictionary containing connection-specific settings, such as synaptic properties and details.\n        connection : Optional[str]\n            Name of the connection type to be used from the conn_type_settings dictionary.\n        general_settings : Optional[dict]\n            General settings dictionary including parameters like simulation time step, duration, and temperature.\n        json_folder_path : Optional[str]\n            Path to folder containing JSON files with additional synaptic properties to update settings.\n        current_name : str, optional\n            Name of the synaptic current variable to be recorded (default is 'i').\n        other_vars_to_record : Optional[list]\n            List of additional synaptic variables to record during the simulation (e.g., 'Pr', 'Use').\n        slider_vars : Optional[list]\n            List of synaptic variables you would like sliders set up for the STP sliders method by default will use all parameters in spec_syn_param.\n        hoc_cell : Optional[object]\n            An already loaded NEURON cell object. If provided, template loading and cell setup will be skipped.\n        network : Optional[str]\n            Name of the specific network dataset to access from the loaded edges data (e.g., 'network_to_network').\n            If not provided, will use all available networks. When a config file is provided, this enables\n            the network dropdown feature in InteractiveTuner for switching between different networks.\n\n        Network Dropdown Feature:\n        -------------------------\n        When initialized with a BMTK config file, the tuner automatically:\n        1. Loads all available network datasets from the config\n        2. Creates a network dropdown in InteractiveTuner (if multiple networks exist)\n        3. Allows dynamic switching between networks, which rebuilds connection types\n        4. Updates connection dropdown options when network is changed\n        5. Preserves current connection if it exists in the new network, otherwise selects the first available\n        \"\"\"\n        self.hoc_cell = hoc_cell\n        # Store config and network information for network dropdown functionality\n        self.config = config  # Store config path for network dropdown functionality\n        self.available_networks = []  # Store available networks from config file\n        self.current_network = network  # Store current network selection\n        # Cache for loaded dynamics params JSON by filename to avoid repeated disk reads\n        self._syn_params_cache = {}\n        h.load_file('stdrun.hoc')\n\n        if hoc_cell is None:\n            if config is None and (mechanisms_dir is None or templates_dir is None):\n                raise ValueError(\n                    \"Either a config file, both mechanisms_dir and templates_dir, or a hoc_cell must be provided.\"\n                )\n\n            if config is None:\n                neuron.load_mechanisms(mechanisms_dir)\n                h.load_file(templates_dir)\n            else:\n                # loads both mech and templates\n                load_templates_from_config(config)\n                # Load available networks from config for network dropdown feature\n                self._load_available_networks()\n                # Prebuild connection type settings for each available network to\n                # make network switching in the UI fast. This will make __init__ slower\n                # but dramatically speed up response when changing the network dropdown.\n                self._prebuilt_conn_type_settings = {}\n                try:\n                    for net in self.available_networks:\n                        self._prebuilt_conn_type_settings[net] = self._build_conn_type_settings_from_config(config, network=net)\n                except Exception as e:\n                    print(f\"Warning: error prebuilding conn_type_settings for networks: {e}\")\n\n        if conn_type_settings is None:\n            if config is not None:\n                print(\"Building conn_type_settings from BMTK config files...\")\n                # If we prebuilt per-network settings, use the one for the requested network\n                if hasattr(self, '_prebuilt_conn_type_settings') and network in getattr(self, '_prebuilt_conn_type_settings', {}):\n                    conn_type_settings = self._prebuilt_conn_type_settings[network]\n                else:\n                    conn_type_settings = self._build_conn_type_settings_from_config(config, network=network)\n                print(f\"Found {len(conn_type_settings)} connection types: {list(conn_type_settings.keys())}\")\n\n                # If connection is not specified, use the first available connection\n                if connection is None and conn_type_settings:\n                    connection = list(conn_type_settings.keys())[0]\n                    print(f\"No connection specified, using first available: {connection}\")\n            else:\n                raise ValueError(\"conn_type_settings must be provided if config is not specified.\")\n\n        if connection is None:\n            raise ValueError(\"connection must be provided or inferable from conn_type_settings.\")\n        if connection not in conn_type_settings:\n            raise ValueError(f\"connection '{connection}' not found in conn_type_settings.\")\n\n        self.conn_type_settings: dict = conn_type_settings\n        if json_folder_path:\n            print(f\"updating settings from json path {json_folder_path}\")\n            self._update_spec_syn_param(json_folder_path)\n        # Use default general settings if not provided\n        if general_settings is None:\n            self.general_settings: dict = DEFAULT_GENERAL_SETTINGS.copy()\n        else:\n            # Merge defaults with user-provided\n            self.general_settings = {**DEFAULT_GENERAL_SETTINGS, **general_settings}\n\n        # Store the initial connection name and set up connection\n        self.current_connection = connection\n        self.conn = self.conn_type_settings[connection]\n        self._current_cell_type = self.conn[\"spec_settings\"][\"post_cell\"]\n        self.synaptic_props = self.conn[\"spec_syn_param\"]\n        self.vclamp = self.general_settings[\"vclamp\"]\n        self.current_name = current_name\n        self.other_vars_to_record = other_vars_to_record or []\n        self.ispk = None\n        self.input_mode = False  # Add input_mode attribute\n        self.last_figure = None  # Store reference to last generated figure\n\n        # Store original slider_vars for connection switching\n        self.original_slider_vars = slider_vars or list(self.synaptic_props.keys())\n\n        if slider_vars:\n            # Start by filtering based on keys in slider_vars\n            self.slider_vars = {\n                key: value for key, value in self.synaptic_props.items() if key in slider_vars\n            }\n            # Iterate over slider_vars and check for missing keys in self.synaptic_props\n            for key in slider_vars:\n                # If the key is missing from synaptic_props, get the value using getattr\n                if key not in self.synaptic_props:\n                    try:\n                        self._set_up_cell()\n                        self._set_up_synapse()\n                        value = getattr(self.syn, key)\n                        self.slider_vars[key] = value\n                    except AttributeError as e:\n                        print(f\"Error accessing '{key}' in syn {self.syn}: {e}\")\n        else:\n            self.slider_vars = self.synaptic_props\n\n        h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"]\n        h.dt = self.general_settings[\"dt\"]  # Time step (resolution) of the simulation in ms\n        h.steps_per_ms = 1 / h.dt\n        h.celsius = self.general_settings[\"celsius\"]\n\n        # get some stuff set up we need for both SingleEvent and Interactive Tuner\n        # Only set up cell if hoc_cell was not provided\n        if self.hoc_cell is None:\n            self._set_up_cell()\n        else:\n            self.cell = self.hoc_cell\n        self._set_up_synapse()\n\n        self.nstim = h.NetStim()\n        self.nstim2 = h.NetStim()\n\n        self.vcl = h.VClamp(self.cell.soma[0](0.5))\n\n        self.nc = h.NetCon(\n            self.nstim,\n            self.syn,\n            self.general_settings[\"threshold\"],\n            self.general_settings[\"delay\"],\n            self.general_settings[\"weight\"],\n        )\n        self.nc2 = h.NetCon(\n            self.nstim2,\n            self.syn,\n            self.general_settings[\"threshold\"],\n            self.general_settings[\"delay\"],\n            self.general_settings[\"weight\"],\n        )\n\n        self._set_up_recorders()\n\n    def _build_conn_type_settings_from_config(self, config_path: str, node_set: Optional[str] = None, network: Optional[str] = None) -&gt; Dict[str, dict]:\n        \"\"\"\n        Build conn_type_settings from BMTK simulation and circuit config files using the method used by relation matrix function in util.\n\n        Parameters:\n        -----------\n        config_path : str\n            Path to the simulation config JSON file.\n        node_set : Optional[str]\n            Specific node set to filter connections for. If None, processes all connections.\n        network : Optional[str]\n            Name of the specific network dataset to access (e.g., 'network_to_network').\n            If None, processes all available networks.\n\n        Returns:\n        --------\n        Dict[str, dict]\n            Dictionary with connection names as keys and connection settings as values.\n\n        NOTE: a lot of this code could probs be made a bit more simple or just removed i kinda tried a bunch of things and it works now\n        but is kinda complex and some code is probs note needed \n\n        \"\"\"\n        # Load configuration and get nodes and edges using util.py methods\n        config = load_config(config_path)\n        # Ensure the config dict knows its source path so path substitutions can be resolved\n        try:\n            # load_config may return a dict; store path used so callers can resolve $COMPONENTS_DIR\n            config['config_path'] = config_path\n        except Exception:\n            pass\n        nodes = load_nodes_from_config(config_path)\n        edges = load_edges_from_config(config_path)\n\n        conn_type_settings = {}\n\n        # If a specific network is requested, only process that one\n        if network:\n            if network not in edges:\n                print(f\"Warning: Network '{network}' not found in edges. Available networks: {list(edges.keys())}\")\n                return conn_type_settings\n            edge_datasets = {network: edges[network]}\n        else:\n            edge_datasets = edges\n\n        # Process each edge dataset using the util.py approach\n        for edge_dataset_name, edge_df in edge_datasets.items():\n            if edge_df.empty:\n                continue\n\n            # Create merged DataFrames with source and target node information like util.py does\n            source_node_df = None\n            target_node_df = None\n\n            # First, try to deterministically parse the edge_dataset_name for patterns like '&lt;src&gt;_to_&lt;tgt&gt;'\n            # e.g., 'network_to_network', 'extnet_to_network'\n            if '_to_' in edge_dataset_name:\n                parts = edge_dataset_name.split('_to_')\n                if len(parts) == 2:\n                    src_name, tgt_name = parts\n                    if src_name in nodes:\n                        source_node_df = nodes[src_name].add_prefix('source_')\n                    if tgt_name in nodes:\n                        target_node_df = nodes[tgt_name].add_prefix('target_')\n\n            # If not found by parsing name, fall back to inspecting a sample edge row which contains\n            # explicit 'source_population' and 'target_population' fields (this avoids reversing source/target)\n            if source_node_df is None or target_node_df is None:\n                sample_edge = edge_df.iloc[0] if len(edge_df) &gt; 0 else None\n                if sample_edge is not None:\n                    # Use explicit population names from the edge entry\n                    source_pop_name = sample_edge.get('source_population', '')\n                    target_pop_name = sample_edge.get('target_population', '')\n                    if source_pop_name in nodes:\n                        source_node_df = nodes[source_pop_name].add_prefix('source_')\n                    if target_pop_name in nodes:\n                        target_node_df = nodes[target_pop_name].add_prefix('target_')\n\n            # As a last resort, attempt to heuristically match by prefix/suffix of the dataset name\n            if source_node_df is None or target_node_df is None:\n                for pop_name, node_df in nodes.items():\n                    if source_node_df is None and (edge_dataset_name.startswith(pop_name) or edge_dataset_name.endswith(pop_name)):\n                        source_node_df = node_df.add_prefix('source_')\n                    elif target_node_df is None and (edge_dataset_name.startswith(pop_name) or edge_dataset_name.endswith(pop_name)):\n                        target_node_df = node_df.add_prefix('target_')\n\n            # If we still don't have the node data, skip this edge dataset\n            if source_node_df is None or target_node_df is None:\n                print(f\"Warning: Could not find node data for edge dataset {edge_dataset_name}\")\n                continue\n\n            # Merge edge data with source node info\n            edges_with_source = pd.merge(\n                edge_df.reset_index(), \n                source_node_df, \n                how='left', \n                left_on='source_node_id', \n                right_index=True\n            )\n\n            # Merge with target node info\n            edges_with_nodes = pd.merge(\n                edges_with_source, \n                target_node_df, \n                how='left', \n                left_on='target_node_id', \n                right_index=True\n            )\n\n            # Get unique edge types from the merged dataset\n            if 'edge_type_id' in edges_with_nodes.columns:\n                edge_types = edges_with_nodes['edge_type_id'].unique()\n            else:\n                edge_types = [0]  # Single edge type\n\n            # Process each edge type\n            for edge_type_id in edge_types:\n                # Filter edges for this type\n                if 'edge_type_id' in edges_with_nodes.columns:\n                    edge_type_data = edges_with_nodes[edges_with_nodes['edge_type_id'] == edge_type_id]\n                else:\n                    edge_type_data = edges_with_nodes\n\n                if len(edge_type_data) == 0:\n                    continue\n\n                # Get representative edge for this type\n                edge_info = edge_type_data.iloc[0]\n\n                # Skip gap junctions\n                if 'is_gap_junction' in edge_info and pd.notna(edge_info['is_gap_junction']) and edge_info['is_gap_junction']:\n                    continue\n\n                # Get population names from the merged data (this is the key improvement!)\n                source_pop = edge_info.get('source_pop_name', '')\n                target_pop = edge_info.get('target_pop_name', '')\n\n                # Get target cell template from the merged data\n                target_model_template = edge_info.get('target_model_template', '')\n                if target_model_template.startswith('hoc:'):\n                    target_cell_type = target_model_template.replace('hoc:', '')\n                else:\n                    target_cell_type = target_model_template\n\n                # Create connection name using the actual population names\n                if source_pop and target_pop:\n                    conn_name = f\"{source_pop}2{target_pop}\"\n                else:\n                    conn_name = f\"{edge_dataset_name}_type_{edge_type_id}\"\n\n                # Get synaptic model template\n                model_template = edge_info.get('model_template', 'exp2syn')\n\n                # Build connection settings early so we can attach metadata like dynamics file name\n                conn_settings = {\n                    'spec_settings': {\n                        'post_cell': target_cell_type,\n                        'vclamp_amp': -70.0,  # Default voltage clamp amplitude\n                        'sec_x': 0.5,  # Default location on section\n                        'sec_id': 0,   # Default to soma\n                        # level_of_detail may be overridden by dynamics params below\n                        'level_of_detail': model_template,\n                    },\n                    'spec_syn_param': {}\n                }\n\n                # Load synaptic parameters from dynamics_params file if available.\n                # NOTE: the edge DataFrame produced by load_edges_from_config/load_sonata_edges_to_dataframe\n                # already contains the 'dynamics_params' column (from the CSV) or the\n                # flattened H5 dynamics_params attributes (prefixed with 'dynamics_params/').\n                # Prefer the direct 'dynamics_params' column value from the merged DataFrame\n                # rather than performing ad-hoc string parsing here.\n                syn_params = {}\n                dynamics_file_name = None\n                # Prefer a top-level 'dynamics_params' column if present\n                if 'dynamics_params' in edge_info and pd.notna(edge_info.get('dynamics_params')):\n                    val = edge_info.get('dynamics_params')\n                    # Some CSV loaders can produce bytes or numpy types; coerce to str\n                    try:\n                        dynamics_file_name = str(val).strip()\n                    except Exception:\n                        dynamics_file_name = None\n\n                # If we found a dynamics file name, use it directly (skip token parsing)\n                if dynamics_file_name and dynamics_file_name.upper() != 'NULL':\n                    try:\n                        conn_settings['spec_settings']['dynamics_params_file'] = dynamics_file_name\n                        # use a cache to avoid re-reading the same JSON multiple times\n                        if dynamics_file_name in self._syn_params_cache:\n                            syn_params = self._syn_params_cache[dynamics_file_name]\n                        else:\n                            syn_params = self._load_synaptic_params_from_config(config, dynamics_file_name)\n                            # cache result (even if empty dict) to avoid repeated lookups\n                            self._syn_params_cache[dynamics_file_name] = syn_params\n                    except Exception as e:\n                        print(f\"Warning: could not load dynamics_params file '{dynamics_file_name}' for edge {edge_dataset_name}: {e}\")\n\n                # If a dynamics params JSON filename was provided, prefer using its basename\n                # as the connection name so that the UI matches the JSON definitions.\n                if dynamics_file_name:\n                    try:\n                        json_base = os.path.splitext(os.path.basename(dynamics_file_name))[0]\n                        # Ensure uniqueness in conn_type_settings\n                        if json_base in conn_type_settings:\n                            # Append edge_type_id to disambiguate\n                            json_base = f\"{json_base}_type_{edge_type_id}\"\n                        conn_name = json_base\n                    except Exception:\n                        pass\n\n                # If the dynamics params defined a level_of_detail, override the default\n                if isinstance(syn_params, dict) and 'level_of_detail' in syn_params:\n                    conn_settings['spec_settings']['level_of_detail'] = syn_params.get('level_of_detail', model_template)\n\n                # Add synaptic parameters, excluding level_of_detail\n                for key, value in syn_params.items():\n                    if key != 'level_of_detail':\n                        conn_settings['spec_syn_param'][key] = value\n                else:\n                    # Fallback: some SONATA/H5 edge files expose dynamics params as flattened\n                    # columns named like 'dynamics_params/&lt;param&gt;'. If no filename was given,\n                    # gather any such columns from edge_info and use them as spec_syn_param.\n                    for col in edge_info.index:\n                        if isinstance(col, str) and col.startswith('dynamics_params/'):\n                            param_key = col.split('/', 1)[1]\n                            try:\n                                val = edge_info[col]\n                                if pd.notna(val):\n                                    conn_settings['spec_syn_param'][param_key] = val\n                            except Exception:\n                                # Ignore malformed entries\n                                pass\n\n                # Add weight from edge info if available\n                if 'syn_weight' in edge_info and pd.notna(edge_info['syn_weight']):\n                    conn_settings['spec_syn_param']['initW'] = float(edge_info['syn_weight'])\n\n                # Handle afferent section information\n                if 'afferent_section_id' in edge_info and pd.notna(edge_info['afferent_section_id']):\n                    conn_settings['spec_settings']['sec_id'] = int(edge_info['afferent_section_id'])\n\n                if 'afferent_section_pos' in edge_info and pd.notna(edge_info['afferent_section_pos']):\n                    conn_settings['spec_settings']['sec_x'] = float(edge_info['afferent_section_pos'])\n\n                # Store in connection settings\n                conn_type_settings[conn_name] = conn_settings\n\n        return conn_type_settings\n\n    def _load_available_networks(self) -&gt; None:\n        \"\"\"\n        Load available network names from the config file for the network dropdown feature.\n\n        This method is automatically called during initialization when a config file is provided.\n        It populates the available_networks list which enables the network dropdown in \n        InteractiveTuner when multiple networks are available.\n\n        Network Dropdown Behavior:\n        -------------------------\n        - If only one network exists: No network dropdown is shown\n        - If multiple networks exist: Network dropdown appears next to connection dropdown\n        - Networks are loaded from the edges data in the config file\n        - Current network defaults to the first available if not specified during init\n        \"\"\"\n        if self.config is None:\n            self.available_networks = []\n            return\n\n        try:\n            edges = load_edges_from_config(self.config)\n            self.available_networks = list(edges.keys())\n\n            # Set current network to first available if not specified\n            if self.current_network is None and self.available_networks:\n                self.current_network = self.available_networks[0]\n        except Exception as e:\n            print(f\"Warning: Could not load networks from config: {e}\")\n            self.available_networks = []\n\n    def _load_synaptic_params_from_config(self, config: dict, dynamics_params: str) -&gt; dict:\n        \"\"\"\n        Load synaptic parameters from dynamics params file using config information.\n\n        Parameters:\n        -----------\n        config : dict\n            BMTK configuration dictionary\n        dynamics_params : str\n            Dynamics parameters filename\n\n        Returns:\n        --------\n        dict\n            Synaptic parameters dictionary\n        \"\"\"\n        try:\n            # Get the synaptic models directory from config\n            synaptic_models_dir = config.get('components', {}).get('synaptic_models_dir', '')\n            if synaptic_models_dir:\n                # Handle path variables\n                if synaptic_models_dir.startswith('$'):\n                    # This is a placeholder, try to resolve it\n                    config_dir = os.path.dirname(config.get('config_path', ''))\n                    synaptic_models_dir = synaptic_models_dir.replace('$COMPONENTS_DIR', \n                                                                    os.path.join(config_dir, 'components'))\n                    synaptic_models_dir = synaptic_models_dir.replace('$BASE_DIR', config_dir)\n\n                dynamics_file = os.path.join(synaptic_models_dir, dynamics_params)\n\n                if os.path.exists(dynamics_file):\n                    with open(dynamics_file, 'r') as f:\n                        return json.load(f)\n                else:\n                    print(f\"Warning: Dynamics params file not found: {dynamics_file}\")\n        except Exception as e:\n            print(f\"Warning: Error loading synaptic parameters: {e}\")\n\n        return {}\n\n    @classmethod\n    def list_connections_from_config(cls, config_path: str, network: Optional[str] = None) -&gt; Dict[str, dict]:\n        \"\"\"\n        Class method to list all available connections from a BMTK config file without creating a tuner.\n\n        Parameters:\n        -----------\n        config_path : str\n            Path to the simulation config JSON file.\n        network : Optional[str]\n            Name of the specific network dataset to access (e.g., 'network_to_network').\n            If None, processes all available networks.\n\n        Returns:\n        --------\n        Dict[str, dict]\n            Dictionary with connection names as keys and connection info as values.\n        \"\"\"\n        # Create a temporary instance just to use the parsing methods\n        temp_tuner = cls.__new__(cls)  # Create without calling __init__\n        conn_type_settings = temp_tuner._build_conn_type_settings_from_config(config_path, network=network)\n\n        # Create a summary of connections with key info\n        connections_summary = {}\n        for conn_name, settings in conn_type_settings.items():\n            connections_summary[conn_name] = {\n                'post_cell': settings['spec_settings']['post_cell'],\n                'synapse_type': settings['spec_settings']['level_of_detail'],\n                'parameters': list(settings['spec_syn_param'].keys())\n            }\n\n        return connections_summary\n\n    def _switch_connection(self, new_connection: str) -&gt; None:\n        \"\"\"\n        Switch to a different connection type and update all related properties.\n\n        Parameters:\n        -----------\n        new_connection : str\n            Name of the new connection type to switch to.\n        \"\"\"\n        if new_connection not in self.conn_type_settings:\n            raise ValueError(f\"Connection '{new_connection}' not found in conn_type_settings.\")\n\n        # Update current connection\n        self.current_connection = new_connection\n        self.conn = self.conn_type_settings[new_connection]\n        self.synaptic_props = self.conn[\"spec_syn_param\"]\n\n        # Update slider vars for new connection\n        if hasattr(self, 'original_slider_vars'):\n            # Filter slider vars based on new connection's parameters\n            self.slider_vars = {\n                key: value for key, value in self.synaptic_props.items() \n                if key in self.original_slider_vars\n            }\n\n            # Check for missing keys and try to get them from the synapse\n            for key in self.original_slider_vars:\n                if key not in self.synaptic_props:\n                    try:\n                        # We'll get this after recreating the synapse\n                        pass\n                    except AttributeError as e:\n                        print(f\"Warning: Could not access '{key}' for connection '{new_connection}': {e}\")\n        else:\n            self.slider_vars = self.synaptic_props\n\n        # Need to recreate the cell if it's different\n        if self.hoc_cell is None:\n            # Check if we need a different cell type\n            new_cell_type = self.conn[\"spec_settings\"][\"post_cell\"]\n            if not hasattr(self, '_current_cell_type') or self._current_cell_type != new_cell_type:\n                self._current_cell_type = new_cell_type\n                self._set_up_cell()\n\n        # Recreate synapse for new connection\n        self._set_up_synapse()\n\n        # Update any missing slider vars from the new synapse\n        if hasattr(self, 'original_slider_vars'):\n            for key in self.original_slider_vars:\n                if key not in self.synaptic_props:\n                    try:\n                        value = getattr(self.syn, key)\n                        self.slider_vars[key] = value\n                    except AttributeError as e:\n                        print(f\"Warning: Could not access '{key}' for connection '{new_connection}': {e}\")\n\n        # Recreate NetCon connections with new synapse\n        self.nc = h.NetCon(\n            self.nstim,\n            self.syn,\n            self.general_settings[\"threshold\"],\n            self.general_settings[\"delay\"],\n            self.general_settings[\"weight\"],\n        )\n        self.nc2 = h.NetCon(\n            self.nstim2,\n            self.syn,\n            self.general_settings[\"threshold\"],\n            self.general_settings[\"delay\"],\n            self.general_settings[\"weight\"],\n        )\n\n        # Recreate voltage clamp with potentially new cell\n        self.vcl = h.VClamp(self.cell.soma[0](0.5))\n\n        # Recreate recorders for new synapse\n        self._set_up_recorders()\n\n        # Reset NEURON state\n        h.finitialize()\n\n        print(f\"Successfully switched to connection: {new_connection}\")\n\n    def _switch_network(self, new_network: str) -&gt; None:\n        \"\"\"\n        Switch to a different network and rebuild conn_type_settings for the new network.\n\n        This method is called when the user selects a different network from the network \n        dropdown in InteractiveTuner. It performs a complete rebuild of the connection \n        types available for the new network.\n\n        Parameters:\n        -----------\n        new_network : str\n            Name of the new network to switch to.\n\n        Network Switching Process:\n        -------------------------\n        1. Validates the new network exists in available_networks\n        2. Rebuilds conn_type_settings using the new network's edge data\n        3. Updates the connection dropdown with new network's available connections\n        4. Preserves current connection if it exists in new network\n        5. Falls back to first available connection if current doesn't exist\n        6. Recreates synapses and NEURON objects for the new connection\n        7. Updates UI components to reflect the changes\n        \"\"\"\n        if new_network not in self.available_networks:\n            print(f\"Warning: Network '{new_network}' not found in available networks: {self.available_networks}\")\n            return\n\n        if new_network == self.current_network:\n            return  # No change needed\n\n        # Update current network\n        self.current_network = new_network\n\n        # Switch conn_type_settings using prebuilt data if available, otherwise build on-demand\n        if self.config:\n            print(f\"Switching connections for network: {new_network}\")\n            if hasattr(self, '_prebuilt_conn_type_settings') and new_network in self._prebuilt_conn_type_settings:\n                self.conn_type_settings = self._prebuilt_conn_type_settings[new_network]\n            else:\n                # Fallback: build on-demand (slower)\n                self.conn_type_settings = self._build_conn_type_settings_from_config(self.config, network=new_network)\n\n            # Update available connections and select first one if current doesn't exist\n            available_connections = list(self.conn_type_settings.keys())\n            if self.current_connection not in available_connections and available_connections:\n                self.current_connection = available_connections[0]\n                print(f\"Connection '{self.current_connection}' not available in new network. Switched to: {available_connections[0]}\")\n\n            # Switch to the (potentially new) connection\n            if self.current_connection in self.conn_type_settings:\n                self._switch_connection(self.current_connection)\n\n            print(f\"Successfully switched to network: {new_network}\")\n            print(f\"Available connections: {available_connections}\")\n\n    def _update_spec_syn_param(self, json_folder_path: str) -&gt; None:\n        \"\"\"\n        Update specific synaptic parameters using JSON files located in the specified folder.\n\n        Parameters:\n        -----------\n        json_folder_path : str\n            Path to folder containing JSON files, where each JSON file corresponds to a connection type.\n        \"\"\"\n        if not self.conn_type_settings:\n            return\n        for conn_type, settings in self.conn_type_settings.items():\n            json_file_path = os.path.join(json_folder_path, f\"{conn_type}.json\")\n            if os.path.exists(json_file_path):\n                with open(json_file_path, \"r\") as json_file:\n                    json_data = json.load(json_file)\n                    settings[\"spec_syn_param\"].update(json_data)\n            else:\n                print(f\"JSON file for {conn_type} not found.\")\n\n    def _set_up_cell(self) -&gt; None:\n        \"\"\"\n        Set up the neuron cell based on the specified connection settings.\n        This method is only called when hoc_cell is not provided.\n        \"\"\"\n        if self.hoc_cell is None:\n            self.cell = getattr(h, self.conn[\"spec_settings\"][\"post_cell\"])()\n        else:\n            self.cell = self.hoc_cell\n\n    def _set_up_synapse(self) -&gt; None:\n        \"\"\"\n        Set up the synapse on the target cell according to the synaptic parameters in `conn_type_settings`.\n\n        Notes:\n        ------\n        - `_set_up_cell()` should be called before setting up the synapse.\n        - Synapse location, type, and properties are specified within `spec_syn_param` and `spec_settings`.\n        \"\"\"\n        try:\n            self.syn = getattr(h, self.conn[\"spec_settings\"][\"level_of_detail\"])(\n                list(self.cell.all)[self.conn[\"spec_settings\"][\"sec_id\"]](\n                    self.conn[\"spec_settings\"][\"sec_x\"]\n                )\n            )\n        except:\n            raise Exception(\"Make sure the mod file exist you are trying to load check spelling!\")\n        for key, value in self.conn[\"spec_syn_param\"].items():\n            if isinstance(value, (int, float)):\n                if hasattr(self.syn, key):\n                    setattr(self.syn, key, value)\n                else:\n                    print(\n                        f\"Warning: {key} cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\"\n                    )\n\n    def _set_up_recorders(self) -&gt; None:\n        \"\"\"\n        Set up recording vectors to capture simulation data.\n\n        The method sets up recorders for:\n        - Synaptic current specified by `current_name`\n        - Other specified synaptic variables (`other_vars_to_record`)\n        - Time, soma voltage, and voltage clamp current for all simulations.\n        \"\"\"\n        self.rec_vectors = {}\n        for var in self.other_vars_to_record:\n            self.rec_vectors[var] = h.Vector()\n            ref_attr = f\"_ref_{var}\"\n            if hasattr(self.syn, ref_attr):\n                self.rec_vectors[var].record(getattr(self.syn, ref_attr))\n            else:\n                print(\n                    f\"Warning: {ref_attr} not found in the syn object. Use vars() to inspect available attributes.\"\n                )\n\n        # Record synaptic current\n        self.rec_vectors[self.current_name] = h.Vector()\n        ref_attr = f\"_ref_{self.current_name}\"\n        if hasattr(self.syn, ref_attr):\n            self.rec_vectors[self.current_name].record(getattr(self.syn, ref_attr))\n        else:\n            print(\"Warning: Synaptic current recorder not set up correctly.\")\n\n        # Record time, synaptic events, soma voltage, and voltage clamp current\n        self.t = h.Vector()\n        self.tspk = h.Vector()\n        self.soma_v = h.Vector()\n        self.ivcl = h.Vector()\n\n        self.t.record(h._ref_t)\n        self.nc.record(self.tspk)\n        self.nc2.record(self.tspk)\n        self.soma_v.record(self.cell.soma[0](0.5)._ref_v)\n        self.ivcl.record(self.vcl._ref_i)\n\n    def SingleEvent(self, plot_and_print=True):\n        \"\"\"\n        Simulate a single synaptic event by delivering an input stimulus to the synapse.\n\n        The method sets up the neuron cell, synapse, stimulus, and voltage clamp,\n        and then runs the NEURON simulation for a single event. The single synaptic event will occur at general_settings['tstart']\n        Will display graphs and synaptic properies works best with a jupyter notebook\n        \"\"\"\n        self.ispk = None\n\n        # user slider values if the sliders are set up\n        if hasattr(self, \"dynamic_sliders\"):\n            syn_props = {var: slider.value for var, slider in self.dynamic_sliders.items()}\n            self._set_syn_prop(**syn_props)\n\n        # sets values based off optimizer\n        if hasattr(self, \"using_optimizer\"):\n            for name, value in zip(self.param_names, self.params):\n                setattr(self.syn, name, value)\n\n        # Set up the stimulus\n        self.nstim.start = self.general_settings[\"tstart\"]\n        self.nstim.noise = 0\n        self.nstim2.start = h.tstop\n        self.nstim2.noise = 0\n\n        # Set up voltage clamp\n        vcldur = [[0, 0, 0], [self.general_settings[\"tstart\"], h.tstop, 1e9]]\n        for i in range(3):\n            self.vcl.amp[i] = self.conn[\"spec_settings\"][\"vclamp_amp\"]\n            self.vcl.dur[i] = vcldur[1][i]\n\n        # Run simulation\n        h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"]\n        self.nstim.interval = self.general_settings[\"tdur\"]\n        self.nstim.number = 1\n        self.nstim2.start = h.tstop\n        h.run()\n\n        current = np.array(self.rec_vectors[self.current_name])\n        syn_props = self._get_syn_prop(\n            rise_interval=self.general_settings[\"rise_interval\"], dt=h.dt\n        )\n        current = (current - syn_props[\"baseline\"]) * 1000  # Convert to pA\n        current_integral = np.trapz(current, dx=h.dt)  # pA\u00b7ms\n\n        if plot_and_print:\n            self._plot_model(\n                [\n                    self.general_settings[\"tstart\"] - 5,\n                    self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"],\n                ]\n            )\n            for prop in syn_props.items():\n                print(prop)\n            print(f\"Current Integral in pA*ms: {current_integral:.2f}\")\n\n        self.rise_time = syn_props[\"rise_time\"]\n        self.decay_time = syn_props[\"decay_time\"]\n\n    def _find_first(self, x):\n        \"\"\"\n        Find the index of the first non-zero element in a given array.\n\n        Parameters:\n        -----------\n        x : np.array\n            The input array to search.\n\n        Returns:\n        --------\n        int\n            Index of the first non-zero element, or None if none exist.\n        \"\"\"\n        x = np.asarray(x)\n        idx = np.nonzero(x)[0]\n        return idx[0] if idx.size else None\n\n    def _get_syn_prop(self, rise_interval=(0.2, 0.8), dt=h.dt, short=False):\n        \"\"\"\n        Calculate synaptic properties such as peak amplitude, latency, rise time, decay time, and half-width.\n\n        Parameters:\n        -----------\n        rise_interval : tuple of floats, optional\n            Fractional rise time interval to calculate (default is (0.2, 0.8)).\n        dt : float, optional\n            Time step of the simulation (default is NEURON's `h.dt`).\n        short : bool, optional\n            If True, only return baseline and sign without calculating full properties.\n\n        Returns:\n        --------\n        dict\n            A dictionary containing the synaptic properties: baseline, sign, peak amplitude, latency, rise time,\n            decay time, and half-width.\n        \"\"\"\n        if self.vclamp:\n            isyn = self.ivcl\n        else:\n            isyn = self.rec_vectors[self.current_name]\n        isyn = np.asarray(isyn)\n        tspk = np.asarray(self.tspk)\n        if tspk.size:\n            tspk = tspk[0]\n\n        ispk = int(np.floor(tspk / dt))\n        baseline = isyn[ispk]\n        isyn = isyn[ispk:] - baseline\n        # print(np.abs(isyn))\n        # print(np.argmax(np.abs(isyn)))\n        # print(isyn[np.argmax(np.abs(isyn))])\n        # print(np.sign(isyn[np.argmax(np.abs(isyn))]))\n        sign = np.sign(isyn[np.argmax(np.abs(isyn))])\n        if short:\n            return {\"baseline\": baseline, \"sign\": sign}\n        isyn *= sign\n        # print(isyn)\n        # peak amplitude\n        ipk, _ = find_peaks(isyn)\n        ipk = ipk[0]\n        peak = isyn[ipk]\n        # latency\n        istart = self._find_first(np.diff(isyn[: ipk + 1]) &gt; 0)\n        latency = dt * (istart + 1)\n        # rise time\n        rt1 = self._find_first(isyn[istart : ipk + 1] &gt; rise_interval[0] * peak)\n        rt2 = self._find_first(isyn[istart : ipk + 1] &gt; rise_interval[1] * peak)\n        rise_time = (rt2 - rt1) * dt\n        # decay time\n        iend = self._find_first(np.diff(isyn[ipk:]) &gt; 0)\n        iend = isyn.size - 1 if iend is None else iend + ipk\n        decay_len = iend - ipk + 1\n        popt, _ = curve_fit(\n            lambda t, a, tau: a * np.exp(-t / tau),\n            dt * np.arange(decay_len),\n            isyn[ipk : iend + 1],\n            p0=(peak, dt * decay_len / 2),\n        )\n        decay_time = popt[1]\n        # half-width\n        hw1 = self._find_first(isyn[istart : ipk + 1] &gt; 0.5 * peak)\n        hw2 = self._find_first(isyn[ipk:] &lt; 0.5 * peak)\n        hw2 = isyn.size if hw2 is None else hw2 + ipk\n        half_width = dt * (hw2 - hw1)\n        output = {\n            \"baseline\": baseline,\n            \"sign\": sign,\n            \"latency\": latency,\n            \"amp\": peak,\n            \"rise_time\": rise_time,\n            \"decay_time\": decay_time,\n            \"half_width\": half_width,\n        }\n        return output\n\n    def _plot_model(self, xlim):\n        \"\"\"\n        Plots the results of the simulation, including synaptic current, soma voltage,\n        and any additional recorded variables.\n\n        Parameters:\n        -----------\n        xlim : tuple\n            A tuple specifying the limits of the x-axis for the plot (start_time, end_time).\n\n        Notes:\n        ------\n        - The function determines how many plots to generate based on the number of variables recorded.\n        - Synaptic current and either voltage clamp or soma voltage will always be plotted.\n        - If other variables are provided in `other_vars_to_record`, they are also plotted.\n        - The function adjusts the plot layout and removes any extra subplots that are not needed.\n        \"\"\"\n        # Determine the number of plots to generate (at least 2: current and voltage)\n        num_vars_to_plot = 2 + (len(self.other_vars_to_record) if self.other_vars_to_record else 0)\n\n        # Set up figure based on number of plots (2x2 grid max)\n        num_rows = (num_vars_to_plot + 1) // 2  # This ensures we have enough rows\n        fig, axs = plt.subplots(num_rows, 2, figsize=(12, 7))\n        axs = axs.ravel()\n\n        # Plot synaptic current (always included)\n        current = self.rec_vectors[self.current_name]\n        syn_prop = self._get_syn_prop(short=True, dt=h.dt)\n        current = current - syn_prop[\"baseline\"]\n        current = current * 1000\n\n        axs[0].plot(self.t, current)\n        if self.ispk is not None:\n            for num in range(len(self.ispk)):\n                axs[0].text(self.t[self.ispk[num]], current[self.ispk[num]], f\"{str(num+1)}\")\n\n        axs[0].set_ylabel(\"Synaptic Current (pA)\")\n\n        # Plot voltage clamp or soma voltage (always included)\n        ispk = int(np.round(self.tspk[0] / h.dt))\n        if self.vclamp:\n            baseline = self.ivcl[ispk]\n            ivcl_plt = np.array(self.ivcl) - baseline\n            ivcl_plt[:ispk] = 0\n            axs[1].plot(self.t, 1000 * ivcl_plt)\n            axs[1].set_ylabel(\"VClamp Current (pA)\")\n        else:\n            soma_v_plt = np.array(self.soma_v)\n            soma_v_plt[:ispk] = soma_v_plt[ispk]\n\n            axs[1].plot(self.t, soma_v_plt)\n            axs[1].set_ylabel(\"Soma Voltage (mV)\")\n\n        # Plot any other variables from other_vars_to_record, if provided\n        if self.other_vars_to_record:\n            for i, var in enumerate(self.other_vars_to_record, start=2):\n                if var in self.rec_vectors:\n                    axs[i].plot(self.t, self.rec_vectors[var])\n                    axs[i].set_ylabel(f\"{var.capitalize()}\")\n\n        # Adjust the layout\n        for i, ax in enumerate(axs[:num_vars_to_plot]):\n            ax.set_xlim(*xlim)\n            if i &gt;= num_vars_to_plot - 2:  # Add x-label to the last row\n                ax.set_xlabel(\"Time (ms)\")\n\n        # Remove extra subplots if less than 4 plots are present\n        if num_vars_to_plot &lt; len(axs):\n            for j in range(num_vars_to_plot, len(axs)):\n                fig.delaxes(axs[j])\n\n        #plt.tight_layout()\n        fig.suptitle(f\"Connection: {self.current_connection}\")\n        self.last_figure = plt.gcf()\n        plt.show()\n\n    def _set_drive_train(self, freq=50.0, delay=250.0):\n        \"\"\"\n        Configures trains of 12 action potentials at a specified frequency and delay period\n        between pulses 8 and 9.\n\n        Parameters:\n        -----------\n        freq : float, optional\n            Frequency of the pulse train in Hz. Default is 50 Hz.\n        delay : float, optional\n            Delay period in milliseconds between the 8th and 9th pulses. Default is 250 ms.\n\n        Returns:\n        --------\n        tstop : float\n            The time at which the last pulse stops.\n\n        Notes:\n        ------\n        - This function is based on experiments from the Allen Database.\n        \"\"\"\n        # lets also set the train drive and delay here\n        self.train_freq = freq\n        self.train_delay = delay\n\n        n_init_pulse = 8\n        n_ending_pulse = 4\n        self.nstim.start = self.general_settings[\"tstart\"]\n        self.nstim.interval = 1000 / freq\n        self.nstim2.interval = 1000 / freq\n        self.nstim.number = n_init_pulse\n        self.nstim2.number = n_ending_pulse\n        self.nstim2.start = self.nstim.start + (n_init_pulse - 1) * self.nstim.interval + delay\n        tstop = self.nstim2.start + n_ending_pulse * self.nstim2.interval\n        return tstop\n\n    def _response_amplitude(self):\n        \"\"\"\n        Calculates the amplitude of synaptic responses for each pulse in a train.\n\n        Returns:\n        --------\n        amp : list\n            A list containing the peak amplitudes for each pulse in the recorded synaptic current.\n\n        Notes:\n        ------\n        This method:\n        1. Extracts and normalizes the synaptic current\n        2. Identifies spike times and segments the current accordingly\n        3. Calculates the peak response amplitude for each segment\n        4. Records the indices of peak amplitudes for visualization\n\n        The amplitude values are returned in the original current units (before pA conversion).\n        \"\"\"\n        isyn = np.array(self.rec_vectors[self.current_name].to_python())\n        tspk = np.append(np.asarray(self.tspk), h.tstop)\n        syn_prop = self._get_syn_prop(short=True, dt=h.dt)\n        # print(\"syn_prp[sign] = \" + str(syn_prop['sign']))\n        isyn = isyn - syn_prop[\"baseline\"]\n        isyn *= syn_prop[\"sign\"]\n        ispk = np.floor((tspk + self.general_settings[\"delay\"]) / h.dt).astype(int)\n\n        try:\n            amp = [isyn[ispk[i] : ispk[i + 1]].max() for i in range(ispk.size - 1)]\n            # indexs of where the max of the synaptic current is at. This is then plotted\n            self.ispk = [\n                np.argmax(isyn[ispk[i] : ispk[i + 1]]) + ispk[i] for i in range(ispk.size - 1)\n            ]\n        # Sometimes the sim can cutoff at the peak of synaptic activity. So we just reduce the range by 1 and ingore that point\n        except:\n            amp = [isyn[ispk[i] : ispk[i + 1]].max() for i in range(ispk.size - 2)]\n            self.ispk = [\n                np.argmax(isyn[ispk[i] : ispk[i + 1]]) + ispk[i] for i in range(ispk.size - 2)\n            ]\n\n        return amp\n\n    def _find_max_amp(self, amp):\n        \"\"\"\n        Determines the maximum amplitude from the response data and returns the max in pA\n\n        Parameters:\n        -----------\n        amp : array-like\n            Array containing the amplitudes of synaptic responses.\n\n        Returns:\n        --------\n        max_amp : float\n            The maximum or minimum amplitude based on the sign of the response.\n        \"\"\"\n        max_amp = max(amp)\n        min_amp = min(amp)\n        if abs(min_amp) &gt; max_amp:\n            return min_amp * 1000  # scale unit\n        return max_amp * 1000  # scale unit\n\n    def _calc_ppr_induction_recovery(self, amp, normalize_by_trial=True, print_math=True):\n        \"\"\"\n        Calculates paired-pulse ratio, induction, recovery, and simple PPR metrics from response amplitudes.\n\n        Parameters:\n        -----------\n        amp : array-like\n            Array containing the amplitudes of synaptic responses to a pulse train.\n        normalize_by_trial : bool, optional\n            If True, normalize the amplitudes within each trial. Default is True.\n        print_math : bool, optional\n            If True, print detailed calculation steps and explanations. Default is True.\n\n        Returns:\n        --------\n        tuple\n            A tuple containing:\n            - ppr: Paired-pulse ratio (2nd pulse - 1st pulse) normalized by 90th percentile amplitude\n            - induction: Measure of facilitation/depression during initial pulses\n            - recovery: Measure of recovery after the delay period\n            - simple_ppr: Simple paired-pulse ratio (2nd pulse / 1st pulse)\n\n        Notes:\n        ------\n        - PPR &gt; 0 indicates facilitation, PPR &lt; 0 indicates depression\n        - Simple PPR &gt; 1 indicates facilitation, Simple PPR &lt; 1 indicates depression\n        - Induction &gt; 0 indicates facilitation, Induction &lt; 0 indicates depression\n        - Recovery compares the response after delay to the initial pulses\n        \"\"\"\n        amp = np.array(amp)\n        amp = amp * 1000  # scale up\n        amp = amp.reshape(-1, amp.shape[-1])\n\n        # Calculate 90th percentile amplitude for normalization\n        percentile_90 = np.percentile(amp, 90)\n\n        def format_array(arr):\n            \"\"\"Format an array to 2 significant figures for cleaner output.\"\"\"\n            return np.array2string(arr, precision=2, separator=\", \", suppress_small=True)\n\n        if print_math:\n            print(\"\\n\" + \"=\" * 40)\n            print(\n                f\"Short Term Plasticity Results for {self.train_freq}Hz with {self.train_delay} Delay\"\n            )\n            print(\"=\" * 40)\n            print(\"Simple PPR: Above 1 is facilitating, below 1 is depressing\")\n            print(\"PPR:        Above 0 is facilitating, below 0 is depressing.\")\n            print(\"Induction:  Above 0 is facilitating, below 0 is depressing.\")\n            print(\"Recovery:   A measure of how fast STP decays.\\n\")\n\n            # Simple PPR Calculation: Avg 2nd pulse / Avg 1st pulse\n            simple_ppr = np.mean(amp[:, 1:2]) / np.mean(amp[:, 0:1])\n            print(\"Simple Paired Pulse Ratio (PPR)\")\n            print(\"    Calculation: Avg 2nd pulse / Avg 1st pulse\")\n            print(\n                f\"    Values: {np.mean(amp[:, 1:2]):.3f} / {np.mean(amp[:, 0:1]):.3f} = {simple_ppr:.3f}\\n\"\n            )\n\n            # PPR Calculation: (Avg 2nd pulse - Avg 1st pulse) / 90th percentile amplitude\n            ppr = (np.mean(amp[:, 1:2]) - np.mean(amp[:, 0:1])) / percentile_90\n            print(\"Paired Pulse Response (PPR)\")\n            print(\"    Calculation: (Avg 2nd pulse - Avg 1st pulse) / 90th percentile amplitude\")\n            print(\n                f\"    Values: ({np.mean(amp[:, 1:2]):.3f} - {np.mean(amp[:, 0:1]):.3f}) / {percentile_90:.3f} = {ppr:.3f}\\n\"\n            )\n\n\n            # Induction Calculation: (Avg (6th, 7th, 8th pulses) - Avg 1st pulse) / 90th percentile amplitude\n            induction = (np.mean(amp[:, 5:8]) - np.mean(amp[:, :1])) / percentile_90\n            print(\"Induction\")\n            print(\"    Calculation: (Avg(6th, 7th, 8th pulses) - Avg 1st pulse) / 90th percentile amplitude\")\n            print(\n                f\"    Values: {np.mean(amp[:, 5:8]):.3f} - {np.mean(amp[:, :1]):.3f} / {percentile_90:.3f} = {induction:.3f}\\n\"\n            )\n\n            # Recovery Calculation: (Avg (9th, 10th, 11th, 12th pulses) - Avg (1st, 2nd, 3rd, 4th pulses)) / 90th percentile amplitude\n            recovery = (np.mean(amp[:, 8:12]) - np.mean(amp[:, :4])) / percentile_90\n            print(\"Recovery\")\n            print(\n                \"    Calculation: (Avg(9th, 10th, 11th, 12th pulses) - Avg(1st to 4th pulses)) / 90th percentile amplitude\"\n            )\n            print(\n                f\"    Values: {np.mean(amp[:, 8:12]):.3f} - {np.mean(amp[:, :4]):.3f} / {percentile_90:.3f} = {recovery:.3f}\\n\"\n            )\n\n            print(\"=\" * 40 + \"\\n\")\n\n        # Calculate final metrics\n        ppr = (np.mean(amp[:, 1:2]) - np.mean(amp[:, 0:1])) / percentile_90\n        induction = (np.mean(amp[:, 5:8]) - np.mean(amp[:, :1])) / percentile_90\n        recovery = (np.mean(amp[:, 8:12]) - np.mean(amp[:, :4])) / percentile_90\n        simple_ppr = np.mean(amp[:, 1:2]) / np.mean(amp[:, 0:1])\n\n        return ppr, induction, recovery, simple_ppr\n\n    def _set_syn_prop(self, **kwargs):\n        \"\"\"\n        Sets the synaptic parameters based on user inputs from sliders.\n\n        Parameters:\n        -----------\n        **kwargs : dict\n            Synaptic properties (such as weight, Use, tau_f, tau_d) as keyword arguments.\n        \"\"\"\n        for key, value in kwargs.items():\n            setattr(self.syn, key, value)\n\n    def _simulate_model(self, input_frequency, delay, vclamp=None):\n        \"\"\"\n        Runs the simulation with the specified input frequency, delay, and voltage clamp settings.\n\n        Parameters:\n        -----------\n        input_frequency : float\n            Frequency of the input drive train in Hz.\n        delay : float\n            Delay period in milliseconds between the 8th and 9th pulses.\n        vclamp : bool or None, optional\n            Whether to use voltage clamp. If None, the current setting is used. Default is None.\n\n        Notes:\n        ------\n        This method handles two different input modes:\n        - Standard train mode with 8 initial pulses followed by a delay and 4 additional pulses\n        - Continuous input mode where stimulation continues for a specified duration\n        \"\"\"\n        if not self.input_mode:\n            self.tstop = self._set_drive_train(input_frequency, delay)\n            h.tstop = self.tstop\n\n            vcldur = [[0, 0, 0], [self.general_settings[\"tstart\"], self.tstop, 1e9]]\n            for i in range(3):\n                self.vcl.amp[i] = self.conn[\"spec_settings\"][\"vclamp_amp\"]\n                self.vcl.dur[i] = vcldur[1][i]\n            h.finitialize(70 * mV)\n            h.continuerun(self.tstop * ms)\n            #h.run()\n        else:\n            # Continuous input mode: ensure simulation runs long enough for the full stimulation duration\n            self.tstop = self.general_settings[\"tstart\"] + self.w_duration.value + 300 # 300ms buffer time \n            self.nstim.interval = 1000 / input_frequency\n            self.nstim.number = np.ceil(self.w_duration.value / 1000 * input_frequency + 1)\n            self.nstim2.number = 0\n\n            h.finitialize(70 * mV)\n            h.continuerun(self.tstop * ms)\n            #h.run()\n\n    def InteractiveTuner(self):\n        \"\"\"\n        Sets up interactive sliders for tuning short-term plasticity (STP) parameters in a Jupyter Notebook.\n\n        This method creates an interactive UI with sliders for:\n        - Network selection dropdown (if multiple networks available and config provided)\n        - Connection type selection dropdown\n        - Input frequency\n        - Delay between pulse trains\n        - Duration of stimulation (for continuous input mode)\n        - Synaptic parameters (e.g., Use, tau_f, tau_d) based on the syn model\n\n        It also provides buttons for:\n        - Running a single event simulation\n        - Running a train input simulation\n        - Toggling voltage clamp mode\n        - Switching between standard and continuous input modes\n\n        Network Dropdown Feature:\n        ------------------------\n        When the SynapseTuner is initialized with a BMTK config file containing multiple networks:\n        - A network dropdown appears next to the connection dropdown\n        - Users can dynamically switch between networks (e.g., 'network_to_network', 'external_to_network')\n        - Switching networks rebuilds available connections and updates the connection dropdown\n        - The current connection is preserved if it exists in the new network\n        - If multiple networks exist but only one is specified during init, that network is used as default\n\n        Notes:\n        ------\n        Ideal for exploratory parameter tuning and interactive visualization of\n        synapse behavior with different parameter values and stimulation protocols.\n        The network dropdown feature enables comprehensive exploration of multi-network\n        BMTK simulations without needing to reinitialize the tuner.\n        \"\"\"\n        # Widgets setup (Sliders)\n        freqs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200]\n        delays = [125, 250, 500, 1000, 2000, 4000]\n        durations = [100, 300, 500, 1000, 2000, 5000, 10000]\n        freq0 = 50\n        delay0 = 250\n        duration0 = 300\n        vlamp_status = self.vclamp\n\n        # Connection dropdown\n        connection_options = sorted(list(self.conn_type_settings.keys()))\n        w_connection = widgets.Dropdown(\n            options=connection_options,\n            value=self.current_connection,\n            description=\"Connection:\",\n            style={'description_width': 'initial'}\n        )\n\n        # Network dropdown - only shown if config was provided and multiple networks are available\n        # This enables users to switch between different network datasets dynamically\n        w_network = None\n        if self.config is not None and len(self.available_networks) &gt; 1:\n            w_network = widgets.Dropdown(\n                options=self.available_networks,\n                value=self.current_network,\n                description=\"Network:\",\n                style={'description_width': 'initial'}\n            )\n\n        w_run = widgets.Button(description=\"Run Train\", icon=\"history\", button_style=\"primary\")\n        w_single = widgets.Button(description=\"Single Event\", icon=\"check\", button_style=\"success\")\n        w_vclamp = widgets.ToggleButton(\n            value=vlamp_status,\n            description=\"Voltage Clamp\",\n            icon=\"fast-backward\",\n            button_style=\"warning\",\n        )\n\n        # Voltage clamp amplitude input\n        default_vclamp_amp = getattr(self.conn['spec_settings'], 'vclamp_amp', -70.0)\n        w_vclamp_amp = widgets.FloatText(\n            value=default_vclamp_amp,\n            description=\"V_clamp (mV):\",\n            step=5.0,\n            style={'description_width': 'initial'},\n            layout=widgets.Layout(width='150px')\n        )\n\n        w_input_mode = widgets.ToggleButton(\n            value=False, description=\"Continuous input\", icon=\"eject\", button_style=\"info\"\n        )\n        w_input_freq = widgets.SelectionSlider(options=freqs, value=freq0, description=\"Input Freq\")\n\n        # Sliders for delay and duration\n        self.w_delay = widgets.SelectionSlider(options=delays, value=delay0, description=\"Delay\")\n        self.w_duration = widgets.SelectionSlider(\n            options=durations, value=duration0, description=\"Duration\"\n        )\n\n        # Save functionality widgets\n        save_path_text = widgets.Text(\n            value=\"plot.png\",\n            description=\"Save path:\",\n            layout=widgets.Layout(width='300px')\n        )\n        save_button = widgets.Button(description=\"Save Plot\", icon=\"save\", button_style=\"success\")\n\n        def save_plot(b):\n            if hasattr(self, 'last_figure') and self.last_figure is not None:\n                try:\n                    # Create a new figure with just the first subplot (synaptic current)\n                    fig, ax = plt.subplots(figsize=(8, 6))\n\n                    # Get the axes from the original figure\n                    original_axes = self.last_figure.get_axes()\n                    if len(original_axes) &gt; 0:\n                        first_ax = original_axes[0]\n\n                        # Copy the data from the first subplot\n                        for line in first_ax.get_lines():\n                            ax.plot(line.get_xdata(), line.get_ydata(), \n                                   color=line.get_color(), label=line.get_label())\n\n                        # Copy axis labels and title\n                        ax.set_xlabel(first_ax.get_xlabel())\n                        ax.set_ylabel(first_ax.get_ylabel())\n                        ax.set_title(first_ax.get_title())\n                        ax.set_xlim(first_ax.get_xlim())\n                        ax.legend()\n                        ax.grid(True)\n\n                        # Save the new figure\n                        fig.savefig(save_path_text.value)\n                        plt.close(fig)  # Close the temporary figure\n                        print(f\"Synaptic current plot saved to {save_path_text.value}\")\n                    else:\n                        print(\"No subplots found in the figure\")\n\n                except Exception as e:\n                    print(f\"Error saving plot: {e}\")\n            else:\n                print(\"No plot to save\")\n\n        save_button.on_click(save_plot)\n\n        def create_dynamic_sliders():\n            \"\"\"Create sliders based on current connection's parameters\"\"\"\n            sliders = {}\n            for key, value in self.slider_vars.items():\n                if isinstance(value, (int, float)):  # Only create sliders for numeric values\n                    if hasattr(self.syn, key):\n                        if value == 0:\n                            print(\n                                f\"{key} was set to zero, going to try to set a range of values, try settings the {key} to a nonzero value if you dont like the range!\"\n                            )\n                            slider = widgets.FloatSlider(\n                                value=value, min=0, max=1000, step=1, description=key\n                            )\n                        else:\n                            slider = widgets.FloatSlider(\n                                value=value, min=0, max=value * 20, step=value / 5, description=key\n                            )\n                        sliders[key] = slider\n                    else:\n                        print(f\"skipping slider for {key} due to not being a synaptic variable\")\n            return sliders\n\n        # Generate sliders dynamically based on valid numeric entries in self.slider_vars\n        self.dynamic_sliders = create_dynamic_sliders()\n        print(\n            \"Setting up slider! The sliders ranges are set by their init value so try changing that if you dont like the slider range!\"\n        )\n\n        # Create output widget for displaying results\n        output_widget = widgets.Output()\n\n        def run_single_event(*args):\n            clear_output()\n            display(ui)\n            display(output_widget)\n\n            self.vclamp = w_vclamp.value\n            # Update voltage clamp amplitude if voltage clamp is enabled\n            if self.vclamp:\n                # Update the voltage clamp amplitude settings\n                self.conn['spec_settings']['vclamp_amp'] = w_vclamp_amp.value\n                # Update general settings if they exist\n                if hasattr(self, 'general_settings'):\n                    self.general_settings['vclamp_amp'] = w_vclamp_amp.value\n            # Update synaptic properties based on slider values\n            self.ispk = None\n\n            # Clear previous results and run simulation\n            output_widget.clear_output()\n            with output_widget:\n                self.SingleEvent()\n\n        def on_connection_change(*args):\n            \"\"\"Handle connection dropdown change\"\"\"\n            try:\n                new_connection = w_connection.value\n                if new_connection != self.current_connection:\n                    # Switch to new connection\n                    self._switch_connection(new_connection)\n\n                    # Recreate dynamic sliders for new connection\n                    self.dynamic_sliders = create_dynamic_sliders()\n\n                    # Update UI\n                    update_ui_layout()\n                    update_ui()\n\n            except Exception as e:\n                print(f\"Error switching connection: {e}\")\n\n        def on_network_change(*args):\n            \"\"\"\n            Handle network dropdown change events.\n\n            This callback is triggered when the user selects a different network from \n            the network dropdown. It coordinates the complete switching process:\n            1. Calls _switch_network() to rebuild connections for the new network\n            2. Updates the connection dropdown options with new network's connections\n            3. Recreates dynamic sliders for new connection parameters\n            4. Refreshes the entire UI to reflect all changes\n            \"\"\"\n            if w_network is None:\n                return\n            try:\n                new_network = w_network.value\n                if new_network != self.current_network:\n                    # Switch to new network\n                    self._switch_network(new_network)\n\n                    # Update connection dropdown options with new network's connections\n                    connection_options = list(self.conn_type_settings.keys())\n                    w_connection.options = connection_options\n                    if connection_options:\n                        w_connection.value = self.current_connection\n\n                    # Recreate dynamic sliders for new connection\n                    self.dynamic_sliders = create_dynamic_sliders()\n\n                    # Update UI\n                    update_ui_layout()\n                    update_ui()\n\n            except Exception as e:\n                print(f\"Error switching network: {e}\")\n\n        def update_ui_layout():\n            \"\"\"\n            Update the UI layout with new sliders and network dropdown.\n\n            This function reconstructs the entire UI layout including:\n            - Network dropdown (if available) and connection dropdown in the top row\n            - Button controls and input mode toggles\n            - Parameter sliders arranged in columns\n            \"\"\"\n            nonlocal ui, slider_columns\n\n            # Add the dynamic sliders to the UI\n            slider_widgets = [slider for slider in self.dynamic_sliders.values()]\n\n            if slider_widgets:\n                half = len(slider_widgets) // 2\n                col1 = VBox(slider_widgets[:half])\n                col2 = VBox(slider_widgets[half:])\n                slider_columns = HBox([col1, col2])\n            else:\n                slider_columns = VBox([])\n\n            # Create button row with voltage clamp controls\n            if w_vclamp.value:  # Show voltage clamp amplitude input when toggle is on\n                button_row = HBox([w_run, w_single, w_vclamp, w_vclamp_amp, w_input_mode])\n            else:  # Hide voltage clamp amplitude input when toggle is off\n                button_row = HBox([w_run, w_single, w_vclamp, w_input_mode])\n\n            # Construct the top row - include network dropdown if available\n            # This creates a horizontal layout with network dropdown (if present) and connection dropdown\n            if w_network is not None:\n                connection_row = HBox([w_network, w_connection])\n            else:\n                connection_row = HBox([w_connection])\n            slider_row = HBox([w_input_freq, self.w_delay, self.w_duration])\n            save_row = HBox([save_path_text, save_button])\n\n            ui = VBox([connection_row, button_row, slider_row, slider_columns, save_row])\n\n        # Function to update UI based on input mode\n        def update_ui(*args):\n            clear_output()\n            display(ui)\n            display(output_widget)\n\n            self.vclamp = w_vclamp.value\n            # Update voltage clamp amplitude if voltage clamp is enabled\n            if self.vclamp:\n                self.conn['spec_settings']['vclamp_amp'] = w_vclamp_amp.value\n                if hasattr(self, 'general_settings'):\n                    self.general_settings['vclamp_amp'] = w_vclamp_amp.value\n\n            self.input_mode = w_input_mode.value\n            syn_props = {var: slider.value for var, slider in self.dynamic_sliders.items()}\n            self._set_syn_prop(**syn_props)\n\n            # Clear previous results and run simulation\n            output_widget.clear_output()\n            with output_widget:\n                if not self.input_mode:\n                    self._simulate_model(w_input_freq.value, self.w_delay.value, w_vclamp.value)\n                else:\n                    self._simulate_model(w_input_freq.value, self.w_duration.value, w_vclamp.value)\n                amp = self._response_amplitude()\n                self._plot_model(\n                    [self.general_settings[\"tstart\"] - self.nstim.interval / 3, self.tstop]\n                )\n                _ = self._calc_ppr_induction_recovery(amp)\n\n        # Function to switch between delay and duration sliders\n        def switch_slider(*args):\n            if w_input_mode.value:\n                self.w_delay.layout.display = \"none\"  # Hide delay slider\n                self.w_duration.layout.display = \"\"  # Show duration slider\n            else:\n                self.w_delay.layout.display = \"\"  # Show delay slider\n                self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n        # Function to handle voltage clamp toggle\n        def on_vclamp_toggle(*args):\n            \"\"\"Handle voltage clamp toggle changes to show/hide amplitude input\"\"\"\n            update_ui_layout()\n            clear_output()\n            display(ui)\n            display(output_widget)\n\n        # Link widgets to their callback functions\n        w_connection.observe(on_connection_change, names=\"value\")\n        # Link network dropdown callback only if network dropdown was created\n        if w_network is not None:\n            w_network.observe(on_network_change, names=\"value\")\n        w_input_mode.observe(switch_slider, names=\"value\")\n        w_vclamp.observe(on_vclamp_toggle, names=\"value\")\n\n        # Hide the duration slider initially until the user selects it\n        self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n        w_single.on_click(run_single_event)\n        w_run.on_click(update_ui)\n\n        # Initial UI setup\n        slider_columns = VBox([])\n        ui = VBox([])\n        update_ui_layout()\n\n        display(ui)\n        update_ui()\n\n    def stp_frequency_response(\n        self,\n        freqs=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200],\n        delay=250,\n        plot=True,\n        log_plot=True,\n    ):\n        \"\"\"\n        Analyze synaptic response across different stimulation frequencies.\n\n        This method systematically tests how the synapse model responds to different\n        stimulation frequencies, calculating key short-term plasticity (STP) metrics\n        for each frequency.\n\n        Parameters:\n        -----------\n        freqs : list, optional\n            List of frequencies to analyze (in Hz). Default covers a wide range from 1-200 Hz.\n        delay : float, optional\n            Delay between pulse trains in ms. Default is 250 ms.\n        plot : bool, optional\n            Whether to plot the results. Default is True.\n        log_plot : bool, optional\n            Whether to use logarithmic scale for frequency axis. Default is True.\n\n        Returns:\n        --------\n        dict\n            Dictionary containing frequency-dependent metrics with keys:\n            - 'frequencies': List of tested frequencies\n            - 'ppr': Paired-pulse ratios at each frequency\n            - 'simple_ppr': Simple paired-pulse ratios (2nd/1st pulse) at each frequency\n            - 'induction': Induction values at each frequency\n            - 'recovery': Recovery values at each frequency\n\n        Notes:\n        ------\n        This method is particularly useful for characterizing the frequency-dependent\n        behavior of synapses, such as identifying facilitating vs. depressing regimes\n        or the frequency at which a synapse transitions between these behaviors.\n        \"\"\"\n        results = {\"frequencies\": freqs, \"ppr\": [], \"induction\": [], \"recovery\": [], \"simple_ppr\": []}\n\n        # Store original state\n        original_ispk = self.ispk\n\n        for freq in tqdm(freqs, desc=\"Analyzing frequencies\"):\n            self._simulate_model(freq, delay)\n            amp = self._response_amplitude()\n            ppr, induction, recovery, simple_ppr = self._calc_ppr_induction_recovery(amp, print_math=False)\n\n            results[\"ppr\"].append(float(ppr))\n            results[\"induction\"].append(float(induction))\n            results[\"recovery\"].append(float(recovery))\n            results[\"simple_ppr\"].append(float(simple_ppr))\n\n        # Restore original state\n        self.ispk = original_ispk\n\n        if plot:\n            self._plot_frequency_analysis(results, log_plot=log_plot)\n\n        return results\n\n    def _plot_frequency_analysis(self, results, log_plot):\n        \"\"\"\n        Plot the frequency-dependent synaptic properties.\n\n        Parameters:\n        -----------\n        results : dict\n            Dictionary containing frequency analysis results with keys:\n            - 'frequencies': List of tested frequencies\n            - 'ppr': Paired-pulse ratios at each frequency\n            - 'simple_ppr': Simple paired-pulse ratios at each frequency\n            - 'induction': Induction values at each frequency\n            - 'recovery': Recovery values at each frequency\n        log_plot : bool\n            Whether to use logarithmic scale for frequency axis\n\n        Notes:\n        ------\n        Creates a figure with three subplots showing:\n        1. Paired-pulse ratios (both normalized and simple) vs. frequency\n        2. Induction vs. frequency\n        3. Recovery vs. frequency\n\n        Each plot includes a horizontal reference line at y=0 or y=1 to indicate\n        the boundary between facilitation and depression.\n        \"\"\"\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n\n        # Plot both PPR measures\n        if log_plot:\n            ax1.semilogx(results[\"frequencies\"], results[\"ppr\"], \"o-\", label=\"Normalized PPR\")\n            ax1.semilogx(results[\"frequencies\"], results[\"simple_ppr\"], \"s-\", label=\"Simple PPR\")\n        else:\n            ax1.plot(results[\"frequencies\"], results[\"ppr\"], \"o-\", label=\"Normalized PPR\")\n            ax1.plot(results[\"frequencies\"], results[\"simple_ppr\"], \"s-\", label=\"Simple PPR\")\n        ax1.axhline(y=1, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax1.set_xlabel(\"Frequency (Hz)\")\n        ax1.set_ylabel(\"Paired Pulse Ratio\")\n        ax1.set_title(\"PPR vs Frequency\")\n        ax1.legend()\n        ax1.grid(True)\n\n        # Plot Induction\n        if log_plot:\n            ax2.semilogx(results[\"frequencies\"], results[\"induction\"], \"o-\")\n        else:\n            ax2.plot(results[\"frequencies\"], results[\"induction\"], \"o-\")\n        ax2.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax2.set_xlabel(\"Frequency (Hz)\")\n        ax2.set_ylabel(\"Induction\")\n        ax2.set_title(\"Induction vs Frequency\")\n        ax2.grid(True)\n\n        # Plot Recovery\n        if log_plot:\n            ax3.semilogx(results[\"frequencies\"], results[\"recovery\"], \"o-\")\n        else:\n            ax3.plot(results[\"frequencies\"], results[\"recovery\"], \"o-\")\n        ax3.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax3.set_xlabel(\"Frequency (Hz)\")\n        ax3.set_ylabel(\"Recovery\")\n        ax3.set_title(\"Recovery vs Frequency\")\n        ax3.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n\n\n    def generate_synaptic_table(self, stp_frequency=50.0, stp_delay=250.0, plot=True):\n        \"\"\"\n        Generate a comprehensive table of synaptic parameters for all connections.\n\n        This method iterates through all available connections, runs simulations to \n        characterize each synapse, and compiles the results into a pandas DataFrame.\n\n        Parameters:\n        -----------\n        stp_frequency : float, optional\n            Frequency in Hz to use for STP (short-term plasticity) analysis. Default is 50.0 Hz.\n        stp_delay : float, optional\n            Delay in ms between pulse trains for STP analysis. Default is 250.0 ms.\n        plot : bool, optional\n            Whether to display the resulting table. Default is True.\n\n        Returns:\n        --------\n        pd.DataFrame\n            DataFrame containing synaptic parameters for each connection with columns:\n            - connection: Connection name\n            - rise_time: 20-80% rise time (ms)\n            - decay_time: Decay time constant (ms)\n            - latency: Response latency (ms)\n            - half_width: Response half-width (ms)\n            - peak_amplitude: Peak synaptic current amplitude (pA)\n            - baseline: Baseline current (pA)\n            - ppr: Paired-pulse ratio (normalized)\n            - simple_ppr: Simple paired-pulse ratio (2nd/1st pulse)\n            - induction: STP induction measure\n            - recovery: STP recovery measure\n\n        Notes:\n        ------\n        This method temporarily switches between connections to characterize each one,\n        then restores the original connection. The STP metrics are calculated at the\n        specified frequency and delay.\n        \"\"\"\n        # Store original connection to restore later\n        original_connection = self.current_connection\n\n        # Initialize results list\n        results = []\n\n        print(f\"Analyzing {len(self.conn_type_settings)} connections...\")\n\n        for conn_name in tqdm(self.conn_type_settings.keys(), desc=\"Analyzing connections\"):\n            try:\n                # Switch to this connection\n                self._switch_connection(conn_name)\n\n                # Run single event analysis\n                self.SingleEvent(plot_and_print=False)\n\n                # Get synaptic properties from the single event\n                syn_props = self._get_syn_prop()\n\n                # Run STP analysis at specified frequency\n                stp_results = self.stp_frequency_response(\n                    freqs=[stp_frequency], \n                    delay=stp_delay, \n                    plot=False, \n                    log_plot=False\n                )\n\n                # Extract STP metrics for this frequency\n                freq_idx = 0  # Only one frequency tested\n                ppr = stp_results['ppr'][freq_idx]\n                induction = stp_results['induction'][freq_idx]\n                recovery = stp_results['recovery'][freq_idx]\n                simple_ppr = stp_results['simple_ppr'][freq_idx]\n\n                # Compile results for this connection\n                conn_results = {\n                    'connection': conn_name,\n                    'rise_time': float(self.rise_time),\n                    'decay_time': float(self.decay_time),\n                    'latency': float(syn_props.get('latency', 0)),\n                    'half_width': float(syn_props.get('half_width', 0)),\n                    'peak_amplitude': float(syn_props.get('amp', 0)),\n                    'baseline': float(syn_props.get('baseline', 0)),\n                    'ppr': float(ppr),\n                    'simple_ppr': float(simple_ppr),\n                    'induction': float(induction),\n                    'recovery': float(recovery)\n                }\n\n                results.append(conn_results)\n\n            except Exception as e:\n                print(f\"Warning: Failed to analyze connection '{conn_name}': {e}\")\n                # Add partial results if possible\n                results.append({\n                    'connection': conn_name,\n                    'rise_time': float('nan'),\n                    'decay_time': float('nan'),\n                    'latency': float('nan'),\n                    'half_width': float('nan'),\n                    'peak_amplitude': float('nan'),\n                    'baseline': float('nan'),\n                    'ppr': float('nan'),\n                    'simple_ppr': float('nan'),\n                    'induction': float('nan'),\n                    'recovery': float('nan')\n                })\n\n        # Restore original connection\n        if original_connection in self.conn_type_settings:\n            self._switch_connection(original_connection)\n\n        # Create DataFrame\n        df = pd.DataFrame(results)\n\n        # Set connection as index for better display\n        df = df.set_index('connection')\n\n        if plot:\n            # Display the table\n            print(\"\\nSynaptic Parameters Table:\")\n            print(\"=\" * 80)\n            display(df.round(4))\n\n            # Optional: Create a simple bar plot for key metrics\n            try:\n                fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n                fig.suptitle(f'Synaptic Parameters Across Connections (STP at {stp_frequency}Hz)', fontsize=16)\n\n                # Plot rise/decay times\n                df[['rise_time', 'decay_time']].plot(kind='bar', ax=axes[0,0])\n                axes[0,0].set_title('Rise and Decay Times')\n                axes[0,0].set_ylabel('Time (ms)')\n                axes[0,0].tick_params(axis='x', rotation=45)\n\n                # Plot PPR metrics\n                df[['ppr', 'simple_ppr']].plot(kind='bar', ax=axes[0,1])\n                axes[0,1].set_title('Paired-Pulse Ratios')\n                axes[0,1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n                axes[0,1].tick_params(axis='x', rotation=45)\n\n                # Plot induction\n                df['induction'].plot(kind='bar', ax=axes[1,0], color='green')\n                axes[1,0].set_title('STP Induction')\n                axes[1,0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n                axes[1,0].set_ylabel('Induction')\n                axes[1,0].tick_params(axis='x', rotation=45)\n\n                # Plot recovery\n                df['recovery'].plot(kind='bar', ax=axes[1,1], color='orange')\n                axes[1,1].set_title('STP Recovery')\n                axes[1,1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n                axes[1,1].set_ylabel('Recovery')\n                axes[1,1].tick_params(axis='x', rotation=45)\n\n                plt.tight_layout()\n                plt.show()\n\n            except Exception as e:\n                print(f\"Warning: Could not create plots: {e}\")\n\n        return df\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner.__init__","title":"<code>__init__(conn_type_settings=None, connection=None, current_name='i', mechanisms_dir=None, templates_dir=None, config=None, general_settings=None, json_folder_path=None, other_vars_to_record=None, slider_vars=None, hoc_cell=None, network=None)</code>","text":"<p>Initialize the SynapseTuner class with connection type settings, mechanisms, and template directories.</p> Parameters: <p>mechanisms_dir : Optional[str]     Directory path containing the compiled mod files needed for NEURON mechanisms. templates_dir : Optional[str]     Directory path containing cell template files (.hoc or .py) loaded into NEURON. conn_type_settings : Optional[dict]     A dictionary containing connection-specific settings, such as synaptic properties and details. connection : Optional[str]     Name of the connection type to be used from the conn_type_settings dictionary. general_settings : Optional[dict]     General settings dictionary including parameters like simulation time step, duration, and temperature. json_folder_path : Optional[str]     Path to folder containing JSON files with additional synaptic properties to update settings. current_name : str, optional     Name of the synaptic current variable to be recorded (default is 'i'). other_vars_to_record : Optional[list]     List of additional synaptic variables to record during the simulation (e.g., 'Pr', 'Use'). slider_vars : Optional[list]     List of synaptic variables you would like sliders set up for the STP sliders method by default will use all parameters in spec_syn_param. hoc_cell : Optional[object]     An already loaded NEURON cell object. If provided, template loading and cell setup will be skipped. network : Optional[str]     Name of the specific network dataset to access from the loaded edges data (e.g., 'network_to_network').     If not provided, will use all available networks. When a config file is provided, this enables     the network dropdown feature in InteractiveTuner for switching between different networks.</p> Network Dropdown Feature: <p>When initialized with a BMTK config file, the tuner automatically: 1. Loads all available network datasets from the config 2. Creates a network dropdown in InteractiveTuner (if multiple networks exist) 3. Allows dynamic switching between networks, which rebuilds connection types 4. Updates connection dropdown options when network is changed 5. Preserves current connection if it exists in the new network, otherwise selects the first available</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def __init__(\n    self,\n    conn_type_settings: Optional[Dict[str, dict]] = None,\n    connection: Optional[str] = None,\n    current_name: str = \"i\",\n    mechanisms_dir: Optional[str] = None,\n    templates_dir: Optional[str] = None,\n    config: Optional[str] = None,\n    general_settings: Optional[dict] = None,\n    json_folder_path: Optional[str] = None,\n    other_vars_to_record: Optional[List[str]] = None,\n    slider_vars: Optional[List[str]] = None,\n    hoc_cell: Optional[object] = None,\n    network: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the SynapseTuner class with connection type settings, mechanisms, and template directories.\n\n    Parameters:\n    -----------\n    mechanisms_dir : Optional[str]\n        Directory path containing the compiled mod files needed for NEURON mechanisms.\n    templates_dir : Optional[str]\n        Directory path containing cell template files (.hoc or .py) loaded into NEURON.\n    conn_type_settings : Optional[dict]\n        A dictionary containing connection-specific settings, such as synaptic properties and details.\n    connection : Optional[str]\n        Name of the connection type to be used from the conn_type_settings dictionary.\n    general_settings : Optional[dict]\n        General settings dictionary including parameters like simulation time step, duration, and temperature.\n    json_folder_path : Optional[str]\n        Path to folder containing JSON files with additional synaptic properties to update settings.\n    current_name : str, optional\n        Name of the synaptic current variable to be recorded (default is 'i').\n    other_vars_to_record : Optional[list]\n        List of additional synaptic variables to record during the simulation (e.g., 'Pr', 'Use').\n    slider_vars : Optional[list]\n        List of synaptic variables you would like sliders set up for the STP sliders method by default will use all parameters in spec_syn_param.\n    hoc_cell : Optional[object]\n        An already loaded NEURON cell object. If provided, template loading and cell setup will be skipped.\n    network : Optional[str]\n        Name of the specific network dataset to access from the loaded edges data (e.g., 'network_to_network').\n        If not provided, will use all available networks. When a config file is provided, this enables\n        the network dropdown feature in InteractiveTuner for switching between different networks.\n\n    Network Dropdown Feature:\n    -------------------------\n    When initialized with a BMTK config file, the tuner automatically:\n    1. Loads all available network datasets from the config\n    2. Creates a network dropdown in InteractiveTuner (if multiple networks exist)\n    3. Allows dynamic switching between networks, which rebuilds connection types\n    4. Updates connection dropdown options when network is changed\n    5. Preserves current connection if it exists in the new network, otherwise selects the first available\n    \"\"\"\n    self.hoc_cell = hoc_cell\n    # Store config and network information for network dropdown functionality\n    self.config = config  # Store config path for network dropdown functionality\n    self.available_networks = []  # Store available networks from config file\n    self.current_network = network  # Store current network selection\n    # Cache for loaded dynamics params JSON by filename to avoid repeated disk reads\n    self._syn_params_cache = {}\n    h.load_file('stdrun.hoc')\n\n    if hoc_cell is None:\n        if config is None and (mechanisms_dir is None or templates_dir is None):\n            raise ValueError(\n                \"Either a config file, both mechanisms_dir and templates_dir, or a hoc_cell must be provided.\"\n            )\n\n        if config is None:\n            neuron.load_mechanisms(mechanisms_dir)\n            h.load_file(templates_dir)\n        else:\n            # loads both mech and templates\n            load_templates_from_config(config)\n            # Load available networks from config for network dropdown feature\n            self._load_available_networks()\n            # Prebuild connection type settings for each available network to\n            # make network switching in the UI fast. This will make __init__ slower\n            # but dramatically speed up response when changing the network dropdown.\n            self._prebuilt_conn_type_settings = {}\n            try:\n                for net in self.available_networks:\n                    self._prebuilt_conn_type_settings[net] = self._build_conn_type_settings_from_config(config, network=net)\n            except Exception as e:\n                print(f\"Warning: error prebuilding conn_type_settings for networks: {e}\")\n\n    if conn_type_settings is None:\n        if config is not None:\n            print(\"Building conn_type_settings from BMTK config files...\")\n            # If we prebuilt per-network settings, use the one for the requested network\n            if hasattr(self, '_prebuilt_conn_type_settings') and network in getattr(self, '_prebuilt_conn_type_settings', {}):\n                conn_type_settings = self._prebuilt_conn_type_settings[network]\n            else:\n                conn_type_settings = self._build_conn_type_settings_from_config(config, network=network)\n            print(f\"Found {len(conn_type_settings)} connection types: {list(conn_type_settings.keys())}\")\n\n            # If connection is not specified, use the first available connection\n            if connection is None and conn_type_settings:\n                connection = list(conn_type_settings.keys())[0]\n                print(f\"No connection specified, using first available: {connection}\")\n        else:\n            raise ValueError(\"conn_type_settings must be provided if config is not specified.\")\n\n    if connection is None:\n        raise ValueError(\"connection must be provided or inferable from conn_type_settings.\")\n    if connection not in conn_type_settings:\n        raise ValueError(f\"connection '{connection}' not found in conn_type_settings.\")\n\n    self.conn_type_settings: dict = conn_type_settings\n    if json_folder_path:\n        print(f\"updating settings from json path {json_folder_path}\")\n        self._update_spec_syn_param(json_folder_path)\n    # Use default general settings if not provided\n    if general_settings is None:\n        self.general_settings: dict = DEFAULT_GENERAL_SETTINGS.copy()\n    else:\n        # Merge defaults with user-provided\n        self.general_settings = {**DEFAULT_GENERAL_SETTINGS, **general_settings}\n\n    # Store the initial connection name and set up connection\n    self.current_connection = connection\n    self.conn = self.conn_type_settings[connection]\n    self._current_cell_type = self.conn[\"spec_settings\"][\"post_cell\"]\n    self.synaptic_props = self.conn[\"spec_syn_param\"]\n    self.vclamp = self.general_settings[\"vclamp\"]\n    self.current_name = current_name\n    self.other_vars_to_record = other_vars_to_record or []\n    self.ispk = None\n    self.input_mode = False  # Add input_mode attribute\n    self.last_figure = None  # Store reference to last generated figure\n\n    # Store original slider_vars for connection switching\n    self.original_slider_vars = slider_vars or list(self.synaptic_props.keys())\n\n    if slider_vars:\n        # Start by filtering based on keys in slider_vars\n        self.slider_vars = {\n            key: value for key, value in self.synaptic_props.items() if key in slider_vars\n        }\n        # Iterate over slider_vars and check for missing keys in self.synaptic_props\n        for key in slider_vars:\n            # If the key is missing from synaptic_props, get the value using getattr\n            if key not in self.synaptic_props:\n                try:\n                    self._set_up_cell()\n                    self._set_up_synapse()\n                    value = getattr(self.syn, key)\n                    self.slider_vars[key] = value\n                except AttributeError as e:\n                    print(f\"Error accessing '{key}' in syn {self.syn}: {e}\")\n    else:\n        self.slider_vars = self.synaptic_props\n\n    h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"]\n    h.dt = self.general_settings[\"dt\"]  # Time step (resolution) of the simulation in ms\n    h.steps_per_ms = 1 / h.dt\n    h.celsius = self.general_settings[\"celsius\"]\n\n    # get some stuff set up we need for both SingleEvent and Interactive Tuner\n    # Only set up cell if hoc_cell was not provided\n    if self.hoc_cell is None:\n        self._set_up_cell()\n    else:\n        self.cell = self.hoc_cell\n    self._set_up_synapse()\n\n    self.nstim = h.NetStim()\n    self.nstim2 = h.NetStim()\n\n    self.vcl = h.VClamp(self.cell.soma[0](0.5))\n\n    self.nc = h.NetCon(\n        self.nstim,\n        self.syn,\n        self.general_settings[\"threshold\"],\n        self.general_settings[\"delay\"],\n        self.general_settings[\"weight\"],\n    )\n    self.nc2 = h.NetCon(\n        self.nstim2,\n        self.syn,\n        self.general_settings[\"threshold\"],\n        self.general_settings[\"delay\"],\n        self.general_settings[\"weight\"],\n    )\n\n    self._set_up_recorders()\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._update_spec_syn_param","title":"<code>_update_spec_syn_param(json_folder_path)</code>","text":"<p>Update specific synaptic parameters using JSON files located in the specified folder.</p> Parameters: <p>json_folder_path : str     Path to folder containing JSON files, where each JSON file corresponds to a connection type.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _update_spec_syn_param(self, json_folder_path: str) -&gt; None:\n    \"\"\"\n    Update specific synaptic parameters using JSON files located in the specified folder.\n\n    Parameters:\n    -----------\n    json_folder_path : str\n        Path to folder containing JSON files, where each JSON file corresponds to a connection type.\n    \"\"\"\n    if not self.conn_type_settings:\n        return\n    for conn_type, settings in self.conn_type_settings.items():\n        json_file_path = os.path.join(json_folder_path, f\"{conn_type}.json\")\n        if os.path.exists(json_file_path):\n            with open(json_file_path, \"r\") as json_file:\n                json_data = json.load(json_file)\n                settings[\"spec_syn_param\"].update(json_data)\n        else:\n            print(f\"JSON file for {conn_type} not found.\")\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._set_up_cell","title":"<code>_set_up_cell()</code>","text":"<p>Set up the neuron cell based on the specified connection settings. This method is only called when hoc_cell is not provided.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _set_up_cell(self) -&gt; None:\n    \"\"\"\n    Set up the neuron cell based on the specified connection settings.\n    This method is only called when hoc_cell is not provided.\n    \"\"\"\n    if self.hoc_cell is None:\n        self.cell = getattr(h, self.conn[\"spec_settings\"][\"post_cell\"])()\n    else:\n        self.cell = self.hoc_cell\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._set_up_synapse","title":"<code>_set_up_synapse()</code>","text":"<p>Set up the synapse on the target cell according to the synaptic parameters in <code>conn_type_settings</code>.</p> Notes: <ul> <li><code>_set_up_cell()</code> should be called before setting up the synapse.</li> <li>Synapse location, type, and properties are specified within <code>spec_syn_param</code> and <code>spec_settings</code>.</li> </ul> Source code in <code>bmtool/synapses.py</code> <pre><code>def _set_up_synapse(self) -&gt; None:\n    \"\"\"\n    Set up the synapse on the target cell according to the synaptic parameters in `conn_type_settings`.\n\n    Notes:\n    ------\n    - `_set_up_cell()` should be called before setting up the synapse.\n    - Synapse location, type, and properties are specified within `spec_syn_param` and `spec_settings`.\n    \"\"\"\n    try:\n        self.syn = getattr(h, self.conn[\"spec_settings\"][\"level_of_detail\"])(\n            list(self.cell.all)[self.conn[\"spec_settings\"][\"sec_id\"]](\n                self.conn[\"spec_settings\"][\"sec_x\"]\n            )\n        )\n    except:\n        raise Exception(\"Make sure the mod file exist you are trying to load check spelling!\")\n    for key, value in self.conn[\"spec_syn_param\"].items():\n        if isinstance(value, (int, float)):\n            if hasattr(self.syn, key):\n                setattr(self.syn, key, value)\n            else:\n                print(\n                    f\"Warning: {key} cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\"\n                )\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._set_up_recorders","title":"<code>_set_up_recorders()</code>","text":"<p>Set up recording vectors to capture simulation data.</p> <p>The method sets up recorders for: - Synaptic current specified by <code>current_name</code> - Other specified synaptic variables (<code>other_vars_to_record</code>) - Time, soma voltage, and voltage clamp current for all simulations.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _set_up_recorders(self) -&gt; None:\n    \"\"\"\n    Set up recording vectors to capture simulation data.\n\n    The method sets up recorders for:\n    - Synaptic current specified by `current_name`\n    - Other specified synaptic variables (`other_vars_to_record`)\n    - Time, soma voltage, and voltage clamp current for all simulations.\n    \"\"\"\n    self.rec_vectors = {}\n    for var in self.other_vars_to_record:\n        self.rec_vectors[var] = h.Vector()\n        ref_attr = f\"_ref_{var}\"\n        if hasattr(self.syn, ref_attr):\n            self.rec_vectors[var].record(getattr(self.syn, ref_attr))\n        else:\n            print(\n                f\"Warning: {ref_attr} not found in the syn object. Use vars() to inspect available attributes.\"\n            )\n\n    # Record synaptic current\n    self.rec_vectors[self.current_name] = h.Vector()\n    ref_attr = f\"_ref_{self.current_name}\"\n    if hasattr(self.syn, ref_attr):\n        self.rec_vectors[self.current_name].record(getattr(self.syn, ref_attr))\n    else:\n        print(\"Warning: Synaptic current recorder not set up correctly.\")\n\n    # Record time, synaptic events, soma voltage, and voltage clamp current\n    self.t = h.Vector()\n    self.tspk = h.Vector()\n    self.soma_v = h.Vector()\n    self.ivcl = h.Vector()\n\n    self.t.record(h._ref_t)\n    self.nc.record(self.tspk)\n    self.nc2.record(self.tspk)\n    self.soma_v.record(self.cell.soma[0](0.5)._ref_v)\n    self.ivcl.record(self.vcl._ref_i)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner.SingleEvent","title":"<code>SingleEvent(plot_and_print=True)</code>","text":"<p>Simulate a single synaptic event by delivering an input stimulus to the synapse.</p> <p>The method sets up the neuron cell, synapse, stimulus, and voltage clamp, and then runs the NEURON simulation for a single event. The single synaptic event will occur at general_settings['tstart'] Will display graphs and synaptic properies works best with a jupyter notebook</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def SingleEvent(self, plot_and_print=True):\n    \"\"\"\n    Simulate a single synaptic event by delivering an input stimulus to the synapse.\n\n    The method sets up the neuron cell, synapse, stimulus, and voltage clamp,\n    and then runs the NEURON simulation for a single event. The single synaptic event will occur at general_settings['tstart']\n    Will display graphs and synaptic properies works best with a jupyter notebook\n    \"\"\"\n    self.ispk = None\n\n    # user slider values if the sliders are set up\n    if hasattr(self, \"dynamic_sliders\"):\n        syn_props = {var: slider.value for var, slider in self.dynamic_sliders.items()}\n        self._set_syn_prop(**syn_props)\n\n    # sets values based off optimizer\n    if hasattr(self, \"using_optimizer\"):\n        for name, value in zip(self.param_names, self.params):\n            setattr(self.syn, name, value)\n\n    # Set up the stimulus\n    self.nstim.start = self.general_settings[\"tstart\"]\n    self.nstim.noise = 0\n    self.nstim2.start = h.tstop\n    self.nstim2.noise = 0\n\n    # Set up voltage clamp\n    vcldur = [[0, 0, 0], [self.general_settings[\"tstart\"], h.tstop, 1e9]]\n    for i in range(3):\n        self.vcl.amp[i] = self.conn[\"spec_settings\"][\"vclamp_amp\"]\n        self.vcl.dur[i] = vcldur[1][i]\n\n    # Run simulation\n    h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"]\n    self.nstim.interval = self.general_settings[\"tdur\"]\n    self.nstim.number = 1\n    self.nstim2.start = h.tstop\n    h.run()\n\n    current = np.array(self.rec_vectors[self.current_name])\n    syn_props = self._get_syn_prop(\n        rise_interval=self.general_settings[\"rise_interval\"], dt=h.dt\n    )\n    current = (current - syn_props[\"baseline\"]) * 1000  # Convert to pA\n    current_integral = np.trapz(current, dx=h.dt)  # pA\u00b7ms\n\n    if plot_and_print:\n        self._plot_model(\n            [\n                self.general_settings[\"tstart\"] - 5,\n                self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"],\n            ]\n        )\n        for prop in syn_props.items():\n            print(prop)\n        print(f\"Current Integral in pA*ms: {current_integral:.2f}\")\n\n    self.rise_time = syn_props[\"rise_time\"]\n    self.decay_time = syn_props[\"decay_time\"]\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._get_syn_prop","title":"<code>_get_syn_prop(rise_interval=(0.2, 0.8), dt=h.dt, short=False)</code>","text":"<p>Calculate synaptic properties such as peak amplitude, latency, rise time, decay time, and half-width.</p> Parameters: <p>rise_interval : tuple of floats, optional     Fractional rise time interval to calculate (default is (0.2, 0.8)). dt : float, optional     Time step of the simulation (default is NEURON's <code>h.dt</code>). short : bool, optional     If True, only return baseline and sign without calculating full properties.</p> Returns: <p>dict     A dictionary containing the synaptic properties: baseline, sign, peak amplitude, latency, rise time,     decay time, and half-width.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _get_syn_prop(self, rise_interval=(0.2, 0.8), dt=h.dt, short=False):\n    \"\"\"\n    Calculate synaptic properties such as peak amplitude, latency, rise time, decay time, and half-width.\n\n    Parameters:\n    -----------\n    rise_interval : tuple of floats, optional\n        Fractional rise time interval to calculate (default is (0.2, 0.8)).\n    dt : float, optional\n        Time step of the simulation (default is NEURON's `h.dt`).\n    short : bool, optional\n        If True, only return baseline and sign without calculating full properties.\n\n    Returns:\n    --------\n    dict\n        A dictionary containing the synaptic properties: baseline, sign, peak amplitude, latency, rise time,\n        decay time, and half-width.\n    \"\"\"\n    if self.vclamp:\n        isyn = self.ivcl\n    else:\n        isyn = self.rec_vectors[self.current_name]\n    isyn = np.asarray(isyn)\n    tspk = np.asarray(self.tspk)\n    if tspk.size:\n        tspk = tspk[0]\n\n    ispk = int(np.floor(tspk / dt))\n    baseline = isyn[ispk]\n    isyn = isyn[ispk:] - baseline\n    # print(np.abs(isyn))\n    # print(np.argmax(np.abs(isyn)))\n    # print(isyn[np.argmax(np.abs(isyn))])\n    # print(np.sign(isyn[np.argmax(np.abs(isyn))]))\n    sign = np.sign(isyn[np.argmax(np.abs(isyn))])\n    if short:\n        return {\"baseline\": baseline, \"sign\": sign}\n    isyn *= sign\n    # print(isyn)\n    # peak amplitude\n    ipk, _ = find_peaks(isyn)\n    ipk = ipk[0]\n    peak = isyn[ipk]\n    # latency\n    istart = self._find_first(np.diff(isyn[: ipk + 1]) &gt; 0)\n    latency = dt * (istart + 1)\n    # rise time\n    rt1 = self._find_first(isyn[istart : ipk + 1] &gt; rise_interval[0] * peak)\n    rt2 = self._find_first(isyn[istart : ipk + 1] &gt; rise_interval[1] * peak)\n    rise_time = (rt2 - rt1) * dt\n    # decay time\n    iend = self._find_first(np.diff(isyn[ipk:]) &gt; 0)\n    iend = isyn.size - 1 if iend is None else iend + ipk\n    decay_len = iend - ipk + 1\n    popt, _ = curve_fit(\n        lambda t, a, tau: a * np.exp(-t / tau),\n        dt * np.arange(decay_len),\n        isyn[ipk : iend + 1],\n        p0=(peak, dt * decay_len / 2),\n    )\n    decay_time = popt[1]\n    # half-width\n    hw1 = self._find_first(isyn[istart : ipk + 1] &gt; 0.5 * peak)\n    hw2 = self._find_first(isyn[ipk:] &lt; 0.5 * peak)\n    hw2 = isyn.size if hw2 is None else hw2 + ipk\n    half_width = dt * (hw2 - hw1)\n    output = {\n        \"baseline\": baseline,\n        \"sign\": sign,\n        \"latency\": latency,\n        \"amp\": peak,\n        \"rise_time\": rise_time,\n        \"decay_time\": decay_time,\n        \"half_width\": half_width,\n    }\n    return output\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._set_syn_prop","title":"<code>_set_syn_prop(**kwargs)</code>","text":"<p>Sets the synaptic parameters based on user inputs from sliders.</p> Parameters: <p>**kwargs : dict     Synaptic properties (such as weight, Use, tau_f, tau_d) as keyword arguments.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _set_syn_prop(self, **kwargs):\n    \"\"\"\n    Sets the synaptic parameters based on user inputs from sliders.\n\n    Parameters:\n    -----------\n    **kwargs : dict\n        Synaptic properties (such as weight, Use, tau_f, tau_d) as keyword arguments.\n    \"\"\"\n    for key, value in kwargs.items():\n        setattr(self.syn, key, value)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._simulate_model","title":"<code>_simulate_model(input_frequency, delay, vclamp=None)</code>","text":"<p>Runs the simulation with the specified input frequency, delay, and voltage clamp settings.</p> Parameters: <p>input_frequency : float     Frequency of the input drive train in Hz. delay : float     Delay period in milliseconds between the 8th and 9th pulses. vclamp : bool or None, optional     Whether to use voltage clamp. If None, the current setting is used. Default is None.</p> Notes: <p>This method handles two different input modes: - Standard train mode with 8 initial pulses followed by a delay and 4 additional pulses - Continuous input mode where stimulation continues for a specified duration</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _simulate_model(self, input_frequency, delay, vclamp=None):\n    \"\"\"\n    Runs the simulation with the specified input frequency, delay, and voltage clamp settings.\n\n    Parameters:\n    -----------\n    input_frequency : float\n        Frequency of the input drive train in Hz.\n    delay : float\n        Delay period in milliseconds between the 8th and 9th pulses.\n    vclamp : bool or None, optional\n        Whether to use voltage clamp. If None, the current setting is used. Default is None.\n\n    Notes:\n    ------\n    This method handles two different input modes:\n    - Standard train mode with 8 initial pulses followed by a delay and 4 additional pulses\n    - Continuous input mode where stimulation continues for a specified duration\n    \"\"\"\n    if not self.input_mode:\n        self.tstop = self._set_drive_train(input_frequency, delay)\n        h.tstop = self.tstop\n\n        vcldur = [[0, 0, 0], [self.general_settings[\"tstart\"], self.tstop, 1e9]]\n        for i in range(3):\n            self.vcl.amp[i] = self.conn[\"spec_settings\"][\"vclamp_amp\"]\n            self.vcl.dur[i] = vcldur[1][i]\n        h.finitialize(70 * mV)\n        h.continuerun(self.tstop * ms)\n        #h.run()\n    else:\n        # Continuous input mode: ensure simulation runs long enough for the full stimulation duration\n        self.tstop = self.general_settings[\"tstart\"] + self.w_duration.value + 300 # 300ms buffer time \n        self.nstim.interval = 1000 / input_frequency\n        self.nstim.number = np.ceil(self.w_duration.value / 1000 * input_frequency + 1)\n        self.nstim2.number = 0\n\n        h.finitialize(70 * mV)\n        h.continuerun(self.tstop * ms)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner.InteractiveTuner","title":"<code>InteractiveTuner()</code>","text":"<p>Sets up interactive sliders for tuning short-term plasticity (STP) parameters in a Jupyter Notebook.</p> <p>This method creates an interactive UI with sliders for: - Network selection dropdown (if multiple networks available and config provided) - Connection type selection dropdown - Input frequency - Delay between pulse trains - Duration of stimulation (for continuous input mode) - Synaptic parameters (e.g., Use, tau_f, tau_d) based on the syn model</p> <p>It also provides buttons for: - Running a single event simulation - Running a train input simulation - Toggling voltage clamp mode - Switching between standard and continuous input modes</p> Network Dropdown Feature: <p>When the SynapseTuner is initialized with a BMTK config file containing multiple networks: - A network dropdown appears next to the connection dropdown - Users can dynamically switch between networks (e.g., 'network_to_network', 'external_to_network') - Switching networks rebuilds available connections and updates the connection dropdown - The current connection is preserved if it exists in the new network - If multiple networks exist but only one is specified during init, that network is used as default</p> Notes: <p>Ideal for exploratory parameter tuning and interactive visualization of synapse behavior with different parameter values and stimulation protocols. The network dropdown feature enables comprehensive exploration of multi-network BMTK simulations without needing to reinitialize the tuner.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def InteractiveTuner(self):\n    \"\"\"\n    Sets up interactive sliders for tuning short-term plasticity (STP) parameters in a Jupyter Notebook.\n\n    This method creates an interactive UI with sliders for:\n    - Network selection dropdown (if multiple networks available and config provided)\n    - Connection type selection dropdown\n    - Input frequency\n    - Delay between pulse trains\n    - Duration of stimulation (for continuous input mode)\n    - Synaptic parameters (e.g., Use, tau_f, tau_d) based on the syn model\n\n    It also provides buttons for:\n    - Running a single event simulation\n    - Running a train input simulation\n    - Toggling voltage clamp mode\n    - Switching between standard and continuous input modes\n\n    Network Dropdown Feature:\n    ------------------------\n    When the SynapseTuner is initialized with a BMTK config file containing multiple networks:\n    - A network dropdown appears next to the connection dropdown\n    - Users can dynamically switch between networks (e.g., 'network_to_network', 'external_to_network')\n    - Switching networks rebuilds available connections and updates the connection dropdown\n    - The current connection is preserved if it exists in the new network\n    - If multiple networks exist but only one is specified during init, that network is used as default\n\n    Notes:\n    ------\n    Ideal for exploratory parameter tuning and interactive visualization of\n    synapse behavior with different parameter values and stimulation protocols.\n    The network dropdown feature enables comprehensive exploration of multi-network\n    BMTK simulations without needing to reinitialize the tuner.\n    \"\"\"\n    # Widgets setup (Sliders)\n    freqs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200]\n    delays = [125, 250, 500, 1000, 2000, 4000]\n    durations = [100, 300, 500, 1000, 2000, 5000, 10000]\n    freq0 = 50\n    delay0 = 250\n    duration0 = 300\n    vlamp_status = self.vclamp\n\n    # Connection dropdown\n    connection_options = sorted(list(self.conn_type_settings.keys()))\n    w_connection = widgets.Dropdown(\n        options=connection_options,\n        value=self.current_connection,\n        description=\"Connection:\",\n        style={'description_width': 'initial'}\n    )\n\n    # Network dropdown - only shown if config was provided and multiple networks are available\n    # This enables users to switch between different network datasets dynamically\n    w_network = None\n    if self.config is not None and len(self.available_networks) &gt; 1:\n        w_network = widgets.Dropdown(\n            options=self.available_networks,\n            value=self.current_network,\n            description=\"Network:\",\n            style={'description_width': 'initial'}\n        )\n\n    w_run = widgets.Button(description=\"Run Train\", icon=\"history\", button_style=\"primary\")\n    w_single = widgets.Button(description=\"Single Event\", icon=\"check\", button_style=\"success\")\n    w_vclamp = widgets.ToggleButton(\n        value=vlamp_status,\n        description=\"Voltage Clamp\",\n        icon=\"fast-backward\",\n        button_style=\"warning\",\n    )\n\n    # Voltage clamp amplitude input\n    default_vclamp_amp = getattr(self.conn['spec_settings'], 'vclamp_amp', -70.0)\n    w_vclamp_amp = widgets.FloatText(\n        value=default_vclamp_amp,\n        description=\"V_clamp (mV):\",\n        step=5.0,\n        style={'description_width': 'initial'},\n        layout=widgets.Layout(width='150px')\n    )\n\n    w_input_mode = widgets.ToggleButton(\n        value=False, description=\"Continuous input\", icon=\"eject\", button_style=\"info\"\n    )\n    w_input_freq = widgets.SelectionSlider(options=freqs, value=freq0, description=\"Input Freq\")\n\n    # Sliders for delay and duration\n    self.w_delay = widgets.SelectionSlider(options=delays, value=delay0, description=\"Delay\")\n    self.w_duration = widgets.SelectionSlider(\n        options=durations, value=duration0, description=\"Duration\"\n    )\n\n    # Save functionality widgets\n    save_path_text = widgets.Text(\n        value=\"plot.png\",\n        description=\"Save path:\",\n        layout=widgets.Layout(width='300px')\n    )\n    save_button = widgets.Button(description=\"Save Plot\", icon=\"save\", button_style=\"success\")\n\n    def save_plot(b):\n        if hasattr(self, 'last_figure') and self.last_figure is not None:\n            try:\n                # Create a new figure with just the first subplot (synaptic current)\n                fig, ax = plt.subplots(figsize=(8, 6))\n\n                # Get the axes from the original figure\n                original_axes = self.last_figure.get_axes()\n                if len(original_axes) &gt; 0:\n                    first_ax = original_axes[0]\n\n                    # Copy the data from the first subplot\n                    for line in first_ax.get_lines():\n                        ax.plot(line.get_xdata(), line.get_ydata(), \n                               color=line.get_color(), label=line.get_label())\n\n                    # Copy axis labels and title\n                    ax.set_xlabel(first_ax.get_xlabel())\n                    ax.set_ylabel(first_ax.get_ylabel())\n                    ax.set_title(first_ax.get_title())\n                    ax.set_xlim(first_ax.get_xlim())\n                    ax.legend()\n                    ax.grid(True)\n\n                    # Save the new figure\n                    fig.savefig(save_path_text.value)\n                    plt.close(fig)  # Close the temporary figure\n                    print(f\"Synaptic current plot saved to {save_path_text.value}\")\n                else:\n                    print(\"No subplots found in the figure\")\n\n            except Exception as e:\n                print(f\"Error saving plot: {e}\")\n        else:\n            print(\"No plot to save\")\n\n    save_button.on_click(save_plot)\n\n    def create_dynamic_sliders():\n        \"\"\"Create sliders based on current connection's parameters\"\"\"\n        sliders = {}\n        for key, value in self.slider_vars.items():\n            if isinstance(value, (int, float)):  # Only create sliders for numeric values\n                if hasattr(self.syn, key):\n                    if value == 0:\n                        print(\n                            f\"{key} was set to zero, going to try to set a range of values, try settings the {key} to a nonzero value if you dont like the range!\"\n                        )\n                        slider = widgets.FloatSlider(\n                            value=value, min=0, max=1000, step=1, description=key\n                        )\n                    else:\n                        slider = widgets.FloatSlider(\n                            value=value, min=0, max=value * 20, step=value / 5, description=key\n                        )\n                    sliders[key] = slider\n                else:\n                    print(f\"skipping slider for {key} due to not being a synaptic variable\")\n        return sliders\n\n    # Generate sliders dynamically based on valid numeric entries in self.slider_vars\n    self.dynamic_sliders = create_dynamic_sliders()\n    print(\n        \"Setting up slider! The sliders ranges are set by their init value so try changing that if you dont like the slider range!\"\n    )\n\n    # Create output widget for displaying results\n    output_widget = widgets.Output()\n\n    def run_single_event(*args):\n        clear_output()\n        display(ui)\n        display(output_widget)\n\n        self.vclamp = w_vclamp.value\n        # Update voltage clamp amplitude if voltage clamp is enabled\n        if self.vclamp:\n            # Update the voltage clamp amplitude settings\n            self.conn['spec_settings']['vclamp_amp'] = w_vclamp_amp.value\n            # Update general settings if they exist\n            if hasattr(self, 'general_settings'):\n                self.general_settings['vclamp_amp'] = w_vclamp_amp.value\n        # Update synaptic properties based on slider values\n        self.ispk = None\n\n        # Clear previous results and run simulation\n        output_widget.clear_output()\n        with output_widget:\n            self.SingleEvent()\n\n    def on_connection_change(*args):\n        \"\"\"Handle connection dropdown change\"\"\"\n        try:\n            new_connection = w_connection.value\n            if new_connection != self.current_connection:\n                # Switch to new connection\n                self._switch_connection(new_connection)\n\n                # Recreate dynamic sliders for new connection\n                self.dynamic_sliders = create_dynamic_sliders()\n\n                # Update UI\n                update_ui_layout()\n                update_ui()\n\n        except Exception as e:\n            print(f\"Error switching connection: {e}\")\n\n    def on_network_change(*args):\n        \"\"\"\n        Handle network dropdown change events.\n\n        This callback is triggered when the user selects a different network from \n        the network dropdown. It coordinates the complete switching process:\n        1. Calls _switch_network() to rebuild connections for the new network\n        2. Updates the connection dropdown options with new network's connections\n        3. Recreates dynamic sliders for new connection parameters\n        4. Refreshes the entire UI to reflect all changes\n        \"\"\"\n        if w_network is None:\n            return\n        try:\n            new_network = w_network.value\n            if new_network != self.current_network:\n                # Switch to new network\n                self._switch_network(new_network)\n\n                # Update connection dropdown options with new network's connections\n                connection_options = list(self.conn_type_settings.keys())\n                w_connection.options = connection_options\n                if connection_options:\n                    w_connection.value = self.current_connection\n\n                # Recreate dynamic sliders for new connection\n                self.dynamic_sliders = create_dynamic_sliders()\n\n                # Update UI\n                update_ui_layout()\n                update_ui()\n\n        except Exception as e:\n            print(f\"Error switching network: {e}\")\n\n    def update_ui_layout():\n        \"\"\"\n        Update the UI layout with new sliders and network dropdown.\n\n        This function reconstructs the entire UI layout including:\n        - Network dropdown (if available) and connection dropdown in the top row\n        - Button controls and input mode toggles\n        - Parameter sliders arranged in columns\n        \"\"\"\n        nonlocal ui, slider_columns\n\n        # Add the dynamic sliders to the UI\n        slider_widgets = [slider for slider in self.dynamic_sliders.values()]\n\n        if slider_widgets:\n            half = len(slider_widgets) // 2\n            col1 = VBox(slider_widgets[:half])\n            col2 = VBox(slider_widgets[half:])\n            slider_columns = HBox([col1, col2])\n        else:\n            slider_columns = VBox([])\n\n        # Create button row with voltage clamp controls\n        if w_vclamp.value:  # Show voltage clamp amplitude input when toggle is on\n            button_row = HBox([w_run, w_single, w_vclamp, w_vclamp_amp, w_input_mode])\n        else:  # Hide voltage clamp amplitude input when toggle is off\n            button_row = HBox([w_run, w_single, w_vclamp, w_input_mode])\n\n        # Construct the top row - include network dropdown if available\n        # This creates a horizontal layout with network dropdown (if present) and connection dropdown\n        if w_network is not None:\n            connection_row = HBox([w_network, w_connection])\n        else:\n            connection_row = HBox([w_connection])\n        slider_row = HBox([w_input_freq, self.w_delay, self.w_duration])\n        save_row = HBox([save_path_text, save_button])\n\n        ui = VBox([connection_row, button_row, slider_row, slider_columns, save_row])\n\n    # Function to update UI based on input mode\n    def update_ui(*args):\n        clear_output()\n        display(ui)\n        display(output_widget)\n\n        self.vclamp = w_vclamp.value\n        # Update voltage clamp amplitude if voltage clamp is enabled\n        if self.vclamp:\n            self.conn['spec_settings']['vclamp_amp'] = w_vclamp_amp.value\n            if hasattr(self, 'general_settings'):\n                self.general_settings['vclamp_amp'] = w_vclamp_amp.value\n\n        self.input_mode = w_input_mode.value\n        syn_props = {var: slider.value for var, slider in self.dynamic_sliders.items()}\n        self._set_syn_prop(**syn_props)\n\n        # Clear previous results and run simulation\n        output_widget.clear_output()\n        with output_widget:\n            if not self.input_mode:\n                self._simulate_model(w_input_freq.value, self.w_delay.value, w_vclamp.value)\n            else:\n                self._simulate_model(w_input_freq.value, self.w_duration.value, w_vclamp.value)\n            amp = self._response_amplitude()\n            self._plot_model(\n                [self.general_settings[\"tstart\"] - self.nstim.interval / 3, self.tstop]\n            )\n            _ = self._calc_ppr_induction_recovery(amp)\n\n    # Function to switch between delay and duration sliders\n    def switch_slider(*args):\n        if w_input_mode.value:\n            self.w_delay.layout.display = \"none\"  # Hide delay slider\n            self.w_duration.layout.display = \"\"  # Show duration slider\n        else:\n            self.w_delay.layout.display = \"\"  # Show delay slider\n            self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n    # Function to handle voltage clamp toggle\n    def on_vclamp_toggle(*args):\n        \"\"\"Handle voltage clamp toggle changes to show/hide amplitude input\"\"\"\n        update_ui_layout()\n        clear_output()\n        display(ui)\n        display(output_widget)\n\n    # Link widgets to their callback functions\n    w_connection.observe(on_connection_change, names=\"value\")\n    # Link network dropdown callback only if network dropdown was created\n    if w_network is not None:\n        w_network.observe(on_network_change, names=\"value\")\n    w_input_mode.observe(switch_slider, names=\"value\")\n    w_vclamp.observe(on_vclamp_toggle, names=\"value\")\n\n    # Hide the duration slider initially until the user selects it\n    self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n    w_single.on_click(run_single_event)\n    w_run.on_click(update_ui)\n\n    # Initial UI setup\n    slider_columns = VBox([])\n    ui = VBox([])\n    update_ui_layout()\n\n    display(ui)\n    update_ui()\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner.stp_frequency_response","title":"<code>stp_frequency_response(freqs=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200], delay=250, plot=True, log_plot=True)</code>","text":"<p>Analyze synaptic response across different stimulation frequencies.</p> <p>This method systematically tests how the synapse model responds to different stimulation frequencies, calculating key short-term plasticity (STP) metrics for each frequency.</p> Parameters: <p>freqs : list, optional     List of frequencies to analyze (in Hz). Default covers a wide range from 1-200 Hz. delay : float, optional     Delay between pulse trains in ms. Default is 250 ms. plot : bool, optional     Whether to plot the results. Default is True. log_plot : bool, optional     Whether to use logarithmic scale for frequency axis. Default is True.</p> Returns: <p>dict     Dictionary containing frequency-dependent metrics with keys:     - 'frequencies': List of tested frequencies     - 'ppr': Paired-pulse ratios at each frequency     - 'simple_ppr': Simple paired-pulse ratios (2nd/1st pulse) at each frequency     - 'induction': Induction values at each frequency     - 'recovery': Recovery values at each frequency</p> Notes: <p>This method is particularly useful for characterizing the frequency-dependent behavior of synapses, such as identifying facilitating vs. depressing regimes or the frequency at which a synapse transitions between these behaviors.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def stp_frequency_response(\n    self,\n    freqs=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200],\n    delay=250,\n    plot=True,\n    log_plot=True,\n):\n    \"\"\"\n    Analyze synaptic response across different stimulation frequencies.\n\n    This method systematically tests how the synapse model responds to different\n    stimulation frequencies, calculating key short-term plasticity (STP) metrics\n    for each frequency.\n\n    Parameters:\n    -----------\n    freqs : list, optional\n        List of frequencies to analyze (in Hz). Default covers a wide range from 1-200 Hz.\n    delay : float, optional\n        Delay between pulse trains in ms. Default is 250 ms.\n    plot : bool, optional\n        Whether to plot the results. Default is True.\n    log_plot : bool, optional\n        Whether to use logarithmic scale for frequency axis. Default is True.\n\n    Returns:\n    --------\n    dict\n        Dictionary containing frequency-dependent metrics with keys:\n        - 'frequencies': List of tested frequencies\n        - 'ppr': Paired-pulse ratios at each frequency\n        - 'simple_ppr': Simple paired-pulse ratios (2nd/1st pulse) at each frequency\n        - 'induction': Induction values at each frequency\n        - 'recovery': Recovery values at each frequency\n\n    Notes:\n    ------\n    This method is particularly useful for characterizing the frequency-dependent\n    behavior of synapses, such as identifying facilitating vs. depressing regimes\n    or the frequency at which a synapse transitions between these behaviors.\n    \"\"\"\n    results = {\"frequencies\": freqs, \"ppr\": [], \"induction\": [], \"recovery\": [], \"simple_ppr\": []}\n\n    # Store original state\n    original_ispk = self.ispk\n\n    for freq in tqdm(freqs, desc=\"Analyzing frequencies\"):\n        self._simulate_model(freq, delay)\n        amp = self._response_amplitude()\n        ppr, induction, recovery, simple_ppr = self._calc_ppr_induction_recovery(amp, print_math=False)\n\n        results[\"ppr\"].append(float(ppr))\n        results[\"induction\"].append(float(induction))\n        results[\"recovery\"].append(float(recovery))\n        results[\"simple_ppr\"].append(float(simple_ppr))\n\n    # Restore original state\n    self.ispk = original_ispk\n\n    if plot:\n        self._plot_frequency_analysis(results, log_plot=log_plot)\n\n    return results\n</code></pre>"},{"location":"api/synapses/#gap-junction-tuning","title":"Gap Junction Tuning","text":""},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner","title":"<code>bmtool.synapses.GapJunctionTuner</code>","text":"Source code in <code>bmtool/synapses.py</code> <pre><code>class GapJunctionTuner:\n    def __init__(\n        self,\n        mechanisms_dir: Optional[str] = None,\n        templates_dir: Optional[str] = None,\n        config: Optional[str] = None,\n        general_settings: Optional[dict] = None,\n        conn_type_settings: Optional[dict] = None,\n        hoc_cell: Optional[object] = None,\n    ):\n        \"\"\"\n        Initialize the GapJunctionTuner class.\n\n        Parameters:\n        -----------\n        mechanisms_dir : str\n            Directory path containing the compiled mod files needed for NEURON mechanisms.\n        templates_dir : str\n            Directory path containing cell template files (.hoc or .py) loaded into NEURON.\n        config : str\n            Path to a BMTK config.json file. Can be used to load mechanisms, templates, and other settings.\n        general_settings : dict\n            General settings dictionary including parameters like simulation time step, duration, and temperature.\n        conn_type_settings : dict\n            A dictionary containing connection-specific settings for gap junctions.\n        hoc_cell : object, optional\n            An already loaded NEURON cell object. If provided, template loading and cell creation will be skipped.\n        \"\"\"\n        self.hoc_cell = hoc_cell\n\n        if hoc_cell is None:\n            if config is None and (mechanisms_dir is None or templates_dir is None):\n                raise ValueError(\n                    \"Either a config file, both mechanisms_dir and templates_dir, or a hoc_cell must be provided.\"\n                )\n\n            if config is None:\n                neuron.load_mechanisms(mechanisms_dir)\n                h.load_file(templates_dir)\n            else:\n                # this will load both mechs and templates\n                load_templates_from_config(config)\n\n        # Use default general settings if not provided, merge with user-provided\n        if general_settings is None:\n            self.general_settings: dict = DEFAULT_GAP_JUNCTION_GENERAL_SETTINGS.copy()\n        else:\n            self.general_settings = {**DEFAULT_GAP_JUNCTION_GENERAL_SETTINGS, **general_settings}\n        self.conn_type_settings = conn_type_settings\n\n        self._syn_params_cache = {}\n        self.config = config\n        self.available_networks = []\n        self.current_network = None\n        self.last_figure = None\n        if self.conn_type_settings is None and self.config is not None:\n            self.conn_type_settings = self._build_conn_type_settings_from_config(self.config)\n        if self.conn_type_settings is None or len(self.conn_type_settings) == 0:\n            raise ValueError(\"conn_type_settings must be provided or config must be given to load gap junction connections from\")\n        self.current_connection = list(self.conn_type_settings.keys())[0]\n        self.conn = self.conn_type_settings[self.current_connection]\n\n        h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"] + 100.0\n        h.dt = self.general_settings[\"dt\"]  # Time step (resolution) of the simulation in ms\n        h.steps_per_ms = 1 / h.dt\n        h.celsius = self.general_settings[\"celsius\"]\n\n        # Clean up any existing parallel context before setting up gap junctions\n        try:\n            pc_temp = h.ParallelContext()\n            pc_temp.done()  # Clean up any existing parallel context\n        except:\n            pass  # Ignore errors if no existing context\n\n        # Force cleanup\n        import gc\n        gc.collect()\n\n        # set up gap junctions\n        self.pc = h.ParallelContext()\n\n        # Use provided hoc_cell or create new cells\n        if self.hoc_cell is not None:\n            self.cell1 = self.hoc_cell\n            # For gap junctions, we need two cells, so create a second one if using hoc_cell\n            self.cell_name = self.conn['cell']\n            self.cell2 = getattr(h, self.cell_name)()\n        else:\n            print(self.conn)\n            self.cell_name = self.conn['cell']\n            self.cell1 = getattr(h, self.cell_name)()\n            self.cell2 = getattr(h, self.cell_name)()\n\n        self.icl = h.IClamp(self.cell1.soma[0](0.5))\n        self.icl.delay = self.general_settings[\"tstart\"]\n        self.icl.dur = self.general_settings[\"tdur\"]\n        self.icl.amp = self.general_settings[\"iclamp_amp\"]  # nA\n\n        sec1 = list(self.cell1.all)[self.conn[\"sec_id\"]]\n        sec2 = list(self.cell2.all)[self.conn[\"sec_id\"]]\n\n        # Use unique IDs to avoid conflicts with existing parallel context setups\n        import time\n        unique_id = int(time.time() * 1000) % 10000  # Use timestamp as unique base ID\n\n        self.pc.source_var(sec1(self.conn[\"sec_x\"])._ref_v, unique_id, sec=sec1)\n        self.gap_junc_1 = h.Gap(sec1(0.5))\n        self.pc.target_var(self.gap_junc_1._ref_vgap, unique_id + 1)\n\n        self.pc.source_var(sec2(self.conn[\"sec_x\"])._ref_v, unique_id + 1, sec=sec2)\n        self.gap_junc_2 = h.Gap(sec2(0.5))\n        self.pc.target_var(self.gap_junc_2._ref_vgap, unique_id)\n\n        self.pc.setup_transfer()\n\n        # Now it's safe to initialize NEURON\n        h.finitialize()\n\n    def _load_synaptic_params_from_config(self, config: dict, dynamics_params: str) -&gt; dict:\n        try:\n            # Get the synaptic models directory from config\n            synaptic_models_dir = config.get('components', {}).get('synaptic_models_dir', '')\n            if synaptic_models_dir:\n                # Handle path variables\n                if synaptic_models_dir.startswith('$'):\n                    # This is a placeholder, try to resolve it\n                    config_dir = os.path.dirname(config.get('config_path', ''))\n                    synaptic_models_dir = synaptic_models_dir.replace('$COMPONENTS_DIR', \n                                                                    os.path.join(config_dir, 'components'))\n                    synaptic_models_dir = synaptic_models_dir.replace('$BASE_DIR', config_dir)\n\n                dynamics_file = os.path.join(synaptic_models_dir, dynamics_params)\n\n                if os.path.exists(dynamics_file):\n                    with open(dynamics_file, 'r') as f:\n                        return json.load(f)\n                else:\n                    print(f\"Warning: Dynamics params file not found: {dynamics_file}\")\n        except Exception as e:\n            print(f\"Warning: Error loading synaptic parameters: {e}\")\n\n        return {}\n\n    def _load_available_networks(self) -&gt; None:\n        \"\"\"\n        Load available network names from the config file for the network dropdown feature.\n\n        This method is automatically called during initialization when a config file is provided.\n        It populates the available_networks list which enables the network dropdown in \n        InteractiveTuner when multiple networks are available.\n\n        Network Dropdown Behavior:\n        -------------------------\n        - If only one network exists: No network dropdown is shown\n        - If multiple networks exist: Network dropdown appears next to connection dropdown\n        - Networks are loaded from the edges data in the config file\n        - Current network defaults to the first available if not specified during init\n        \"\"\"\n        if self.config is None:\n            self.available_networks = []\n            return\n\n        try:\n            edges = load_edges_from_config(self.config)\n            self.available_networks = list(edges.keys())\n\n            # Set current network to first available if not specified\n            if self.current_network is None and self.available_networks:\n                self.current_network = self.available_networks[0]\n        except Exception as e:\n            print(f\"Warning: Could not load networks from config: {e}\")\n            self.available_networks = []\n\n    def _build_conn_type_settings_from_config(self, config_path: str) -&gt; Dict[str, dict]:\n        # Load configuration and get nodes and edges using util.py methods\n        config = load_config(config_path)\n        # Ensure the config dict knows its source path so path substitutions can be resolved\n        try:\n            config['config_path'] = config_path\n        except Exception:\n            pass\n        nodes = load_nodes_from_config(config_path)\n        edges = load_edges_from_config(config_path)\n\n        conn_type_settings = {}\n\n        # Process all edge datasets\n        for edge_dataset_name, edge_df in edges.items():\n            if edge_df.empty:\n                continue\n\n            # Merging with node data to get model templates\n            source_node_df = None\n            target_node_df = None\n\n            # First, try to deterministically parse the edge_dataset_name for patterns like '&lt;src&gt;_to_&lt;tgt&gt;'\n            if '_to_' in edge_dataset_name:\n                parts = edge_dataset_name.split('_to_')\n                if len(parts) == 2:\n                    src_name, tgt_name = parts\n                    if src_name in nodes:\n                        source_node_df = nodes[src_name].add_prefix('source_')\n                    if tgt_name in nodes:\n                        target_node_df = nodes[tgt_name].add_prefix('target_')\n\n            # If not found by parsing name, fall back to inspecting a sample edge row\n            if source_node_df is None or target_node_df is None:\n                sample_edge = edge_df.iloc[0] if len(edge_df) &gt; 0 else None\n                if sample_edge is not None:\n                    source_pop_name = sample_edge.get('source_population', '')\n                    target_pop_name = sample_edge.get('target_population', '')\n                    if source_pop_name in nodes:\n                        source_node_df = nodes[source_pop_name].add_prefix('source_')\n                    if target_pop_name in nodes:\n                        target_node_df = nodes[target_pop_name].add_prefix('target_')\n\n            # As a last resort, attempt to heuristically match\n            if source_node_df is None or target_node_df is None:\n                for pop_name, node_df in nodes.items():\n                    if source_node_df is None and (edge_dataset_name.startswith(pop_name) or edge_dataset_name.endswith(pop_name)):\n                        source_node_df = node_df.add_prefix('source_')\n                    if target_node_df is None and (edge_dataset_name.startswith(pop_name) or edge_dataset_name.endswith(pop_name)):\n                        target_node_df = node_df.add_prefix('target_')\n\n            if source_node_df is None or target_node_df is None:\n                print(f\"Warning: Could not find node data for edge dataset {edge_dataset_name}\")\n                continue\n\n            # Merge edge data with source node info\n            edges_with_source = pd.merge(\n                edge_df.reset_index(), \n                source_node_df, \n                how='left', \n                left_on='source_node_id', \n                right_index=True\n            )\n\n            # Merge with target node info\n            edges_with_nodes = pd.merge(\n                edges_with_source, \n                target_node_df, \n                how='left', \n                left_on='target_node_id', \n                right_index=True\n            )\n\n            # Skip edge datasets that don't have gap junction information\n            if 'is_gap_junction' not in edges_with_nodes.columns:\n                continue\n\n            # Filter to only gap junction edges\n            # Handle NaN values in is_gap_junction column\n            gap_junction_mask = edges_with_nodes['is_gap_junction'].fillna(False) == True\n            gap_junction_edges = edges_with_nodes[gap_junction_mask]\n            if gap_junction_edges.empty:\n                continue\n\n            # Get unique edge types from the gap junction edges\n            if 'edge_type_id' in gap_junction_edges.columns:\n                edge_types = gap_junction_edges['edge_type_id'].unique()\n            else:\n                edge_types = [None]  # Single edge type\n\n            # Process each edge type\n            for edge_type_id in edge_types:\n                # Filter edges for this type\n                if edge_type_id is not None:\n                    edge_type_data = gap_junction_edges[gap_junction_edges['edge_type_id'] == edge_type_id]\n                else:\n                    edge_type_data = gap_junction_edges\n\n                if len(edge_type_data) == 0:\n                    continue\n\n                # Get representative edge for this type\n                edge_info = edge_type_data.iloc[0]\n\n                # Process gap junction\n                source_model_template = edge_info.get('source_model_template', '')\n                target_model_template = edge_info.get('target_model_template', '')\n\n                source_cell_type = source_model_template.replace('hoc:', '') if source_model_template.startswith('hoc:') else source_model_template\n                target_cell_type = target_model_template.replace('hoc:', '') if target_model_template.startswith('hoc:') else target_model_template\n\n                if source_cell_type != target_cell_type:\n                    continue  # Only process gap junctions between same cell types\n\n                source_pop = edge_info.get('source_pop_name', '')\n                target_pop = edge_info.get('target_pop_name', '')\n\n                conn_name = f\"{source_pop}2{target_pop}_gj\"\n                if edge_type_id is not None:\n                    conn_name += f\"_type_{edge_type_id}\"\n\n                conn_settings = {\n                    'cell': source_cell_type,\n                    'sec_id': 0,\n                    'sec_x': 0.5,\n                    'iclamp_amp': -0.01,\n                    'spec_syn_param': {}\n                }\n\n                # Load dynamics params\n                dynamics_file_name = edge_info.get('dynamics_params', '')\n                if dynamics_file_name and dynamics_file_name.upper() != 'NULL':\n                    try:\n                        syn_params = self._load_synaptic_params_from_config(config, dynamics_file_name)\n                        conn_settings['spec_syn_param'] = syn_params\n                    except Exception as e:\n                        print(f\"Warning: could not load dynamics_params file '{dynamics_file_name}': {e}\")\n\n                conn_type_settings[conn_name] = conn_settings\n\n        return conn_type_settings\n\n    def _switch_connection(self, new_connection: str) -&gt; None:\n        \"\"\"\n        Switch to a different gap junction connection and update all related properties.\n\n        Parameters:\n        -----------\n        new_connection : str\n            Name of the new connection type to switch to.\n        \"\"\"\n        if new_connection not in self.conn_type_settings:\n            raise ValueError(f\"Connection '{new_connection}' not found in conn_type_settings\")\n\n        # Update current connection\n        self.current_connection = new_connection\n        self.conn = self.conn_type_settings[new_connection]\n\n        # Check if cell type changed\n        new_cell_name = self.conn['cell']\n        if self.cell_name != new_cell_name:\n            self.cell_name = new_cell_name\n\n            # Recreate cells\n            if self.hoc_cell is None:\n                self.cell1 = getattr(h, self.cell_name)()\n                self.cell2 = getattr(h, self.cell_name)()\n            else:\n                # For hoc_cell, recreate the second cell\n                self.cell2 = getattr(h, self.cell_name)()\n\n            # Recreate IClamp\n            self.icl = h.IClamp(self.cell1.soma[0](0.5))\n            self.icl.delay = self.general_settings[\"tstart\"]\n            self.icl.dur = self.general_settings[\"tdur\"]\n            self.icl.amp = self.general_settings[\"iclamp_amp\"]\n        else:\n            # Update IClamp parameters even if same cell type\n            self.icl.amp = self.general_settings[\"iclamp_amp\"]\n\n        # Always recreate gap junctions when switching connections \n        # (even for same cell type, sec_id or sec_x might differ)\n\n        # Clean up previous gap junctions and parallel context\n        if hasattr(self, 'gap_junc_1'):\n            del self.gap_junc_1\n        if hasattr(self, 'gap_junc_2'):\n            del self.gap_junc_2\n\n        # Properly clean up the existing parallel context\n        if hasattr(self, 'pc'):\n            self.pc.done()  # Clean up existing parallel context\n\n        # Force garbage collection and reset NEURON state\n        import gc\n        gc.collect()\n        h.finitialize()\n\n        # Create a fresh parallel context after cleanup\n        self.pc = h.ParallelContext()\n\n        try:\n            sec1 = list(self.cell1.all)[self.conn[\"sec_id\"]]\n            sec2 = list(self.cell2.all)[self.conn[\"sec_id\"]]\n\n            # Use unique IDs to avoid conflicts with existing parallel context setups\n            import time\n            unique_id = int(time.time() * 1000) % 10000  # Use timestamp as unique base ID\n\n            self.pc.source_var(sec1(self.conn[\"sec_x\"])._ref_v, unique_id, sec=sec1)\n            self.gap_junc_1 = h.Gap(sec1(0.5))\n            self.pc.target_var(self.gap_junc_1._ref_vgap, unique_id + 1)\n\n            self.pc.source_var(sec2(self.conn[\"sec_x\"])._ref_v, unique_id + 1, sec=sec2)\n            self.gap_junc_2 = h.Gap(sec2(0.5))\n            self.pc.target_var(self.gap_junc_2._ref_vgap, unique_id)\n\n            self.pc.setup_transfer()\n        except Exception as e:\n            print(f\"Error setting up gap junctions: {e}\")\n            # Try to continue with basic setup\n            self.gap_junc_1 = h.Gap(list(self.cell1.all)[self.conn[\"sec_id\"]](0.5))\n            self.gap_junc_2 = h.Gap(list(self.cell2.all)[self.conn[\"sec_id\"]](0.5))\n\n        # Reset NEURON state after complete setup\n        h.finitialize()\n\n        print(f\"Successfully switched to connection: {new_connection}\")\n\n    def model(self, resistance):\n        \"\"\"\n        Run a simulation with a specified gap junction resistance.\n\n        Parameters:\n        -----------\n        resistance : float\n            The gap junction resistance value (in MOhm) to use for the simulation.\n\n        Notes:\n        ------\n        This method sets up the gap junction resistance, initializes recording vectors for time\n        and membrane voltages of both cells, and runs the NEURON simulation.\n        \"\"\"\n        self.gap_junc_1.g = resistance\n        self.gap_junc_2.g = resistance\n\n        t_vec = h.Vector()\n        soma_v_1 = h.Vector()\n        soma_v_2 = h.Vector()\n        t_vec.record(h._ref_t)\n        soma_v_1.record(self.cell1.soma[0](0.5)._ref_v)\n        soma_v_2.record(self.cell2.soma[0](0.5)._ref_v)\n\n        self.t_vec = t_vec\n        self.soma_v_1 = soma_v_1\n        self.soma_v_2 = soma_v_2\n\n        h.finitialize(-70 * mV)\n        h.continuerun(h.tstop * ms)\n\n    def plot_model(self):\n        \"\"\"\n        Plot the voltage traces of both cells to visualize gap junction coupling.\n\n        This method creates a plot showing the membrane potential of both cells over time,\n        highlighting the effect of gap junction coupling when a current step is applied to cell 1.\n        \"\"\"\n        t_range = [\n            self.general_settings[\"tstart\"] - 100.0,\n            self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"] + 100.0,\n        ]\n        t = np.array(self.t_vec)\n        v1 = np.array(self.soma_v_1)\n        v2 = np.array(self.soma_v_2)\n        tidx = (t &gt;= t_range[0]) &amp; (t &lt;= t_range[1])\n\n        plt.figure()\n        plt.plot(t[tidx], v1[tidx], \"b\", label=f\"{self.cell_name} 1\")\n        plt.plot(t[tidx], v2[tidx], \"r\", label=f\"{self.cell_name} 2\")\n        plt.title(f\"{self.cell_name} gap junction\")\n        plt.xlabel(\"Time (ms)\")\n        plt.ylabel(\"Membrane Voltage (mV)\")\n        plt.legend()\n        self.last_figure = plt.gcf()\n\n    def coupling_coefficient(self, t, v1, v2, t_start, t_end, dt=h.dt):\n        \"\"\"\n        Calculate the coupling coefficient between two cells connected by a gap junction.\n\n        Parameters:\n        -----------\n        t : array-like\n            Time vector.\n        v1 : array-like\n            Voltage trace of the cell receiving the current injection.\n        v2 : array-like\n            Voltage trace of the coupled cell.\n        t_start : float\n            Start time for calculating the steady-state voltage change.\n        t_end : float\n            End time for calculating the steady-state voltage change.\n        dt : float, optional\n            Time step of the simulation. Default is h.dt.\n\n        Returns:\n        --------\n        float\n            The coupling coefficient, defined as the ratio of voltage change in cell 2\n            to voltage change in cell 1 (\u0394V\u2082/\u0394V\u2081).\n        \"\"\"\n        t = np.asarray(t)\n        v1 = np.asarray(v1)\n        v2 = np.asarray(v2)\n        idx1 = np.nonzero(t &lt; t_start)[0][-1]\n        idx2 = np.nonzero(t &lt; t_end)[0][-1]\n        return (v2[idx2] - v2[idx1]) / (v1[idx2] - v1[idx1])\n\n    def InteractiveTuner(self):\n        \"\"\"\n        Sets up interactive sliders for tuning short-term plasticity (STP) parameters in a Jupyter Notebook.\n\n        This method creates an interactive UI with sliders for:\n        - Network selection dropdown (if multiple networks available and config provided)\n        - Connection type selection dropdown\n        - Input frequency\n        - Delay between pulse trains\n        - Duration of stimulation (for continuous input mode)\n        - Synaptic parameters (e.g., Use, tau_f, tau_d) based on the syn model\n\n        It also provides buttons for:\n        - Running a single event simulation\n        - Running a train input simulation\n        - Toggling voltage clamp mode\n        - Switching between standard and continuous input modes\n\n        Network Dropdown Feature:\n        ------------------------\n        When the SynapseTuner is initialized with a BMTK config file containing multiple networks:\n        - A network dropdown appears next to the connection dropdown\n        - Users can dynamically switch between networks (e.g., 'network_to_network', 'external_to_network')\n        - Switching networks rebuilds available connections and updates the connection dropdown\n        - The current connection is preserved if it exists in the new network\n        - If multiple networks exist but only one is specified during init, that network is used as default\n\n        Notes:\n        ------\n        Ideal for exploratory parameter tuning and interactive visualization of\n        synapse behavior with different parameter values and stimulation protocols.\n        The network dropdown feature enables comprehensive exploration of multi-network\n        BMTK simulations without needing to reinitialize the tuner.\n        \"\"\"\n        # Widgets setup (Sliders)\n        freqs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200]\n        delays = [125, 250, 500, 1000, 2000, 4000]\n        durations = [100, 300, 500, 1000, 2000, 5000, 10000]\n        freq0 = 50\n        delay0 = 250\n        duration0 = 300\n        vlamp_status = self.vclamp\n\n        # Connection dropdown\n        connection_options = sorted(list(self.conn_type_settings.keys()))\n        w_connection = widgets.Dropdown(\n            options=connection_options,\n            value=self.current_connection,\n            description=\"Connection:\",\n            style={'description_width': 'initial'}\n        )\n\n        # Network dropdown - only shown if config was provided and multiple networks are available\n        # This enables users to switch between different network datasets dynamically\n        w_network = None\n        if self.config is not None and len(self.available_networks) &gt; 1:\n            w_network = widgets.Dropdown(\n                options=self.available_networks,\n                value=self.current_network,\n                description=\"Network:\",\n                style={'description_width': 'initial'}\n            )\n\n        w_run = widgets.Button(description=\"Run Train\", icon=\"history\", button_style=\"primary\")\n        w_single = widgets.Button(description=\"Single Event\", icon=\"check\", button_style=\"success\")\n        w_vclamp = widgets.ToggleButton(\n            value=vlamp_status,\n            description=\"Voltage Clamp\",\n            icon=\"fast-backward\",\n            button_style=\"warning\",\n        )\n\n        # Voltage clamp amplitude input\n        default_vclamp_amp = getattr(self.conn['spec_settings'], 'vclamp_amp', -70.0)\n        w_vclamp_amp = widgets.FloatText(\n            value=default_vclamp_amp,\n            description=\"V_clamp (mV):\",\n            step=5.0,\n            style={'description_width': 'initial'},\n            layout=widgets.Layout(width='150px')\n        )\n\n        w_input_mode = widgets.ToggleButton(\n            value=False, description=\"Continuous input\", icon=\"eject\", button_style=\"info\"\n        )\n        w_input_freq = widgets.SelectionSlider(options=freqs, value=freq0, description=\"Input Freq\")\n\n        # Sliders for delay and duration\n        self.w_delay = widgets.SelectionSlider(options=delays, value=delay0, description=\"Delay\")\n        self.w_duration = widgets.SelectionSlider(\n            options=durations, value=duration0, description=\"Duration\"\n        )\n\n        # Save functionality widgets\n        save_path_text = widgets.Text(\n            value=\"plot.png\",\n            description=\"Save path:\",\n            layout=widgets.Layout(width='300px')\n        )\n        save_button = widgets.Button(description=\"Save Plot\", icon=\"save\", button_style=\"success\")\n\n        def save_plot(b):\n            if hasattr(self, 'last_figure') and self.last_figure is not None:\n                try:\n                    # Create a new figure with just the first subplot (synaptic current)\n                    fig, ax = plt.subplots(figsize=(8, 6))\n\n                    # Get the axes from the original figure\n                    original_axes = self.last_figure.get_axes()\n                    if len(original_axes) &gt; 0:\n                        first_ax = original_axes[0]\n\n                        # Copy the data from the first subplot\n                        for line in first_ax.get_lines():\n                            ax.plot(line.get_xdata(), line.get_ydata(), \n                                   color=line.get_color(), label=line.get_label())\n\n                        # Copy axis labels and title\n                        ax.set_xlabel(first_ax.get_xlabel())\n                        ax.set_ylabel(first_ax.get_ylabel())\n                        ax.set_title(first_ax.get_title())\n                        ax.set_xlim(first_ax.get_xlim())\n                        ax.legend()\n                        ax.grid(True)\n\n                        # Save the new figure\n                        fig.savefig(save_path_text.value)\n                        plt.close(fig)  # Close the temporary figure\n                        print(f\"Synaptic current plot saved to {save_path_text.value}\")\n                    else:\n                        print(\"No subplots found in the figure\")\n\n                except Exception as e:\n                    print(f\"Error saving plot: {e}\")\n            else:\n                print(\"No plot to save\")\n\n        save_button.on_click(save_plot)\n\n        def create_dynamic_sliders():\n            \"\"\"Create sliders based on current connection's parameters\"\"\"\n            sliders = {}\n            for key, value in self.slider_vars.items():\n                if isinstance(value, (int, float)):  # Only create sliders for numeric values\n                    if hasattr(self.syn, key):\n                        if value == 0:\n                            print(\n                                f\"{key} was set to zero, going to try to set a range of values, try settings the {key} to a nonzero value if you dont like the range!\"\n                            )\n                            slider = widgets.FloatSlider(\n                                value=value, min=0, max=1000, step=1, description=key\n                            )\n                        else:\n                            slider = widgets.FloatSlider(\n                                value=value, min=0, max=value * 20, step=value / 5, description=key\n                            )\n                        sliders[key] = slider\n                    else:\n                        print(f\"skipping slider for {key} due to not being a synaptic variable\")\n            return sliders\n\n        # Generate sliders dynamically based on valid numeric entries in self.slider_vars\n        self.dynamic_sliders = create_dynamic_sliders()\n        print(\n            \"Setting up slider! The sliders ranges are set by their init value so try changing that if you dont like the slider range!\"\n        )\n\n        # Create output widget for displaying results\n        output_widget = widgets.Output()\n\n        def run_single_event(*args):\n            clear_output()\n            display(ui)\n            display(output_widget)\n\n            self.vclamp = w_vclamp.value\n            # Update voltage clamp amplitude if voltage clamp is enabled\n            if self.vclamp:\n                # Update the voltage clamp amplitude settings\n                self.conn['spec_settings']['vclamp_amp'] = w_vclamp_amp.value\n                # Update general settings if they exist\n                if hasattr(self, 'general_settings'):\n                    self.general_settings['vclamp_amp'] = w_vclamp_amp.value\n            # Update synaptic properties based on slider values\n            self.ispk = None\n\n            # Clear previous results and run simulation\n            output_widget.clear_output()\n            with output_widget:\n                self.SingleEvent()\n\n        def on_connection_change(*args):\n            \"\"\"Handle connection dropdown change\"\"\"\n            try:\n                new_connection = w_connection.value\n                if new_connection != self.current_connection:\n                    # Switch to new connection\n                    self._switch_connection(new_connection)\n\n                    # Recreate dynamic sliders for new connection\n                    self.dynamic_sliders = create_dynamic_sliders()\n\n                    # Update UI\n                    update_ui_layout()\n                    update_ui()\n\n            except Exception as e:\n                print(f\"Error switching connection: {e}\")\n\n        def on_network_change(*args):\n            \"\"\"\n            Handle network dropdown change events.\n\n            This callback is triggered when the user selects a different network from \n            the network dropdown. It coordinates the complete switching process:\n            1. Calls _switch_network() to rebuild connections for the new network\n            2. Updates the connection dropdown options with new network's connections\n            3. Recreates dynamic sliders for new connection parameters\n            4. Refreshes the entire UI to reflect all changes\n            \"\"\"\n            if w_network is None:\n                return\n            try:\n                new_network = w_network.value\n                if new_network != self.current_network:\n                    # Switch to new network\n                    self._switch_network(new_network)\n\n                    # Update connection dropdown options with new network's connections\n                    connection_options = list(self.conn_type_settings.keys())\n                    w_connection.options = connection_options\n                    if connection_options:\n                        w_connection.value = self.current_connection\n\n                    # Recreate dynamic sliders for new connection\n                    self.dynamic_sliders = create_dynamic_sliders()\n\n                    # Update UI\n                    update_ui_layout()\n                    update_ui()\n\n            except Exception as e:\n                print(f\"Error switching network: {e}\")\n\n        def update_ui_layout():\n            \"\"\"\n            Update the UI layout with new sliders and network dropdown.\n\n            This function reconstructs the entire UI layout including:\n            - Network dropdown (if available) and connection dropdown in the top row\n            - Button controls and input mode toggles\n            - Parameter sliders arranged in columns\n            \"\"\"\n            nonlocal ui, slider_columns\n\n            # Add the dynamic sliders to the UI\n            slider_widgets = [slider for slider in self.dynamic_sliders.values()]\n\n            if slider_widgets:\n                half = len(slider_widgets) // 2\n                col1 = VBox(slider_widgets[:half])\n                col2 = VBox(slider_widgets[half:])\n                slider_columns = HBox([col1, col2])\n            else:\n                slider_columns = VBox([])\n\n            # Create button row with voltage clamp controls\n            if w_vclamp.value:  # Show voltage clamp amplitude input when toggle is on\n                button_row = HBox([w_run, w_single, w_vclamp, w_vclamp_amp, w_input_mode])\n            else:  # Hide voltage clamp amplitude input when toggle is off\n                button_row = HBox([w_run, w_single, w_vclamp, w_input_mode])\n\n            # Construct the top row - include network dropdown if available\n            # This creates a horizontal layout with network dropdown (if present) and connection dropdown\n            if w_network is not None:\n                connection_row = HBox([w_network, w_connection])\n            else:\n                connection_row = HBox([w_connection])\n            slider_row = HBox([w_input_freq, self.w_delay, self.w_duration])\n            save_row = HBox([save_path_text, save_button])\n\n            ui = VBox([connection_row, button_row, slider_row, slider_columns, save_row])\n\n        # Function to update UI based on input mode\n        def update_ui(*args):\n            clear_output()\n            display(ui)\n            display(output_widget)\n\n            self.vclamp = w_vclamp.value\n            # Update voltage clamp amplitude if voltage clamp is enabled\n            if self.vclamp:\n                self.conn['spec_settings']['vclamp_amp'] = w_vclamp_amp.value\n                if hasattr(self, 'general_settings'):\n                    self.general_settings['vclamp_amp'] = w_vclamp_amp.value\n\n            self.input_mode = w_input_mode.value\n            syn_props = {var: slider.value for var, slider in self.dynamic_sliders.items()}\n            self._set_syn_prop(**syn_props)\n\n            # Clear previous results and run simulation\n            output_widget.clear_output()\n            with output_widget:\n                if not self.input_mode:\n                    self._simulate_model(w_input_freq.value, self.w_delay.value, w_vclamp.value)\n                else:\n                    self._simulate_model(w_input_freq.value, self.w_duration.value, w_vclamp.value)\n                amp = self._response_amplitude()\n                self._plot_model(\n                    [self.general_settings[\"tstart\"] - self.nstim.interval / 3, self.tstop]\n                )\n                _ = self._calc_ppr_induction_recovery(amp)\n\n        # Function to switch between delay and duration sliders\n        def switch_slider(*args):\n            if w_input_mode.value:\n                self.w_delay.layout.display = \"none\"  # Hide delay slider\n                self.w_duration.layout.display = \"\"  # Show duration slider\n            else:\n                self.w_delay.layout.display = \"\"  # Show delay slider\n                self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n        # Function to handle voltage clamp toggle\n        def on_vclamp_toggle(*args):\n            \"\"\"Handle voltage clamp toggle changes to show/hide amplitude input\"\"\"\n            update_ui_layout()\n            clear_output()\n            display(ui)\n            display(output_widget)\n\n        # Link widgets to their callback functions\n        w_connection.observe(on_connection_change, names=\"value\")\n        # Link network dropdown callback only if network dropdown was created\n        if w_network is not None:\n            w_network.observe(on_network_change, names=\"value\")\n        w_input_mode.observe(switch_slider, names=\"value\")\n        w_vclamp.observe(on_vclamp_toggle, names=\"value\")\n\n        # Hide the duration slider initially until the user selects it\n        self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n        w_single.on_click(run_single_event)\n        w_run.on_click(update_ui)\n\n        # Initial UI setup\n        slider_columns = VBox([])\n        ui = VBox([])\n        update_ui_layout()\n\n        display(ui)\n        update_ui()\n\n    def stp_frequency_response(\n        self,\n        freqs=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200],\n        delay=250,\n        plot=True,\n        log_plot=True,\n    ):\n        \"\"\"\n        Analyze synaptic response across different stimulation frequencies.\n\n        This method systematically tests how the synapse model responds to different\n        stimulation frequencies, calculating key short-term plasticity (STP) metrics\n        for each frequency.\n\n        Parameters:\n        -----------\n        freqs : list, optional\n            List of frequencies to analyze (in Hz). Default covers a wide range from 1-200 Hz.\n        delay : float, optional\n            Delay between pulse trains in ms. Default is 250 ms.\n        plot : bool, optional\n            Whether to plot the results. Default is True.\n        log_plot : bool, optional\n            Whether to use logarithmic scale for frequency axis. Default is True.\n\n        Returns:\n        --------\n        dict\n            Dictionary containing frequency-dependent metrics with keys:\n            - 'frequencies': List of tested frequencies\n            - 'ppr': Paired-pulse ratios at each frequency\n            - 'simple_ppr': Simple paired-pulse ratios (2nd/1st pulse) at each frequency\n            - 'induction': Induction values at each frequency\n            - 'recovery': Recovery values at each frequency\n\n        Notes:\n        ------\n        This method is particularly useful for characterizing the frequency-dependent\n        behavior of synapses, such as identifying facilitating vs. depressing regimes\n        or the frequency at which a synapse transitions between these behaviors.\n        \"\"\"\n        results = {\"frequencies\": freqs, \"ppr\": [], \"induction\": [], \"recovery\": [], \"simple_ppr\": []}\n\n        # Store original state\n        original_ispk = self.ispk\n\n        for freq in tqdm(freqs, desc=\"Analyzing frequencies\"):\n            self._simulate_model(freq, delay)\n            amp = self._response_amplitude()\n            ppr, induction, recovery, simple_ppr = self._calc_ppr_induction_recovery(amp, print_math=False)\n\n            results[\"ppr\"].append(float(ppr))\n            results[\"induction\"].append(float(induction))\n            results[\"recovery\"].append(float(recovery))\n            results[\"simple_ppr\"].append(float(simple_ppr))\n\n        # Restore original state\n        self.ispk = original_ispk\n\n        if plot:\n            self._plot_frequency_analysis(results, log_plot=log_plot)\n\n        return results\n\n    def _plot_frequency_analysis(self, results, log_plot):\n        \"\"\"\n        Plot the frequency-dependent synaptic properties.\n\n        Parameters:\n        -----------\n        results : dict\n            Dictionary containing frequency analysis results with keys:\n            - 'frequencies': List of tested frequencies\n            - 'ppr': Paired-pulse ratios at each frequency\n            - 'simple_ppr': Simple paired-pulse ratios at each frequency\n            - 'induction': Induction values at each frequency\n            - 'recovery': Recovery values at each frequency\n        log_plot : bool\n            Whether to use logarithmic scale for frequency axis\n\n        Notes:\n        ------\n        Creates a figure with three subplots showing:\n        1. Paired-pulse ratios (both normalized and simple) vs. frequency\n        2. Induction vs. frequency\n        3. Recovery vs. frequency\n\n        Each plot includes a horizontal reference line at y=0 or y=1 to indicate\n        the boundary between facilitation and depression.\n        \"\"\"\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n\n        # Plot both PPR measures\n        if log_plot:\n            ax1.semilogx(results[\"frequencies\"], results[\"ppr\"], \"o-\", label=\"Normalized PPR\")\n            ax1.semilogx(results[\"frequencies\"], results[\"simple_ppr\"], \"s-\", label=\"Simple PPR\")\n        else:\n            ax1.plot(results[\"frequencies\"], results[\"ppr\"], \"o-\", label=\"Normalized PPR\")\n            ax1.plot(results[\"frequencies\"], results[\"simple_ppr\"], \"s-\", label=\"Simple PPR\")\n        ax1.axhline(y=1, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax1.set_xlabel(\"Frequency (Hz)\")\n        ax1.set_ylabel(\"Paired Pulse Ratio\")\n        ax1.set_title(\"PPR vs Frequency\")\n        ax1.legend()\n        ax1.grid(True)\n\n        # Plot Induction\n        if log_plot:\n            ax2.semilogx(results[\"frequencies\"], results[\"induction\"], \"o-\")\n        else:\n            ax2.plot(results[\"frequencies\"], results[\"induction\"], \"o-\")\n        ax2.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax2.set_xlabel(\"Frequency (Hz)\")\n        ax2.set_ylabel(\"Induction\")\n        ax2.set_title(\"Induction vs Frequency\")\n        ax2.grid(True)\n\n        # Plot Recovery\n        if log_plot:\n            ax3.semilogx(results[\"frequencies\"], results[\"recovery\"], \"o-\")\n        else:\n            ax3.plot(results[\"frequencies\"], results[\"recovery\"], \"o-\")\n        ax3.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax3.set_xlabel(\"Frequency (Hz)\")\n        ax3.set_ylabel(\"Recovery\")\n        ax3.set_title(\"Recovery vs Frequency\")\n        ax3.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n\n    def generate_synaptic_table(self, stp_frequency=50.0, stp_delay=250.0, plot=True):\n        \"\"\"\n        Generate a comprehensive table of synaptic parameters for all connections.\n\n        This method iterates through all available connections, runs simulations to \n        characterize each synapse, and compiles the results into a pandas DataFrame.\n\n        Parameters:\n        -----------\n        stp_frequency : float, optional\n            Frequency in Hz to use for STP (short-term plasticity) analysis. Default is 50.0 Hz.\n        stp_delay : float, optional\n            Delay in ms between pulse trains for STP analysis. Default is 250.0 ms.\n        plot : bool, optional\n            Whether to display the resulting table. Default is True.\n\n        Returns:\n        --------\n        pd.DataFrame\n            DataFrame containing synaptic parameters for each connection with columns:\n            - connection: Connection name\n            - rise_time: 20-80% rise time (ms)\n            - decay_time: Decay time constant (ms)\n            - latency: Response latency (ms)\n            - half_width: Response half-width (ms)\n            - peak_amplitude: Peak synaptic current amplitude (pA)\n            - baseline: Baseline current (pA)\n            - ppr: Paired-pulse ratio (normalized)\n            - simple_ppr: Simple paired-pulse ratio (2nd/1st pulse)\n            - induction: STP induction measure\n            - recovery: STP recovery measure\n\n        Notes:\n        ------\n        This method temporarily switches between connections to characterize each one,\n        then restores the original connection. The STP metrics are calculated at the\n        specified frequency and delay.\n        \"\"\"\n        # Store original connection to restore later\n        original_connection = self.current_connection\n\n        # Initialize results list\n        results = []\n\n        print(f\"Analyzing {len(self.conn_type_settings)} connections...\")\n\n        for conn_name in tqdm(self.conn_type_settings.keys(), desc=\"Analyzing connections\"):\n            try:\n                # Switch to this connection\n                self._switch_connection(conn_name)\n\n                # Run single event analysis\n                self.SingleEvent(plot_and_print=False)\n\n                # Get synaptic properties from the single event\n                syn_props = self._get_syn_prop()\n\n                # Run STP analysis at specified frequency\n                stp_results = self.stp_frequency_response(\n                    freqs=[stp_frequency], \n                    delay=stp_delay, \n                    plot=False, \n                    log_plot=False\n                )\n\n                # Extract STP metrics for this frequency\n                freq_idx = 0  # Only one frequency tested\n                ppr = stp_results['ppr'][freq_idx]\n                induction = stp_results['induction'][freq_idx]\n                recovery = stp_results['recovery'][freq_idx]\n                simple_ppr = stp_results['simple_ppr'][freq_idx]\n\n                # Compile results for this connection\n                conn_results = {\n                    'connection': conn_name,\n                    'rise_time': float(self.rise_time),\n                    'decay_time': float(self.decay_time),\n                    'latency': float(syn_props.get('latency', 0)),\n                    'half_width': float(syn_props.get('half_width', 0)),\n                    'peak_amplitude': float(syn_props.get('amp', 0)),\n                    'baseline': float(syn_props.get('baseline', 0)),\n                    'ppr': float(ppr),\n                    'simple_ppr': float(simple_ppr),\n                    'induction': float(induction),\n                    'recovery': float(recovery)\n                }\n\n                results.append(conn_results)\n\n            except Exception as e:\n                print(f\"Warning: Failed to analyze connection '{conn_name}': {e}\")\n                # Add partial results if possible\n                results.append({\n                    'connection': conn_name,\n                    'rise_time': float('nan'),\n                    'decay_time': float('nan'),\n                    'latency': float('nan'),\n                    'half_width': float('nan'),\n                    'peak_amplitude': float('nan'),\n                    'baseline': float('nan'),\n                    'ppr': float('nan'),\n                    'simple_ppr': float('nan'),\n                    'induction': float('nan'),\n                    'recovery': float('nan')\n                })\n\n        # Restore original connection\n        if original_connection in self.conn_type_settings:\n            self._switch_connection(original_connection)\n\n        # Create DataFrame\n        df = pd.DataFrame(results)\n\n        # Set connection as index for better display\n        df = df.set_index('connection')\n\n        if plot:\n            # Display the table\n            print(\"\\nSynaptic Parameters Table:\")\n            print(\"=\" * 80)\n            display(df.round(4))\n\n            # Optional: Create a simple bar plot for key metrics\n            try:\n                fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n                fig.suptitle(f'Synaptic Parameters Across Connections (STP at {stp_frequency}Hz)', fontsize=16)\n\n                # Plot rise/decay times\n                df[['rise_time', 'decay_time']].plot(kind='bar', ax=axes[0,0])\n                axes[0,0].set_title('Rise and Decay Times')\n                axes[0,0].set_ylabel('Time (ms)')\n                axes[0,0].tick_params(axis='x', rotation=45)\n\n                # Plot PPR metrics\n                df[['ppr', 'simple_ppr']].plot(kind='bar', ax=axes[0,1])\n                axes[0,1].set_title('Paired-Pulse Ratios')\n                axes[0,1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n                axes[0,1].tick_params(axis='x', rotation=45)\n\n                # Plot induction\n                df['induction'].plot(kind='bar', ax=axes[1,0], color='green')\n                axes[1,0].set_title('STP Induction')\n                axes[1,0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n                axes[1,0].set_ylabel('Induction')\n                axes[1,0].tick_params(axis='x', rotation=45)\n\n                # Plot recovery\n                df['recovery'].plot(kind='bar', ax=axes[1,1], color='orange')\n                axes[1,1].set_title('STP Recovery')\n                axes[1,1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n                axes[1,1].set_ylabel('Recovery')\n                axes[1,1].tick_params(axis='x', rotation=45)\n\n                plt.tight_layout()\n                plt.show()\n\n            except Exception as e:\n                print(f\"Warning: Could not create plots: {e}\")\n\n        return df\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner.__init__","title":"<code>__init__(mechanisms_dir=None, templates_dir=None, config=None, general_settings=None, conn_type_settings=None, hoc_cell=None)</code>","text":"<p>Initialize the GapJunctionTuner class.</p> Parameters: <p>mechanisms_dir : str     Directory path containing the compiled mod files needed for NEURON mechanisms. templates_dir : str     Directory path containing cell template files (.hoc or .py) loaded into NEURON. config : str     Path to a BMTK config.json file. Can be used to load mechanisms, templates, and other settings. general_settings : dict     General settings dictionary including parameters like simulation time step, duration, and temperature. conn_type_settings : dict     A dictionary containing connection-specific settings for gap junctions. hoc_cell : object, optional     An already loaded NEURON cell object. If provided, template loading and cell creation will be skipped.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def __init__(\n    self,\n    mechanisms_dir: Optional[str] = None,\n    templates_dir: Optional[str] = None,\n    config: Optional[str] = None,\n    general_settings: Optional[dict] = None,\n    conn_type_settings: Optional[dict] = None,\n    hoc_cell: Optional[object] = None,\n):\n    \"\"\"\n    Initialize the GapJunctionTuner class.\n\n    Parameters:\n    -----------\n    mechanisms_dir : str\n        Directory path containing the compiled mod files needed for NEURON mechanisms.\n    templates_dir : str\n        Directory path containing cell template files (.hoc or .py) loaded into NEURON.\n    config : str\n        Path to a BMTK config.json file. Can be used to load mechanisms, templates, and other settings.\n    general_settings : dict\n        General settings dictionary including parameters like simulation time step, duration, and temperature.\n    conn_type_settings : dict\n        A dictionary containing connection-specific settings for gap junctions.\n    hoc_cell : object, optional\n        An already loaded NEURON cell object. If provided, template loading and cell creation will be skipped.\n    \"\"\"\n    self.hoc_cell = hoc_cell\n\n    if hoc_cell is None:\n        if config is None and (mechanisms_dir is None or templates_dir is None):\n            raise ValueError(\n                \"Either a config file, both mechanisms_dir and templates_dir, or a hoc_cell must be provided.\"\n            )\n\n        if config is None:\n            neuron.load_mechanisms(mechanisms_dir)\n            h.load_file(templates_dir)\n        else:\n            # this will load both mechs and templates\n            load_templates_from_config(config)\n\n    # Use default general settings if not provided, merge with user-provided\n    if general_settings is None:\n        self.general_settings: dict = DEFAULT_GAP_JUNCTION_GENERAL_SETTINGS.copy()\n    else:\n        self.general_settings = {**DEFAULT_GAP_JUNCTION_GENERAL_SETTINGS, **general_settings}\n    self.conn_type_settings = conn_type_settings\n\n    self._syn_params_cache = {}\n    self.config = config\n    self.available_networks = []\n    self.current_network = None\n    self.last_figure = None\n    if self.conn_type_settings is None and self.config is not None:\n        self.conn_type_settings = self._build_conn_type_settings_from_config(self.config)\n    if self.conn_type_settings is None or len(self.conn_type_settings) == 0:\n        raise ValueError(\"conn_type_settings must be provided or config must be given to load gap junction connections from\")\n    self.current_connection = list(self.conn_type_settings.keys())[0]\n    self.conn = self.conn_type_settings[self.current_connection]\n\n    h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"] + 100.0\n    h.dt = self.general_settings[\"dt\"]  # Time step (resolution) of the simulation in ms\n    h.steps_per_ms = 1 / h.dt\n    h.celsius = self.general_settings[\"celsius\"]\n\n    # Clean up any existing parallel context before setting up gap junctions\n    try:\n        pc_temp = h.ParallelContext()\n        pc_temp.done()  # Clean up any existing parallel context\n    except:\n        pass  # Ignore errors if no existing context\n\n    # Force cleanup\n    import gc\n    gc.collect()\n\n    # set up gap junctions\n    self.pc = h.ParallelContext()\n\n    # Use provided hoc_cell or create new cells\n    if self.hoc_cell is not None:\n        self.cell1 = self.hoc_cell\n        # For gap junctions, we need two cells, so create a second one if using hoc_cell\n        self.cell_name = self.conn['cell']\n        self.cell2 = getattr(h, self.cell_name)()\n    else:\n        print(self.conn)\n        self.cell_name = self.conn['cell']\n        self.cell1 = getattr(h, self.cell_name)()\n        self.cell2 = getattr(h, self.cell_name)()\n\n    self.icl = h.IClamp(self.cell1.soma[0](0.5))\n    self.icl.delay = self.general_settings[\"tstart\"]\n    self.icl.dur = self.general_settings[\"tdur\"]\n    self.icl.amp = self.general_settings[\"iclamp_amp\"]  # nA\n\n    sec1 = list(self.cell1.all)[self.conn[\"sec_id\"]]\n    sec2 = list(self.cell2.all)[self.conn[\"sec_id\"]]\n\n    # Use unique IDs to avoid conflicts with existing parallel context setups\n    import time\n    unique_id = int(time.time() * 1000) % 10000  # Use timestamp as unique base ID\n\n    self.pc.source_var(sec1(self.conn[\"sec_x\"])._ref_v, unique_id, sec=sec1)\n    self.gap_junc_1 = h.Gap(sec1(0.5))\n    self.pc.target_var(self.gap_junc_1._ref_vgap, unique_id + 1)\n\n    self.pc.source_var(sec2(self.conn[\"sec_x\"])._ref_v, unique_id + 1, sec=sec2)\n    self.gap_junc_2 = h.Gap(sec2(0.5))\n    self.pc.target_var(self.gap_junc_2._ref_vgap, unique_id)\n\n    self.pc.setup_transfer()\n\n    # Now it's safe to initialize NEURON\n    h.finitialize()\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner.model","title":"<code>model(resistance)</code>","text":"<p>Run a simulation with a specified gap junction resistance.</p> Parameters: <p>resistance : float     The gap junction resistance value (in MOhm) to use for the simulation.</p> Notes: <p>This method sets up the gap junction resistance, initializes recording vectors for time and membrane voltages of both cells, and runs the NEURON simulation.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def model(self, resistance):\n    \"\"\"\n    Run a simulation with a specified gap junction resistance.\n\n    Parameters:\n    -----------\n    resistance : float\n        The gap junction resistance value (in MOhm) to use for the simulation.\n\n    Notes:\n    ------\n    This method sets up the gap junction resistance, initializes recording vectors for time\n    and membrane voltages of both cells, and runs the NEURON simulation.\n    \"\"\"\n    self.gap_junc_1.g = resistance\n    self.gap_junc_2.g = resistance\n\n    t_vec = h.Vector()\n    soma_v_1 = h.Vector()\n    soma_v_2 = h.Vector()\n    t_vec.record(h._ref_t)\n    soma_v_1.record(self.cell1.soma[0](0.5)._ref_v)\n    soma_v_2.record(self.cell2.soma[0](0.5)._ref_v)\n\n    self.t_vec = t_vec\n    self.soma_v_1 = soma_v_1\n    self.soma_v_2 = soma_v_2\n\n    h.finitialize(-70 * mV)\n    h.continuerun(h.tstop * ms)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner.plot_model","title":"<code>plot_model()</code>","text":"<p>Plot the voltage traces of both cells to visualize gap junction coupling.</p> <p>This method creates a plot showing the membrane potential of both cells over time, highlighting the effect of gap junction coupling when a current step is applied to cell 1.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def plot_model(self):\n    \"\"\"\n    Plot the voltage traces of both cells to visualize gap junction coupling.\n\n    This method creates a plot showing the membrane potential of both cells over time,\n    highlighting the effect of gap junction coupling when a current step is applied to cell 1.\n    \"\"\"\n    t_range = [\n        self.general_settings[\"tstart\"] - 100.0,\n        self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"] + 100.0,\n    ]\n    t = np.array(self.t_vec)\n    v1 = np.array(self.soma_v_1)\n    v2 = np.array(self.soma_v_2)\n    tidx = (t &gt;= t_range[0]) &amp; (t &lt;= t_range[1])\n\n    plt.figure()\n    plt.plot(t[tidx], v1[tidx], \"b\", label=f\"{self.cell_name} 1\")\n    plt.plot(t[tidx], v2[tidx], \"r\", label=f\"{self.cell_name} 2\")\n    plt.title(f\"{self.cell_name} gap junction\")\n    plt.xlabel(\"Time (ms)\")\n    plt.ylabel(\"Membrane Voltage (mV)\")\n    plt.legend()\n    self.last_figure = plt.gcf()\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner.coupling_coefficient","title":"<code>coupling_coefficient(t, v1, v2, t_start, t_end, dt=h.dt)</code>","text":"<p>Calculate the coupling coefficient between two cells connected by a gap junction.</p> Parameters: <p>t : array-like     Time vector. v1 : array-like     Voltage trace of the cell receiving the current injection. v2 : array-like     Voltage trace of the coupled cell. t_start : float     Start time for calculating the steady-state voltage change. t_end : float     End time for calculating the steady-state voltage change. dt : float, optional     Time step of the simulation. Default is h.dt.</p> Returns: <p>float     The coupling coefficient, defined as the ratio of voltage change in cell 2     to voltage change in cell 1 (\u0394V\u2082/\u0394V\u2081).</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def coupling_coefficient(self, t, v1, v2, t_start, t_end, dt=h.dt):\n    \"\"\"\n    Calculate the coupling coefficient between two cells connected by a gap junction.\n\n    Parameters:\n    -----------\n    t : array-like\n        Time vector.\n    v1 : array-like\n        Voltage trace of the cell receiving the current injection.\n    v2 : array-like\n        Voltage trace of the coupled cell.\n    t_start : float\n        Start time for calculating the steady-state voltage change.\n    t_end : float\n        End time for calculating the steady-state voltage change.\n    dt : float, optional\n        Time step of the simulation. Default is h.dt.\n\n    Returns:\n    --------\n    float\n        The coupling coefficient, defined as the ratio of voltage change in cell 2\n        to voltage change in cell 1 (\u0394V\u2082/\u0394V\u2081).\n    \"\"\"\n    t = np.asarray(t)\n    v1 = np.asarray(v1)\n    v2 = np.asarray(v2)\n    idx1 = np.nonzero(t &lt; t_start)[0][-1]\n    idx2 = np.nonzero(t &lt; t_end)[0][-1]\n    return (v2[idx2] - v2[idx1]) / (v1[idx2] - v1[idx1])\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner.InteractiveTuner","title":"<code>InteractiveTuner()</code>","text":"<p>Sets up interactive sliders for tuning short-term plasticity (STP) parameters in a Jupyter Notebook.</p> <p>This method creates an interactive UI with sliders for: - Network selection dropdown (if multiple networks available and config provided) - Connection type selection dropdown - Input frequency - Delay between pulse trains - Duration of stimulation (for continuous input mode) - Synaptic parameters (e.g., Use, tau_f, tau_d) based on the syn model</p> <p>It also provides buttons for: - Running a single event simulation - Running a train input simulation - Toggling voltage clamp mode - Switching between standard and continuous input modes</p> Network Dropdown Feature: <p>When the SynapseTuner is initialized with a BMTK config file containing multiple networks: - A network dropdown appears next to the connection dropdown - Users can dynamically switch between networks (e.g., 'network_to_network', 'external_to_network') - Switching networks rebuilds available connections and updates the connection dropdown - The current connection is preserved if it exists in the new network - If multiple networks exist but only one is specified during init, that network is used as default</p> Notes: <p>Ideal for exploratory parameter tuning and interactive visualization of synapse behavior with different parameter values and stimulation protocols. The network dropdown feature enables comprehensive exploration of multi-network BMTK simulations without needing to reinitialize the tuner.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def InteractiveTuner(self):\n    \"\"\"\n    Sets up interactive sliders for tuning short-term plasticity (STP) parameters in a Jupyter Notebook.\n\n    This method creates an interactive UI with sliders for:\n    - Network selection dropdown (if multiple networks available and config provided)\n    - Connection type selection dropdown\n    - Input frequency\n    - Delay between pulse trains\n    - Duration of stimulation (for continuous input mode)\n    - Synaptic parameters (e.g., Use, tau_f, tau_d) based on the syn model\n\n    It also provides buttons for:\n    - Running a single event simulation\n    - Running a train input simulation\n    - Toggling voltage clamp mode\n    - Switching between standard and continuous input modes\n\n    Network Dropdown Feature:\n    ------------------------\n    When the SynapseTuner is initialized with a BMTK config file containing multiple networks:\n    - A network dropdown appears next to the connection dropdown\n    - Users can dynamically switch between networks (e.g., 'network_to_network', 'external_to_network')\n    - Switching networks rebuilds available connections and updates the connection dropdown\n    - The current connection is preserved if it exists in the new network\n    - If multiple networks exist but only one is specified during init, that network is used as default\n\n    Notes:\n    ------\n    Ideal for exploratory parameter tuning and interactive visualization of\n    synapse behavior with different parameter values and stimulation protocols.\n    The network dropdown feature enables comprehensive exploration of multi-network\n    BMTK simulations without needing to reinitialize the tuner.\n    \"\"\"\n    # Widgets setup (Sliders)\n    freqs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200]\n    delays = [125, 250, 500, 1000, 2000, 4000]\n    durations = [100, 300, 500, 1000, 2000, 5000, 10000]\n    freq0 = 50\n    delay0 = 250\n    duration0 = 300\n    vlamp_status = self.vclamp\n\n    # Connection dropdown\n    connection_options = sorted(list(self.conn_type_settings.keys()))\n    w_connection = widgets.Dropdown(\n        options=connection_options,\n        value=self.current_connection,\n        description=\"Connection:\",\n        style={'description_width': 'initial'}\n    )\n\n    # Network dropdown - only shown if config was provided and multiple networks are available\n    # This enables users to switch between different network datasets dynamically\n    w_network = None\n    if self.config is not None and len(self.available_networks) &gt; 1:\n        w_network = widgets.Dropdown(\n            options=self.available_networks,\n            value=self.current_network,\n            description=\"Network:\",\n            style={'description_width': 'initial'}\n        )\n\n    w_run = widgets.Button(description=\"Run Train\", icon=\"history\", button_style=\"primary\")\n    w_single = widgets.Button(description=\"Single Event\", icon=\"check\", button_style=\"success\")\n    w_vclamp = widgets.ToggleButton(\n        value=vlamp_status,\n        description=\"Voltage Clamp\",\n        icon=\"fast-backward\",\n        button_style=\"warning\",\n    )\n\n    # Voltage clamp amplitude input\n    default_vclamp_amp = getattr(self.conn['spec_settings'], 'vclamp_amp', -70.0)\n    w_vclamp_amp = widgets.FloatText(\n        value=default_vclamp_amp,\n        description=\"V_clamp (mV):\",\n        step=5.0,\n        style={'description_width': 'initial'},\n        layout=widgets.Layout(width='150px')\n    )\n\n    w_input_mode = widgets.ToggleButton(\n        value=False, description=\"Continuous input\", icon=\"eject\", button_style=\"info\"\n    )\n    w_input_freq = widgets.SelectionSlider(options=freqs, value=freq0, description=\"Input Freq\")\n\n    # Sliders for delay and duration\n    self.w_delay = widgets.SelectionSlider(options=delays, value=delay0, description=\"Delay\")\n    self.w_duration = widgets.SelectionSlider(\n        options=durations, value=duration0, description=\"Duration\"\n    )\n\n    # Save functionality widgets\n    save_path_text = widgets.Text(\n        value=\"plot.png\",\n        description=\"Save path:\",\n        layout=widgets.Layout(width='300px')\n    )\n    save_button = widgets.Button(description=\"Save Plot\", icon=\"save\", button_style=\"success\")\n\n    def save_plot(b):\n        if hasattr(self, 'last_figure') and self.last_figure is not None:\n            try:\n                # Create a new figure with just the first subplot (synaptic current)\n                fig, ax = plt.subplots(figsize=(8, 6))\n\n                # Get the axes from the original figure\n                original_axes = self.last_figure.get_axes()\n                if len(original_axes) &gt; 0:\n                    first_ax = original_axes[0]\n\n                    # Copy the data from the first subplot\n                    for line in first_ax.get_lines():\n                        ax.plot(line.get_xdata(), line.get_ydata(), \n                               color=line.get_color(), label=line.get_label())\n\n                    # Copy axis labels and title\n                    ax.set_xlabel(first_ax.get_xlabel())\n                    ax.set_ylabel(first_ax.get_ylabel())\n                    ax.set_title(first_ax.get_title())\n                    ax.set_xlim(first_ax.get_xlim())\n                    ax.legend()\n                    ax.grid(True)\n\n                    # Save the new figure\n                    fig.savefig(save_path_text.value)\n                    plt.close(fig)  # Close the temporary figure\n                    print(f\"Synaptic current plot saved to {save_path_text.value}\")\n                else:\n                    print(\"No subplots found in the figure\")\n\n            except Exception as e:\n                print(f\"Error saving plot: {e}\")\n        else:\n            print(\"No plot to save\")\n\n    save_button.on_click(save_plot)\n\n    def create_dynamic_sliders():\n        \"\"\"Create sliders based on current connection's parameters\"\"\"\n        sliders = {}\n        for key, value in self.slider_vars.items():\n            if isinstance(value, (int, float)):  # Only create sliders for numeric values\n                if hasattr(self.syn, key):\n                    if value == 0:\n                        print(\n                            f\"{key} was set to zero, going to try to set a range of values, try settings the {key} to a nonzero value if you dont like the range!\"\n                        )\n                        slider = widgets.FloatSlider(\n                            value=value, min=0, max=1000, step=1, description=key\n                        )\n                    else:\n                        slider = widgets.FloatSlider(\n                            value=value, min=0, max=value * 20, step=value / 5, description=key\n                        )\n                    sliders[key] = slider\n                else:\n                    print(f\"skipping slider for {key} due to not being a synaptic variable\")\n        return sliders\n\n    # Generate sliders dynamically based on valid numeric entries in self.slider_vars\n    self.dynamic_sliders = create_dynamic_sliders()\n    print(\n        \"Setting up slider! The sliders ranges are set by their init value so try changing that if you dont like the slider range!\"\n    )\n\n    # Create output widget for displaying results\n    output_widget = widgets.Output()\n\n    def run_single_event(*args):\n        clear_output()\n        display(ui)\n        display(output_widget)\n\n        self.vclamp = w_vclamp.value\n        # Update voltage clamp amplitude if voltage clamp is enabled\n        if self.vclamp:\n            # Update the voltage clamp amplitude settings\n            self.conn['spec_settings']['vclamp_amp'] = w_vclamp_amp.value\n            # Update general settings if they exist\n            if hasattr(self, 'general_settings'):\n                self.general_settings['vclamp_amp'] = w_vclamp_amp.value\n        # Update synaptic properties based on slider values\n        self.ispk = None\n\n        # Clear previous results and run simulation\n        output_widget.clear_output()\n        with output_widget:\n            self.SingleEvent()\n\n    def on_connection_change(*args):\n        \"\"\"Handle connection dropdown change\"\"\"\n        try:\n            new_connection = w_connection.value\n            if new_connection != self.current_connection:\n                # Switch to new connection\n                self._switch_connection(new_connection)\n\n                # Recreate dynamic sliders for new connection\n                self.dynamic_sliders = create_dynamic_sliders()\n\n                # Update UI\n                update_ui_layout()\n                update_ui()\n\n        except Exception as e:\n            print(f\"Error switching connection: {e}\")\n\n    def on_network_change(*args):\n        \"\"\"\n        Handle network dropdown change events.\n\n        This callback is triggered when the user selects a different network from \n        the network dropdown. It coordinates the complete switching process:\n        1. Calls _switch_network() to rebuild connections for the new network\n        2. Updates the connection dropdown options with new network's connections\n        3. Recreates dynamic sliders for new connection parameters\n        4. Refreshes the entire UI to reflect all changes\n        \"\"\"\n        if w_network is None:\n            return\n        try:\n            new_network = w_network.value\n            if new_network != self.current_network:\n                # Switch to new network\n                self._switch_network(new_network)\n\n                # Update connection dropdown options with new network's connections\n                connection_options = list(self.conn_type_settings.keys())\n                w_connection.options = connection_options\n                if connection_options:\n                    w_connection.value = self.current_connection\n\n                # Recreate dynamic sliders for new connection\n                self.dynamic_sliders = create_dynamic_sliders()\n\n                # Update UI\n                update_ui_layout()\n                update_ui()\n\n        except Exception as e:\n            print(f\"Error switching network: {e}\")\n\n    def update_ui_layout():\n        \"\"\"\n        Update the UI layout with new sliders and network dropdown.\n\n        This function reconstructs the entire UI layout including:\n        - Network dropdown (if available) and connection dropdown in the top row\n        - Button controls and input mode toggles\n        - Parameter sliders arranged in columns\n        \"\"\"\n        nonlocal ui, slider_columns\n\n        # Add the dynamic sliders to the UI\n        slider_widgets = [slider for slider in self.dynamic_sliders.values()]\n\n        if slider_widgets:\n            half = len(slider_widgets) // 2\n            col1 = VBox(slider_widgets[:half])\n            col2 = VBox(slider_widgets[half:])\n            slider_columns = HBox([col1, col2])\n        else:\n            slider_columns = VBox([])\n\n        # Create button row with voltage clamp controls\n        if w_vclamp.value:  # Show voltage clamp amplitude input when toggle is on\n            button_row = HBox([w_run, w_single, w_vclamp, w_vclamp_amp, w_input_mode])\n        else:  # Hide voltage clamp amplitude input when toggle is off\n            button_row = HBox([w_run, w_single, w_vclamp, w_input_mode])\n\n        # Construct the top row - include network dropdown if available\n        # This creates a horizontal layout with network dropdown (if present) and connection dropdown\n        if w_network is not None:\n            connection_row = HBox([w_network, w_connection])\n        else:\n            connection_row = HBox([w_connection])\n        slider_row = HBox([w_input_freq, self.w_delay, self.w_duration])\n        save_row = HBox([save_path_text, save_button])\n\n        ui = VBox([connection_row, button_row, slider_row, slider_columns, save_row])\n\n    # Function to update UI based on input mode\n    def update_ui(*args):\n        clear_output()\n        display(ui)\n        display(output_widget)\n\n        self.vclamp = w_vclamp.value\n        # Update voltage clamp amplitude if voltage clamp is enabled\n        if self.vclamp:\n            self.conn['spec_settings']['vclamp_amp'] = w_vclamp_amp.value\n            if hasattr(self, 'general_settings'):\n                self.general_settings['vclamp_amp'] = w_vclamp_amp.value\n\n        self.input_mode = w_input_mode.value\n        syn_props = {var: slider.value for var, slider in self.dynamic_sliders.items()}\n        self._set_syn_prop(**syn_props)\n\n        # Clear previous results and run simulation\n        output_widget.clear_output()\n        with output_widget:\n            if not self.input_mode:\n                self._simulate_model(w_input_freq.value, self.w_delay.value, w_vclamp.value)\n            else:\n                self._simulate_model(w_input_freq.value, self.w_duration.value, w_vclamp.value)\n            amp = self._response_amplitude()\n            self._plot_model(\n                [self.general_settings[\"tstart\"] - self.nstim.interval / 3, self.tstop]\n            )\n            _ = self._calc_ppr_induction_recovery(amp)\n\n    # Function to switch between delay and duration sliders\n    def switch_slider(*args):\n        if w_input_mode.value:\n            self.w_delay.layout.display = \"none\"  # Hide delay slider\n            self.w_duration.layout.display = \"\"  # Show duration slider\n        else:\n            self.w_delay.layout.display = \"\"  # Show delay slider\n            self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n    # Function to handle voltage clamp toggle\n    def on_vclamp_toggle(*args):\n        \"\"\"Handle voltage clamp toggle changes to show/hide amplitude input\"\"\"\n        update_ui_layout()\n        clear_output()\n        display(ui)\n        display(output_widget)\n\n    # Link widgets to their callback functions\n    w_connection.observe(on_connection_change, names=\"value\")\n    # Link network dropdown callback only if network dropdown was created\n    if w_network is not None:\n        w_network.observe(on_network_change, names=\"value\")\n    w_input_mode.observe(switch_slider, names=\"value\")\n    w_vclamp.observe(on_vclamp_toggle, names=\"value\")\n\n    # Hide the duration slider initially until the user selects it\n    self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n    w_single.on_click(run_single_event)\n    w_run.on_click(update_ui)\n\n    # Initial UI setup\n    slider_columns = VBox([])\n    ui = VBox([])\n    update_ui_layout()\n\n    display(ui)\n    update_ui()\n</code></pre>"},{"location":"api/synapses/#optimization-results","title":"Optimization Results","text":""},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizationResult","title":"<code>bmtool.synapses.SynapseOptimizationResult</code>  <code>dataclass</code>","text":"<p>Container for synaptic parameter optimization results</p> Source code in <code>bmtool/synapses.py</code> <pre><code>@dataclass\nclass SynapseOptimizationResult:\n    \"\"\"Container for synaptic parameter optimization results\"\"\"\n\n    optimal_params: Dict[str, float]\n    achieved_metrics: Dict[str, float]\n    target_metrics: Dict[str, float]\n    error: float\n    optimization_path: List[Dict[str, float]]\n</code></pre>"},{"location":"api/synapses/#synapse-optimization","title":"Synapse Optimization","text":""},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer","title":"<code>bmtool.synapses.SynapseOptimizer</code>","text":"Source code in <code>bmtool/synapses.py</code> <pre><code>class SynapseOptimizer:\n    def __init__(self, tuner):\n        \"\"\"\n        Initialize the synapse optimizer with parameter scaling\n\n        Parameters:\n        -----------\n        tuner : SynapseTuner\n            Instance of the SynapseTuner class\n        \"\"\"\n        self.tuner = tuner\n        self.optimization_history = []\n        self.param_scales = {}\n\n    def _normalize_params(self, params: np.ndarray, param_names: List[str]) -&gt; np.ndarray:\n        \"\"\"\n        Normalize parameters to similar scales for better optimization performance.\n\n        Parameters:\n        -----------\n        params : np.ndarray\n            Original parameter values.\n        param_names : List[str]\n            Names of the parameters corresponding to the values.\n\n        Returns:\n        --------\n        np.ndarray\n            Normalized parameter values.\n        \"\"\"\n        return np.array([params[i] / self.param_scales[name] for i, name in enumerate(param_names)])\n\n    def _denormalize_params(\n        self, normalized_params: np.ndarray, param_names: List[str]\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Convert normalized parameters back to original scale.\n\n        Parameters:\n        -----------\n        normalized_params : np.ndarray\n            Normalized parameter values.\n        param_names : List[str]\n            Names of the parameters corresponding to the normalized values.\n\n        Returns:\n        --------\n        np.ndarray\n            Denormalized parameter values in their original scale.\n        \"\"\"\n        return np.array(\n            [normalized_params[i] * self.param_scales[name] for i, name in enumerate(param_names)]\n        )\n\n    def _calculate_metrics(self) -&gt; Dict[str, float]:\n        \"\"\"\n        Calculate standard metrics from the current simulation.\n\n        This method runs either a single event simulation, a train input simulation,\n        or both based on configuration flags, and calculates relevant synaptic metrics.\n\n        Returns:\n        --------\n        Dict[str, float]\n            Dictionary of calculated metrics including:\n            - induction: measure of synaptic facilitation/depression\n            - ppr: paired-pulse ratio\n            - recovery: recovery from facilitation/depression\n            - max_amplitude: maximum synaptic response amplitude\n            - rise_time: time for synaptic response to rise from 20% to 80% of peak\n            - decay_time: time constant of synaptic response decay\n            - latency: synaptic response latency\n            - half_width: synaptic response half-width\n            - baseline: baseline current\n            - amp: peak amplitude from syn_props\n        \"\"\"\n        # Set these to 0 for when we return the dict\n        induction = 0\n        ppr = 0\n        recovery = 0\n        simple_ppr = 0\n        amp = 0\n        rise_time = 0\n        decay_time = 0\n        latency = 0\n        half_width = 0\n        baseline = 0\n        syn_amp = 0\n\n        if self.run_single_event:\n            self.tuner.SingleEvent(plot_and_print=False)\n            # Use the attributes set by SingleEvent method\n            rise_time = getattr(self.tuner, \"rise_time\", 0)\n            decay_time = getattr(self.tuner, \"decay_time\", 0)\n            # Get additional syn_props directly\n            syn_props = self.tuner._get_syn_prop()\n            latency = syn_props.get(\"latency\", 0)\n            half_width = syn_props.get(\"half_width\", 0)\n            baseline = syn_props.get(\"baseline\", 0)\n            syn_amp = syn_props.get(\"amp\", 0)\n\n        if self.run_train_input:\n            self.tuner._simulate_model(self.train_frequency, self.train_delay)\n            amp = self.tuner._response_amplitude()\n            ppr, induction, recovery, simple_ppr = self.tuner._calc_ppr_induction_recovery(\n                amp, print_math=False\n            )\n            amp = self.tuner._find_max_amp(amp)\n\n        return {\n            \"induction\": float(induction),\n            \"ppr\": float(ppr),\n            \"recovery\": float(recovery),\n            \"simple_ppr\": float(simple_ppr),\n            \"max_amplitude\": float(amp),\n            \"rise_time\": float(rise_time),\n            \"decay_time\": float(decay_time),\n            \"latency\": float(latency),\n            \"half_width\": float(half_width),\n            \"baseline\": float(baseline),\n            \"amp\": float(syn_amp),\n        }\n\n    def _default_cost_function(\n        self, metrics: Dict[str, float], target_metrics: Dict[str, float]\n    ) -&gt; float:\n        \"\"\"\n        Default cost function that minimizes the squared difference between achieved and target induction.\n\n        Parameters:\n        -----------\n        metrics : Dict[str, float]\n            Dictionary of calculated metrics from the current simulation.\n        target_metrics : Dict[str, float]\n            Dictionary of target metrics to optimize towards.\n\n        Returns:\n        --------\n        float\n            The squared error between achieved and target induction.\n        \"\"\"\n        return float((metrics[\"induction\"] - target_metrics[\"induction\"]) ** 2)\n\n    def _objective_function(\n        self,\n        normalized_params: np.ndarray,\n        param_names: List[str],\n        cost_function: Callable,\n        target_metrics: Dict[str, float],\n    ) -&gt; float:\n        \"\"\"\n        Calculate error using provided cost function\n        \"\"\"\n        # Denormalize parameters\n        params = self._denormalize_params(normalized_params, param_names)\n\n        # Set parameters\n        for name, value in zip(param_names, params):\n            setattr(self.tuner.syn, name, value)\n\n        # just do this and have the SingleEvent handle it\n        if self.run_single_event:\n            self.tuner.using_optimizer = True\n            self.tuner.param_names = param_names\n            self.tuner.params = params\n\n        # Calculate metrics and error\n        metrics = self._calculate_metrics()\n        error = float(cost_function(metrics, target_metrics))  # Ensure error is scalar\n\n        # Store history with denormalized values\n        history_entry = {\n            \"params\": dict(zip(param_names, params)),\n            \"metrics\": metrics,\n            \"error\": error,\n        }\n        self.optimization_history.append(history_entry)\n\n        return error\n\n    def optimize_parameters(\n        self,\n        target_metrics: Dict[str, float],\n        param_bounds: Dict[str, Tuple[float, float]],\n        run_single_event: bool = False,\n        run_train_input: bool = True,\n        train_frequency: float = 50,\n        train_delay: float = 250,\n        cost_function: Optional[Callable] = None,\n        method: str = \"SLSQP\",\n        init_guess=\"random\",\n    ) -&gt; SynapseOptimizationResult:\n        \"\"\"\n        Optimize synaptic parameters to achieve target metrics.\n\n        Parameters:\n        -----------\n        target_metrics : Dict[str, float]\n            Target values for synaptic metrics (e.g., {'induction': 0.2, 'rise_time': 0.5})\n        param_bounds : Dict[str, Tuple[float, float]]\n            Bounds for each parameter to optimize (e.g., {'tau_d': (5, 50), 'Use': (0.1, 0.9)})\n        run_single_event : bool, optional\n            Whether to run single event simulations during optimization (default: False)\n        run_train_input : bool, optional\n            Whether to run train input simulations during optimization (default: True)\n        train_frequency : float, optional\n            Frequency of the stimulus train in Hz (default: 50)\n        train_delay : float, optional\n            Delay between pulse trains in ms (default: 250)\n        cost_function : Optional[Callable]\n            Custom cost function for optimization. If None, uses default cost function\n            that optimizes induction.\n        method : str, optional\n            Optimization method to use (default: 'SLSQP')\n        init_guess : str, optional\n            Method for initial parameter guess ('random' or 'middle_guess')\n\n        Returns:\n        --------\n        SynapseOptimizationResult\n            Results of the optimization including optimal parameters, achieved metrics,\n            target metrics, final error, and optimization path.\n\n        Notes:\n        ------\n        This function uses scipy.optimize.minimize to find the optimal parameter values\n        that minimize the difference between achieved and target metrics.\n        \"\"\"\n        self.optimization_history = []\n        self.train_frequency = train_frequency\n        self.train_delay = train_delay\n        self.run_single_event = run_single_event\n        self.run_train_input = run_train_input\n\n        param_names = list(param_bounds.keys())\n        bounds = [param_bounds[name] for name in param_names]\n\n        if cost_function is None:\n            cost_function = self._default_cost_function\n\n        # Calculate scaling factors\n        self.param_scales = {\n            name: max(abs(bounds[i][0]), abs(bounds[i][1])) for i, name in enumerate(param_names)\n        }\n\n        # Normalize bounds\n        normalized_bounds = [\n            (b[0] / self.param_scales[name], b[1] / self.param_scales[name])\n            for name, b in zip(param_names, bounds)\n        ]\n\n        # picks with method of init value we want to use\n        if init_guess == \"random\":\n            x0 = np.array([np.random.uniform(b[0], b[1]) for b in bounds])\n        elif init_guess == \"middle_guess\":\n            x0 = [(b[0] + b[1]) / 2 for b in bounds]\n        else:\n            raise Exception(\"Pick a valid init guess method: either 'random' or 'middle_guess'\")\n        normalized_x0 = self._normalize_params(np.array(x0), param_names)\n\n        # Run optimization\n        result = minimize(\n            self._objective_function,\n            normalized_x0,\n            args=(param_names, cost_function, target_metrics),\n            method=method,\n            bounds=normalized_bounds,\n        )\n\n        # Get final parameters and metrics\n        final_params = dict(zip(param_names, self._denormalize_params(result.x, param_names)))\n        for name, value in final_params.items():\n            setattr(self.tuner.syn, name, value)\n        final_metrics = self._calculate_metrics()\n\n        return SynapseOptimizationResult(\n            optimal_params=final_params,\n            achieved_metrics=final_metrics,\n            target_metrics=target_metrics,\n            error=result.fun,\n            optimization_path=self.optimization_history,\n        )\n\n    def plot_optimization_results(self, result: SynapseOptimizationResult):\n        \"\"\"\n        Plot optimization results including convergence and final traces.\n\n        Parameters:\n        -----------\n        result : SynapseOptimizationResult\n            Results from optimization as returned by optimize_parameters()\n\n        Notes:\n        ------\n        This method generates three plots:\n        1. Error convergence plot showing how the error decreased over iterations\n        2. Parameter convergence plots showing how each parameter changed\n        3. Final model response with the optimal parameters\n\n        It also prints a summary of the optimization results including target vs. achieved\n        metrics and the optimal parameter values.\n        \"\"\"\n        # Ensure errors are properly shaped for plotting\n        iterations = range(len(result.optimization_path))\n        errors = np.array([float(h[\"error\"]) for h in result.optimization_path]).flatten()\n\n        # Plot error convergence\n        fig1, ax1 = plt.subplots(figsize=(8, 5))\n        ax1.plot(iterations, errors, label=\"Error\")\n        ax1.set_xlabel(\"Iteration\")\n        ax1.set_ylabel(\"Error\")\n        ax1.set_title(\"Error Convergence\")\n        ax1.set_yscale(\"log\")\n        ax1.legend()\n        plt.tight_layout()\n        plt.show()\n\n        # Plot parameter convergence\n        param_names = list(result.optimal_params.keys())\n        num_params = len(param_names)\n        fig2, axs = plt.subplots(nrows=num_params, ncols=1, figsize=(8, 5 * num_params))\n\n        if num_params == 1:\n            axs = [axs]\n\n        for ax, param in zip(axs, param_names):\n            values = [float(h[\"params\"][param]) for h in result.optimization_path]\n            ax.plot(iterations, values, label=f\"{param}\")\n            ax.set_xlabel(\"Iteration\")\n            ax.set_ylabel(\"Parameter Value\")\n            ax.set_title(f\"Convergence of {param}\")\n            ax.legend()\n\n        plt.tight_layout()\n        plt.show()\n\n        # Print final results\n        print(\"Optimization Results:\")\n        print(f\"Final Error: {float(result.error):.2e}\\n\")\n        print(\"Target Metrics:\")\n        for metric, value in result.target_metrics.items():\n            achieved = result.achieved_metrics.get(metric)\n            if achieved is not None and metric != \"amplitudes\":  # Skip amplitude array\n                print(f\"{metric}: {float(achieved):.3f} (target: {float(value):.3f})\")\n\n        print(\"\\nOptimal Parameters:\")\n        for param, value in result.optimal_params.items():\n            print(f\"{param}: {float(value):.3f}\")\n\n        # Plot final model response\n        if self.run_train_input:\n            self.tuner._plot_model(\n                [\n                    self.tuner.general_settings[\"tstart\"] - self.tuner.nstim.interval / 3,\n                    self.tuner.tstop,\n                ]\n            )\n            amp = self.tuner._response_amplitude()\n            self.tuner._calc_ppr_induction_recovery(amp)\n        if self.run_single_event:\n            self.tuner.ispk = None\n            self.tuner.SingleEvent(plot_and_print=True)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer.__init__","title":"<code>__init__(tuner)</code>","text":"<p>Initialize the synapse optimizer with parameter scaling</p> Parameters: <p>tuner : SynapseTuner     Instance of the SynapseTuner class</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def __init__(self, tuner):\n    \"\"\"\n    Initialize the synapse optimizer with parameter scaling\n\n    Parameters:\n    -----------\n    tuner : SynapseTuner\n        Instance of the SynapseTuner class\n    \"\"\"\n    self.tuner = tuner\n    self.optimization_history = []\n    self.param_scales = {}\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer._normalize_params","title":"<code>_normalize_params(params, param_names)</code>","text":"<p>Normalize parameters to similar scales for better optimization performance.</p> Parameters: <p>params : np.ndarray     Original parameter values. param_names : List[str]     Names of the parameters corresponding to the values.</p> Returns: <p>np.ndarray     Normalized parameter values.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _normalize_params(self, params: np.ndarray, param_names: List[str]) -&gt; np.ndarray:\n    \"\"\"\n    Normalize parameters to similar scales for better optimization performance.\n\n    Parameters:\n    -----------\n    params : np.ndarray\n        Original parameter values.\n    param_names : List[str]\n        Names of the parameters corresponding to the values.\n\n    Returns:\n    --------\n    np.ndarray\n        Normalized parameter values.\n    \"\"\"\n    return np.array([params[i] / self.param_scales[name] for i, name in enumerate(param_names)])\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer._denormalize_params","title":"<code>_denormalize_params(normalized_params, param_names)</code>","text":"<p>Convert normalized parameters back to original scale.</p> Parameters: <p>normalized_params : np.ndarray     Normalized parameter values. param_names : List[str]     Names of the parameters corresponding to the normalized values.</p> Returns: <p>np.ndarray     Denormalized parameter values in their original scale.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _denormalize_params(\n    self, normalized_params: np.ndarray, param_names: List[str]\n) -&gt; np.ndarray:\n    \"\"\"\n    Convert normalized parameters back to original scale.\n\n    Parameters:\n    -----------\n    normalized_params : np.ndarray\n        Normalized parameter values.\n    param_names : List[str]\n        Names of the parameters corresponding to the normalized values.\n\n    Returns:\n    --------\n    np.ndarray\n        Denormalized parameter values in their original scale.\n    \"\"\"\n    return np.array(\n        [normalized_params[i] * self.param_scales[name] for i, name in enumerate(param_names)]\n    )\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer._calculate_metrics","title":"<code>_calculate_metrics()</code>","text":"<p>Calculate standard metrics from the current simulation.</p> <p>This method runs either a single event simulation, a train input simulation, or both based on configuration flags, and calculates relevant synaptic metrics.</p> Returns: <p>Dict[str, float]     Dictionary of calculated metrics including:     - induction: measure of synaptic facilitation/depression     - ppr: paired-pulse ratio     - recovery: recovery from facilitation/depression     - max_amplitude: maximum synaptic response amplitude     - rise_time: time for synaptic response to rise from 20% to 80% of peak     - decay_time: time constant of synaptic response decay     - latency: synaptic response latency     - half_width: synaptic response half-width     - baseline: baseline current     - amp: peak amplitude from syn_props</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _calculate_metrics(self) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate standard metrics from the current simulation.\n\n    This method runs either a single event simulation, a train input simulation,\n    or both based on configuration flags, and calculates relevant synaptic metrics.\n\n    Returns:\n    --------\n    Dict[str, float]\n        Dictionary of calculated metrics including:\n        - induction: measure of synaptic facilitation/depression\n        - ppr: paired-pulse ratio\n        - recovery: recovery from facilitation/depression\n        - max_amplitude: maximum synaptic response amplitude\n        - rise_time: time for synaptic response to rise from 20% to 80% of peak\n        - decay_time: time constant of synaptic response decay\n        - latency: synaptic response latency\n        - half_width: synaptic response half-width\n        - baseline: baseline current\n        - amp: peak amplitude from syn_props\n    \"\"\"\n    # Set these to 0 for when we return the dict\n    induction = 0\n    ppr = 0\n    recovery = 0\n    simple_ppr = 0\n    amp = 0\n    rise_time = 0\n    decay_time = 0\n    latency = 0\n    half_width = 0\n    baseline = 0\n    syn_amp = 0\n\n    if self.run_single_event:\n        self.tuner.SingleEvent(plot_and_print=False)\n        # Use the attributes set by SingleEvent method\n        rise_time = getattr(self.tuner, \"rise_time\", 0)\n        decay_time = getattr(self.tuner, \"decay_time\", 0)\n        # Get additional syn_props directly\n        syn_props = self.tuner._get_syn_prop()\n        latency = syn_props.get(\"latency\", 0)\n        half_width = syn_props.get(\"half_width\", 0)\n        baseline = syn_props.get(\"baseline\", 0)\n        syn_amp = syn_props.get(\"amp\", 0)\n\n    if self.run_train_input:\n        self.tuner._simulate_model(self.train_frequency, self.train_delay)\n        amp = self.tuner._response_amplitude()\n        ppr, induction, recovery, simple_ppr = self.tuner._calc_ppr_induction_recovery(\n            amp, print_math=False\n        )\n        amp = self.tuner._find_max_amp(amp)\n\n    return {\n        \"induction\": float(induction),\n        \"ppr\": float(ppr),\n        \"recovery\": float(recovery),\n        \"simple_ppr\": float(simple_ppr),\n        \"max_amplitude\": float(amp),\n        \"rise_time\": float(rise_time),\n        \"decay_time\": float(decay_time),\n        \"latency\": float(latency),\n        \"half_width\": float(half_width),\n        \"baseline\": float(baseline),\n        \"amp\": float(syn_amp),\n    }\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer._default_cost_function","title":"<code>_default_cost_function(metrics, target_metrics)</code>","text":"<p>Default cost function that minimizes the squared difference between achieved and target induction.</p> Parameters: <p>metrics : Dict[str, float]     Dictionary of calculated metrics from the current simulation. target_metrics : Dict[str, float]     Dictionary of target metrics to optimize towards.</p> Returns: <p>float     The squared error between achieved and target induction.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _default_cost_function(\n    self, metrics: Dict[str, float], target_metrics: Dict[str, float]\n) -&gt; float:\n    \"\"\"\n    Default cost function that minimizes the squared difference between achieved and target induction.\n\n    Parameters:\n    -----------\n    metrics : Dict[str, float]\n        Dictionary of calculated metrics from the current simulation.\n    target_metrics : Dict[str, float]\n        Dictionary of target metrics to optimize towards.\n\n    Returns:\n    --------\n    float\n        The squared error between achieved and target induction.\n    \"\"\"\n    return float((metrics[\"induction\"] - target_metrics[\"induction\"]) ** 2)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer._objective_function","title":"<code>_objective_function(normalized_params, param_names, cost_function, target_metrics)</code>","text":"<p>Calculate error using provided cost function</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _objective_function(\n    self,\n    normalized_params: np.ndarray,\n    param_names: List[str],\n    cost_function: Callable,\n    target_metrics: Dict[str, float],\n) -&gt; float:\n    \"\"\"\n    Calculate error using provided cost function\n    \"\"\"\n    # Denormalize parameters\n    params = self._denormalize_params(normalized_params, param_names)\n\n    # Set parameters\n    for name, value in zip(param_names, params):\n        setattr(self.tuner.syn, name, value)\n\n    # just do this and have the SingleEvent handle it\n    if self.run_single_event:\n        self.tuner.using_optimizer = True\n        self.tuner.param_names = param_names\n        self.tuner.params = params\n\n    # Calculate metrics and error\n    metrics = self._calculate_metrics()\n    error = float(cost_function(metrics, target_metrics))  # Ensure error is scalar\n\n    # Store history with denormalized values\n    history_entry = {\n        \"params\": dict(zip(param_names, params)),\n        \"metrics\": metrics,\n        \"error\": error,\n    }\n    self.optimization_history.append(history_entry)\n\n    return error\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer.optimize_parameters","title":"<code>optimize_parameters(target_metrics, param_bounds, run_single_event=False, run_train_input=True, train_frequency=50, train_delay=250, cost_function=None, method='SLSQP', init_guess='random')</code>","text":"<p>Optimize synaptic parameters to achieve target metrics.</p> Parameters: <p>target_metrics : Dict[str, float]     Target values for synaptic metrics (e.g., {'induction': 0.2, 'rise_time': 0.5}) param_bounds : Dict[str, Tuple[float, float]]     Bounds for each parameter to optimize (e.g., {'tau_d': (5, 50), 'Use': (0.1, 0.9)}) run_single_event : bool, optional     Whether to run single event simulations during optimization (default: False) run_train_input : bool, optional     Whether to run train input simulations during optimization (default: True) train_frequency : float, optional     Frequency of the stimulus train in Hz (default: 50) train_delay : float, optional     Delay between pulse trains in ms (default: 250) cost_function : Optional[Callable]     Custom cost function for optimization. If None, uses default cost function     that optimizes induction. method : str, optional     Optimization method to use (default: 'SLSQP') init_guess : str, optional     Method for initial parameter guess ('random' or 'middle_guess')</p> Returns: <p>SynapseOptimizationResult     Results of the optimization including optimal parameters, achieved metrics,     target metrics, final error, and optimization path.</p> Notes: <p>This function uses scipy.optimize.minimize to find the optimal parameter values that minimize the difference between achieved and target metrics.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def optimize_parameters(\n    self,\n    target_metrics: Dict[str, float],\n    param_bounds: Dict[str, Tuple[float, float]],\n    run_single_event: bool = False,\n    run_train_input: bool = True,\n    train_frequency: float = 50,\n    train_delay: float = 250,\n    cost_function: Optional[Callable] = None,\n    method: str = \"SLSQP\",\n    init_guess=\"random\",\n) -&gt; SynapseOptimizationResult:\n    \"\"\"\n    Optimize synaptic parameters to achieve target metrics.\n\n    Parameters:\n    -----------\n    target_metrics : Dict[str, float]\n        Target values for synaptic metrics (e.g., {'induction': 0.2, 'rise_time': 0.5})\n    param_bounds : Dict[str, Tuple[float, float]]\n        Bounds for each parameter to optimize (e.g., {'tau_d': (5, 50), 'Use': (0.1, 0.9)})\n    run_single_event : bool, optional\n        Whether to run single event simulations during optimization (default: False)\n    run_train_input : bool, optional\n        Whether to run train input simulations during optimization (default: True)\n    train_frequency : float, optional\n        Frequency of the stimulus train in Hz (default: 50)\n    train_delay : float, optional\n        Delay between pulse trains in ms (default: 250)\n    cost_function : Optional[Callable]\n        Custom cost function for optimization. If None, uses default cost function\n        that optimizes induction.\n    method : str, optional\n        Optimization method to use (default: 'SLSQP')\n    init_guess : str, optional\n        Method for initial parameter guess ('random' or 'middle_guess')\n\n    Returns:\n    --------\n    SynapseOptimizationResult\n        Results of the optimization including optimal parameters, achieved metrics,\n        target metrics, final error, and optimization path.\n\n    Notes:\n    ------\n    This function uses scipy.optimize.minimize to find the optimal parameter values\n    that minimize the difference between achieved and target metrics.\n    \"\"\"\n    self.optimization_history = []\n    self.train_frequency = train_frequency\n    self.train_delay = train_delay\n    self.run_single_event = run_single_event\n    self.run_train_input = run_train_input\n\n    param_names = list(param_bounds.keys())\n    bounds = [param_bounds[name] for name in param_names]\n\n    if cost_function is None:\n        cost_function = self._default_cost_function\n\n    # Calculate scaling factors\n    self.param_scales = {\n        name: max(abs(bounds[i][0]), abs(bounds[i][1])) for i, name in enumerate(param_names)\n    }\n\n    # Normalize bounds\n    normalized_bounds = [\n        (b[0] / self.param_scales[name], b[1] / self.param_scales[name])\n        for name, b in zip(param_names, bounds)\n    ]\n\n    # picks with method of init value we want to use\n    if init_guess == \"random\":\n        x0 = np.array([np.random.uniform(b[0], b[1]) for b in bounds])\n    elif init_guess == \"middle_guess\":\n        x0 = [(b[0] + b[1]) / 2 for b in bounds]\n    else:\n        raise Exception(\"Pick a valid init guess method: either 'random' or 'middle_guess'\")\n    normalized_x0 = self._normalize_params(np.array(x0), param_names)\n\n    # Run optimization\n    result = minimize(\n        self._objective_function,\n        normalized_x0,\n        args=(param_names, cost_function, target_metrics),\n        method=method,\n        bounds=normalized_bounds,\n    )\n\n    # Get final parameters and metrics\n    final_params = dict(zip(param_names, self._denormalize_params(result.x, param_names)))\n    for name, value in final_params.items():\n        setattr(self.tuner.syn, name, value)\n    final_metrics = self._calculate_metrics()\n\n    return SynapseOptimizationResult(\n        optimal_params=final_params,\n        achieved_metrics=final_metrics,\n        target_metrics=target_metrics,\n        error=result.fun,\n        optimization_path=self.optimization_history,\n    )\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer.plot_optimization_results","title":"<code>plot_optimization_results(result)</code>","text":"<p>Plot optimization results including convergence and final traces.</p> Parameters: <p>result : SynapseOptimizationResult     Results from optimization as returned by optimize_parameters()</p> Notes: <p>This method generates three plots: 1. Error convergence plot showing how the error decreased over iterations 2. Parameter convergence plots showing how each parameter changed 3. Final model response with the optimal parameters</p> <p>It also prints a summary of the optimization results including target vs. achieved metrics and the optimal parameter values.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def plot_optimization_results(self, result: SynapseOptimizationResult):\n    \"\"\"\n    Plot optimization results including convergence and final traces.\n\n    Parameters:\n    -----------\n    result : SynapseOptimizationResult\n        Results from optimization as returned by optimize_parameters()\n\n    Notes:\n    ------\n    This method generates three plots:\n    1. Error convergence plot showing how the error decreased over iterations\n    2. Parameter convergence plots showing how each parameter changed\n    3. Final model response with the optimal parameters\n\n    It also prints a summary of the optimization results including target vs. achieved\n    metrics and the optimal parameter values.\n    \"\"\"\n    # Ensure errors are properly shaped for plotting\n    iterations = range(len(result.optimization_path))\n    errors = np.array([float(h[\"error\"]) for h in result.optimization_path]).flatten()\n\n    # Plot error convergence\n    fig1, ax1 = plt.subplots(figsize=(8, 5))\n    ax1.plot(iterations, errors, label=\"Error\")\n    ax1.set_xlabel(\"Iteration\")\n    ax1.set_ylabel(\"Error\")\n    ax1.set_title(\"Error Convergence\")\n    ax1.set_yscale(\"log\")\n    ax1.legend()\n    plt.tight_layout()\n    plt.show()\n\n    # Plot parameter convergence\n    param_names = list(result.optimal_params.keys())\n    num_params = len(param_names)\n    fig2, axs = plt.subplots(nrows=num_params, ncols=1, figsize=(8, 5 * num_params))\n\n    if num_params == 1:\n        axs = [axs]\n\n    for ax, param in zip(axs, param_names):\n        values = [float(h[\"params\"][param]) for h in result.optimization_path]\n        ax.plot(iterations, values, label=f\"{param}\")\n        ax.set_xlabel(\"Iteration\")\n        ax.set_ylabel(\"Parameter Value\")\n        ax.set_title(f\"Convergence of {param}\")\n        ax.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # Print final results\n    print(\"Optimization Results:\")\n    print(f\"Final Error: {float(result.error):.2e}\\n\")\n    print(\"Target Metrics:\")\n    for metric, value in result.target_metrics.items():\n        achieved = result.achieved_metrics.get(metric)\n        if achieved is not None and metric != \"amplitudes\":  # Skip amplitude array\n            print(f\"{metric}: {float(achieved):.3f} (target: {float(value):.3f})\")\n\n    print(\"\\nOptimal Parameters:\")\n    for param, value in result.optimal_params.items():\n        print(f\"{param}: {float(value):.3f}\")\n\n    # Plot final model response\n    if self.run_train_input:\n        self.tuner._plot_model(\n            [\n                self.tuner.general_settings[\"tstart\"] - self.tuner.nstim.interval / 3,\n                self.tuner.tstop,\n            ]\n        )\n        amp = self.tuner._response_amplitude()\n        self.tuner._calc_ppr_induction_recovery(amp)\n    if self.run_single_event:\n        self.tuner.ispk = None\n        self.tuner.SingleEvent(plot_and_print=True)\n</code></pre>"},{"location":"api/synapses/#gap-junction-optimization-results","title":"Gap Junction Optimization Results","text":""},{"location":"api/synapses/#bmtool.synapses.GapOptimizationResult","title":"<code>bmtool.synapses.GapOptimizationResult</code>  <code>dataclass</code>","text":"<p>Container for gap junction optimization results</p> Source code in <code>bmtool/synapses.py</code> <pre><code>@dataclass\nclass GapOptimizationResult:\n    \"\"\"Container for gap junction optimization results\"\"\"\n\n    optimal_resistance: float\n    achieved_cc: float\n    target_cc: float\n    error: float\n    optimization_path: List[Dict[str, float]]\n</code></pre>"},{"location":"api/synapses/#gap-junction-optimization","title":"Gap Junction Optimization","text":""},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer","title":"<code>bmtool.synapses.GapJunctionOptimizer</code>","text":"Source code in <code>bmtool/synapses.py</code> <pre><code>class GapJunctionOptimizer:\n    def __init__(self, tuner):\n        \"\"\"\n        Initialize the gap junction optimizer\n\n        Parameters:\n        -----------\n        tuner : GapJunctionTuner\n            Instance of the GapJunctionTuner class\n        \"\"\"\n        self.tuner = tuner\n        self.optimization_history = []\n\n    def _objective_function(self, resistance: float, target_cc: float) -&gt; float:\n        \"\"\"\n        Calculate error between achieved and target coupling coefficient\n\n        Parameters:\n        -----------\n        resistance : float\n            Gap junction resistance to try\n        target_cc : float\n            Target coupling coefficient to match\n\n        Returns:\n        --------\n        float : Error between achieved and target coupling coefficient\n        \"\"\"\n        # Run model with current resistance\n        self.tuner.model(resistance)\n\n        # Calculate coupling coefficient\n        achieved_cc = self.tuner.coupling_coefficient(\n            self.tuner.t_vec,\n            self.tuner.soma_v_1,\n            self.tuner.soma_v_2,\n            self.tuner.general_settings[\"tstart\"],\n            self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n        )\n\n        # Calculate error\n        error = (achieved_cc - target_cc) ** 2  # MSE\n\n        # Store history\n        self.optimization_history.append(\n            {\"resistance\": resistance, \"achieved_cc\": achieved_cc, \"error\": error}\n        )\n\n        return error\n\n    def optimize_resistance(\n        self, target_cc: float, resistance_bounds: tuple = (1e-4, 1e-2), method: str = \"bounded\"\n    ) -&gt; GapOptimizationResult:\n        \"\"\"\n        Optimize gap junction resistance to achieve a target coupling coefficient.\n\n        Parameters:\n        -----------\n        target_cc : float\n            Target coupling coefficient to achieve (between 0 and 1)\n        resistance_bounds : tuple, optional\n            (min, max) bounds for resistance search in MOhm. Default is (1e-4, 1e-2).\n        method : str, optional\n            Optimization method to use. Default is 'bounded' which works well\n            for single-parameter optimization.\n\n        Returns:\n        --------\n        GapOptimizationResult\n            Container with optimization results including:\n            - optimal_resistance: The optimized resistance value\n            - achieved_cc: The coupling coefficient achieved with the optimal resistance\n            - target_cc: The target coupling coefficient\n            - error: The final error (squared difference between target and achieved)\n            - optimization_path: List of all values tried during optimization\n\n        Notes:\n        ------\n        Uses scipy.optimize.minimize_scalar with bounded method, which is\n        appropriate for this single-parameter optimization problem.\n        \"\"\"\n        self.optimization_history = []\n\n        # Run optimization\n        result = minimize_scalar(\n            self._objective_function, args=(target_cc,), bounds=resistance_bounds, method=method\n        )\n\n        # Run final model with optimal resistance\n        self.tuner.model(result.x)\n        final_cc = self.tuner.coupling_coefficient(\n            self.tuner.t_vec,\n            self.tuner.soma_v_1,\n            self.tuner.soma_v_2,\n            self.tuner.general_settings[\"tstart\"],\n            self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n        )\n\n        # Package up our results\n        optimization_result = GapOptimizationResult(\n            optimal_resistance=result.x,\n            achieved_cc=final_cc,\n            target_cc=target_cc,\n            error=result.fun,\n            optimization_path=self.optimization_history,\n        )\n\n        return optimization_result\n\n    def plot_optimization_results(self, result: GapOptimizationResult):\n        \"\"\"\n        Plot optimization results including convergence and final voltage traces\n\n        Parameters:\n        -----------\n        result : GapOptimizationResult\n            Results from optimization\n        \"\"\"\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n        # Plot voltage traces\n        t_range = [\n            self.tuner.general_settings[\"tstart\"] - 100.0,\n            self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"] + 100.0,\n        ]\n        t = np.array(self.tuner.t_vec)\n        v1 = np.array(self.tuner.soma_v_1)\n        v2 = np.array(self.tuner.soma_v_2)\n        tidx = (t &gt;= t_range[0]) &amp; (t &lt;= t_range[1])\n\n        ax1.plot(t[tidx], v1[tidx], \"b\", label=f\"{self.tuner.cell_name} 1\")\n        ax1.plot(t[tidx], v2[tidx], \"r\", label=f\"{self.tuner.cell_name} 2\")\n        ax1.set_xlabel(\"Time (ms)\")\n        ax1.set_ylabel(\"Membrane Voltage (mV)\")\n        ax1.legend()\n        ax1.set_title(\"Optimized Voltage Traces\")\n\n        # Plot error convergence\n        errors = [h[\"error\"] for h in result.optimization_path]\n        ax2.plot(errors)\n        ax2.set_xlabel(\"Iteration\")\n        ax2.set_ylabel(\"Error\")\n        ax2.set_title(\"Error Convergence\")\n        ax2.set_yscale(\"log\")\n\n        # Plot resistance convergence\n        resistances = [h[\"resistance\"] for h in result.optimization_path]\n        ax3.plot(resistances)\n        ax3.set_xlabel(\"Iteration\")\n        ax3.set_ylabel(\"Resistance\")\n        ax3.set_title(\"Resistance Convergence\")\n        ax3.set_yscale(\"log\")\n\n        # Print final results\n        result_text = (\n            f\"Optimal Resistance: {result.optimal_resistance:.2e}\\n\"\n            f\"Target CC: {result.target_cc:.3f}\\n\"\n            f\"Achieved CC: {result.achieved_cc:.3f}\\n\"\n            f\"Final Error: {result.error:.2e}\"\n        )\n        ax4.text(0.1, 0.7, result_text, transform=ax4.transAxes, fontsize=10)\n        ax4.axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n\n    def parameter_sweep(self, resistance_range: np.ndarray) -&gt; dict:\n        \"\"\"\n        Perform a parameter sweep across different resistance values.\n\n        Parameters:\n        -----------\n        resistance_range : np.ndarray\n            Array of resistance values to test.\n\n        Returns:\n        --------\n        dict\n            Dictionary containing the results of the parameter sweep, with keys:\n            - 'resistance': List of resistance values tested\n            - 'coupling_coefficient': Corresponding coupling coefficients\n\n        Notes:\n        ------\n        This method is useful for understanding the relationship between gap junction\n        resistance and coupling coefficient before attempting optimization.\n        \"\"\"\n        results = {\"resistance\": [], \"coupling_coefficient\": []}\n\n        for resistance in tqdm(resistance_range, desc=\"Sweeping resistance values\"):\n            self.tuner.model(resistance)\n            cc = self.tuner.coupling_coefficient(\n                self.tuner.t_vec,\n                self.tuner.soma_v_1,\n                self.tuner.soma_v_2,\n                self.tuner.general_settings[\"tstart\"],\n                self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n            )\n\n            results[\"resistance\"].append(resistance)\n            results[\"coupling_coefficient\"].append(cc)\n\n        return results\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer.__init__","title":"<code>__init__(tuner)</code>","text":"<p>Initialize the gap junction optimizer</p> Parameters: <p>tuner : GapJunctionTuner     Instance of the GapJunctionTuner class</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def __init__(self, tuner):\n    \"\"\"\n    Initialize the gap junction optimizer\n\n    Parameters:\n    -----------\n    tuner : GapJunctionTuner\n        Instance of the GapJunctionTuner class\n    \"\"\"\n    self.tuner = tuner\n    self.optimization_history = []\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer._objective_function","title":"<code>_objective_function(resistance, target_cc)</code>","text":"<p>Calculate error between achieved and target coupling coefficient</p> Parameters: <p>resistance : float     Gap junction resistance to try target_cc : float     Target coupling coefficient to match</p> Returns: <p>float : Error between achieved and target coupling coefficient</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _objective_function(self, resistance: float, target_cc: float) -&gt; float:\n    \"\"\"\n    Calculate error between achieved and target coupling coefficient\n\n    Parameters:\n    -----------\n    resistance : float\n        Gap junction resistance to try\n    target_cc : float\n        Target coupling coefficient to match\n\n    Returns:\n    --------\n    float : Error between achieved and target coupling coefficient\n    \"\"\"\n    # Run model with current resistance\n    self.tuner.model(resistance)\n\n    # Calculate coupling coefficient\n    achieved_cc = self.tuner.coupling_coefficient(\n        self.tuner.t_vec,\n        self.tuner.soma_v_1,\n        self.tuner.soma_v_2,\n        self.tuner.general_settings[\"tstart\"],\n        self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n    )\n\n    # Calculate error\n    error = (achieved_cc - target_cc) ** 2  # MSE\n\n    # Store history\n    self.optimization_history.append(\n        {\"resistance\": resistance, \"achieved_cc\": achieved_cc, \"error\": error}\n    )\n\n    return error\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer.optimize_resistance","title":"<code>optimize_resistance(target_cc, resistance_bounds=(0.0001, 0.01), method='bounded')</code>","text":"<p>Optimize gap junction resistance to achieve a target coupling coefficient.</p> Parameters: <p>target_cc : float     Target coupling coefficient to achieve (between 0 and 1) resistance_bounds : tuple, optional     (min, max) bounds for resistance search in MOhm. Default is (1e-4, 1e-2). method : str, optional     Optimization method to use. Default is 'bounded' which works well     for single-parameter optimization.</p> Returns: <p>GapOptimizationResult     Container with optimization results including:     - optimal_resistance: The optimized resistance value     - achieved_cc: The coupling coefficient achieved with the optimal resistance     - target_cc: The target coupling coefficient     - error: The final error (squared difference between target and achieved)     - optimization_path: List of all values tried during optimization</p> Notes: <p>Uses scipy.optimize.minimize_scalar with bounded method, which is appropriate for this single-parameter optimization problem.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def optimize_resistance(\n    self, target_cc: float, resistance_bounds: tuple = (1e-4, 1e-2), method: str = \"bounded\"\n) -&gt; GapOptimizationResult:\n    \"\"\"\n    Optimize gap junction resistance to achieve a target coupling coefficient.\n\n    Parameters:\n    -----------\n    target_cc : float\n        Target coupling coefficient to achieve (between 0 and 1)\n    resistance_bounds : tuple, optional\n        (min, max) bounds for resistance search in MOhm. Default is (1e-4, 1e-2).\n    method : str, optional\n        Optimization method to use. Default is 'bounded' which works well\n        for single-parameter optimization.\n\n    Returns:\n    --------\n    GapOptimizationResult\n        Container with optimization results including:\n        - optimal_resistance: The optimized resistance value\n        - achieved_cc: The coupling coefficient achieved with the optimal resistance\n        - target_cc: The target coupling coefficient\n        - error: The final error (squared difference between target and achieved)\n        - optimization_path: List of all values tried during optimization\n\n    Notes:\n    ------\n    Uses scipy.optimize.minimize_scalar with bounded method, which is\n    appropriate for this single-parameter optimization problem.\n    \"\"\"\n    self.optimization_history = []\n\n    # Run optimization\n    result = minimize_scalar(\n        self._objective_function, args=(target_cc,), bounds=resistance_bounds, method=method\n    )\n\n    # Run final model with optimal resistance\n    self.tuner.model(result.x)\n    final_cc = self.tuner.coupling_coefficient(\n        self.tuner.t_vec,\n        self.tuner.soma_v_1,\n        self.tuner.soma_v_2,\n        self.tuner.general_settings[\"tstart\"],\n        self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n    )\n\n    # Package up our results\n    optimization_result = GapOptimizationResult(\n        optimal_resistance=result.x,\n        achieved_cc=final_cc,\n        target_cc=target_cc,\n        error=result.fun,\n        optimization_path=self.optimization_history,\n    )\n\n    return optimization_result\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer.plot_optimization_results","title":"<code>plot_optimization_results(result)</code>","text":"<p>Plot optimization results including convergence and final voltage traces</p> Parameters: <p>result : GapOptimizationResult     Results from optimization</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def plot_optimization_results(self, result: GapOptimizationResult):\n    \"\"\"\n    Plot optimization results including convergence and final voltage traces\n\n    Parameters:\n    -----------\n    result : GapOptimizationResult\n        Results from optimization\n    \"\"\"\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n    # Plot voltage traces\n    t_range = [\n        self.tuner.general_settings[\"tstart\"] - 100.0,\n        self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"] + 100.0,\n    ]\n    t = np.array(self.tuner.t_vec)\n    v1 = np.array(self.tuner.soma_v_1)\n    v2 = np.array(self.tuner.soma_v_2)\n    tidx = (t &gt;= t_range[0]) &amp; (t &lt;= t_range[1])\n\n    ax1.plot(t[tidx], v1[tidx], \"b\", label=f\"{self.tuner.cell_name} 1\")\n    ax1.plot(t[tidx], v2[tidx], \"r\", label=f\"{self.tuner.cell_name} 2\")\n    ax1.set_xlabel(\"Time (ms)\")\n    ax1.set_ylabel(\"Membrane Voltage (mV)\")\n    ax1.legend()\n    ax1.set_title(\"Optimized Voltage Traces\")\n\n    # Plot error convergence\n    errors = [h[\"error\"] for h in result.optimization_path]\n    ax2.plot(errors)\n    ax2.set_xlabel(\"Iteration\")\n    ax2.set_ylabel(\"Error\")\n    ax2.set_title(\"Error Convergence\")\n    ax2.set_yscale(\"log\")\n\n    # Plot resistance convergence\n    resistances = [h[\"resistance\"] for h in result.optimization_path]\n    ax3.plot(resistances)\n    ax3.set_xlabel(\"Iteration\")\n    ax3.set_ylabel(\"Resistance\")\n    ax3.set_title(\"Resistance Convergence\")\n    ax3.set_yscale(\"log\")\n\n    # Print final results\n    result_text = (\n        f\"Optimal Resistance: {result.optimal_resistance:.2e}\\n\"\n        f\"Target CC: {result.target_cc:.3f}\\n\"\n        f\"Achieved CC: {result.achieved_cc:.3f}\\n\"\n        f\"Final Error: {result.error:.2e}\"\n    )\n    ax4.text(0.1, 0.7, result_text, transform=ax4.transAxes, fontsize=10)\n    ax4.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer.parameter_sweep","title":"<code>parameter_sweep(resistance_range)</code>","text":"<p>Perform a parameter sweep across different resistance values.</p> Parameters: <p>resistance_range : np.ndarray     Array of resistance values to test.</p> Returns: <p>dict     Dictionary containing the results of the parameter sweep, with keys:     - 'resistance': List of resistance values tested     - 'coupling_coefficient': Corresponding coupling coefficients</p> Notes: <p>This method is useful for understanding the relationship between gap junction resistance and coupling coefficient before attempting optimization.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def parameter_sweep(self, resistance_range: np.ndarray) -&gt; dict:\n    \"\"\"\n    Perform a parameter sweep across different resistance values.\n\n    Parameters:\n    -----------\n    resistance_range : np.ndarray\n        Array of resistance values to test.\n\n    Returns:\n    --------\n    dict\n        Dictionary containing the results of the parameter sweep, with keys:\n        - 'resistance': List of resistance values tested\n        - 'coupling_coefficient': Corresponding coupling coefficients\n\n    Notes:\n    ------\n    This method is useful for understanding the relationship between gap junction\n    resistance and coupling coefficient before attempting optimization.\n    \"\"\"\n    results = {\"resistance\": [], \"coupling_coefficient\": []}\n\n    for resistance in tqdm(resistance_range, desc=\"Sweeping resistance values\"):\n        self.tuner.model(resistance)\n        cc = self.tuner.coupling_coefficient(\n            self.tuner.t_vec,\n            self.tuner.soma_v_1,\n            self.tuner.soma_v_2,\n            self.tuner.general_settings[\"tstart\"],\n            self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n        )\n\n        results[\"resistance\"].append(resistance)\n        results[\"coupling_coefficient\"].append(cc)\n\n    return results\n</code></pre>"},{"location":"api/analysis/entrainment/","title":"Entrainment Analysis","text":"<p>The <code>entrainment</code> module provides tools for analyzing the entrainment of spikes and LFP signals, including phase-locking value (PLV) and pairwise phase consistency (PPC) calculations.</p>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.align_spike_times_with_lfp","title":"<code>bmtool.analysis.entrainment.align_spike_times_with_lfp(lfp, timestamps)</code>","text":"<p>the lfp xarray should have a time axis. use that to align the spike times since the lfp can start at a non-zero time after sliced. Both need to be on same fs for this to be correct.</p> <p>Parameters:</p> Name Type Description Default <code>lfp</code> <code>DataArray</code> <p>LFP data with time coordinates</p> required <code>timestamps</code> <code>ndarray</code> <p>Array of spike timestamps</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Copy of timestamps with adjusted timestamps to align with lfp.</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def align_spike_times_with_lfp(lfp: xr.DataArray, timestamps: np.ndarray) -&gt; np.ndarray:\n    \"\"\"the lfp xarray should have a time axis. use that to align the spike times since the lfp can start at a\n    non-zero time after sliced. Both need to be on same fs for this to be correct.\n\n    Parameters\n    ----------\n    lfp : xarray.DataArray\n        LFP data with time coordinates\n    timestamps : np.ndarray\n        Array of spike timestamps\n\n    Returns\n    -------\n    np.ndarray\n        Copy of timestamps with adjusted timestamps to align with lfp.\n    \"\"\"\n    # print(\"Pairing LFP and Spike Times\")\n    # print(lfp.time.values)\n    # print(f\"LFP starts at {lfp.time.values[0]}ms\")\n    # need to make sure lfp and spikes have the same time axis\n    # align spikes with lfp\n    timestamps = timestamps[\n        (timestamps &gt;= lfp.time.values[0]) &amp; (timestamps &lt;= lfp.time.values[-1])\n    ].copy()\n    # set the time axis of the spikes to match the lfp\n    # timestamps = timestamps - lfp.time.values[0]\n    return timestamps\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_signal_signal_plv","title":"<code>bmtool.analysis.entrainment.calculate_signal_signal_plv(signal1, signal2, fs, freq_of_interest=None, filter_method='wavelet', lowcut=None, highcut=None, bandwidth=2.0)</code>","text":"<p>Calculate Phase Locking Value (PLV) between two signals using wavelet or Hilbert method.</p> <p>Parameters:</p> Name Type Description Default <code>signal1</code> <code>ndarray</code> <p>First input signal (1D array)</p> required <code>signal2</code> <code>ndarray</code> <p>Second input signal (1D array, same length as signal1)</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz</p> required <code>freq_of_interest</code> <code>float</code> <p>Desired frequency for wavelet PLV calculation, required if filter_method='wavelet'</p> <code>None</code> <code>filter_method</code> <code>str</code> <p>Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')</p> <code>'wavelet'</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Phase Locking Value (1D array)</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def calculate_signal_signal_plv(\n    signal1: np.ndarray,\n    signal2: np.ndarray,\n    fs: float,\n    freq_of_interest: float = None,\n    filter_method: str = \"wavelet\",\n    lowcut: float = None,\n    highcut: float = None,\n    bandwidth: float = 2.0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculate Phase Locking Value (PLV) between two signals using wavelet or Hilbert method.\n\n    Parameters\n    ----------\n    signal1 : np.ndarray\n        First input signal (1D array)\n    signal2 : np.ndarray\n        Second input signal (1D array, same length as signal1)\n    fs : float\n        Sampling frequency in Hz\n    freq_of_interest : float, optional\n        Desired frequency for wavelet PLV calculation, required if filter_method='wavelet'\n    filter_method : str, optional\n        Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n\n    Returns\n    -------\n    np.ndarray\n        Phase Locking Value (1D array)\n    \"\"\"\n    if len(signal1) != len(signal2):\n        raise ValueError(\"Input signals must have the same length.\")\n\n    if filter_method == \"wavelet\":\n        if freq_of_interest is None:\n            raise ValueError(\"freq_of_interest must be provided for the wavelet method.\")\n\n        # Apply CWT to both signals\n        theta1 = wavelet_filter(x=signal1, freq=freq_of_interest, fs=fs, bandwidth=bandwidth)\n        theta2 = wavelet_filter(x=signal2, freq=freq_of_interest, fs=fs, bandwidth=bandwidth)\n\n    elif filter_method == \"butter\":\n        if lowcut is None or highcut is None:\n            print(\n                \"Lowcut and/or highcut were not defined, signal will not be filtered and will just take Hilbert transform for PLV calculation\"\n            )\n\n        if lowcut and highcut:\n            # Bandpass filter and get the analytic signal using the Hilbert transform\n            filtered_signal1 = butter_bandpass_filter(\n                data=signal1, lowcut=lowcut, highcut=highcut, fs=fs\n            )\n            filtered_signal2 = butter_bandpass_filter(\n                data=signal2, lowcut=lowcut, highcut=highcut, fs=fs\n            )\n            # Get phase using the Hilbert transform\n            theta1 = signal.hilbert(filtered_signal1)\n            theta2 = signal.hilbert(filtered_signal2)\n        else:\n            # Get phase using the Hilbert transform without filtering\n            theta1 = signal.hilbert(signal1)\n            theta2 = signal.hilbert(signal2)\n\n    else:\n        raise ValueError(\"Invalid method. Choose 'wavelet' or 'butter'.\")\n\n    # Calculate phase difference\n    phase_diff = np.angle(theta1) - np.angle(theta2)\n\n    # Calculate PLV from standard equation from Measuring phase synchrony in brain signals(1999)\n    plv = np.abs(np.mean(np.exp(1j * phase_diff), axis=-1))\n\n    return plv\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_spike_lfp_plv","title":"<code>bmtool.analysis.entrainment.calculate_spike_lfp_plv(spike_times=None, lfp_data=None, spike_fs=None, lfp_fs=None, filter_method='butter', freq_of_interest=None, lowcut=None, highcut=None, bandwidth=2.0, filtered_lfp_phase=None)</code>","text":"<p>Calculate spike-lfp unbiased phase locking value</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>ndarray</code> <p>Array of spike times</p> <code>None</code> <code>lfp_data</code> <code>ndarray</code> <p>Local field potential time series data. Not required if filtered_lfp_phase is provided.</p> <code>None</code> <code>spike_fs</code> <code>float</code> <p>Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates</p> <code>None</code> <code>lfp_fs</code> <code>float</code> <p>Sampling frequency in Hz of the LFP data</p> <code>None</code> <code>filter_method</code> <code>str</code> <p>Method to use for filtering, either 'wavelet' or 'butter' (default: 'butter')</p> <code>'butter'</code> <code>freq_of_interest</code> <code>float</code> <p>Desired frequency for wavelet phase extraction, required if filter_method='wavelet'</p> <code>None</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2.0</code> <code>filtered_lfp_phase</code> <code>ndarray</code> <p>Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Phase Locking Value (unbiased)</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def calculate_spike_lfp_plv(\n    spike_times: np.ndarray = None,\n    lfp_data=None,\n    spike_fs: float = None,\n    lfp_fs: float = None,\n    filter_method: str = \"butter\",\n    freq_of_interest: float = None,\n    lowcut: float = None,\n    highcut: float = None,\n    bandwidth: float = 2.0,\n    filtered_lfp_phase: np.ndarray = None,\n) -&gt; float:\n    \"\"\"\n    Calculate spike-lfp unbiased phase locking value\n\n    Parameters\n    ----------\n    spike_times : np.ndarray\n        Array of spike times\n    lfp_data : np.ndarray\n        Local field potential time series data. Not required if filtered_lfp_phase is provided.\n    spike_fs : float, optional\n        Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates\n    lfp_fs : float\n        Sampling frequency in Hz of the LFP data\n    filter_method : str, optional\n        Method to use for filtering, either 'wavelet' or 'butter' (default: 'butter')\n    freq_of_interest : float, optional\n        Desired frequency for wavelet phase extraction, required if filter_method='wavelet'\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n    filtered_lfp_phase : np.ndarray, optional\n        Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.\n\n    Returns\n    -------\n    float\n        Phase Locking Value (unbiased)\n    \"\"\"\n\n    spike_phases = _get_spike_phases(\n        spike_times=spike_times,\n        lfp_data=lfp_data,\n        spike_fs=spike_fs,\n        lfp_fs=lfp_fs,\n        filter_method=filter_method,\n        freq_of_interest=freq_of_interest,\n        lowcut=lowcut,\n        highcut=highcut,\n        bandwidth=bandwidth,\n        filtered_lfp_phase=filtered_lfp_phase,\n    )\n\n    if len(spike_phases) &lt;= 1:\n        return 0\n\n    # Number of spikes\n    N = len(spike_phases)\n\n    # Convert phases to unit vectors in the complex plane\n    unit_vectors = np.exp(1j * spike_phases)\n\n    # Sum of all unit vectors (resultant vector)\n    resultant_vector = np.sum(unit_vectors)\n\n    # Calculate plv^2 * N\n    plv2n = (resultant_vector * resultant_vector.conjugate()).real / N  # plv^2 * N\n    plv = (plv2n / N) ** 0.5\n    ppc = (plv2n - 1) / (N - 1)  # ppc = (plv^2 * N - 1) / (N - 1)\n    plv_unbiased = np.fmax(ppc, 0.0) ** 0.5  # ensure non-negative\n\n    return plv_unbiased\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc","title":"<code>bmtool.analysis.entrainment.calculate_ppc(spike_times=None, lfp_data=None, spike_fs=None, lfp_fs=None, filter_method='wavelet', freq_of_interest=None, lowcut=None, highcut=None, bandwidth=2.0, ppc_method='numpy', filtered_lfp_phase=None)</code>","text":"<p>Calculate Pairwise Phase Consistency (PPC) between spike times and LFP signal. Based on https://www.sciencedirect.com/science/article/pii/S1053811910000959</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>ndarray</code> <p>Array of spike times</p> <code>None</code> <code>lfp_data</code> <code>ndarray</code> <p>Local field potential time series data. Not required if filtered_lfp_phase is provided.</p> <code>None</code> <code>spike_fs</code> <code>float</code> <p>Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates</p> <code>None</code> <code>lfp_fs</code> <code>float</code> <p>Sampling frequency in Hz of the LFP data</p> <code>None</code> <code>filter_method</code> <code>str</code> <p>Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')</p> <code>'wavelet'</code> <code>freq_of_interest</code> <code>float</code> <p>Desired frequency for wavelet phase extraction, required if filter_method='wavelet'</p> <code>None</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2.0</code> <code>ppc_method</code> <code>str</code> <p>Algorithm to use for PPC calculation: 'numpy', 'numba', or 'gpu' (default: 'numpy')</p> <code>'numpy'</code> <code>filtered_lfp_phase</code> <code>ndarray</code> <p>Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Pairwise Phase Consistency value</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def calculate_ppc(\n    spike_times: np.ndarray = None,\n    lfp_data=None,\n    spike_fs: float = None,\n    lfp_fs: float = None,\n    filter_method: str = \"wavelet\",\n    freq_of_interest: float = None,\n    lowcut: float = None,\n    highcut: float = None,\n    bandwidth: float = 2.0,\n    ppc_method: str = \"numpy\",\n    filtered_lfp_phase: np.ndarray = None,\n) -&gt; float:\n    \"\"\"\n    Calculate Pairwise Phase Consistency (PPC) between spike times and LFP signal.\n    Based on https://www.sciencedirect.com/science/article/pii/S1053811910000959\n\n    Parameters\n    ----------\n    spike_times : np.ndarray\n        Array of spike times\n    lfp_data : np.ndarray\n        Local field potential time series data. Not required if filtered_lfp_phase is provided.\n    spike_fs : float, optional\n        Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates\n    lfp_fs : float\n        Sampling frequency in Hz of the LFP data\n    filter_method : str, optional\n        Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')\n    freq_of_interest : float, optional\n        Desired frequency for wavelet phase extraction, required if filter_method='wavelet'\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n    ppc_method : str, optional\n        Algorithm to use for PPC calculation: 'numpy', 'numba', or 'gpu' (default: 'numpy')\n    filtered_lfp_phase : np.ndarray, optional\n        Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.\n\n    Returns\n    -------\n    float\n        Pairwise Phase Consistency value\n    \"\"\"\n\n    spike_phases = _get_spike_phases(\n        spike_times=spike_times,\n        lfp_data=lfp_data,\n        spike_fs=spike_fs,\n        lfp_fs=lfp_fs,\n        filter_method=filter_method,\n        freq_of_interest=freq_of_interest,\n        lowcut=lowcut,\n        highcut=highcut,\n        bandwidth=bandwidth,\n        filtered_lfp_phase=filtered_lfp_phase,\n    )\n\n    if len(spike_phases) &lt;= 1:\n        return 0\n\n    n_spikes = len(spike_phases)\n\n    # Calculate PPC (Pairwise Phase Consistency)\n    # Explicit calculation of pairwise phase consistency\n    # Vectorized computation for efficiency\n    if ppc_method == \"numpy\":\n        i, j = np.triu_indices(n_spikes, k=1)\n        phase_diff = spike_phases[i] - spike_phases[j]\n        sum_cos_diff = np.sum(np.cos(phase_diff))\n        ppc = (2 / (n_spikes * (n_spikes - 1))) * sum_cos_diff\n    elif ppc_method == \"numba\":\n        ppc = _ppc_parallel_numba(spike_phases)\n    elif ppc_method == \"gpu\":\n        ppc = _ppc_gpu(spike_phases)\n    else:\n        raise ValueError(\"Please use a supported ppc method currently that is numpy, numba or gpu\")\n    return ppc\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2","title":"<code>bmtool.analysis.entrainment.calculate_ppc2(spike_times=None, lfp_data=None, spike_fs=None, lfp_fs=None, filter_method='wavelet', freq_of_interest=None, lowcut=None, highcut=None, bandwidth=2.0, filtered_lfp_phase=None)</code>","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2---","title":"-----------------------------------------------------------------------------","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2--ppc2-calculation-vinck-et-al-2010","title":"PPC2 Calculation (Vinck et al., 2010)","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2---_1","title":"-----------------------------------------------------------------------------","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2--equationoriginal","title":"Equation(Original):","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2--ppc-2-n-n-1-sumcos_i-_j-for-all-i-j","title":"PPC = (2 / (n * (n - 1))) * sum(cos(\u03c6_i - \u03c6_j) for all i &lt; j)","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2--optimized-formula-algebraically-equivalent","title":"Optimized Formula (Algebraically Equivalent):","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2--ppc-sumei_j2-n-n-n-1","title":"PPC = (|sum(e^(i*\u03c6_j))|^2 - n) / (n * (n - 1))","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2---_2","title":"-----------------------------------------------------------------------------","text":"<p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>ndarray</code> <p>Array of spike times</p> <code>None</code> <code>lfp_data</code> <code>ndarray</code> <p>Local field potential time series data. Not required if filtered_lfp_phase is provided.</p> <code>None</code> <code>spike_fs</code> <code>float</code> <p>Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates</p> <code>None</code> <code>lfp_fs</code> <code>float</code> <p>Sampling frequency in Hz of the LFP data</p> <code>None</code> <code>filter_method</code> <code>str</code> <p>Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')</p> <code>'wavelet'</code> <code>freq_of_interest</code> <code>float</code> <p>Desired frequency for wavelet phase extraction, required if filter_method='wavelet'</p> <code>None</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2.0</code> <code>filtered_lfp_phase</code> <code>ndarray</code> <p>Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Pairwise Phase Consistency 2 (PPC2) value</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def calculate_ppc2(\n    spike_times: np.ndarray = None,\n    lfp_data=None,\n    spike_fs: float = None,\n    lfp_fs: float = None,\n    filter_method: str = \"wavelet\",\n    freq_of_interest: float = None,\n    lowcut: float = None,\n    highcut: float = None,\n    bandwidth: float = 2.0,\n    filtered_lfp_phase: np.ndarray = None,\n) -&gt; float:\n    \"\"\"\n    # -----------------------------------------------------------------------------\n    # PPC2 Calculation (Vinck et al., 2010)\n    # -----------------------------------------------------------------------------\n    # Equation(Original):\n    #   PPC = (2 / (n * (n - 1))) * sum(cos(\u03c6_i - \u03c6_j) for all i &lt; j)\n    # Optimized Formula (Algebraically Equivalent):\n    #   PPC = (|sum(e^(i*\u03c6_j))|^2 - n) / (n * (n - 1))\n    # -----------------------------------------------------------------------------\n\n    Parameters\n    ----------\n    spike_times : np.ndarray\n        Array of spike times\n    lfp_data : np.ndarray\n        Local field potential time series data. Not required if filtered_lfp_phase is provided.\n    spike_fs : float, optional\n        Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates\n    lfp_fs : float\n        Sampling frequency in Hz of the LFP data\n    filter_method : str, optional\n        Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')\n    freq_of_interest : float, optional\n        Desired frequency for wavelet phase extraction, required if filter_method='wavelet'\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n    filtered_lfp_phase : np.ndarray, optional\n        Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.\n\n    Returns\n    -------\n    float\n        Pairwise Phase Consistency 2 (PPC2) value\n    \"\"\"\n\n    spike_phases = _get_spike_phases(\n        spike_times=spike_times,\n        lfp_data=lfp_data,\n        spike_fs=spike_fs,\n        lfp_fs=lfp_fs,\n        filter_method=filter_method,\n        freq_of_interest=freq_of_interest,\n        lowcut=lowcut,\n        highcut=highcut,\n        bandwidth=bandwidth,\n        filtered_lfp_phase=filtered_lfp_phase,\n    )\n\n    if len(spike_phases) &lt;= 1:\n        return 0\n\n    # Calculate PPC2 according to Vinck et al. (2010), Equation 6\n    n = len(spike_phases)\n\n    # Convert phases to unit vectors in the complex plane\n    unit_vectors = np.exp(1j * spike_phases)\n\n    # Calculate the resultant vector\n    resultant_vector = np.sum(unit_vectors)\n\n    # PPC2 = (|\u2211(e^(i*\u03c6_j))|\u00b2 - n) / (n * (n - 1))\n    ppc2 = (np.abs(resultant_vector) ** 2 - n) / (n * (n - 1))\n\n    return ppc2\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_entrainment_per_cell","title":"<code>bmtool.analysis.entrainment.calculate_entrainment_per_cell(spike_df=None, lfp_data=None, filter_method='wavelet', pop_names=None, entrainment_method='plv', lowcut=None, highcut=None, spike_fs=None, lfp_fs=None, bandwidth=2, freqs=None, ppc_method='numpy')</code>","text":"<p>Calculate neural entrainment (PPC, PLV) per neuron (cell) for specified frequencies across different populations.</p> <p>This function computes the entrainment metrics for each neuron within the specified populations based on their spike times and the provided LFP signal. It returns a nested dictionary structure containing the entrainment values organized by population, node ID, and frequency.</p> <p>Parameters:</p> Name Type Description Default <code>spike_df</code> <code>DataFrame</code> <p>DataFrame containing spike data with columns 'pop_name', 'node_ids', and 'timestamps'</p> <code>None</code> <code>lfp_data</code> <code>ndarray</code> <p>Local field potential (LFP) time series data</p> <code>None</code> <code>filter_method</code> <code>str</code> <p>Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')</p> <code>'wavelet'</code> <code>entrainment_method</code> <code>str</code> <p>Method to use for entrainment calculation, either 'plv', 'ppc', or 'ppc2' (default: 'plv')</p> <code>'plv'</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>spike_fs</code> <code>float</code> <p>Sampling frequency of the spike times in Hz</p> <code>None</code> <code>lfp_fs</code> <code>float</code> <p>Sampling frequency of the LFP signal in Hz</p> <code>None</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2</code> <code>ppc_method</code> <code>str</code> <p>Algorithm to use for PPC calculation: 'numpy', 'numba', or 'gpu' (default: 'numpy')</p> <code>'numpy'</code> <code>pop_names</code> <code>List[str]</code> <p>List of population names to analyze</p> <code>None</code> <code>freqs</code> <code>List[float]</code> <p>List of frequencies (in Hz) at which to calculate entrainment</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[int, Dict[float, float]]]</code> <p>Nested dictionary where the structure is: {     population_name: {         node_id: {             frequency: entrainment value         }     } } Entrainment values are floats representing the metric (PPC, PLV) at each frequency</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def calculate_entrainment_per_cell(\n    spike_df: pd.DataFrame = None,\n    lfp_data: np.ndarray = None,\n    filter_method: str = \"wavelet\",\n    pop_names: List[str] = None,\n    entrainment_method: str = \"plv\",\n    lowcut: float = None,\n    highcut: float = None,\n    spike_fs: float = None,\n    lfp_fs: float = None,\n    bandwidth: float = 2,\n    freqs: List[float] = None,\n    ppc_method: str = \"numpy\",\n) -&gt; Dict[str, Dict[int, Dict[float, float]]]:\n    \"\"\"\n    Calculate neural entrainment (PPC, PLV) per neuron (cell) for specified frequencies across different populations.\n\n    This function computes the entrainment metrics for each neuron within the specified populations based on their spike times\n    and the provided LFP signal. It returns a nested dictionary structure containing the entrainment values\n    organized by population, node ID, and frequency.\n\n    Parameters\n    ----------\n    spike_df : pd.DataFrame\n        DataFrame containing spike data with columns 'pop_name', 'node_ids', and 'timestamps'\n    lfp_data : np.ndarray\n        Local field potential (LFP) time series data\n    filter_method : str, optional\n        Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')\n    entrainment_method : str, optional\n        Method to use for entrainment calculation, either 'plv', 'ppc', or 'ppc2' (default: 'plv')\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    spike_fs : float\n        Sampling frequency of the spike times in Hz\n    lfp_fs : float\n        Sampling frequency of the LFP signal in Hz\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n    ppc_method : str, optional\n        Algorithm to use for PPC calculation: 'numpy', 'numba', or 'gpu' (default: 'numpy')\n    pop_names : List[str]\n        List of population names to analyze\n    freqs : List[float]\n        List of frequencies (in Hz) at which to calculate entrainment\n\n    Returns\n    -------\n    Dict[str, Dict[int, Dict[float, float]]]\n        Nested dictionary where the structure is:\n        {\n            population_name: {\n                node_id: {\n                    frequency: entrainment value\n                }\n            }\n        }\n        Entrainment values are floats representing the metric (PPC, PLV) at each frequency\n    \"\"\"\n    # pre filter lfp to speed up calculate of entrainment\n    filtered_lfp_phases = {}\n    for freq in range(len(freqs)):\n        phase = get_lfp_phase(\n            lfp_data=lfp_data,\n            freq_of_interest=freqs[freq],\n            fs=lfp_fs,\n            filter_method=filter_method,\n            lowcut=lowcut,\n            highcut=highcut,\n            bandwidth=bandwidth,\n        )\n        filtered_lfp_phases[freqs[freq]] = phase\n\n    entrainment_dict = {}\n    for pop in pop_names:\n        skip_count = 0\n        pop_spikes = spike_df[spike_df[\"pop_name\"] == pop]\n        nodes = sorted(pop_spikes[\"node_ids\"].unique())  # sort so all nodes are processed in order\n        entrainment_dict[pop] = {}\n        print(f\"Processing {pop} population\")\n        for node in tqdm(nodes):\n            node_spikes = pop_spikes[pop_spikes[\"node_ids\"] == node]\n\n            # Skip nodes with less than or equal to 1 spike\n            if len(node_spikes) &lt;= 1:\n                skip_count += 1\n                continue\n\n            entrainment_dict[pop][node] = {}\n            for freq in freqs:\n                # Calculate entrainment based on the selected method using the pre-filtered phases\n                if entrainment_method == \"plv\":\n                    entrainment_dict[pop][node][freq] = calculate_spike_lfp_plv(\n                        node_spikes[\"timestamps\"].values,\n                        lfp_data,\n                        spike_fs=spike_fs,\n                        lfp_fs=lfp_fs,\n                        freq_of_interest=freq,\n                        bandwidth=bandwidth,\n                        lowcut=lowcut,\n                        highcut=highcut,\n                        filter_method=filter_method,\n                        filtered_lfp_phase=filtered_lfp_phases[freq],\n                    )\n                elif entrainment_method == \"ppc2\":\n                    entrainment_dict[pop][node][freq] = calculate_ppc2(\n                        node_spikes[\"timestamps\"].values,\n                        lfp_data,\n                        spike_fs=spike_fs,\n                        lfp_fs=lfp_fs,\n                        freq_of_interest=freq,\n                        bandwidth=bandwidth,\n                        lowcut=lowcut,\n                        highcut=highcut,\n                        filter_method=filter_method,\n                        filtered_lfp_phase=filtered_lfp_phases[freq],\n                    )\n                elif entrainment_method == \"ppc\":\n                    entrainment_dict[pop][node][freq] = calculate_ppc(\n                        node_spikes[\"timestamps\"].values,\n                        lfp_data,\n                        spike_fs=spike_fs,\n                        lfp_fs=lfp_fs,\n                        freq_of_interest=freq,\n                        bandwidth=bandwidth,\n                        lowcut=lowcut,\n                        highcut=highcut,\n                        filter_method=filter_method,\n                        ppc_method=ppc_method,\n                        filtered_lfp_phase=filtered_lfp_phases[freq],\n                    )\n\n        print(\n            f\"Calculated {entrainment_method.upper()} for {pop} population with {len(nodes)-skip_count} valid cells, skipped {skip_count} cells for lack of spikes\"\n        )\n\n    return entrainment_dict\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.get_spikes_in_cycle","title":"<code>bmtool.analysis.entrainment.get_spikes_in_cycle(spike_df, lfp_data, spike_fs=1000, lfp_fs=400, filter_method='butter', lowcut=None, highcut=None, bandwidth=2.0, freq_of_interest=None)</code>","text":"<p>Analyze spike timing relative to oscillation phases.</p> Parameters: <p>spike_df : pd.DataFrame lfp_data : np.array     Raw LFP signal fs : float     Sampling frequency of LFP in Hz gamma_band : tuple     Lower and upper bounds of gamma frequency band in Hz</p> Returns: <p>phase_data : dict     Dictionary containing phase values for each spike and neuron population</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def get_spikes_in_cycle(\n    spike_df,\n    lfp_data,\n    spike_fs=1000,\n    lfp_fs=400,\n    filter_method=\"butter\",\n    lowcut=None,\n    highcut=None,\n    bandwidth=2.0,\n    freq_of_interest=None,\n):\n    \"\"\"\n    Analyze spike timing relative to oscillation phases.\n\n    Parameters:\n    -----------\n    spike_df : pd.DataFrame\n    lfp_data : np.array\n        Raw LFP signal\n    fs : float\n        Sampling frequency of LFP in Hz\n    gamma_band : tuple\n        Lower and upper bounds of gamma frequency band in Hz\n\n    Returns:\n    --------\n    phase_data : dict\n        Dictionary containing phase values for each spike and neuron population\n    \"\"\"\n    phase = get_lfp_phase(\n        lfp_data=lfp_data,\n        fs=lfp_fs,\n        filter_method=filter_method,\n        lowcut=lowcut,\n        highcut=highcut,\n        bandwidth=bandwidth,\n        freq_of_interest=freq_of_interest,\n    )\n\n    # Get unique neuron populations\n    neuron_pops = spike_df[\"pop_name\"].unique()\n\n    # Get the phase at each spike time for each neuron population\n    phase_data = {}\n\n    for pop in neuron_pops:\n        # Get spike times for this population\n        pop_spikes = spike_df[spike_df[\"pop_name\"] == pop][\"timestamps\"].values\n\n        # Convert spike times to sample indices\n        spike_times_seconds = pop_spikes / spike_fs\n\n        # Then convert from seconds to samples at the new sampling rate\n        spike_indices = np.round(spike_times_seconds * lfp_fs).astype(int)\n\n        # Ensure spike times are within LFP data range\n        valid_indices = (spike_indices &gt;= 0) &amp; (spike_indices &lt; len(phase))\n\n        if np.any(valid_indices):\n            valid_samples = spike_indices[valid_indices]\n            phase_data[pop] = phase[valid_samples]\n\n    return phase_data\n</code></pre>"},{"location":"api/analysis/lfp/","title":"LFP/ECP Analysis","text":"<p>The <code>lfp</code> module provides tools for analyzing local field potentials (LFP) and extracellular potentials (ECP).</p>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.load_ecp_to_xarray","title":"<code>bmtool.analysis.lfp.load_ecp_to_xarray(ecp_file, demean=False)</code>","text":"<p>Load ECP data from an HDF5 file (BMTK sim) into an xarray DataArray.</p> Parameters: <p>ecp_file : str     Path to the HDF5 file containing ECP data. demean : bool, optional     If True, the mean of the data will be subtracted (default is False).</p> Returns: <p>xr.DataArray     An xarray DataArray containing the ECP data, with time as one dimension     and channel_id as another.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def load_ecp_to_xarray(ecp_file: str, demean: bool = False) -&gt; xr.DataArray:\n    \"\"\"\n    Load ECP data from an HDF5 file (BMTK sim) into an xarray DataArray.\n\n    Parameters:\n    ----------\n    ecp_file : str\n        Path to the HDF5 file containing ECP data.\n    demean : bool, optional\n        If True, the mean of the data will be subtracted (default is False).\n\n    Returns:\n    -------\n    xr.DataArray\n        An xarray DataArray containing the ECP data, with time as one dimension\n        and channel_id as another.\n    \"\"\"\n    with h5py.File(ecp_file, \"r\") as f:\n        ecp = xr.DataArray(\n            f[\"ecp\"][\"data\"][()].T,\n            coords=dict(\n                channel_id=f[\"ecp\"][\"channel_id\"][()],\n                time=np.arange(*f[\"ecp\"][\"time\"]),  # ms\n            ),\n            attrs=dict(\n                fs=1000 / f[\"ecp\"][\"time\"][2]  # Hz\n            ),\n        )\n    if demean:\n        ecp -= ecp.mean(dim=\"time\")\n    return ecp\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.ecp_to_lfp","title":"<code>bmtool.analysis.lfp.ecp_to_lfp(ecp_data, cutoff=250, fs=10000, downsample_freq=1000)</code>","text":"<p>Apply a low-pass Butterworth filter to an xarray DataArray and optionally downsample. This filters out the high end frequencies turning the ECP into a LFP</p> Parameters: <p>ecp_data : xr.DataArray     The input data array containing LFP data with time as one dimension. cutoff : float     The cutoff frequency for the low-pass filter in Hz (default is 250Hz). fs : float, optional     The sampling frequency of the data (default is 10000 Hz). downsample_freq : float, optional     The frequency to downsample to (default is 1000 Hz).</p> Returns: <p>xr.DataArray     The filtered (and possibly downsampled) data as an xarray DataArray.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def ecp_to_lfp(\n    ecp_data: xr.DataArray, cutoff: float = 250, fs: float = 10000, downsample_freq: float = 1000\n) -&gt; xr.DataArray:\n    \"\"\"\n    Apply a low-pass Butterworth filter to an xarray DataArray and optionally downsample.\n    This filters out the high end frequencies turning the ECP into a LFP\n\n    Parameters:\n    ----------\n    ecp_data : xr.DataArray\n        The input data array containing LFP data with time as one dimension.\n    cutoff : float\n        The cutoff frequency for the low-pass filter in Hz (default is 250Hz).\n    fs : float, optional\n        The sampling frequency of the data (default is 10000 Hz).\n    downsample_freq : float, optional\n        The frequency to downsample to (default is 1000 Hz).\n\n    Returns:\n    -------\n    xr.DataArray\n        The filtered (and possibly downsampled) data as an xarray DataArray.\n    \"\"\"\n    # Bandpass filter design\n    nyq = 0.5 * fs\n    cut = cutoff / nyq\n    b, a = signal.butter(8, cut, btype=\"low\", analog=False)\n\n    # Initialize an array to hold filtered data\n    filtered_data = xr.DataArray(\n        np.zeros_like(ecp_data), coords=ecp_data.coords, dims=ecp_data.dims\n    )\n\n    # Apply the filter to each channel\n    for channel in ecp_data.channel_id:\n        filtered_data.loc[channel, :] = signal.filtfilt(\n            b, a, ecp_data.sel(channel_id=channel).values\n        )\n\n    # Downsample the filtered data if a downsample frequency is provided\n    if downsample_freq is not None:\n        downsample_factor = int(fs / downsample_freq)\n        filtered_data = filtered_data.isel(time=slice(None, None, downsample_factor))\n        # Update the sampling frequency attribute\n        filtered_data.attrs[\"fs\"] = downsample_freq\n\n    return filtered_data\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.slice_time_series","title":"<code>bmtool.analysis.lfp.slice_time_series(data, time_ranges)</code>","text":"<p>Slice the xarray DataArray based on provided time ranges. Can be used to get LFP during certain stimulus times</p> Parameters: <p>data : xr.DataArray     The input xarray DataArray containing time-series data. time_ranges : tuple or list of tuples     One or more tuples representing the (start, stop) time points for slicing.     For example: (start, stop) or [(start1, stop1), (start2, stop2)]</p> Returns: <p>xr.DataArray     A new xarray DataArray containing the concatenated slices.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def slice_time_series(data: xr.DataArray, time_ranges: tuple) -&gt; xr.DataArray:\n    \"\"\"\n    Slice the xarray DataArray based on provided time ranges.\n    Can be used to get LFP during certain stimulus times\n\n    Parameters:\n    ----------\n    data : xr.DataArray\n        The input xarray DataArray containing time-series data.\n    time_ranges : tuple or list of tuples\n        One or more tuples representing the (start, stop) time points for slicing.\n        For example: (start, stop) or [(start1, stop1), (start2, stop2)]\n\n    Returns:\n    -------\n    xr.DataArray\n        A new xarray DataArray containing the concatenated slices.\n    \"\"\"\n    # Ensure time_ranges is a list of tuples\n    if isinstance(time_ranges, tuple) and len(time_ranges) == 2:\n        time_ranges = [time_ranges]\n\n    # List to hold sliced data\n    slices = []\n\n    # Slice the data for each time range\n    for start, stop in time_ranges:\n        sliced_data = data.sel(time=slice(start, stop))\n        slices.append(sliced_data)\n\n    # Concatenate all slices along the time dimension if more than one slice\n    if len(slices) &gt; 1:\n        return xr.concat(slices, dim=\"time\")\n    else:\n        return slices[0]\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.fit_fooof","title":"<code>bmtool.analysis.lfp.fit_fooof(f, pxx, aperiodic_mode='fixed', dB_threshold=3.0, max_n_peaks=10, freq_range=None, peak_width_limits=None, report=False, plot=False, plt_log=False, plt_range=None, figsize=None, title=None)</code>","text":"<p>Fit a FOOOF model to power spectral density data.</p> Parameters: <p>f : array-like     Frequencies corresponding to the power spectral density data. pxx : array-like     Power spectral density data to fit. aperiodic_mode : str, optional     The mode for fitting aperiodic components ('fixed' or 'knee', default is 'fixed'). dB_threshold : float, optional     Minimum peak height in dB (default is 3). max_n_peaks : int, optional     Maximum number of peaks to fit (default is 10). freq_range : tuple, optional     Frequency range to fit (default is None, which uses the full range). peak_width_limits : tuple, optional     Limits on the width of peaks (default is None). report : bool, optional     If True, will print fitting results (default is False). plot : bool, optional     If True, will plot the fitting results (default is False). plt_log : bool, optional     If True, use a logarithmic scale for the y-axis in plots (default is False). plt_range : tuple, optional     Range for plotting (default is None). figsize : tuple, optional     Size of the figure (default is None). title : str, optional     Title for the plot (default is None).</p> Returns: <p>tuple     A tuple containing the fitting results and the FOOOF model object.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def fit_fooof(\n    f: np.ndarray,\n    pxx: np.ndarray,\n    aperiodic_mode: str = \"fixed\",\n    dB_threshold: float = 3.0,\n    max_n_peaks: int = 10,\n    freq_range: tuple = None,\n    peak_width_limits: tuple = None,\n    report: bool = False,\n    plot: bool = False,\n    plt_log: bool = False,\n    plt_range: tuple = None,\n    figsize: tuple = None,\n    title: str = None,\n) -&gt; tuple:\n    \"\"\"\n    Fit a FOOOF model to power spectral density data.\n\n    Parameters:\n    ----------\n    f : array-like\n        Frequencies corresponding to the power spectral density data.\n    pxx : array-like\n        Power spectral density data to fit.\n    aperiodic_mode : str, optional\n        The mode for fitting aperiodic components ('fixed' or 'knee', default is 'fixed').\n    dB_threshold : float, optional\n        Minimum peak height in dB (default is 3).\n    max_n_peaks : int, optional\n        Maximum number of peaks to fit (default is 10).\n    freq_range : tuple, optional\n        Frequency range to fit (default is None, which uses the full range).\n    peak_width_limits : tuple, optional\n        Limits on the width of peaks (default is None).\n    report : bool, optional\n        If True, will print fitting results (default is False).\n    plot : bool, optional\n        If True, will plot the fitting results (default is False).\n    plt_log : bool, optional\n        If True, use a logarithmic scale for the y-axis in plots (default is False).\n    plt_range : tuple, optional\n        Range for plotting (default is None).\n    figsize : tuple, optional\n        Size of the figure (default is None).\n    title : str, optional\n        Title for the plot (default is None).\n\n    Returns:\n    -------\n    tuple\n        A tuple containing the fitting results and the FOOOF model object.\n    \"\"\"\n    if aperiodic_mode != \"knee\":\n        aperiodic_mode = \"fixed\"\n\n    def set_range(x, upper=f[-1]):\n        x = np.array(upper) if x is None else np.array(x)\n        return [f[2], x.item()] if x.size == 1 else x.tolist()\n\n    freq_range = set_range(freq_range)\n    peak_width_limits = set_range(peak_width_limits, np.inf)\n\n    # Initialize a FOOOF object\n    fm = FOOOF(\n        peak_width_limits=peak_width_limits,\n        min_peak_height=dB_threshold / 10,\n        peak_threshold=0.0,\n        max_n_peaks=max_n_peaks,\n        aperiodic_mode=aperiodic_mode,\n    )\n\n    # Fit the model\n    try:\n        fm.fit(f, pxx, freq_range)\n    except Exception as e:\n        fl = np.linspace(f[0], f[-1], int((f[-1] - f[0]) / np.min(np.diff(f))) + 1)\n        fm.fit(fl, np.interp(fl, f, pxx), freq_range)\n\n    results = fm.get_results()\n\n    if report:\n        fm.print_results()\n        if aperiodic_mode == \"knee\":\n            ap_params = results.aperiodic_params\n            if ap_params[1] &lt;= 0:\n                print(\n                    \"Negative value of knee parameter occurred. Suggestion: Fit without knee parameter.\"\n                )\n            knee_freq = np.abs(ap_params[1]) ** (1 / ap_params[2])\n            print(f\"Knee location: {knee_freq:.2f} Hz\")\n\n    if plot:\n        plt_range = set_range(plt_range)\n        fm.plot(ax=plt.gca(), plt_log=plt_log)\n        plt.xlim(np.log10(plt_range) if plt_log else plt_range)\n        # plt.ylim(-8, -5.5)\n        if figsize:\n            plt.gcf().set_size_inches(figsize)\n        if title:\n            plt.title(title)\n        if is_notebook():\n            pass\n        else:\n            plt.show()\n\n    return results, fm\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.generate_resd_from_fooof","title":"<code>bmtool.analysis.lfp.generate_resd_from_fooof(fooof_model)</code>","text":"<p>Generate residuals from a fitted FOOOF model.</p> Parameters: <p>fooof_model : FOOOF     A fitted FOOOF model object.</p> Returns: <p>tuple     A tuple containing the residual power spectral density and the aperiodic fit.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def generate_resd_from_fooof(fooof_model: FOOOF) -&gt; tuple:\n    \"\"\"\n    Generate residuals from a fitted FOOOF model.\n\n    Parameters:\n    ----------\n    fooof_model : FOOOF\n        A fitted FOOOF model object.\n\n    Returns:\n    -------\n    tuple\n        A tuple containing the residual power spectral density and the aperiodic fit.\n    \"\"\"\n    results = fooof_model.get_results()\n    full_fit, _, ap_fit = gen_model(\n        fooof_model.freqs[1:],\n        results.aperiodic_params,\n        results.gaussian_params,\n        return_components=True,\n    )\n\n    full_fit, ap_fit = 10**full_fit, 10**ap_fit  # Convert back from log\n    res_psd = np.insert(\n        (10 ** fooof_model.power_spectrum[1:]) - ap_fit, 0, 0.0\n    )  # Convert back from log\n    res_fit = np.insert(full_fit - ap_fit, 0, 0.0)\n    ap_fit = np.insert(ap_fit, 0, 0.0)\n\n    return res_psd, ap_fit\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.calculate_SNR","title":"<code>bmtool.analysis.lfp.calculate_SNR(fooof_model, freq_band)</code>","text":"<p>Calculate the signal-to-noise ratio (SNR) from a fitted FOOOF model.</p> Parameters: <p>fooof_model : FOOOF     A fitted FOOOF model object. freq_band : tuple     Frequency band (min, max) for SNR calculation.</p> Returns: <p>float     The calculated SNR for the specified frequency band.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def calculate_SNR(fooof_model: FOOOF, freq_band: tuple) -&gt; float:\n    \"\"\"\n    Calculate the signal-to-noise ratio (SNR) from a fitted FOOOF model.\n\n    Parameters:\n    ----------\n    fooof_model : FOOOF\n        A fitted FOOOF model object.\n    freq_band : tuple\n        Frequency band (min, max) for SNR calculation.\n\n    Returns:\n    -------\n    float\n        The calculated SNR for the specified frequency band.\n    \"\"\"\n    periodic, ap = generate_resd_from_fooof(fooof_model)\n    freq = fooof_model.freqs  # Get frequencies from model\n    indices = (freq &gt;= freq_band[0]) &amp; (freq &lt;= freq_band[1])  # Get only the band we care about\n    band_periodic = periodic[indices]  # Filter based on band\n    band_ap = ap[indices]  # Filter\n    band_freq = freq[indices]  # Another filter\n    periodic_power = np.trapz(band_periodic, band_freq)  # Integrate periodic power\n    ap_power = np.trapz(band_ap, band_freq)  # Integrate aperiodic power\n    normalized_power = periodic_power / ap_power  # Compute the SNR\n    return normalized_power\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.wavelet_filter","title":"<code>bmtool.analysis.lfp.wavelet_filter(x, freq, fs, bandwidth=1.0, axis=-1, show_passband=False)</code>","text":"<p>Compute the Continuous Wavelet Transform (CWT) for a specified frequency using a complex Morlet wavelet.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input signal</p> required <code>freq</code> <code>float</code> <p>Target frequency for the wavelet filter</p> required <code>fs</code> <code>float</code> <p>Sampling frequency of the signal</p> required <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter of the wavelet filter (default is 1.0)</p> <code>1.0</code> <code>axis</code> <code>int</code> <p>Axis along which to compute the CWT (default is -1)</p> <code>-1</code> <code>show_passband</code> <code>bool</code> <p>If True, print the passband of the wavelet filter (default is False)</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Continuous Wavelet Transform of the input signal</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def wavelet_filter(\n    x: np.ndarray,\n    freq: float,\n    fs: float,\n    bandwidth: float = 1.0,\n    axis: int = -1,\n    show_passband: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute the Continuous Wavelet Transform (CWT) for a specified frequency using a complex Morlet wavelet.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input signal\n    freq : float\n        Target frequency for the wavelet filter\n    fs : float\n        Sampling frequency of the signal\n    bandwidth : float, optional\n        Bandwidth parameter of the wavelet filter (default is 1.0)\n    axis : int, optional\n        Axis along which to compute the CWT (default is -1)\n    show_passband : bool, optional\n        If True, print the passband of the wavelet filter (default is False)\n\n    Returns\n    -------\n    np.ndarray\n        Continuous Wavelet Transform of the input signal\n    \"\"\"\n    if show_passband:\n        lower_bound, upper_bound, passband_width = calculate_wavelet_passband(\n            freq, bandwidth, threshold=0.3\n        )  # kinda made up threshold gives the rough idea\n        print(f\"Wavelet filter at {freq:.1f} Hz Bandwidth: {bandwidth:.1f} Hz:\")\n        print(\n            f\"  Passband: {lower_bound:.1f} - {upper_bound:.1f} Hz (width: {passband_width:.1f} Hz)\"\n        )\n    wavelet = \"cmor\" + str(2 * bandwidth**2) + \"-1.0\"\n    scale = pywt.scale2frequency(wavelet, 1) * fs / freq\n    x_a = pywt.cwt(x, [scale], wavelet=wavelet, axis=axis)[0][0]\n    return x_a\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.butter_bandpass_filter","title":"<code>bmtool.analysis.lfp.butter_bandpass_filter(data, lowcut, highcut, fs, order=5, axis=-1)</code>","text":"<p>Apply a Butterworth bandpass filter to the input data.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def butter_bandpass_filter(\n    data: np.ndarray, lowcut: float, highcut: float, fs: float, order: int = 5, axis: int = -1\n) -&gt; np.ndarray:\n    \"\"\"\n    Apply a Butterworth bandpass filter to the input data.\n    \"\"\"\n    sos = signal.butter(order, [lowcut, highcut], fs=fs, btype=\"band\", output=\"sos\")\n    x_a = signal.sosfiltfilt(sos, data, axis=axis)\n    return x_a\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.cwt_spectrogram","title":"<code>bmtool.analysis.lfp.cwt_spectrogram(x, fs, nNotes=6, nOctaves=np.inf, freq_range=(0, np.inf), bandwidth=1.0, axis=-1, detrend=False, normalize=False)</code>","text":"<p>Calculate spectrogram using continuous wavelet transform</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def cwt_spectrogram(\n    x,\n    fs,\n    nNotes=6,\n    nOctaves=np.inf,\n    freq_range=(0, np.inf),\n    bandwidth=1.0,\n    axis=-1,\n    detrend=False,\n    normalize=False,\n):\n    \"\"\"Calculate spectrogram using continuous wavelet transform\"\"\"\n    x = np.asarray(x)\n    N = x.shape[axis]\n    times = np.arange(N) / fs\n    # detrend and normalize\n    if detrend:\n        x = signal.detrend(x, axis=axis, type=\"linear\")\n    if normalize:\n        x = x / x.std()\n    # Define some parameters of our wavelet analysis.\n    # range of scales (in time) that makes sense\n    # min = 2 (Nyquist frequency)\n    # max = np.floor(N/2)\n    nOctaves = min(nOctaves, np.log2(2 * np.floor(N / 2)))\n    scales = 2 ** np.arange(1, nOctaves, 1 / nNotes)\n    # cwt and the frequencies used.\n    # Use the complex morelet with bw=2*bandwidth^2 and center frequency of 1.0\n    # bandwidth is sigma of the gaussian envelope\n    wavelet = \"cmor\" + str(2 * bandwidth**2) + \"-1.0\"\n    frequencies = pywt.scale2frequency(wavelet, scales) * fs\n    scales = scales[(frequencies &gt;= freq_range[0]) &amp; (frequencies &lt;= freq_range[1])]\n    coef, frequencies = pywt.cwt(\n        x, scales[::-1], wavelet=wavelet, sampling_period=1 / fs, axis=axis\n    )\n    power = np.real(coef * np.conj(coef))  # equivalent to power = np.abs(coef)**2\n    # cone of influence in terms of wavelength\n    coi = N / 2 - np.abs(np.arange(N) - (N - 1) / 2)\n    # cone of influence in terms of frequency\n    coif = COI_FREQ * fs / coi\n    return power, times, frequencies, coif\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.cwt_spectrogram_xarray","title":"<code>bmtool.analysis.lfp.cwt_spectrogram_xarray(x, fs, time=None, axis=-1, downsample_fs=None, channel_coords=None, **cwt_kwargs)</code>","text":"<p>Calculate spectrogram using continuous wavelet transform and return an xarray.Dataset x: input array fs: sampling frequency (Hz) axis: dimension index of time axis in x downsample_fs: downsample to the frequency if specified channel_coords: dictionary of {coordinate name: index} for channels cwt_kwargs: keyword arguments for cwt_spectrogram()</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def cwt_spectrogram_xarray(\n    x, fs, time=None, axis=-1, downsample_fs=None, channel_coords=None, **cwt_kwargs\n):\n    \"\"\"Calculate spectrogram using continuous wavelet transform and return an xarray.Dataset\n    x: input array\n    fs: sampling frequency (Hz)\n    axis: dimension index of time axis in x\n    downsample_fs: downsample to the frequency if specified\n    channel_coords: dictionary of {coordinate name: index} for channels\n    cwt_kwargs: keyword arguments for cwt_spectrogram()\n    \"\"\"\n    x = np.asarray(x)\n    T = x.shape[axis]  # number of time points\n    t = np.arange(T) / fs if time is None else np.asarray(time)\n    if downsample_fs is None or downsample_fs &gt;= fs:\n        downsample_fs = fs\n        downsampled = x\n    else:\n        num = int(T * downsample_fs / fs)\n        downsample_fs = num / T * fs\n        downsampled, t = signal.resample(x, num=num, t=t, axis=axis)\n    downsampled = np.moveaxis(downsampled, axis, -1)\n    sxx, _, f, coif = cwt_spectrogram(downsampled, downsample_fs, **cwt_kwargs)\n    sxx = np.moveaxis(sxx, 0, -2)  # shape (... , freq, time)\n    if channel_coords is None:\n        channel_coords = {f\"dim_{i:d}\": range(d) for i, d in enumerate(sxx.shape[:-2])}\n    sxx = xr.DataArray(sxx, coords={**channel_coords, \"frequency\": f, \"time\": t}).to_dataset(\n        name=\"PSD\"\n    )\n    sxx.update(dict(cone_of_influence_frequency=xr.DataArray(coif, coords={\"time\": t})))\n    return sxx\n</code></pre>"},{"location":"api/analysis/netcon_reports/","title":"Network Connectivity Reports","text":"<p>The <code>netcon_reports</code> module provides tools for analyzing and reporting network connectivity statistics.</p>"},{"location":"api/analysis/spikes/","title":"Spike Analysis","text":"<p>The <code>spikes</code> module provides functions for loading and analyzing spike data from simulations.</p>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes.load_spikes_to_df","title":"<code>bmtool.analysis.spikes.load_spikes_to_df(spike_file, network_name, sort=True, config=None, groupby='pop_name')</code>","text":"<p>Load spike data from an HDF5 file into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spike_file</code> <code>str</code> <p>Path to the HDF5 file containing spike data</p> required <code>network_name</code> <code>str</code> <p>The name of the network within the HDF5 file from which to load spike data</p> required <code>sort</code> <code>bool</code> <p>Whether to sort the DataFrame by 'timestamps' (default: True)</p> <code>True</code> <code>config</code> <code>str</code> <p>Path to configuration file to label the cell type of each spike (default: None)</p> <code>None</code> <code>groupby</code> <code>Union[str, List[str]]</code> <p>The column(s) to group by (default: 'pop_name')</p> <code>'pop_name'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame containing 'node_ids' and 'timestamps' columns from the spike data, with additional columns if a config file is provided</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = load_spikes_to_df(\"spikes.h5\", \"cortex\")\n&gt;&gt;&gt; df = load_spikes_to_df(\"spikes.h5\", \"cortex\", config=\"config.json\", groupby=[\"pop_name\", \"model_type\"])\n</code></pre> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def load_spikes_to_df(\n    spike_file: str,\n    network_name: str,\n    sort: bool = True,\n    config: str = None,\n    groupby: Union[str, List[str]] = \"pop_name\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load spike data from an HDF5 file into a pandas DataFrame.\n\n    Parameters\n    ----------\n    spike_file : str\n        Path to the HDF5 file containing spike data\n    network_name : str\n        The name of the network within the HDF5 file from which to load spike data\n    sort : bool, optional\n        Whether to sort the DataFrame by 'timestamps' (default: True)\n    config : str, optional\n        Path to configuration file to label the cell type of each spike (default: None)\n    groupby : Union[str, List[str]], optional\n        The column(s) to group by (default: 'pop_name')\n\n    Returns\n    -------\n    pd.DataFrame\n        A pandas DataFrame containing 'node_ids' and 'timestamps' columns from the spike data,\n        with additional columns if a config file is provided\n\n    Examples\n    --------\n    &gt;&gt;&gt; df = load_spikes_to_df(\"spikes.h5\", \"cortex\")\n    &gt;&gt;&gt; df = load_spikes_to_df(\"spikes.h5\", \"cortex\", config=\"config.json\", groupby=[\"pop_name\", \"model_type\"])\n    \"\"\"\n    with h5py.File(spike_file) as f:\n        spikes_df = pd.DataFrame(\n            {\n                \"node_ids\": f[\"spikes\"][network_name][\"node_ids\"],\n                \"timestamps\": f[\"spikes\"][network_name][\"timestamps\"],\n            }\n        )\n\n        if sort:\n            spikes_df.sort_values(by=\"timestamps\", inplace=True, ignore_index=True)\n\n        if config:\n            nodes = load_nodes_from_config(config)\n            nodes = nodes[network_name]\n\n            # Convert single string to a list for uniform handling\n            if isinstance(groupby, str):\n                groupby = [groupby]\n\n            # Ensure all requested columns exist\n            missing_cols = [col for col in groupby if col not in nodes.columns]\n            if missing_cols:\n                raise KeyError(f\"Columns {missing_cols} not found in nodes DataFrame.\")\n\n            spikes_df = spikes_df.merge(\n                nodes[groupby], left_on=\"node_ids\", right_index=True, how=\"left\"\n            )\n\n    return spikes_df\n</code></pre>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes.compute_firing_rate_stats","title":"<code>bmtool.analysis.spikes.compute_firing_rate_stats(df, groupby='pop_name', start_time=None, stop_time=None)</code>","text":"<p>Computes the firing rates of individual nodes and the mean and standard deviation of firing rates per group.</p> <p>Args:     df (pd.DataFrame): Dataframe containing spike timestamps and node IDs.     groupby (str or list of str, optional): Column(s) to group by (e.g., 'pop_name' or ['pop_name', 'layer']).     start_time (float, optional): Start time for the analysis window. Defaults to the minimum timestamp in the data.     stop_time (float, optional): Stop time for the analysis window. Defaults to the maximum timestamp in the data.</p> <p>Returns:     Tuple[pd.DataFrame, pd.DataFrame]:         - The first DataFrame (<code>pop_stats</code>) contains the mean and standard deviation of firing rates per group.         - The second DataFrame (<code>individual_stats</code>) contains the firing rate of each individual node.</p> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def compute_firing_rate_stats(\n    df: pd.DataFrame,\n    groupby: Union[str, List[str]] = \"pop_name\",\n    start_time: float = None,\n    stop_time: float = None,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Computes the firing rates of individual nodes and the mean and standard deviation of firing rates per group.\n\n    Args:\n        df (pd.DataFrame): Dataframe containing spike timestamps and node IDs.\n        groupby (str or list of str, optional): Column(s) to group by (e.g., 'pop_name' or ['pop_name', 'layer']).\n        start_time (float, optional): Start time for the analysis window. Defaults to the minimum timestamp in the data.\n        stop_time (float, optional): Stop time for the analysis window. Defaults to the maximum timestamp in the data.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]:\n            - The first DataFrame (`pop_stats`) contains the mean and standard deviation of firing rates per group.\n            - The second DataFrame (`individual_stats`) contains the firing rate of each individual node.\n    \"\"\"\n\n    # Ensure groupby is a list\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    # Ensure all columns exist in the dataframe\n    for col in groupby:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in dataframe.\")\n\n    # Filter dataframe based on start/stop time\n    if start_time is not None:\n        df = df[df[\"timestamps\"] &gt;= start_time]\n    if stop_time is not None:\n        df = df[df[\"timestamps\"] &lt;= stop_time]\n\n    # Compute total duration for firing rate calculation\n    if start_time is None:\n        min_time = df[\"timestamps\"].min()\n    else:\n        min_time = start_time\n\n    if stop_time is None:\n        max_time = df[\"timestamps\"].max()\n    else:\n        max_time = stop_time\n\n    duration = max_time - min_time  # Avoid division by zero\n\n    if duration &lt;= 0:\n        raise ValueError(\"Invalid time window: Stop time must be greater than start time.\")\n\n    # Compute firing rate for each node\n\n    # Compute spike counts per node\n    spike_counts = df[\"node_ids\"].value_counts().reset_index()\n    spike_counts.columns = [\"node_ids\", \"spike_count\"]  # Rename columns\n\n    # Merge with original dataframe to get corresponding labels (e.g., 'pop_name')\n    spike_counts = spike_counts.merge(\n        df[[\"node_ids\"] + groupby].drop_duplicates(), on=\"node_ids\", how=\"left\"\n    )\n\n    # Compute firing rate\n    spike_counts[\"firing_rate\"] = spike_counts[\"spike_count\"] / duration * 1000  # scale to Hz\n    indivdual_stats = spike_counts\n\n    # Compute mean and standard deviation per group\n    pop_stats = spike_counts.groupby(groupby)[\"firing_rate\"].agg([\"mean\", \"std\"]).reset_index()\n\n    # Rename columns\n    pop_stats.rename(columns={\"mean\": \"firing_rate_mean\", \"std\": \"firing_rate_std\"}, inplace=True)\n\n    return pop_stats, indivdual_stats\n</code></pre>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes._pop_spike_rate","title":"<code>bmtool.analysis.spikes._pop_spike_rate(spike_times, time=None, time_points=None, frequency=False)</code>","text":"<p>Calculate the spike count or frequency histogram over specified time intervals.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>Union[ndarray, list]</code> <p>Array or list of spike times in milliseconds</p> required <code>time</code> <code>Optional[Tuple[float, float, float]]</code> <p>Tuple specifying (start, stop, step) in milliseconds. Used to create evenly spaced time points if <code>time_points</code> is not provided. Default is None.</p> <code>None</code> <code>time_points</code> <code>Optional[Union[ndarray, list]]</code> <p>Array or list of specific time points for binning. If provided, <code>time</code> is ignored. Default is None.</p> <code>None</code> <code>frequency</code> <code>bool</code> <p>If True, returns spike frequency in Hz; otherwise, returns spike count. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of spike counts or frequencies, depending on the <code>frequency</code> flag.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>time</code> and <code>time_points</code> are None.</p> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def _pop_spike_rate(\n    spike_times: Union[np.ndarray, list],\n    time: Optional[Tuple[float, float, float]] = None,\n    time_points: Optional[Union[np.ndarray, list]] = None,\n    frequency: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the spike count or frequency histogram over specified time intervals.\n\n    Parameters\n    ----------\n    spike_times : Union[np.ndarray, list]\n        Array or list of spike times in milliseconds\n    time : Optional[Tuple[float, float, float]], optional\n        Tuple specifying (start, stop, step) in milliseconds. Used to create evenly spaced time points\n        if `time_points` is not provided. Default is None.\n    time_points : Optional[Union[np.ndarray, list]], optional\n        Array or list of specific time points for binning. If provided, `time` is ignored. Default is None.\n    frequency : bool, optional\n        If True, returns spike frequency in Hz; otherwise, returns spike count. Default is False.\n\n    Returns\n    -------\n    np.ndarray\n        Array of spike counts or frequencies, depending on the `frequency` flag.\n\n    Raises\n    ------\n    ValueError\n        If both `time` and `time_points` are None.\n    \"\"\"\n    if time_points is None:\n        if time is None:\n            raise ValueError(\"Either `time` or `time_points` must be provided.\")\n        time_points = np.arange(*time)\n        dt = time[2]\n    else:\n        time_points = np.asarray(time_points).ravel()\n        dt = (time_points[-1] - time_points[0]) / (time_points.size - 1)\n\n    bins = np.append(time_points, time_points[-1] + dt)\n    spike_rate, _ = np.histogram(np.asarray(spike_times), bins)\n\n    if frequency:\n        spike_rate = 1000 / dt * spike_rate\n\n    return spike_rate\n</code></pre>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes.get_population_spike_rate","title":"<code>bmtool.analysis.spikes.get_population_spike_rate(spike_data, fs=400.0, t_start=0, t_stop=None, config=None, network_name=None, save=False, save_path=None, normalize=False, smooth_window=50, smooth_method='gaussian')</code>","text":"<p>Calculate the population spike rate for each population in the given spike data.</p> <p>Parameters:</p> Name Type Description Default <code>spike_data</code> <code>DataFrame</code> <p>A DataFrame containing spike data with columns 'pop_name', 'timestamps', and 'node_ids'</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz, which determines the time bin size for calculating the spike rate (default: 400.0)</p> <code>400.0</code> <code>t_start</code> <code>float</code> <p>Start time (in milliseconds) for spike rate calculation (default: 0)</p> <code>0</code> <code>t_stop</code> <code>Optional[float]</code> <p>Stop time (in milliseconds) for spike rate calculation. If None, defaults to the maximum timestamp in the data</p> <code>None</code> <code>config</code> <code>Optional[str]</code> <p>Path to a configuration file containing node information, used to determine the correct number of nodes per population. If None, node count is estimated from unique node spikes (default: None)</p> <code>None</code> <code>network_name</code> <code>Optional[str]</code> <p>Name of the network used in the configuration file, allowing selection of nodes for that network. Required if <code>config</code> is provided (default: None)</p> <code>None</code> <code>save</code> <code>bool</code> <p>Whether to save the calculated population spike rate to a file (default: False)</p> <code>False</code> <code>save_path</code> <code>Optional[str]</code> <p>Directory path where the file should be saved if <code>save</code> is True (default: None)</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the spike rates for each population to a range of [0, 1] (default: False)</p> <code>False</code> <code>smooth_window</code> <code>int</code> <p>Window size for smoothing in number of time bins (default: 50)</p> <code>50</code> <code>smooth_method</code> <code>str</code> <p>Smoothing method to use: 'gaussian', 'boxcar', or 'exponential' (default: 'gaussian')</p> <code>'gaussian'</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>An xarray DataArray containing the spike rates with dimensions of time, population, and type. The 'type' dimension includes 'raw' and 'smoothed' values. The DataArray includes sampling frequency (fs) as an attribute. If normalize is True, each population's spike rate is scaled to [0, 1].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>save</code> is True but <code>save_path</code> is not provided. If an invalid smooth_method is specified.</p> Notes <ul> <li>If <code>config</code> is None, the function assumes all cells in each population have fired at least once;   otherwise, the node count may be inaccurate.</li> <li>If normalization is enabled, each population's spike rate is scaled using Min-Max normalization.</li> <li>Smoothing is applied using scipy.ndimage's filters based on the specified method.</li> </ul> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def get_population_spike_rate(\n    spike_data: pd.DataFrame,\n    fs: float = 400.0,\n    t_start: float = 0,\n    t_stop: Optional[float] = None,\n    config: Optional[str] = None,\n    network_name: Optional[str] = None,\n    save: bool = False,\n    save_path: Optional[str] = None,\n    normalize: bool = False,\n    smooth_window: int = 50,  # Window size for smoothing (in time bins)\n    smooth_method: str = \"gaussian\",  # Smoothing method: 'gaussian', 'boxcar', or 'exponential'\n) -&gt; xr.DataArray:\n    \"\"\"\n    Calculate the population spike rate for each population in the given spike data.\n\n    Parameters\n    ----------\n    spike_data : pd.DataFrame\n        A DataFrame containing spike data with columns 'pop_name', 'timestamps', and 'node_ids'\n    fs : float, optional\n        Sampling frequency in Hz, which determines the time bin size for calculating the spike rate (default: 400.0)\n    t_start : float, optional\n        Start time (in milliseconds) for spike rate calculation (default: 0)\n    t_stop : Optional[float], optional\n        Stop time (in milliseconds) for spike rate calculation. If None, defaults to the maximum timestamp in the data\n    config : Optional[str], optional\n        Path to a configuration file containing node information, used to determine the correct number of nodes per population.\n        If None, node count is estimated from unique node spikes (default: None)\n    network_name : Optional[str], optional\n        Name of the network used in the configuration file, allowing selection of nodes for that network.\n        Required if `config` is provided (default: None)\n    save : bool, optional\n        Whether to save the calculated population spike rate to a file (default: False)\n    save_path : Optional[str], optional\n        Directory path where the file should be saved if `save` is True (default: None)\n    normalize : bool, optional\n        Whether to normalize the spike rates for each population to a range of [0, 1] (default: False)\n    smooth_window : int, optional\n        Window size for smoothing in number of time bins (default: 50)\n    smooth_method : str, optional\n        Smoothing method to use: 'gaussian', 'boxcar', or 'exponential' (default: 'gaussian')\n\n    Returns\n    -------\n    xr.DataArray\n        An xarray DataArray containing the spike rates with dimensions of time, population, and type.\n        The 'type' dimension includes 'raw' and 'smoothed' values.\n        The DataArray includes sampling frequency (fs) as an attribute.\n        If normalize is True, each population's spike rate is scaled to [0, 1].\n\n    Raises\n    ------\n    ValueError\n        If `save` is True but `save_path` is not provided.\n        If an invalid smooth_method is specified.\n\n    Notes\n    -----\n    - If `config` is None, the function assumes all cells in each population have fired at least once;\n      otherwise, the node count may be inaccurate.\n    - If normalization is enabled, each population's spike rate is scaled using Min-Max normalization.\n    - Smoothing is applied using scipy.ndimage's filters based on the specified method.\n    \"\"\"\n    import numpy as np\n    from scipy import ndimage\n\n    # Validate smoothing method\n    if smooth_method not in [\"gaussian\", \"boxcar\", \"exponential\"]:\n        raise ValueError(\n            f\"Invalid smooth_method: {smooth_method}. Choose from 'gaussian', 'boxcar', or 'exponential'.\"\n        )\n\n    pop_spikes = {}\n    node_number = {}\n\n    if config is None:\n        pass\n        # print(\n        #     \"Note: Node number is obtained by counting unique node spikes in the network.\\nIf the network did not run for a sufficient duration, or not all cells fired,\\nthen this count will not include all nodes so the firing rate will not be of the whole population!\"\n        # )\n        # print(\n        #     \"You can provide a config to calculate the correct amount of nodes! for a true population rate.\"\n        # )\n\n    if config:\n        if not network_name:\n            print(\n                \"Grabbing first network; specify a network name to ensure correct node population is selected.\"\n            )\n\n    # Get t_stop if not provided\n    if t_stop is None:\n        t_stop = spike_data[\"timestamps\"].max()\n\n    # Get population names and prepare data\n    populations = spike_data[\"pop_name\"].unique()\n    for pop_name in populations:\n        ps = spike_data[spike_data[\"pop_name\"] == pop_name]\n\n        if config:\n            nodes = load_nodes_from_config(config)\n            if network_name:\n                nodes = nodes[network_name]\n            else:\n                nodes = list(nodes.values())[0] if nodes else {}\n            nodes = nodes[nodes[\"pop_name\"] == pop_name]\n            node_number[pop_name] = nodes.index.nunique()\n\n        else:\n            node_number[pop_name] = ps[\"node_ids\"].nunique()\n\n        filtered_spikes = spike_data[\n            (spike_data[\"pop_name\"] == pop_name)\n            &amp; (spike_data[\"timestamps\"] &gt; t_start)\n            &amp; (spike_data[\"timestamps\"] &lt; t_stop)\n        ]\n        pop_spikes[pop_name] = filtered_spikes\n\n    # Calculate time points\n    time = np.arange(t_start, t_stop, 1000 / fs)  # Convert sampling frequency to time steps\n\n    # Calculate spike rates for each population\n    spike_rates = []\n    for p in populations:\n        raw_rate = _pop_spike_rate(pop_spikes[p][\"timestamps\"], (t_start, t_stop, 1000 / fs))\n        rate = fs / node_number[p] * raw_rate\n        spike_rates.append(rate)\n\n    spike_rates_array = np.array(spike_rates).T  # Transpose to have time as first dimension\n\n    # Calculate smoothed version for each population\n    smoothed_rates = []\n\n    for i in range(spike_rates_array.shape[1]):\n        pop_rate = spike_rates_array[:, i]\n\n        if smooth_method == \"gaussian\":\n            # Gaussian smoothing (sigma is approximately window/6 for a Gaussian filter)\n            sigma = smooth_window / 6\n            smoothed_pop_rate = ndimage.gaussian_filter1d(pop_rate, sigma=sigma)\n        elif smooth_method == \"boxcar\":\n            # Boxcar/uniform smoothing\n            kernel = np.ones(smooth_window) / smooth_window\n            smoothed_pop_rate = ndimage.convolve1d(pop_rate, kernel, mode=\"nearest\")\n        elif smooth_method == \"exponential\":\n            # Exponential smoothing\n            alpha = 2 / (smooth_window + 1)  # Equivalent to window size in exponential smoothing\n            smoothed_pop_rate = np.zeros_like(pop_rate)\n            smoothed_pop_rate[0] = pop_rate[0]\n            for t in range(1, len(pop_rate)):\n                smoothed_pop_rate[t] = alpha * pop_rate[t] + (1 - alpha) * smoothed_pop_rate[t - 1]\n\n        smoothed_rates.append(smoothed_pop_rate)\n\n    smoothed_rates_array = np.array(smoothed_rates).T  # Transpose to have time as first dimension\n\n    # Stack raw and smoothed data\n    combined_data = np.stack([spike_rates_array, smoothed_rates_array], axis=2)\n\n    # Create DataArray with the additional 'type' dimension\n    spike_rate_array = xr.DataArray(\n        combined_data,\n        coords={\"time\": time, \"population\": populations, \"type\": [\"raw\", \"smoothed\"]},\n        dims=[\"time\", \"population\", \"type\"],\n        attrs={\n            \"fs\": fs,\n            \"normalized\": False,\n            \"smooth_method\": smooth_method,\n            \"smooth_window\": smooth_window,\n        },\n    )\n\n    # Normalize if requested\n    if normalize:\n        # Apply normalization for each population and each type (raw/smoothed)\n        for pop_idx in range(len(populations)):\n            for type_idx, type_name in enumerate([\"raw\", \"smoothed\"]):\n                pop_data = spike_rate_array.sel(population=populations[pop_idx], type=type_name)\n                min_val = pop_data.min(dim=\"time\")\n                max_val = pop_data.max(dim=\"time\")\n\n                # Handle case where min == max (constant signal)\n                if max_val != min_val:\n                    spike_rate_array.loc[:, populations[pop_idx], type_name] = (\n                        pop_data - min_val\n                    ) / (max_val - min_val)\n\n        spike_rate_array.attrs[\"normalized\"] = True\n\n    # Save if requested\n    if save:\n        if save_path is None:\n            raise ValueError(\"save_path must be provided if save is True.\")\n\n        os.makedirs(save_path, exist_ok=True)\n        save_file = os.path.join(save_path, \"spike_rate.h5\")\n        spike_rate_array.to_netcdf(save_file)\n\n    return spike_rate_array\n</code></pre>"},{"location":"api/bmplot/connections/","title":"Network Connections Plotting API","text":""},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.is_notebook","title":"<code>bmtool.bmplot.connections.is_notebook()</code>","text":"<p>Detect if code is running in a Jupyter notebook environment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if running in a Jupyter notebook, False otherwise.</p> Notes <p>This is used to determine whether to call plt.show() explicitly or rely on Jupyter's automatic display functionality.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; if is_notebook():\n...     plt.show()\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def is_notebook() -&gt; bool:\n    \"\"\"\n    Detect if code is running in a Jupyter notebook environment.\n\n    Returns\n    -------\n    bool\n        True if running in a Jupyter notebook, False otherwise.\n\n    Notes\n    -----\n    This is used to determine whether to call plt.show() explicitly or\n    rely on Jupyter's automatic display functionality.\n\n    Examples\n    --------\n    &gt;&gt;&gt; if is_notebook():\n    ...     plt.show()\n    \"\"\"\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == \"ZMQInteractiveShell\":\n            return True  # Jupyter notebook or qtconsole\n        elif shell == \"TerminalInteractiveShell\":\n            return False  # Terminal running IPython\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False  # Probably standard Python interpreter\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.total_connection_matrix","title":"<code>bmtool.bmplot.connections.total_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, synaptic_info='0', include_gap=True)</code>","text":"<p>Generate a plot displaying total connections or other synaptic statistics.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network names to use as sources.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network names to use as targets.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifiers to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifiers to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, don't display population name before sid or tid in the plot. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>synaptic_info</code> <code>str</code> <p>Type of information to display. Options: - '0': Total connections (default) - '1': Mean and standard deviation of connections - '2': All synapse .mod files used - '3': All synapse .json files used</p> <code>'0'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions and chemical synapses in the analysis. If False, only include chemical synapses. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; total_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     title='PN to LN Connections'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def total_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    synaptic_info: str = \"0\",\n    include_gap: bool = True,\n) -&gt; None:\n    \"\"\"\n    Generate a plot displaying total connections or other synaptic statistics.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network names to use as sources.\n    targets : str, optional\n        Comma-separated string of network names to use as targets.\n    sids : str, optional\n        Comma-separated string of source node identifiers to filter.\n    tids : str, optional\n        Comma-separated string of target node identifiers to filter.\n    no_prepend_pop : bool, optional\n        If True, don't display population name before sid or tid in the plot. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    synaptic_info : str, optional\n        Type of information to display. Options:\n        - '0': Total connections (default)\n        - '1': Mean and standard deviation of connections\n        - '2': All synapse .mod files used\n        - '3': All synapse .json files used\n    include_gap : bool, optional\n        If True, include gap junctions and chemical synapses in the analysis.\n        If False, only include chemical synapses. Default is True.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; total_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     title='PN to LN Connections'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    text, num, source_labels, target_labels = util.connection_totals(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        synaptic_info=synaptic_info,\n        include_gap=include_gap,\n    )\n\n    if title is None or title == \"\":\n        title = \"Total Connections\"\n    if synaptic_info == \"1\":\n        title = \"Mean and Stdev # of Conn on Target\"\n    if synaptic_info == \"2\":\n        title = \"All Synapse .mod Files Used\"\n    if synaptic_info == \"3\":\n        title = \"All Synapse .json Files Used\"\n\n    plot_connection_info(\n        text, num, source_labels, target_labels, title, syn_info=synaptic_info, save_file=save_file\n    )\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.percent_connection_matrix","title":"<code>bmtool.bmplot.connections.percent_connection_matrix(config, nodes=None, edges=None, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, method='total', include_gap=True, return_dict=False)</code>","text":"<p>Generates a plot showing the percent connectivity of a network.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid in the plot. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method for calculating percent connectivity. Options: 'total', 'uni', 'bi'. Default is 'total'.</p> <code>'total'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. If False, only include chemical synapses. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>(dict, optional)</code> <p>Dictionary containing connection information if return_dict=True, None otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = percent_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     return_dict=True\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def percent_connection_matrix(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    method: str = \"total\",\n    include_gap: bool = True,\n    return_dict: bool = False,\n) -&gt; Optional[Dict]:\n    \"\"\"\n    Generates a plot showing the percent connectivity of a network.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid in the plot. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    method : str, optional\n        Method for calculating percent connectivity. Options: 'total', 'uni', 'bi'.\n        Default is 'total'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. If False, only include chemical synapses.\n        Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is False.\n\n    Returns\n    -------\n    dict, optional\n        Dictionary containing connection information if return_dict=True, None otherwise.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = percent_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     return_dict=True\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    text, num, source_labels, target_labels = util.percent_connections(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        method=method,\n        include_gap=include_gap,\n    )\n    if title is None or title == \"\":\n        title = \"Percent Connectivity\"\n\n    if return_dict:\n        result_dict = plot_connection_info(\n            text, num, source_labels, target_labels, title, save_file=save_file, return_dict=return_dict\n        )\n        return result_dict\n    else:\n        plot_connection_info(text, num, source_labels, target_labels, title, save_file=save_file)\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.probability_connection_matrix","title":"<code>bmtool.bmplot.connections.probability_connection_matrix(config, nodes=None, edges=None, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, dist_X=True, dist_Y=True, dist_Z=True, bins=8, line_plot=False, verbose=False, include_gap=True)</code>","text":"<p>Generates probability graphs showing connectivity as a function of distance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>dist_X</code> <code>bool</code> <p>If True, include X distance in calculations. Default is True.</p> <code>True</code> <code>dist_Y</code> <code>bool</code> <p>If True, include Y distance in calculations. Default is True.</p> <code>True</code> <code>dist_Z</code> <code>bool</code> <p>If True, include Z distance in calculations. Default is True.</p> <code>True</code> <code>bins</code> <code>int</code> <p>Number of distance bins for the probability calculation. Default is 8.</p> <code>8</code> <code>line_plot</code> <code>bool</code> <p>If True, plot lines instead of bars. Default is False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, print debugging information. Default is False.</p> <code>False</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> Notes <p>This function needs model_template to be defined to work properly.</p> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def probability_connection_matrix(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    dist_X: bool = True,\n    dist_Y: bool = True,\n    dist_Z: bool = True,\n    bins: int = 8,\n    line_plot: bool = False,\n    verbose: bool = False,\n    include_gap: bool = True,\n) -&gt; None:\n    \"\"\"\n    Generates probability graphs showing connectivity as a function of distance.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    dist_X : bool, optional\n        If True, include X distance in calculations. Default is True.\n    dist_Y : bool, optional\n        If True, include Y distance in calculations. Default is True.\n    dist_Z : bool, optional\n        If True, include Z distance in calculations. Default is True.\n    bins : int, optional\n        Number of distance bins for the probability calculation. Default is 8.\n    line_plot : bool, optional\n        If True, plot lines instead of bars. Default is False.\n    verbose : bool, optional\n        If True, print debugging information. Default is False.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Notes\n    -----\n    This function needs model_template to be defined to work properly.\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    throwaway, data, source_labels, target_labels = util.connection_probabilities(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        dist_X=dist_X,\n        dist_Y=dist_Y,\n        dist_Z=dist_Z,\n        num_bins=bins,\n        include_gap=include_gap,\n    )\n    if not data.any():\n        return\n    if data[0][0] == -1:\n        return\n    # plot_connection_info(data,source_labels,target_labels,title, save_file=save_file)\n\n    # plt.clf()# clears previous plots\n    np.seterr(divide=\"ignore\", invalid=\"ignore\")\n    num_src, num_tar = data.shape\n    fig, axes = plt.subplots(nrows=num_src, ncols=num_tar, figsize=(12, 12))\n    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n\n    for x in range(num_src):\n        for y in range(num_tar):\n            ns = data[x][y][\"ns\"]\n            bins_data = data[x][y][\"bins\"]\n\n            XX = bins_data[:-1]\n            YY = ns[0] / ns[1]\n\n            if line_plot:\n                axes[x, y].plot(XX, YY)\n            else:\n                axes[x, y].bar(XX, YY)\n\n            if x == num_src - 1:\n                axes[x, y].set_xlabel(target_labels[y])\n            if y == 0:\n                axes[x, y].set_ylabel(source_labels[x])\n\n            if verbose:\n                print(\"Source: [\" + source_labels[x] + \"] | Target: [\" + target_labels[y] + \"]\")\n                print(\"X:\")\n                print(XX)\n                print(\"Y:\")\n                print(YY)\n\n    tt = \"Distance Probability Matrix\"\n    if title:\n        tt = title\n    st = fig.suptitle(tt, fontsize=14)\n    fig.text(0.5, 0.04, \"Target\", ha=\"center\")\n    fig.text(0.04, 0.5, \"Source\", va=\"center\", rotation=\"vertical\")\n    notebook = is_notebook()\n    if not notebook:\n        fig.show()\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.convergence_connection_matrix","title":"<code>bmtool.bmplot.connections.convergence_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, convergence=True, method='mean+std', include_gap=True, return_dict=None)</code>","text":"<p>Generates connection plot displaying synaptic convergence data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>convergence</code> <code>bool</code> <p>If True, compute convergence; if False, compute divergence. Default is True.</p> <code>True</code> <code>method</code> <code>str</code> <p>Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'. Default is 'mean+std'.</p> <code>'mean+std'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>(dict, optional)</code> <p>Dictionary containing connection information if return_dict=True, None otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = convergence_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='mean+std'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def convergence_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    convergence: bool = True,\n    method: str = \"mean+std\",\n    include_gap: bool = True,\n    return_dict: Optional[bool] = None,\n) -&gt; Optional[Dict]:\n    \"\"\"\n    Generates connection plot displaying synaptic convergence data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    convergence : bool, optional\n        If True, compute convergence; if False, compute divergence. Default is True.\n    method : str, optional\n        Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'.\n        Default is 'mean+std'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is None.\n\n    Returns\n    -------\n    dict, optional\n        Dictionary containing connection information if return_dict=True, None otherwise.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = convergence_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='mean+std'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    return divergence_connection_matrix(\n        config,\n        title,\n        sources,\n        targets,\n        sids,\n        tids,\n        no_prepend_pop,\n        save_file,\n        convergence,\n        method,\n        include_gap=include_gap,\n        return_dict=return_dict,\n    )\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.divergence_connection_matrix","title":"<code>bmtool.bmplot.connections.divergence_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, convergence=False, method='mean+std', include_gap=True, return_dict=None)</code>","text":"<p>Generates connection plot displaying synaptic divergence data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>convergence</code> <code>bool</code> <p>If True, compute convergence; if False, compute divergence. Default is False.</p> <code>False</code> <code>method</code> <code>str</code> <p>Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'. Default is 'mean+std'.</p> <code>'mean+std'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>(dict, optional)</code> <p>Dictionary containing connection information if return_dict=True, None otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = divergence_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='mean+std'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def divergence_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    convergence: bool = False,\n    method: str = \"mean+std\",\n    include_gap: bool = True,\n    return_dict: Optional[bool] = None,\n) -&gt; Optional[Dict]:\n    \"\"\"\n    Generates connection plot displaying synaptic divergence data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    convergence : bool, optional\n        If True, compute convergence; if False, compute divergence. Default is False.\n    method : str, optional\n        Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'.\n        Default is 'mean+std'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is None.\n\n    Returns\n    -------\n    dict, optional\n        Dictionary containing connection information if return_dict=True, None otherwise.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = divergence_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='mean+std'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    syn_info, data, source_labels, target_labels = util.connection_divergence(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        convergence=convergence,\n        method=method,\n        include_gap=include_gap,\n    )\n\n    # data, labels = util.connection_divergence_average(config=config,nodes=nodes,edges=edges,populations=populations)\n\n    if title is None or title == \"\":\n        if method == \"min\":\n            title = \"Minimum \"\n        elif method == \"max\":\n            title = \"Maximum \"\n        elif method == \"std\":\n            title = \"Standard Deviation \"\n        elif method == \"mean\":\n            title = \"Mean \"\n        else:\n            title = \"Mean + Std \"\n\n        if convergence:\n            title = title + \"Synaptic Convergence\"\n        else:\n            title = title + \"Synaptic Divergence\"\n    if return_dict:\n        result_dict = plot_connection_info(\n            syn_info,\n            data,\n            source_labels,\n            target_labels,\n            title,\n            save_file=save_file,\n            return_dict=return_dict,\n        )\n        return result_dict\n    else:\n        plot_connection_info(\n            syn_info, data, source_labels, target_labels, title, save_file=save_file\n        )\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.gap_junction_matrix","title":"<code>bmtool.bmplot.connections.gap_junction_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, save_file=None, method='convergence')</code>","text":"<p>Generates connection plot displaying gap junction data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method for computing gap junction statistics. Options: 'convergence', 'percent'. Default is 'convergence'.</p> <code>'convergence'</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined, sources/targets are not defined, or method is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gap_junction_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='convergence'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def gap_junction_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    save_file: Optional[str] = None,\n    method: str = \"convergence\",\n) -&gt; None:\n    \"\"\"\n    Generates connection plot displaying gap junction data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    method : str, optional\n        Method for computing gap junction statistics. Options: 'convergence', 'percent'.\n        Default is 'convergence'.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If config is not defined, sources/targets are not defined, or method is invalid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; gap_junction_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='convergence'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    if method != \"convergence\" and method != \"percent\":\n        raise Exception(\"type must be 'convergence' or 'percent'\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    syn_info, data, source_labels, target_labels = util.gap_junction_connections(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        method=method,\n    )\n\n    def filter_rows(\n        syn_info: np.ndarray,\n        data: np.ndarray,\n        source_labels: List,\n        target_labels: List,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, List]:\n        \"\"\"\n        Filters out rows in a connectivity matrix that contain only NaN or zero values.\n\n        This function is used to clean up connection matrices by removing rows that have\n        no meaningful data, which helps create more informative visualizations of network connectivity.\n\n        Parameters\n        ----------\n        syn_info : np.ndarray\n            Array containing synaptic information corresponding to the data matrix.\n        data : np.ndarray\n            2D matrix containing connectivity data with rows representing sources\n            and columns representing targets.\n        source_labels : list\n            List of labels for the source populations corresponding to rows in the data matrix.\n        target_labels : list\n            List of labels for the target populations corresponding to columns in the data matrix.\n\n        Returns\n        -------\n        tuple\n            A tuple containing (syn_info, data, source_labels, target_labels) with invalid rows removed.\n        \"\"\"\n        # Identify rows with all NaN or all zeros\n        valid_rows = ~np.all(np.isnan(data), axis=1) &amp; ~np.all(data == 0, axis=1)\n\n        # Filter rows based on valid_rows mask\n        new_syn_info = syn_info[valid_rows]\n        new_data = data[valid_rows]\n        new_source_labels = np.array(source_labels)[valid_rows]\n\n        return new_syn_info, new_data, new_source_labels, target_labels\n\n    def filter_rows_and_columns(\n        syn_info: np.ndarray,\n        data: np.ndarray,\n        source_labels: List,\n        target_labels: List,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, List, List]:\n        \"\"\"\n        Filters out both rows and columns in a connectivity matrix that contain only NaN or zero values.\n\n        This function performs a two-step filtering process: first removing rows with no data,\n        then transposing the matrix and removing columns with no data (by treating them as rows).\n        This creates a cleaner, more informative connectivity matrix visualization.\n\n        Parameters\n        ----------\n        syn_info : np.ndarray\n            Array containing synaptic information corresponding to the data matrix.\n        data : np.ndarray\n            2D matrix containing connectivity data with rows representing sources\n            and columns representing targets.\n        source_labels : list\n            List of labels for the source populations corresponding to rows in the data matrix.\n        target_labels : list\n            List of labels for the target populations corresponding to columns in the data matrix.\n\n        Returns\n        -------\n        tuple\n            A tuple containing (syn_info, data, source_labels, target_labels) with both\n            invalid rows and columns removed.\n        \"\"\"\n        # Filter rows first\n        syn_info, data, source_labels, target_labels = filter_rows(\n            syn_info, data, source_labels, target_labels\n        )\n\n        # Transpose data to filter columns\n        transposed_syn_info = np.transpose(syn_info)\n        transposed_data = np.transpose(data)\n        transposed_source_labels = target_labels\n        transposed_target_labels = source_labels\n\n        # Filter columns (by treating them as rows in transposed data)\n        (\n            transposed_syn_info,\n            transposed_data,\n            transposed_source_labels,\n            transposed_target_labels,\n        ) = filter_rows(\n            transposed_syn_info, transposed_data, transposed_source_labels, transposed_target_labels\n        )\n\n        # Transpose back to original orientation\n        filtered_syn_info = np.transpose(transposed_syn_info)\n        filtered_data = np.transpose(transposed_data)\n        filtered_source_labels = transposed_target_labels  # Back to original source_labels\n        filtered_target_labels = transposed_source_labels  # Back to original target_labels\n\n        return filtered_syn_info, filtered_data, filtered_source_labels, filtered_target_labels\n\n    syn_info, data, source_labels, target_labels = filter_rows_and_columns(\n        syn_info, data, source_labels, target_labels\n    )\n\n    if title is None or title == \"\":\n        title = \"Gap Junction\"\n        if method == \"convergence\":\n            title += \" Syn Convergence\"\n        elif method == \"percent\":\n            title += \" Percent Connectivity\"\n    plot_connection_info(syn_info, data, source_labels, target_labels, title, save_file=save_file)\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.connection_histogram","title":"<code>bmtool.bmplot.connections.connection_histogram(config, nodes=None, edges=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=True, synaptic_info='0', source_cell=None, target_cell=None, include_gap=True)</code>","text":"<p>Generates histogram of the number of connections individual cells receive from another population.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot as sources.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot as targets.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter by.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter by.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not prepended to sid or tid. Default is True.</p> <code>True</code> <code>synaptic_info</code> <code>str</code> <p>Type of synaptic information to display. Default is '0'.</p> <code>'0'</code> <code>source_cell</code> <code>str</code> <p>Specific source cell type to plot connections from.</p> <code>None</code> <code>target_cell</code> <code>str</code> <p>Specific target cell type to plot connections onto.</p> <code>None</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def connection_histogram(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = True,\n    synaptic_info: str = \"0\",\n    source_cell: Optional[str] = None,\n    target_cell: Optional[str] = None,\n    include_gap: bool = True,\n) -&gt; None:\n    \"\"\"\n    Generates histogram of the number of connections individual cells receive from another population.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot as sources.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot as targets.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter by.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter by.\n    no_prepend_pop : bool, optional\n        If True, population name is not prepended to sid or tid. Default is True.\n    synaptic_info : str, optional\n        Type of synaptic information to display. Default is '0'.\n    source_cell : str, optional\n        Specific source cell type to plot connections from.\n    target_cell : str, optional\n        Specific target cell type to plot connections onto.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    sources_list = sources.split(\",\") if sources else []\n    targets_list = targets.split(\",\") if targets else []\n    if sids:\n        sids_list = sids.split(\",\")\n    else:\n        sids_list = []\n    if tids:\n        tids_list = tids.split(\",\")\n    else:\n        tids_list = []\n\n    def connection_pair_histogram(**kwargs: Dict) -&gt; None:\n        \"\"\"\n        Creates a histogram showing the distribution of connection counts between specific cell types.\n\n        This function is designed to be used with the relation_matrix utility and will only\n        create histograms for the specified source and target cell types.\n\n        Parameters\n        ----------\n        kwargs : dict\n            Dictionary containing edge data and filtering information.\n            - edges: DataFrame containing edge information\n            - sid: Column name for source ID type in the edges DataFrame\n            - tid: Column name for target ID type in the edges DataFrame\n            - source_id: Value to filter edges by source ID type\n            - target_id: Value to filter edges by target ID type\n\n        Returns\n        -------\n        None\n        \"\"\"\n        edges_data = kwargs[\"edges\"]\n        source_id_type = kwargs[\"sid\"]\n        target_id_type = kwargs[\"tid\"]\n        source_id = kwargs[\"source_id\"]\n        target_id = kwargs[\"target_id\"]\n        if source_id == source_cell and target_id == target_cell:\n            temp = edges_data[\n                (edges_data[source_id_type] == source_id) &amp; (edges_data[target_id_type] == target_id)\n            ]\n            if not include_gap:\n                gap_col = temp[\"is_gap_junction\"].fillna(False).astype(bool)\n                temp = temp[~gap_col]\n            node_pairs = temp.groupby(\"target_node_id\")[\"source_node_id\"].count()\n            try:\n                conn_mean = statistics.mean(node_pairs.values)\n                conn_std = statistics.stdev(node_pairs.values)\n                conn_median = statistics.median(node_pairs.values)\n                label = \"mean {:.2f} std {:.2f} median {:.2f}\".format(\n                    conn_mean, conn_std, conn_median\n                )\n            except (statistics.StatisticsError, ValueError):  # lazy fix for std not calculated with 1 node\n                conn_mean = statistics.mean(node_pairs.values)\n                conn_median = statistics.median(node_pairs.values)\n                label = \"mean {:.2f} median {:.2f}\".format(conn_mean, conn_median)\n            plt.hist(node_pairs.values, density=False, bins=\"auto\", stacked=True, label=label)\n            plt.legend()\n            plt.xlabel(\"# of conns from {} to {}\".format(source_cell, target_cell))\n            plt.ylabel(\"# of cells\")\n            plt.show()\n        else:  # dont care about other cell pairs so pass\n            pass\n\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    util.relation_matrix(\n        config,\n        nodes,\n        edges,\n        sources_list,\n        targets_list,\n        sids_list,\n        tids_list,\n        not no_prepend_pop,\n        relation_func=connection_pair_histogram,\n        synaptic_info=synaptic_info,\n    )\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.connection_distance","title":"<code>bmtool.bmplot.connections.connection_distance(config, sources, targets, source_cell_id, target_id_type, ignore_z=False)</code>","text":"<p>Plots the 3D spatial distribution of target nodes relative to a source node and a histogram of distances from the source node to each target node.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>sources</code> <code>str</code> <p>Network name(s) to plot as sources.</p> required <code>targets</code> <code>str</code> <p>Network name(s) to plot as targets.</p> required <code>source_cell_id</code> <code>int</code> <p>ID of the source cell for calculating distances to target nodes.</p> required <code>target_id_type</code> <code>str</code> <p>String to filter target nodes based off the target_query.</p> required <code>ignore_z</code> <code>bool</code> <p>If True, ignore Z axis when calculating distance. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; connection_distance(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     source_cell_id=0,\n...     target_id_type='LN',\n...     ignore_z=False\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def connection_distance(\n    config: str,\n    sources: str,\n    targets: str,\n    source_cell_id: int,\n    target_id_type: str,\n    ignore_z: bool = False,\n) -&gt; None:\n    \"\"\"\n    Plots the 3D spatial distribution of target nodes relative to a source node\n    and a histogram of distances from the source node to each target node.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    sources : str\n        Network name(s) to plot as sources.\n    targets : str\n        Network name(s) to plot as targets.\n    source_cell_id : int\n        ID of the source cell for calculating distances to target nodes.\n    target_id_type : str\n        String to filter target nodes based off the target_query.\n    ignore_z : bool, optional\n        If True, ignore Z axis when calculating distance. Default is False.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; connection_distance(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     source_cell_id=0,\n    ...     target_id_type='LN',\n    ...     ignore_z=False\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    # if source != target:\n    # raise Exception(\"Code is setup for source and target to be the same! Look at source code for function to add feature\")\n\n    # Load nodes and edges based on config file\n    nodes, edges = util.load_nodes_edges_from_config(config)\n\n    edge_network = sources + \"_to_\" + targets\n    node_network = sources\n\n    # Filter edges to obtain connections originating from the source node\n    edge = edges[edge_network]\n    edge = edge[edge[\"source_node_id\"] == source_cell_id]\n    if target_id_type:\n        edge = edge[edge[\"target_query\"].str.contains(target_id_type, na=False)]\n\n    target_node_ids = edge[\"target_node_id\"]\n\n    # Filter nodes to obtain only the target and source nodes\n    node = nodes[node_network]\n    target_nodes = node.loc[node.index.isin(target_node_ids)]\n    source_node = node.loc[node.index == source_cell_id]\n\n    # Calculate distances between source node and each target node\n    if ignore_z:\n        target_positions = target_nodes[[\"pos_x\", \"pos_y\"]].values\n        source_position = np.array(\n            [source_node[\"pos_x\"], source_node[\"pos_y\"]]\n        ).ravel()  # Ensure 1D shape\n    else:\n        target_positions = target_nodes[[\"pos_x\", \"pos_y\", \"pos_z\"]].values\n        source_position = np.array(\n            [source_node[\"pos_x\"], source_node[\"pos_y\"], source_node[\"pos_z\"]]\n        ).ravel()  # Ensure 1D shape\n    distances = np.linalg.norm(target_positions - source_position, axis=1)\n\n    # Plot positions of source and target nodes in 3D space or 2D\n    if ignore_z:\n        fig = plt.figure(figsize=(8, 6))\n        ax = fig.add_subplot(111)\n        ax.scatter(target_nodes[\"pos_x\"], target_nodes[\"pos_y\"], c=\"blue\", label=\"target cells\")\n        ax.scatter(source_node[\"pos_x\"], source_node[\"pos_y\"], c=\"red\", label=\"source cell\")\n    else:\n        fig = plt.figure(figsize=(8, 6))\n        ax = fig.add_subplot(111, projection=\"3d\")\n        ax.scatter(\n            target_nodes[\"pos_x\"],\n            target_nodes[\"pos_y\"],\n            target_nodes[\"pos_z\"],\n            c=\"blue\",\n            label=\"target cells\",\n        )\n        ax.scatter(\n            source_node[\"pos_x\"],\n            source_node[\"pos_y\"],\n            source_node[\"pos_z\"],\n            c=\"red\",\n            label=\"source cell\",\n        )\n\n    # Optional: Add text annotations for distances\n    # for i, distance in enumerate(distances):\n    #     ax.text(target_nodes['pos_x'].iloc[i], target_nodes['pos_y'].iloc[i], target_nodes['pos_z'].iloc[i],\n    #             f'{distance:.2f}', color='black', fontsize=8, ha='center')\n\n    plt.legend()\n    plt.show()\n\n    # Plot distances in a separate 2D plot\n    plt.figure(figsize=(8, 6))\n    plt.hist(distances, bins=20, color=\"blue\", edgecolor=\"black\")\n    plt.xlabel(\"Distance\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Distance from Source Node to Each Target Node\")\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.edge_histogram_matrix","title":"<code>bmtool.bmplot.connections.edge_histogram_matrix(config, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=None, edge_property=None, time=None, time_compare=None, report=None, title=None, save_file=None)</code>","text":"<p>Generates a matrix of histograms showing the distribution of edge properties between populations.</p> <p>This function creates a grid of histograms where each cell represents the distribution of a specific edge property between source and target populations.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>sources</code> <code>str</code> <p>Comma-separated list of source network names.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated list of target network names.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated list of source node identifiers to filter by.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated list of target node identifiers to filter by.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population names are not prepended to node identifiers.</p> <code>None</code> <code>edge_property</code> <code>str</code> <p>The edge property to analyze (e.g., 'syn_weight', 'delay').</p> <code>None</code> <code>time</code> <code>int</code> <p>Time point to analyze from a time series report.</p> <code>None</code> <code>time_compare</code> <code>int</code> <p>Second time point for comparison with time.</p> <code>None</code> <code>report</code> <code>str</code> <p>Name of the report to analyze.</p> <code>None</code> <code>title</code> <code>str</code> <p>Custom title for the plot.</p> <code>None</code> <code>save_file</code> <code>str</code> <p>Path to save the generated plot.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; edge_histogram_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     edge_property='syn_weight'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def edge_histogram_matrix(\n    config: str,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: Optional[bool] = None,\n    edge_property: Optional[str] = None,\n    time: Optional[int] = None,\n    time_compare: Optional[int] = None,\n    report: Optional[str] = None,\n    title: Optional[str] = None,\n    save_file: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Generates a matrix of histograms showing the distribution of edge properties between populations.\n\n    This function creates a grid of histograms where each cell represents the distribution\n    of a specific edge property between source and target populations.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    sources : str, optional\n        Comma-separated list of source network names.\n    targets : str, optional\n        Comma-separated list of target network names.\n    sids : str, optional\n        Comma-separated list of source node identifiers to filter by.\n    tids : str, optional\n        Comma-separated list of target node identifiers to filter by.\n    no_prepend_pop : bool, optional\n        If True, population names are not prepended to node identifiers.\n    edge_property : str, optional\n        The edge property to analyze (e.g., 'syn_weight', 'delay').\n    time : int, optional\n        Time point to analyze from a time series report.\n    time_compare : int, optional\n        Second time point for comparison with time.\n    report : str, optional\n        Name of the report to analyze.\n    title : str, optional\n        Custom title for the plot.\n    save_file : str, optional\n        Path to save the generated plot.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; edge_histogram_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     edge_property='syn_weight'\n    ... )\n    \"\"\"\n\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    if time_compare:\n        time_compare = int(time_compare)\n\n    data, source_labels, target_labels = util.edge_property_matrix(\n        edge_property,\n        nodes=None,\n        edges=None,\n        config=config,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        report=report,\n        time=time,\n        time_compare=time_compare,\n    )\n\n    # Fantastic resource\n    # https://stackoverflow.com/questions/7941207/is-there-a-function-to-make-scatterplot-matrices-in-matplotlib\n    num_src, num_tar = data.shape\n    fig, axes = plt.subplots(nrows=num_src, ncols=num_tar, figsize=(12, 12))\n    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n\n    for x in range(num_src):\n        for y in range(num_tar):\n            axes[x, y].hist(data[x][y])\n\n            if x == num_src - 1:\n                axes[x, y].set_xlabel(target_labels[y])\n            if y == 0:\n                axes[x, y].set_ylabel(source_labels[x])\n\n    tt = edge_property + \" Histogram Matrix\"\n    if title:\n        tt = title\n    st = fig.suptitle(tt, fontsize=14)\n    fig.text(0.5, 0.04, \"Target\", ha=\"center\")\n    fig.text(0.04, 0.5, \"Source\", va=\"center\", rotation=\"vertical\")\n    plt.draw()\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.distance_delay_plot","title":"<code>bmtool.bmplot.connections.distance_delay_plot(simulation_config, source, target, group_by, sid, tid)</code>","text":"<p>Plots the relationship between the distance and delay of connections between nodes in a neural network.</p> <p>This function loads node and edge data from a simulation configuration file, filters nodes by population, identifies connections (edges) between source and target node populations, calculates the Euclidean distance between connected nodes, and plots the delay as a function of distance.</p> <p>Parameters:</p> Name Type Description Default <code>simulation_config</code> <code>str</code> <p>Path to the simulation config file.</p> required <code>source</code> <code>str</code> <p>The name of the source population in the edge data.</p> required <code>target</code> <code>str</code> <p>The name of the target population in the edge data.</p> required <code>group_by</code> <code>str</code> <p>Column name to group nodes by (e.g., population name).</p> required <code>sid</code> <code>str</code> <p>Identifier for the source group (e.g., 'PN').</p> required <code>tid</code> <code>str</code> <p>Identifier for the target group (e.g., 'PN').</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; distance_delay_plot(\n...     'config.json',\n...     'cortex',\n...     'cortex',\n...     'node_type_id',\n...     'E',\n...     'E'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def distance_delay_plot(\n    simulation_config: str, source: str, target: str, group_by: str, sid: str, tid: str\n) -&gt; None:\n    \"\"\"\n    Plots the relationship between the distance and delay of connections between nodes in a neural network.\n\n    This function loads node and edge data from a simulation configuration file, filters nodes by population,\n    identifies connections (edges) between source and target node populations, calculates the Euclidean distance\n    between connected nodes, and plots the delay as a function of distance.\n\n    Parameters\n    ----------\n    simulation_config : str\n        Path to the simulation config file.\n    source : str\n        The name of the source population in the edge data.\n    target : str\n        The name of the target population in the edge data.\n    group_by : str\n        Column name to group nodes by (e.g., population name).\n    sid : str\n        Identifier for the source group (e.g., 'PN').\n    tid : str\n        Identifier for the target group (e.g., 'PN').\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; distance_delay_plot(\n    ...     'config.json',\n    ...     'cortex',\n    ...     'cortex',\n    ...     'node_type_id',\n    ...     'E',\n    ...     'E'\n    ... )\n    \"\"\"\n    nodes, edges = util.load_nodes_edges_from_config(simulation_config)\n    nodes = nodes[target]\n    # node id is index of nodes df\n    node_id_source = nodes[nodes[group_by] == sid].index\n    node_id_target = nodes[nodes[group_by] == tid].index\n\n    edges = edges[f\"{source}_to_{target}\"]\n    edges = edges[\n        edges[\"source_node_id\"].isin(node_id_source) &amp; edges[\"target_node_id\"].isin(node_id_target)\n    ]\n\n    stuff_to_plot = []\n    for index, row in edges.iterrows():\n        try:\n            source_node = row[\"source_node_id\"]\n            target_node = row[\"target_node_id\"]\n\n            source_pos = nodes.loc[[source_node], [\"pos_x\", \"pos_y\", \"pos_z\"]]\n            target_pos = nodes.loc[[target_node], [\"pos_x\", \"pos_y\", \"pos_z\"]]\n\n            distance = np.linalg.norm(source_pos.values - target_pos.values)\n\n            delay = row[\"delay\"]  # This line may raise KeyError\n            stuff_to_plot.append([distance, delay])\n\n        except KeyError as e:\n            print(f\"KeyError: Missing key {e} in either edge properties or node positions.\")\n        except IndexError as e:\n            print(f\"IndexError: Node ID {source_node} or {target_node} not found in nodes.\")\n        except Exception as e:\n            print(f\"Unexpected error at edge index {index}: {e}\")\n\n    plt.scatter([x[0] for x in stuff_to_plot], [x[1] for x in stuff_to_plot])\n    plt.xlabel(\"Distance\")\n    plt.ylabel(\"Delay\")\n    plt.title(f\"Distance vs Delay for edge between {sid} and {tid}\")\n    plt.show()\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.plot_synapse_location","title":"<code>bmtool.bmplot.connections.plot_synapse_location(config, source, target, sids, tids, syn_feature='afferent_section_id')</code>","text":"<p>Generates a connectivity matrix showing synaptic distribution across different cell sections.</p> <p>Note: Excludes gap junctions since they don't have an afferent id stored in the h5 file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to BMTK config file.</p> required <code>source</code> <code>str</code> <p>The source BMTK network name.</p> required <code>target</code> <code>str</code> <p>The target BMTK network name.</p> required <code>sids</code> <code>str</code> <p>Column name in nodes file containing source population identifiers.</p> required <code>tids</code> <code>str</code> <p>Column name in nodes file containing target population identifiers.</p> required <code>syn_feature</code> <code>str</code> <p>Synaptic feature to analyze. Default is 'afferent_section_id'. Options: 'afferent_section_id' or 'afferent_section_pos'.</p> <code>'afferent_section_id'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(matplotlib.figure.Figure, matplotlib.axes.Axes) containing the plot.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required parameters are missing or invalid.</p> <code>RuntimeError</code> <p>If template loading or cell instantiation fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, ax = plot_synapse_location(\n...     config='config.json',\n...     source='LGN',\n...     target='cortex',\n...     sids='node_type_id',\n...     tids='node_type_id'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def plot_synapse_location(\n    config: str,\n    source: str,\n    target: str,\n    sids: str,\n    tids: str,\n    syn_feature: str = \"afferent_section_id\",\n) -&gt; Tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Generates a connectivity matrix showing synaptic distribution across different cell sections.\n\n    Note: Excludes gap junctions since they don't have an afferent id stored in the h5 file.\n\n    Parameters\n    ----------\n    config : str\n        Path to BMTK config file.\n    source : str\n        The source BMTK network name.\n    target : str\n        The target BMTK network name.\n    sids : str\n        Column name in nodes file containing source population identifiers.\n    tids : str\n        Column name in nodes file containing target population identifiers.\n    syn_feature : str, optional\n        Synaptic feature to analyze. Default is 'afferent_section_id'.\n        Options: 'afferent_section_id' or 'afferent_section_pos'.\n\n    Returns\n    -------\n    tuple\n        (matplotlib.figure.Figure, matplotlib.axes.Axes) containing the plot.\n\n    Raises\n    ------\n    ValueError\n        If required parameters are missing or invalid.\n    RuntimeError\n        If template loading or cell instantiation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = plot_synapse_location(\n    ...     config='config.json',\n    ...     source='LGN',\n    ...     target='cortex',\n    ...     sids='node_type_id',\n    ...     tids='node_type_id'\n    ... )\n    \"\"\"\n    # Validate inputs\n    if not all([config, source, target, sids, tids]):\n        raise ValueError(\n            \"Missing required parameters: config, source, target, sids, and tids must be provided\"\n        )\n\n    # Fix the validation logic - it was using 'or' instead of 'and'\n    #if syn_feature not in [\"afferent_section_id\", \"afferent_section_pos\"]:\n    #    raise ValueError(\"Currently only syn features supported are afferent_section_id or afferent_section_pos\")\n\n    try:\n        # Load mechanisms and template\n        util.load_templates_from_config(config)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load templates from config: {str(e)}\")\n\n    try:\n        # Load node and edge data\n        nodes, edges = util.load_nodes_edges_from_config(config)\n        if source not in nodes or f\"{source}_to_{target}\" not in edges:\n            raise ValueError(f\"Source '{source}' or target '{target}' networks not found in data\")\n\n        target_nodes = nodes[target]\n        source_nodes = nodes[source]\n        edges = edges[f\"{source}_to_{target}\"]\n\n        # Find edges with NaN values in the specified feature\n        nan_edges = edges[edges[syn_feature].isna()]\n        # Print information about removed edges\n        if not nan_edges.empty:\n            unique_indices = sorted(list(set(nan_edges.index.tolist())))\n            print(f\"Removing {len(nan_edges)} edges with missing {syn_feature}\")\n            print(f\"Unique indices removed: {unique_indices}\")\n\n        # Filter out edges with NaN values in the specified feature\n        edges = edges[edges[syn_feature].notna()]\n\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load nodes and edges: {str(e)}\")\n\n    # Map identifiers while checking for missing values\n    edges[\"target_model_template\"] = edges[\"target_node_id\"].map(target_nodes[\"model_template\"])\n    edges[\"target_pop_name\"] = edges[\"target_node_id\"].map(target_nodes[tids])\n    edges[\"source_pop_name\"] = edges[\"source_node_id\"].map(source_nodes[sids])\n\n    if edges[\"target_model_template\"].isnull().any():\n        print(\"Warning: Some target nodes missing model template\")\n    if edges[\"target_pop_name\"].isnull().any():\n        print(\"Warning: Some target nodes missing population name\")\n    if edges[\"source_pop_name\"].isnull().any():\n        print(\"Warning: Some source nodes missing population name\")\n\n    # Get unique populations\n    source_pops = edges[\"source_pop_name\"].unique()\n    target_pops = edges[\"target_pop_name\"].unique()\n\n    # Initialize matrices\n    num_connections = np.zeros((len(source_pops), len(target_pops)))\n    text_data = np.empty((len(source_pops), len(target_pops)), dtype=object)\n\n    # Create mappings for indices\n    source_pop_to_idx = {pop: idx for idx, pop in enumerate(source_pops)}\n    target_pop_to_idx = {pop: idx for idx, pop in enumerate(target_pops)}\n\n    # Cache for section mappings to avoid recreating cells\n    section_mappings = {}\n\n    # Calculate connectivity statistics\n    for source_pop in source_pops:\n        for target_pop in target_pops:\n            # Filter edges for this source-target pair\n            filtered_edges = edges[\n                (edges[\"source_pop_name\"] == source_pop) &amp; (edges[\"target_pop_name\"] == target_pop)\n            ]\n\n            source_idx = source_pop_to_idx[source_pop]\n            target_idx = target_pop_to_idx[target_pop]\n\n            if len(filtered_edges) == 0:\n                num_connections[source_idx, target_idx] = 0\n                text_data[source_idx, target_idx] = \"No connections\"\n                continue\n\n            total_connections = len(filtered_edges)\n            target_model_template = filtered_edges[\"target_model_template\"].iloc[0]\n\n            try:\n                # Get or create section mapping for this model\n                if target_model_template not in section_mappings:\n                    cell_class_name = (\n                        target_model_template.split(\":\")[1]\n                        if \":\" in target_model_template\n                        else target_model_template\n                    )\n                    cell = getattr(h, cell_class_name)()\n\n                    # Create section mapping\n                    section_mapping = {}\n                    for idx, sec in enumerate(cell.all):\n                        section_mapping[idx] = sec.name().split(\".\")[-1]  # Clean name\n                    section_mappings[target_model_template] = section_mapping\n\n                section_mapping = section_mappings[target_model_template]\n\n                # Calculate section distribution\n                section_counts = filtered_edges[syn_feature].value_counts()\n                section_percentages = (section_counts / total_connections * 100).round(1)\n\n                # Format section distribution text - show all sections\n                section_display = []\n                for section_id, percentage in section_percentages.items():\n                    section_name = section_mapping.get(section_id, f\"sec_{section_id}\")\n                    section_display.append(f\"{section_name}:{percentage}%\")\n\n\n                num_connections[source_idx, target_idx] = total_connections\n                text_data[source_idx, target_idx] = \"\\n\".join(section_display)\n\n            except Exception as e:\n                print(f\"Warning: Error processing {target_model_template}: {str(e)}\")\n                num_connections[source_idx, target_idx] = total_connections\n                text_data[source_idx, target_idx] = \"Feature info N/A\"\n\n    # Create the plot\n    title = f\"Synaptic Distribution by {syn_feature.replace('_', ' ').title()}: {source} to {target}\"\n    fig, ax = plot_connection_info(\n        text=text_data,\n        num=num_connections,\n        source_labels=list(source_pops),\n        target_labels=list(target_pops),\n        title=title,\n        syn_info=\"1\",\n    )\n    if is_notebook():\n        plt.show()\n    else:\n        return fig, ax\n</code></pre>"},{"location":"api/bmplot/entrainment/","title":"Entrainment Plotting API","text":""},{"location":"api/bmplot/entrainment/#bmtool.bmplot.entrainment.plot_spike_power_correlation","title":"<code>bmtool.bmplot.entrainment.plot_spike_power_correlation(spike_df, lfp_data, firing_quantile, fs, pop_names, filter_method='wavelet', bandwidth=2.0, lowcut=None, highcut=None, freq_range=(10, 100), freq_step=5, type_name='raw', time_windows=None, error_type='ci')</code>","text":"<p>Calculate and plot correlation between population spike rates and LFP power across frequencies. Supports both single-signal and trial-based analysis with error bars.</p> <p>Parameters:</p> Name Type Description Default <code>spike_df</code> <code>DataFrame</code> <p>DataFrame containing spike data with columns 'timestamps', 'node_ids', and 'pop_name'.</p> required <code>lfp_data</code> <code>DataArray</code> <p>LFP data</p> required <code>firing_quantile</code> <code>float</code> <p>Upper quantile threshold for selecting high-firing cells (e.g., 0.8 for top 20%)</p> required <code>fs</code> <code>float</code> <p>Sampling frequency</p> required <code>pop_names</code> <code>List[str]</code> <p>List of population names to analyze</p> required <code>filter_method</code> <code>str</code> <p>Filtering method to use, either 'wavelet' or 'butter' (default: 'wavelet')</p> <code>'wavelet'</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2.0</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>freq_range</code> <code>Tuple[float, float]</code> <p>Min and max frequency to analyze (default: (10, 100))</p> <code>(10, 100)</code> <code>freq_step</code> <code>float</code> <p>Step size for frequency analysis (default: 5)</p> <code>5</code> <code>type_name</code> <code>str</code> <p>Which type of spike rate to use if 'type' dimension exists (default: 'raw')</p> <code>'raw'</code> <code>time_windows</code> <code>List[Tuple[float, float]]</code> <p>List of (start, end) time tuples for trial-based analysis. If None, analyze entire signal</p> <code>None</code> <code>error_type</code> <code>str</code> <p>Type of error bars to plot: \"ci\" for 95% confidence interval, \"sem\" for standard error, \"std\" for standard deviation</p> <code>'ci'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The figure containing the correlation plot</p> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def plot_spike_power_correlation(\n    spike_df: pd.DataFrame,\n    lfp_data: xr.DataArray,\n    firing_quantile: float,\n    fs: float,\n    pop_names: List[str],\n    filter_method: str = \"wavelet\",\n    bandwidth: float = 2.0,\n    lowcut: Optional[float] = None,\n    highcut: Optional[float] = None,\n    freq_range: Tuple[float, float] = (10, 100),\n    freq_step: float = 5,\n    type_name: str = \"raw\",\n    time_windows: Optional[List[Tuple[float, float]]] = None,\n    error_type: str = \"ci\",\n):\n    \"\"\"\n    Calculate and plot correlation between population spike rates and LFP power across frequencies.\n    Supports both single-signal and trial-based analysis with error bars.\n\n    Parameters\n    ----------\n    spike_df : pd.DataFrame\n        DataFrame containing spike data with columns 'timestamps', 'node_ids', and 'pop_name'.\n    lfp_data : xr.DataArray\n        LFP data\n    firing_quantile : float\n        Upper quantile threshold for selecting high-firing cells (e.g., 0.8 for top 20%)\n    fs : float\n        Sampling frequency\n    pop_names : List[str]\n        List of population names to analyze\n    filter_method : str, optional\n        Filtering method to use, either 'wavelet' or 'butter' (default: 'wavelet')\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    freq_range : Tuple[float, float], optional\n        Min and max frequency to analyze (default: (10, 100))\n    freq_step : float, optional\n        Step size for frequency analysis (default: 5)\n    type_name : str, optional\n        Which type of spike rate to use if 'type' dimension exists (default: 'raw')\n    time_windows : List[Tuple[float, float]], optional\n        List of (start, end) time tuples for trial-based analysis. If None, analyze entire signal\n    error_type : str, optional\n        Type of error bars to plot: \"ci\" for 95% confidence interval, \"sem\" for standard error, \"std\" for standard deviation\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The figure containing the correlation plot\n    \"\"\"\n\n    if not (0 &lt;= firing_quantile &lt; 1):\n        raise ValueError(\"firing_quantile must be between 0 and 1\")\n\n    if error_type not in [\"ci\", \"sem\", \"std\"]:\n        raise ValueError(\n            \"error_type must be 'ci' for confidence interval, 'sem' for standard error, or 'std' for standard deviation\"\n        )\n\n    # Setup\n    is_trial_based = time_windows is not None\n\n    # Convert spike_df to spike rate with trial-based filtering of high firing cells\n    if is_trial_based:\n        # Initialize storage for trial-based spike rates\n        trial_rates = []\n\n        for start_time, end_time in time_windows:\n            # Get spikes for this trial\n            trial_spikes = spike_df[\n                (spike_df[\"timestamps\"] &gt;= start_time) &amp; (spike_df[\"timestamps\"] &lt;= end_time)\n            ].copy()\n\n            # Filter for high firing cells within this trial\n            trial_spikes = bmspikes.find_highest_firing_cells(\n                trial_spikes, upper_quantile=firing_quantile\n            )\n            # Calculate rate for this trial's filtered spikes\n            trial_rate = bmspikes.get_population_spike_rate(\n                trial_spikes, fs=fs, t_start=start_time, t_stop=end_time\n            )\n            trial_rates.append(trial_rate)\n\n        # Combine all trial rates\n        spike_rate = xr.concat(trial_rates, dim=\"trial\")\n    else:\n        # For non-trial analysis, proceed as before\n        spike_df = bmspikes.find_highest_firing_cells(spike_df, upper_quantile=firing_quantile)\n        spike_rate = bmspikes.get_population_spike_rate(spike_df)\n\n    # Setup frequencies for analysis\n    frequencies = np.arange(freq_range[0], freq_range[1] + 1, freq_step)\n\n    # Pre-calculate LFP power for all frequencies\n    power_by_freq = {}\n    for freq in frequencies:\n        power_by_freq[freq] = get_lfp_power(\n            lfp_data, freq, fs, filter_method, lowcut=lowcut, highcut=highcut, bandwidth=bandwidth\n        )\n\n    # Calculate correlations\n    results = {}\n    for pop in pop_names:\n        pop_spike_rate = spike_rate.sel(population=pop, type=type_name)\n        results[pop] = {}\n\n        for freq in frequencies:\n            lfp_power = power_by_freq[freq]\n\n            if not is_trial_based:\n                # Single signal analysis\n                if len(pop_spike_rate) != len(lfp_power):\n                    print(f\"Warning: Length mismatch for {pop} at {freq} Hz\")\n                    continue\n\n                corr, p_val = stats.spearmanr(pop_spike_rate, lfp_power)\n                results[pop][freq] = {\n                    \"correlation\": corr,\n                    \"p_value\": p_val,\n                }\n            else:\n                # Trial-based analysis using pre-filtered trial rates\n                trial_correlations = []\n\n                for trial_idx in range(len(time_windows)):\n                    # Get time window first\n                    start_time, end_time = time_windows[trial_idx]\n\n                    # Get the pre-filtered spike rate for this trial\n                    trial_spike_rate = pop_spike_rate.sel(trial=trial_idx)\n\n                    # Get corresponding LFP power for this trial window\n                    trial_lfp_power = lfp_power.sel(time=slice(start_time, end_time))\n\n                    # Ensure both signals have same time points\n                    common_times = np.intersect1d(trial_spike_rate.time, trial_lfp_power.time)\n\n                    if len(common_times) &gt; 0:\n                        trial_sr = trial_spike_rate.sel(time=common_times).values\n                        trial_lfp = trial_lfp_power.sel(time=common_times).values\n\n                        if (\n                            len(trial_sr) &gt; 1 and len(trial_lfp) &gt; 1\n                        ):  # Need at least 2 points for correlation\n                            corr, _ = stats.spearmanr(trial_sr, trial_lfp)\n                            if not np.isnan(corr):\n                                trial_correlations.append(corr)\n\n                # Calculate trial statistics\n                if len(trial_correlations) &gt; 0:\n                    trial_correlations = np.array(trial_correlations)\n                    mean_corr = np.mean(trial_correlations)\n\n                    if len(trial_correlations) &gt; 1:\n                        if error_type == \"ci\":\n                            # Calculate 95% confidence interval using t-distribution\n                            df = len(trial_correlations) - 1\n                            sem = stats.sem(trial_correlations)\n                            t_critical = stats.t.ppf(0.975, df)  # 95% CI, two-tailed\n                            error_val = t_critical * sem\n                            error_lower = mean_corr - error_val\n                            error_upper = mean_corr + error_val\n                        elif error_type == \"sem\":\n                            # Calculate standard error of the mean\n                            sem = stats.sem(trial_correlations)\n                            error_lower = mean_corr - sem\n                            error_upper = mean_corr + sem\n                        elif error_type == \"std\":\n                            # Calculate standard deviation\n                            std = np.std(trial_correlations, ddof=1)\n                            error_lower = mean_corr - std\n                            error_upper = mean_corr + std\n                    else:\n                        error_lower = error_upper = mean_corr\n\n                    results[pop][freq] = {\n                        \"correlation\": mean_corr,\n                        \"error_lower\": error_lower,\n                        \"error_upper\": error_upper,\n                        \"n_trials\": len(trial_correlations),\n                        \"trial_correlations\": trial_correlations,\n                    }\n                else:\n                    # No valid trials\n                    results[pop][freq] = {\n                        \"correlation\": np.nan,\n                        \"error_lower\": np.nan,\n                        \"error_upper\": np.nan,\n                        \"n_trials\": 0,\n                        \"trial_correlations\": np.array([]),\n                    }\n\n    # Plotting\n    sns.set_style(\"whitegrid\")\n    fig = plt.figure(figsize=(12, 8))\n\n    for i, pop in enumerate(pop_names):\n        # Extract data for plotting\n        plot_freqs = []\n        plot_corrs = []\n        plot_ci_lower = []\n        plot_ci_upper = []\n\n        for freq in frequencies:\n            if freq in results[pop] and not np.isnan(results[pop][freq][\"correlation\"]):\n                plot_freqs.append(freq)\n                plot_corrs.append(results[pop][freq][\"correlation\"])\n\n                if is_trial_based:\n                    plot_ci_lower.append(results[pop][freq][\"error_lower\"])\n                    plot_ci_upper.append(results[pop][freq][\"error_upper\"])\n\n        if len(plot_freqs) == 0:\n            continue\n\n        # Convert to arrays\n        plot_freqs = np.array(plot_freqs)\n        plot_corrs = np.array(plot_corrs)\n\n        # Get color for this population\n        colors = plt.get_cmap(\"tab10\")\n        color = colors(i)\n\n        # Plot main line\n        plt.plot(\n            plot_freqs, plot_corrs, marker=\"o\", label=pop, linewidth=2, markersize=6, color=color\n        )\n\n        # Plot error bands for trial-based analysis\n        if is_trial_based and len(plot_ci_lower) &gt; 0:\n            plot_ci_lower = np.array(plot_ci_lower)\n            plot_ci_upper = np.array(plot_ci_upper)\n            plt.fill_between(plot_freqs, plot_ci_lower, plot_ci_upper, alpha=0.2, color=color)\n\n    # Formatting\n    plt.xlabel(\"Frequency (Hz)\", fontsize=12)\n    plt.ylabel(\"Spike Rate-Power Correlation\", fontsize=12)\n\n    # Calculate percentage for title\n    firing_percentage = round(float((1 - firing_quantile) * 100), 1)\n    if is_trial_based:\n        title = f\"Trial-averaged Spike Rate-LFP Power Correlation\\nTop {firing_percentage}% Firing Cells (95% CI)\"\n    else:\n        title = f\"Spike Rate-LFP Power Correlation\\nTop {firing_percentage}% Firing Cells\"\n\n    plt.title(title, fontsize=14)\n    plt.grid(True, alpha=0.3)\n    plt.axhline(y=0, color=\"gray\", linestyle=\"-\", alpha=0.5)\n\n    # Legend\n    # Create legend elements for each population\n    from matplotlib.lines import Line2D\n\n    colors = plt.get_cmap(\"tab10\")\n    legend_elements = [\n        Line2D([0], [0], color=colors(i), marker=\"o\", linestyle=\"-\", label=pop)\n        for i, pop in enumerate(pop_names)\n    ]\n\n    # Add error band legend element for trial-based analysis\n    if is_trial_based:\n        # Map error type to legend label\n        error_labels = {\"ci\": \"95% CI\", \"sem\": \"\u00b1SEM\", \"std\": \"\u00b11 SD\"}\n        error_label = error_labels[error_type]\n\n        legend_elements.append(\n            Line2D([0], [0], color=\"gray\", alpha=0.3, linewidth=10, label=error_label)\n        )\n\n    plt.legend(handles=legend_elements, fontsize=10, loc=\"best\")\n\n    # Axis formatting\n    if len(frequencies) &gt; 10:\n        plt.xticks(frequencies[::2])\n    else:\n        plt.xticks(frequencies)\n    plt.xlim(frequencies[0], frequencies[-1])\n\n    y_min, y_max = plt.ylim()\n    plt.ylim(min(y_min, -0.1), max(y_max, 0.1))\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/lfp/","title":"LFP/ECP Plotting API","text":""},{"location":"api/bmplot/lfp/#bmtool.bmplot.lfp.plot_spectrogram","title":"<code>bmtool.bmplot.lfp.plot_spectrogram(sxx_xarray, remove_aperiodic=None, log_power=False, plt_range=None, clr_freq_range=None, pad=0.03, ax=None)</code>","text":"<p>Plot a power spectrogram with optional aperiodic removal and frequency-based coloring.</p> <p>Parameters:</p> Name Type Description Default <code>sxx_xarray</code> <code>array - like</code> <p>Spectrogram data as an xarray DataArray with PSD values.</p> required <code>remove_aperiodic</code> <code>optional</code> <p>FOOOF model object for aperiodic subtraction. If None, raw spectrum is displayed.</p> <code>None</code> <code>log_power</code> <code>bool or str</code> <p>If True or 'dB', convert power to log scale. Default is False.</p> <code>False</code> <code>plt_range</code> <code>tuple of float</code> <p>Frequency range to display as (f_min, f_max). If None, displays full range.</p> <code>None</code> <code>clr_freq_range</code> <code>tuple of float</code> <p>Frequency range to use for determining color limits. If None, uses full range.</p> <code>None</code> <code>pad</code> <code>float</code> <p>Padding for colorbar. Default is 0.03.</p> <code>0.03</code> <code>ax</code> <code>Axes</code> <p>Axes to plot on. If None, creates a new figure and axes.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The figure object containing the spectrogram.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig = plot_spectrogram(\n...     sxx_xarray, log_power='dB',\n...     plt_range=(10, 100), clr_freq_range=(20, 50)\n... )\n</code></pre> Source code in <code>bmtool/bmplot/lfp.py</code> <pre><code>def plot_spectrogram(\n    sxx_xarray: Any,\n    remove_aperiodic: Optional[Any] = None,\n    log_power: bool = False,\n    plt_range: Optional[Tuple[float, float]] = None,\n    clr_freq_range: Optional[Tuple[float, float]] = None,\n    pad: float = 0.03,\n    ax: Optional[plt.Axes] = None,\n) -&gt; Figure:\n    \"\"\"\n    Plot a power spectrogram with optional aperiodic removal and frequency-based coloring.\n\n    Parameters\n    ----------\n    sxx_xarray : array-like\n        Spectrogram data as an xarray DataArray with PSD values.\n    remove_aperiodic : optional\n        FOOOF model object for aperiodic subtraction. If None, raw spectrum is displayed.\n    log_power : bool or str, optional\n        If True or 'dB', convert power to log scale. Default is False.\n    plt_range : tuple of float, optional\n        Frequency range to display as (f_min, f_max). If None, displays full range.\n    clr_freq_range : tuple of float, optional\n        Frequency range to use for determining color limits. If None, uses full range.\n    pad : float, optional\n        Padding for colorbar. Default is 0.03.\n    ax : matplotlib.axes.Axes, optional\n        Axes to plot on. If None, creates a new figure and axes.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The figure object containing the spectrogram.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = plot_spectrogram(\n    ...     sxx_xarray, log_power='dB',\n    ...     plt_range=(10, 100), clr_freq_range=(20, 50)\n    ... )\n    \"\"\"\n    sxx = sxx_xarray.PSD.values.copy()\n    t = sxx_xarray.time.values.copy()\n    f = sxx_xarray.frequency.values.copy()\n\n    cbar_label = \"PSD\" if remove_aperiodic is None else \"PSD Residual\"\n    if log_power:\n        with np.errstate(divide=\"ignore\"):\n            sxx = np.log10(sxx)\n        cbar_label += \" dB\" if log_power == \"dB\" else \" log(power)\"\n\n    if remove_aperiodic is not None:\n        f1_idx = 0 if f[0] else 1\n        ap_fit = gen_aperiodic(f[f1_idx:], remove_aperiodic.aperiodic_params)\n        sxx[f1_idx:, :] -= (ap_fit if log_power else 10**ap_fit)[:, None]\n        sxx[:f1_idx, :] = 0.0\n\n    if log_power == \"dB\":\n        sxx *= 10\n\n    if ax is None:\n        _, ax = plt.subplots(1, 1)\n    plt_range = np.array(f[-1]) if plt_range is None else np.array(plt_range)\n    if plt_range.size == 1:\n        plt_range = [f[0 if f[0] else 1] if log_power else 0.0, plt_range.item()]\n    f_idx = (f &gt;= plt_range[0]) &amp; (f &lt;= plt_range[1])\n    if clr_freq_range is None:\n        vmin, vmax = None, None\n    else:\n        c_idx = (f &gt;= clr_freq_range[0]) &amp; (f &lt;= clr_freq_range[1])\n        vmin, vmax = sxx[c_idx, :].min(), sxx[c_idx, :].max()\n\n    f = f[f_idx]\n    pcm = ax.pcolormesh(t, f, sxx[f_idx, :], shading=\"gouraud\", vmin=vmin, vmax=vmax, rasterized=True)\n    if \"cone_of_influence_frequency\" in sxx_xarray:\n        coif = sxx_xarray.cone_of_influence_frequency\n        ax.plot(t, coif)\n        ax.fill_between(t, coif, step=\"mid\", alpha=0.2)\n    ax.set_xlim(t[0], t[-1])\n    # ax.set_xlim(t[0],0.2)\n    ax.set_ylim(f[0], f[-1])\n    plt.colorbar(mappable=pcm, ax=ax, label=cbar_label, pad=pad)\n    ax.set_xlabel(\"Time (sec)\")\n    ax.set_ylabel(\"Frequency (Hz)\")\n    return ax.figure\n</code></pre>"},{"location":"api/bmplot/spikes/","title":"Spike Plotting API","text":""},{"location":"api/bmplot/spikes/#bmtool.bmplot.spikes.raster","title":"<code>bmtool.bmplot.spikes.raster(spikes_df=None, config=None, network_name=None, groupby='pop_name', sortby=None, ax=None, tstart=None, tstop=None, color_map=None, dot_size=0.3)</code>","text":"<p>Plots a raster plot of neural spikes, with different colors for each population.</p> Parameters: <p>spikes_df : pd.DataFrame, optional     DataFrame containing spike data with columns 'timestamps', 'node_ids', and optional 'pop_name'. config : str, optional     Path to the configuration file used to load node data. network_name : str, optional     Specific network name to select from the configuration; if not provided, uses the first network. groupby : str, optional     Column name to group spikes by for coloring. Default is 'pop_name'. sortby : str, optional     Column name to sort node_ids within each group. If provided, nodes within each population will be sorted by this column. ax : matplotlib.axes.Axes, optional     Axes on which to plot the raster; if None, a new figure and axes are created. tstart : float, optional     Start time for filtering spikes; only spikes with timestamps greater than <code>tstart</code> will be plotted. tstop : float, optional     Stop time for filtering spikes; only spikes with timestamps less than <code>tstop</code> will be plotted. color_map : dict, optional     Dictionary specifying colors for each population. Keys should be population names, and values should be color values. dot_size: float, optional     Size of the dot to display on the scatterplot</p> Returns: <p>matplotlib.axes.Axes     Axes with the raster plot.</p> Notes: <ul> <li>If <code>config</code> is provided, the function merges population names from the node data with <code>spikes_df</code>.</li> <li>Each unique population from groupby in <code>spikes_df</code> will be represented by a different color if <code>color_map</code> is not specified.</li> <li>If <code>color_map</code> is provided, it should contain colors for all unique <code>pop_name</code> values in <code>spikes_df</code>.</li> </ul> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def raster(\n    spikes_df: Optional[pd.DataFrame] = None,\n    config: Optional[str] = None,\n    network_name: Optional[str] = None,\n    groupby: str = \"pop_name\",\n    sortby: Optional[str] = None,\n    ax: Optional[Axes] = None,\n    tstart: Optional[float] = None,\n    tstop: Optional[float] = None,\n    color_map: Optional[Dict[str, str]] = None,\n    dot_size: float = 0.3,\n) -&gt; Axes:\n    \"\"\"\n    Plots a raster plot of neural spikes, with different colors for each population.\n\n    Parameters:\n    ----------\n    spikes_df : pd.DataFrame, optional\n        DataFrame containing spike data with columns 'timestamps', 'node_ids', and optional 'pop_name'.\n    config : str, optional\n        Path to the configuration file used to load node data.\n    network_name : str, optional\n        Specific network name to select from the configuration; if not provided, uses the first network.\n    groupby : str, optional\n        Column name to group spikes by for coloring. Default is 'pop_name'.\n    sortby : str, optional\n        Column name to sort node_ids within each group. If provided, nodes within each population will be sorted by this column.\n    ax : matplotlib.axes.Axes, optional\n        Axes on which to plot the raster; if None, a new figure and axes are created.\n    tstart : float, optional\n        Start time for filtering spikes; only spikes with timestamps greater than `tstart` will be plotted.\n    tstop : float, optional\n        Stop time for filtering spikes; only spikes with timestamps less than `tstop` will be plotted.\n    color_map : dict, optional\n        Dictionary specifying colors for each population. Keys should be population names, and values should be color values.\n    dot_size: float, optional\n        Size of the dot to display on the scatterplot\n\n    Returns:\n    -------\n    matplotlib.axes.Axes\n        Axes with the raster plot.\n\n    Notes:\n    -----\n    - If `config` is provided, the function merges population names from the node data with `spikes_df`.\n    - Each unique population from groupby in `spikes_df` will be represented by a different color if `color_map` is not specified.\n    - If `color_map` is provided, it should contain colors for all unique `pop_name` values in `spikes_df`.\n    \"\"\"\n    # Initialize axes if none provided\n    sns.set_style(\"whitegrid\")\n    if ax is None:\n        _, ax = plt.subplots(1, 1)\n\n    # Filter spikes by time range if specified\n    if tstart is not None:\n        spikes_df = spikes_df[spikes_df[\"timestamps\"] &gt; tstart]\n    if tstop is not None:\n        spikes_df = spikes_df[spikes_df[\"timestamps\"] &lt; tstop]\n\n    # Load and merge node population data if config is provided\n    if config:\n        nodes = load_nodes_from_config(config)\n        if network_name:\n            nodes = nodes.get(network_name, {})\n        else:\n            nodes = list(nodes.values())[0] if nodes else {}\n            print(\n                \"Grabbing first network; specify a network name to ensure correct node population is selected.\"\n            )\n\n        # Find common columns, but exclude the join key from the list\n        common_columns = spikes_df.columns.intersection(nodes.columns).tolist()\n        common_columns = [\n            col for col in common_columns if col != \"node_ids\"\n        ]  # Remove our join key from the common list\n\n        # Drop all intersecting columns except the join key column from df2\n        spikes_df = spikes_df.drop(columns=common_columns)\n        # merge nodes and spikes df\n        spikes_df = spikes_df.merge(\n            nodes[groupby], left_on=\"node_ids\", right_index=True, how=\"left\"\n        )\n\n    # Get unique population names\n    unique_pop_names = spikes_df[groupby].unique()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"tab10\")  # Default colormap\n        color_map = {\n            pop_name: cmap(i / len(unique_pop_names)) for i, pop_name in enumerate(unique_pop_names)\n        }\n    else:\n        # Ensure color_map contains all population names\n        missing_colors = [pop for pop in unique_pop_names if pop not in color_map]\n        if missing_colors:\n            raise ValueError(f\"color_map is missing colors for populations: {missing_colors}\")\n\n    # Plot each population with its specified or generated color\n    legend_handles = []\n    y_offset = 0  # Track y-position offset for stacking populations\n\n    for pop_name, group in spikes_df.groupby(groupby):\n        if sortby:\n            # Sort by the specified column, putting NaN values at the end\n            group_sorted = group.sort_values(by=sortby, na_position='last')\n            # Create a mapping from node_ids to consecutive y-positions based on sorted order\n            # Use the sorted order to maintain the same sequence for all spikes from same node\n            unique_nodes_sorted = group_sorted['node_ids'].drop_duplicates()\n            node_to_y = {node_id: y_offset + i for i, node_id in enumerate(unique_nodes_sorted)}\n            # Map node_ids to new y-positions for ALL spikes (not just the sorted group)\n            y_positions = group['node_ids'].map(node_to_y)\n            # Verify no data was lost\n            assert len(y_positions) == len(group), f\"Data loss detected in population {pop_name}\"\n            assert y_positions.isna().sum() == 0, f\"Unmapped node_ids found in population {pop_name}\"\n        else:\n            y_positions = group['node_ids']\n\n        ax.scatter(group[\"timestamps\"], y_positions, color=color_map[pop_name], s=dot_size)\n        # Dummy scatter for consistent legend appearance\n        handle = ax.scatter([], [], color=color_map[pop_name], label=pop_name, s=20)\n        legend_handles.append(handle)\n\n        # Update y_offset for next population if sortby is used\n        if sortby:\n            y_offset += len(unique_nodes_sorted)\n\n    # Label axes\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Node ID\")\n    ax.legend(handles=legend_handles, title=\"Population\", loc=\"upper right\", framealpha=0.9)\n\n    return ax\n</code></pre>"},{"location":"examples/analysis/","title":"Analysis Tutorials","text":"<p>The Analysis module provides tools for processing and analyzing simulation results from BMTK models, including spike data and other output reports.</p>"},{"location":"examples/analysis/#features","title":"Features","text":"<ul> <li>Load and analyze spike data from simulations</li> <li>Calculate population statistics and metrics</li> <li>Analyze LFP/ECP data with spectrograms and phase locking</li> <li>Visualize results with various plotting functions</li> </ul> <p>The Using Spikes tutorial demonstrates how to work with spike data from simulations. In this notebook, you'll learn:</p> <ul> <li>How to load spike data from BMTK simulations</li> <li>How to calculate firing rate statistics</li> <li>How to visualize spike patterns using raster plots</li> <li>How to compute population metrics</li> </ul>"},{"location":"examples/analysis/#other-tutorials","title":"Other Tutorials","text":"<ul> <li>Plot Spectrogram: Learn to create and visualize spectrograms from LFP/ECP data</li> <li>Phase Locking: Analyze the relationship between spike times and oscillatory phase</li> </ul>"},{"location":"examples/analysis/#basic-api-usage","title":"Basic API Usage","text":"<p>Here are some basic examples of how to use the Analysis module in your code:</p>"},{"location":"examples/analysis/#spike-analysis","title":"Spike Analysis","text":"<pre><code>from bmtool.analysis.spikes import load_spikes_to_df, compute_firing_rate_stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load spike data from a simulation\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network',\n    config='config.json'  # Optional, for cell type labeling\n)\n\n# Get basic spike statistics\npop_stats, individual_stats = compute_firing_rate_stats(\n    df=spikes_df,\n    groupby='pop_name',\n    start_time=500,\n    stop_time=1500\n)\n\nprint(\"Population firing rate statistics:\")\nprint(pop_stats)\n</code></pre>"},{"location":"examples/analysis/#raster-plots","title":"Raster Plots","text":"<pre><code>from bmtool.analysis.spikes import load_spikes_to_df\nfrom bmtool.bmplot import raster\nimport matplotlib.pyplot as plt\n\n# Load spike data\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network'\n)\n\n# Create a basic raster plot\nfig, ax = plt.subplots(figsize=(10, 6))\nraster(\n    spikes_df=spikes_df,\n    groupby='pop_name',\n    time_range=(0, 2000),\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"examples/analysis/#population-statistics","title":"Population Statistics","text":"<pre><code>from bmtool.analysis.spikes import load_spikes_to_df, get_population_spike_rate\nimport matplotlib.pyplot as plt\n\n# Load spike data\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network'\n)\n\n# Calculate population firing rates over time\npopulation_rates = get_population_spike_rate(\n    spikes=spikes_df,\n    fs=400.0,  # Sampling frequency in Hz\n    t_start=0,\n    t_stop=2000\n)\n\n# Plot population rates\nfor pop_name, rates in population_rates.items():\n    plt.plot(rates, label=pop_name)\nplt.xlabel('Time (ms)')\nplt.ylabel('Firing Rate (Hz)')\nplt.title('Population Firing Rates')\nplt.legend()\nplt.show()\n</code></pre> <p>For more advanced examples and detailed usage, please refer to the Jupyter notebook tutorials above.</p>"},{"location":"examples/bmplot/","title":"BMPlot Examples","text":"<p>This page provides examples of how to use the BMPlot module for visualizing network connectivity and structure.</p>"},{"location":"examples/bmplot/#jupyter-notebook-tutorial","title":"Jupyter Notebook Tutorial","text":"<p>For a comprehensive guide with visualizations, check out our Jupyter notebook tutorial:</p>"},{"location":"examples/bmplot/#plotting-examples","title":"Plotting Examples","text":"<p>The BMPlot Tutorial demonstrates the various plotting functions available in BMTool. In this notebook, you'll learn:</p> <ul> <li>How to create connection matrices to visualize network connectivity</li> <li>How to plot cell positions in 3D space</li> <li>How to analyze connection distances and distributions</li> </ul> <pre><code>import bmtool.bmplot.connections as connections\n\n# Generate a table showing the total number of connections\nconnections.total_connection_matrix(config='config.json')\n\n# Generate a table showing the percent connectivity\nconnections.percent_connection_matrix(config='config.json')\n\n# Generate a table showing the mean convergence\nconnections.convergence_connection_matrix(config='config.json')\n\n# Generate a table showing the mean divergence\nconnections.divergence_connection_matrix(config='config.json')\n\n# Generate a matrix specifically for gap junctions\nconnections.gap_junction_matrix(config='config.json', method='percent')\n</code></pre>"},{"location":"examples/bmplot/#spatial-analysis","title":"Spatial Analysis","text":"<pre><code>import bmtool.bmplot.connections as connections\n\n# Generate a 3D plot with the source and target cells location and connection distance histogram\nconnections.connection_distance(config='config.json', source='PopA', target='PopB')\n\n# Generate a histogram of connection distributions\nconnections.connection_histogram(config='config.json', source='PopA', target='PopB')\n</code></pre>"},{"location":"examples/bmplot/#3d-visualization","title":"3D Visualization","text":"<pre><code>import bmtool.bmplot.connections as connections\n\n# Generate a plot of cell positions in 3D space\nconnections.plot_3d_positions(config='config.json', populations=['PopA', 'PopB'])\n\n# Generate a plot showing cell locations and orientation in 3D\nconnections.plot_3d_cell_rotation(config='config.json', populations=['PopA'])\n</code></pre>"},{"location":"examples/bmplot/#network-graph","title":"Network Graph","text":"<pre><code>import bmtool.bmplot.connections as connections\n\n# Plot a network connection diagram\nconnections.plot_network_graph(\n    config='config.json',\n    sources='LA',\n    targets='LA',\n    tids='pop_name',\n    sids='pop_name',\n    no_prepend_pop=True\n)\n</code></pre>"},{"location":"examples/bmplot/#spike-analysis","title":"Spike Analysis","text":"<pre><code>import bmtool.bmplot.spikes as spikes\n\n# Create a raster plot from spike data\nspikes.raster(spikes_df=spikes_df, groupby='pop_name')\n\n# Plot firing rate statistics\nspikes.plot_firing_rate_pop_stats(firing_stats=firing_stats_df, groupby='pop_name')\n</code></pre>"},{"location":"examples/bmplot/#lfp-analysis","title":"LFP Analysis","text":"<pre><code>import bmtool.bmplot.lfp as lfp\n\n# Plot a spectrogram\nlfp.plot_spectrogram(sxx_xarray=spectrogram_data, log_power=True)\n</code></pre> <p>For more advanced examples and detailed visualizations, please refer to the Jupyter notebook tutorial above.</p>"},{"location":"examples/connectors/","title":"Connectors Examples","text":"<p>This page provides examples of how to use the Connectors module for building complex network connectivity patterns in BMTK models.</p>"},{"location":"examples/connectors/#basic-setup","title":"Basic Setup","text":"<p>All connector examples use the following network node structure:</p> <pre><code>from bmtk.builder import NetworkBuilder\n\n# Create main network\nnet = NetworkBuilder('example_net')\nnet.add_nodes(N=100, pop_name='PopA', model_type='biophysical')\nnet.add_nodes(N=100, pop_name='PopB', model_type='biophysical')\n\n# Create background inputs\nbackground = NetworkBuilder('background')\nbackground.add_nodes(N=300, pop_name='tON', potential='exc', model_type='virtual')\n</code></pre>"},{"location":"examples/connectors/#unidirectional-connector","title":"Unidirectional Connector","text":"<p>The <code>UnidirectionConnector</code> allows you to build unidirectional connections with a given probability:</p> <pre><code>from bmtool.connectors import UnidirectionConnector\n\n# Create connector with 15% connection probability and 1 synapse per connection\nconnector = UnidirectionConnector(p=0.15, n_syn=1)\n\n# Set up source and target nodes\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopB'))\n\n# Add the edges to the network\nnet.add_edges(**connector.edge_params())\n</code></pre> <p>This creates connections from neurons in population 'PopA' to neurons in population 'PopB' with a 15% probability of connection.</p>"},{"location":"examples/connectors/#reciprocal-connector","title":"Reciprocal Connector","text":"<p>The <code>ReciprocalConnector</code> enables you to build connections with a specific reciprocal probability:</p> <pre><code>from bmtool.connectors import ReciprocalConnector\n\n# Create connector with 15% base probability and 6.7% reciprocal probability\nconnector = ReciprocalConnector(\n    p0=0.15,           # Base connection probability\n    pr=0.06767705087,  # Reciprocal connection probability\n    n_syn0=1,          # Number of synapses for base connection\n    n_syn1=1,          # Number of synapses for reciprocal connection\n    estimate_rho=False # Whether to estimate rho value\n)\n\n# Setup for recurrent connections within PopA\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\n\n# Add the edges to the network\nnet.add_edges(**connector.edge_params())\n</code></pre> <p>This creates connections within population 'PopA' where: - Any two neurons have a 15% probability of having a unidirectional connection - If a unidirectional connection exists from neuron A to neuron B, there's a 6.7% probability of also having a connection from B to A</p>"},{"location":"examples/connectors/#correlated-gap-junction","title":"Correlated Gap Junction","text":"<p>The <code>CorrelatedGapJunction</code> connector creates gap junction connections that can be correlated with chemical synapses:</p> <pre><code>from bmtool.connectors import ReciprocalConnector, CorrelatedGapJunction\n\n# First create a chemical synapse connectivity pattern\nconnector = ReciprocalConnector(p0=0.15, pr=0.06, n_syn0=1, n_syn1=1, estimate_rho=False)\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\nnet.add_edges(**connector.edge_params())\n\n# Then create gap junctions that are correlated with chemical synapses\ngap_junc = CorrelatedGapJunction(\n    p_non=0.1228,  # Probability for pairs with no chemical synapse\n    p_uni=0.56,    # Probability for pairs with unidirectional chemical synapse\n    p_rec=1,       # Probability for pairs with reciprocal chemical synapses\n    connector=connector  # Use the chemical synapse connector for correlation\n)\ngap_junc.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\n\n# Add gap junction edges\nconn = net.add_edges(\n    is_gap_junction=True,\n    syn_weight=0.0000495,\n    target_sections=None,\n    afferent_section_id=0,\n    afferent_section_pos=0.5,\n    **gap_junc.edge_params()\n)\n</code></pre> <p>This creates gap junctions with probabilities that depend on the existing chemical synapse connections.</p>"},{"location":"examples/connectors/#one-to-one-sequential-connector","title":"One-to-One Sequential Connector","text":"<p>The <code>OneToOneSequentialConnector</code> creates one-to-one mappings between populations:</p> <pre><code>from bmtool.connectors import OneToOneSequentialConnector\n\n# Create the connector\nconnector = OneToOneSequentialConnector()\n\n# Connect background to PopA\nconnector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name='PopA'))\nnet.add_edges(**connector.edge_params())\n\n# Connect background to PopB\nconnector.setup_nodes(target=net.nodes(pop_name='PopB'))\nnet.add_edges(**connector.edge_params())\n</code></pre> <p>This creates one-to-one connections where each neuron in the background population connects to exactly one neuron in PopA and one in PopB.</p>"},{"location":"examples/connectors/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/connectors/#distance-dependent-connectivity","title":"Distance-Dependent Connectivity","text":"<p>You can create distance-dependent connections by providing a connection probability function:</p> <pre><code>import numpy as np\nfrom bmtool.connectors import UnidirectionConnector\n\n# Define a distance-dependent probability function\ndef connection_probability(source, target, distance_range=500.0, p_max=0.15):\n    \"\"\"Probability decreases with distance\"\"\"\n    dist = np.sqrt(np.sum((source['positions'] - target['positions'])**2, axis=1))\n    return p_max * np.exp(-dist/distance_range)\n\n# Create connector with the probability function\nconnector = UnidirectionConnector(p=connection_probability, n_syn=1)\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopB'))\nnet.add_edges(**connector.edge_params())\n</code></pre> <p>This creates connections where the probability decreases exponentially with distance between neurons.</p>"},{"location":"examples/single-cell/","title":"Single Cell Examples","text":"<p>This page provides examples of how to use the Single Cell module for various analyses.</p>"},{"location":"examples/single-cell/#python-api-examples","title":"Python API Examples","text":"<p>The examples below demonstrate how to use the Single Cell module through its Python API:</p>"},{"location":"examples/single-cell/#basic-setup","title":"Basic Setup","text":"<p>All single cell examples require initializing the Profiler:</p> <pre><code>from bmtool.singlecell import Profiler\n\n# Initialize with paths to templates and mechanisms\nprofiler = Profiler(template_dir='templates', mechanism_dir='mechanisms', dt=0.1)\n</code></pre>"},{"location":"examples/single-cell/#passive-properties-analysis","title":"Passive Properties Analysis","text":"<p>Calculating passive properties (V-rest, input resistance, and time constant):</p> <pre><code>from bmtool.singlecell import Passive, run_and_plot\nimport matplotlib.pyplot as plt\n\n# Create a Passive simulation object\nsim = Passive('Cell_Cf', inj_amp=-100., inj_delay=1500., inj_dur=1000.,\n              tstop=2500., method='exp2')\n\n# Run the simulation and plot the results\ntitle = 'Passive Cell Current Injection'\nxlabel = 'Time (ms)'\nylabel = 'Membrane Potential (mV)'\nX, Y = run_and_plot(sim, title, xlabel, ylabel, plot_injection_only=True)\n\n# Plot the double exponential fit\nplt.gca().plot(*sim.double_exponential_fit(), 'r:', label='double exponential fit')\nplt.legend()\nplt.show()\n</code></pre> <p>This will output the passive properties: <pre><code>Injection location: Cell_Cf[0].soma[0](0.5)\nRecording: Cell_Cf[0].soma[0](0.5)._ref_v\nRunning simulation for passive properties...\n\nV Rest: -70.21 (mV)\nResistance: 128.67 (MOhms)\nMembrane time constant: 55.29 (ms)\n</code></pre></p>"},{"location":"examples/single-cell/#current-clamp","title":"Current Clamp","text":"<p>Running a current clamp to observe spiking behavior:</p> <pre><code>from bmtool.singlecell import CurrentClamp, run_and_plot\nimport matplotlib.pyplot as plt\n\n# Create a CurrentClamp simulation object\nsim = CurrentClamp('Cell_Cf', inj_amp=350., inj_delay=1500., inj_dur=1000.,\n                   tstop=3000., threshold=-15.)\n\n# Run the simulation and plot the results\nX, Y = run_and_plot(sim, title='Current Injection', xlabel='Time (ms)',\n                    ylabel='Membrane Potential (mV)', plot_injection_only=True)\nplt.show()\n</code></pre>"},{"location":"examples/single-cell/#fi-curve","title":"FI Curve","text":"<p>Generating a frequency-current (FI) curve:</p> <pre><code>from bmtool.singlecell import FI, run_and_plot\nimport matplotlib.pyplot as plt\n\n# Create an FI simulation object\nsim = FI('Cell_Cf', i_start=0., i_stop=1000., i_increment=50.,\n          tstart=1500., threshold=-15.)\n\n# Run the simulation and plot the results\nX, Y = run_and_plot(sim, title='FI Curve', xlabel='Injection (nA)',\n                    ylabel='# Spikes')\nplt.show()\n</code></pre>"},{"location":"examples/single-cell/#zap-protocol","title":"ZAP Protocol","text":"<p>Analyzing frequency response using a chirp current (ZAP):</p> <pre><code>from bmtool.singlecell import ZAP, run_and_plot\nimport matplotlib.pyplot as plt\n\n# Create a ZAP simulation object\nsim = ZAP('Cell_Cf')\n\n# Run the simulation and plot the results\nX, Y = run_and_plot(sim)\nplt.show()\n</code></pre>"},{"location":"examples/single-cell/#jupyter-notebook-tutorials","title":"Jupyter Notebook Tutorials","text":"<p>For more detailed examples with rich output and visualizations, check out our Jupyter notebook tutorials:</p>"},{"location":"examples/single-cell/#allen-cell-model-tutorial","title":"Allen Cell Model Tutorial","text":"<p>The Allen Cell Tutorial demonstrates how to use BMTool with Allen Institute cell models. In this tutorial, you'll learn:</p> <ul> <li>How to load Allen Institute cell models from SWC and JSON files</li> <li>How to analyze and characterize the cell's electrophysiological properties</li> <li>How to visualize the results of various protocols</li> </ul>"},{"location":"examples/single-cell/#other-examples","title":"Other Examples","text":"<p>You can also access other examples from the command line:</p> <pre><code># Cell Tuning via CLI\nbmtool util cell tune --builder\n\n# VHalf Segregation\nbmtool util cell vhseg\n</code></pre>"},{"location":"examples/slurm/","title":"SLURM Tutorials","text":"<p>The SLURM module provides tools for managing and running simulations on SLURM-based high-performance computing clusters.</p>"},{"location":"examples/slurm/#features","title":"Features","text":"<ul> <li>Automate simulation job submission to SLURM clusters</li> <li>Manage simulation parameters and configurations</li> <li>Track simulation status and results</li> <li>Parallelize parameter sweeps and batch runs</li> </ul> <p>The Block Runner Tutorial demonstrates how to manage simulations on SLURM clusters. In this notebook, you'll learn:</p> <ul> <li>How to set up simulation configurations for SLURM</li> <li>How to submit and monitor jobs</li> <li>How to parallelize parameter sweeps</li> <li>How to collect and analyze results from distributed simulations</li> </ul>"},{"location":"examples/slurm/#basic-api-usage","title":"Basic API Usage","text":"<p>Here are some basic examples of how to use the SLURM module in your code:</p>"},{"location":"examples/slurm/#block-runner","title":"Block Runner","text":"<pre><code>from bmtool.SLURM import BlockRunner\n\n# Initialize a block runner for a BMTK model\nrunner = BlockRunner(\n    model_dir='/path/to/model',\n    config='simulation_config.json',\n    steps_per_block=10,  # Number of simulation steps per SLURM job\n    total_steps=100      # Total simulation steps\n)\n\n# Submit the jobs to SLURM\nrunner.run()\n\n# Check the status of submitted jobs\nstatus = runner.check_status()\nprint(status)\n\n# Collect results from completed jobs\nresults = runner.collect_results()\n</code></pre>"},{"location":"examples/slurm/#parameter-sweeps","title":"Parameter Sweeps","text":"<pre><code>from bmtool.SLURM import ParameterSweep\n\n# Create a parameter sweep\nsweep = ParameterSweep(\n    base_config='simulation_config.json',\n    model_dir='/path/to/model',\n    parameter_specs={\n        'syn_weight': [0.001, 0.002, 0.003, 0.004],\n        'conn_prob': [0.1, 0.2, 0.3],\n        'input_rate': [10, 20, 30, 40, 50]\n    }\n)\n\n# Generate configurations\nconfigs = sweep.generate_configs()\n\n# Run the parameter sweep\nsweep.run(time_limit='2:00:00', memory='16G')\n</code></pre>"},{"location":"examples/slurm/#custom-slurm-runner","title":"Custom SLURM Runner","text":"<pre><code>from bmtool.SLURM import SLURMRunner\n\n# Create a custom SLURM runner\nrunner = SLURMRunner(\n    job_name='bmtk_simulation',\n    partition='normal',\n    nodes=1,\n    cores_per_node=16,\n    memory_gb=32,\n    time_limit='08:00:00',\n    email='user@example.com',\n    email_options=['END', 'FAIL']\n)\n\n# Submit a BMTK simulation\nrunner.submit(\n    model_dir='/path/to/model',\n    config='simulation_config.json',\n    modules_to_load=['neuron', 'python']\n)\n</code></pre> <p>For more advanced examples and detailed usage, please refer to the Jupyter notebook tutorial above.</p>"},{"location":"examples/synapses/","title":"Synapses Tutorials","text":"<p>The Synapses module provides tools for creating and tuning chemical and electrical synapses in NEURON and BMTK models.</p>"},{"location":"examples/synapses/#features","title":"Features","text":"<ul> <li>Interactive tuning of synapse parameters</li> <li>Support for both chemical and electrical (gap junction) synapses</li> <li>Visualization of synaptic responses</li> <li>Parameter fitting to match experimental data</li> </ul> <p>The Synapses module provides two different tutorials for chemical synapse tuning:</p> <p>The BMTK Chemical Synapse Tuner tutorial demonstrates how to use BMTool to interactively tune chemical synapses within BMTK networks. In this notebook, you'll learn:</p> <ul> <li>How to set up and configure chemical synapses in BMTK models</li> <li>How to switch between different network connections for tuning</li> <li>How to adjust synapse parameters and observe responses in a network context</li> <li>How to use the optimizer to automatically fit synaptic parameters</li> </ul> <p>The Neuron Chemical Synapse Tuner tutorial shows how to tune chemical synapses using pure NEURON models. This notebook covers:</p> <ul> <li>How to set up chemical synapses with detailed configuration</li> <li>How to manually tune synapse parameters outside of BMTK</li> <li>How to work with different synapse types (facilitating, depressing, etc.)</li> <li>How to implement custom synaptic mechanisms</li> </ul> <p>The Gap Junction Tuner tutorial shows how to configure and optimize electrical synapses. This notebook covers:</p> <ul> <li>Setting up gap junctions in NEURON models</li> <li>Adjusting gap junction conductance</li> <li>Visualizing current flow through gap junctions</li> <li>Implementing gap junctions in network models</li> </ul>"},{"location":"examples/synapses/#basic-api-usage","title":"Basic API Usage","text":"<p>If you prefer to use the Synapses module directly in your code, here are some basic examples:</p>"},{"location":"examples/synapses/#synapsetuner-with-bmtk-networks","title":"SynapseTuner with BMTK Networks","text":"<pre><code>from bmtool.synapses import SynapseTuner\n\n# Create a tuner for BMTK networks\ntuner = SynapseTuner(\n    config='simulation_config.json',  # Path to BMTK config\n    current_name='i',                 # Synaptic current to record\n    slider_vars=['initW','Dep','Fac','Use','tau1','tau2']  # Parameters for sliders\n)\n\n# Display the interactive tuner\ntuner.InteractiveTuner()\n\n# Switch between different connections in your network\ntuner._switch_connection('PV2Exc')\n</code></pre>"},{"location":"examples/synapses/#synapsetuner-with-pure-neuron-models","title":"SynapseTuner with Pure NEURON Models","text":"<pre><code>from bmtool.synapses import SynapseTuner\n\n# Define general settings\ngeneral_settings = {\n    'vclamp': True,\n    'rise_interval': (0.1, 0.9),\n    'tstart': 500.,\n    'tdur': 100.,\n    'threshold': -15.,\n    'delay': 1.3,\n    'weight': 1.,\n    'dt': 0.025,\n    'celsius': 20\n}\n\n# Define connection-specific settings\nconn_settings = {\n    'Exc2FSI': {\n        'spec_settings': {\n            'post_cell': 'FSI_Cell',\n            'vclamp_amp': -70.,\n            'sec_x': 0.5,\n            'sec_id': 1,\n            \"level_of_detail\": \"AMPA_NMDA_STP\",\n        },\n        'spec_syn_param': {\n            'initW': 0.76,\n            'tau_r_AMPA': 0.45,\n            'tau_d_AMPA': 7.5,\n            'Use': 0.13,\n            'Dep': 0.,\n            'Fac': 200.\n        },\n    }\n}\n\n# Create tuner with custom settings\ntuner = SynapseTuner(\n    general_settings=general_settings,\n    conn_type_settings=conn_settings\n)\n\n# Display the interactive tuner\ntuner.InteractiveTuner()\n</code></pre>"},{"location":"examples/synapses/#gapjunctiontuner","title":"GapJunctionTuner","text":"<pre><code>from bmtool.synapses import GapJunctionTuner\n\n# Create a tuner for gap junctions\ntuner = GapJunctionTuner(\n    cell1_template='Interneuron',\n    cell2_template='Interneuron',\n    template_dir='path/to/templates',\n    mod_dir='path/to/mechanisms'\n)\n\n# Display the interactive tuner\ntuner.show()\n\n# Use the optimizer to find resistance for a target coupling coefficient\noptimal_resistance = tuner.optimize(target_cc=0.05)\nprint(f\"Optimal gap junction resistance: {optimal_resistance} MOhm\")\n</code></pre> <p>For more advanced usage, please refer to the Jupyter notebook tutorials above.</p>"},{"location":"examples/notebooks/SLURM/using_BlockRunner/","title":"Block Runner","text":"In\u00a0[2]: Copied! <pre>from bmtool.SLURM import SimulationBlock, BlockRunner\nfrom pathlib import Path\nimport shutil\n</pre> from bmtool.SLURM import SimulationBlock, BlockRunner from pathlib import Path import shutil In\u00a0[4]: Copied! <pre>output_name = \"example_output_name\" # can be any string\nbasePath = Path.cwd()  # Equivalent to os.getcwd() gets our current working directory\n\n# Path to 'Run-Storage' directory this is just setting up where i want things stored so this would go ../Run-Storage/example_output_name dir\nrun_storage_dir = basePath.parent / 'Run-Storage'  \ntarget_dir = run_storage_dir / output_name  # Path to 'Run-Storage/CurrentResults'\ndisplay(target_dir)\n</pre> output_name = \"example_output_name\" # can be any string basePath = Path.cwd()  # Equivalent to os.getcwd() gets our current working directory  # Path to 'Run-Storage' directory this is just setting up where i want things stored so this would go ../Run-Storage/example_output_name dir run_storage_dir = basePath.parent / 'Run-Storage'   target_dir = run_storage_dir / output_name  # Path to 'Run-Storage/CurrentResults' display(target_dir) <pre>PosixPath('/home/gjgpb9/cortex_modeling/bmtool/examples/Run-Storage/example')</pre> In\u00a0[5]: Copied! <pre>from pathlib import PosixPath\nmanual_path = \"gjgpb9/bmtool/examples/Run-Storage/example\"\n\n# Create a PosixPath object\ntarget_dir = PosixPath(manual_path)\ndisplay(target_dir)\n</pre> from pathlib import PosixPath manual_path = \"gjgpb9/bmtool/examples/Run-Storage/example\"  # Create a PosixPath object target_dir = PosixPath(manual_path) display(target_dir) <pre>PosixPath('gjgpb9/bmtool/examples/Run-Storage/example')</pre> In\u00a0[\u00a0]: Copied! <pre>if target_dir.exists() and target_dir.is_dir():\n    shutil.rmtree(target_dir)\n</pre> if target_dir.exists() and target_dir.is_dir():     shutil.rmtree(target_dir) In\u00a0[\u00a0]: Copied! <pre>simulation_cases = {\n    \"baseline\": \"mpirun nrniv -mpi -python run_network.py simulation_config_baseline.json False\",\n    \"short\": \"mpirun nrniv -mpi -python run_network.py simulation_config_short.json False\",\n    \"long\": \"mpirun nrniv -mpi -python run_network.py simulation_config_long.json False\"\n}\n</pre> simulation_cases = {     \"baseline\": \"mpirun nrniv -mpi -python run_network.py simulation_config_baseline.json False\",     \"short\": \"mpirun nrniv -mpi -python run_network.py simulation_config_short.json False\",     \"long\": \"mpirun nrniv -mpi -python run_network.py simulation_config_long.json False\" } In\u00a0[\u00a0]: Copied! <pre>block_params = {\n'time': '04:00:00',\n'partition': 'batch',\n'nodes': 1,\n'ntasks': 40,\n'mem': '80G',\n'output_base_dir': target_dir\n}\n</pre> block_params = { 'time': '04:00:00', 'partition': 'batch', 'nodes': 1, 'ntasks': 40, 'mem': '80G', 'output_base_dir': target_dir } In\u00a0[\u00a0]: Copied! <pre>additional_commands = [\n    \"module purge\",\n    \"module load slurm\",\n    \"module load cpu/0.17.3b\",\n    \"module load gcc/10.2.0/npcyll4\",\n    \"module load openmpi/4.1.1\",\n    \"export HDF5_USE_FILE_LOCKING=FALSE\"\n]\n</pre> additional_commands = [     \"module purge\",     \"module load slurm\",     \"module load cpu/0.17.3b\",     \"module load gcc/10.2.0/npcyll4\",     \"module load openmpi/4.1.1\",     \"export HDF5_USE_FILE_LOCKING=FALSE\" ] In\u00a0[\u00a0]: Copied! <pre>#full_path = 'components/synaptic_models/synapses_STP/FSI2LTS.json'\ncomponent_path = 'components'\njson_file_path = 'synaptic_models/synapses_STP/FSI2LTS.json'\nparam_name = 'initW' # key in json\nparam_values = [3.1,5] # values in json\n</pre> #full_path = 'components/synaptic_models/synapses_STP/FSI2LTS.json' component_path = 'components' json_file_path = 'synaptic_models/synapses_STP/FSI2LTS.json' param_name = 'initW' # key in json param_values = [3.1,5] # values in json In\u00a0[\u00a0]: Copied! <pre>num_blocks = len(param_values)\n# Create a list to hold the blocks\nblocks = []\nfor i in range(1, num_blocks + 1):\n    block_name = f'block{i}'\n    block = SimulationBlock(block_name, **block_params, simulation_cases=simulation_cases,\n                            additional_commands=additional_commands,component_path=component_path)\n    blocks.append(block)\n</pre> num_blocks = len(param_values) # Create a list to hold the blocks blocks = [] for i in range(1, num_blocks + 1):     block_name = f'block{i}'     block = SimulationBlock(block_name, **block_params, simulation_cases=simulation_cases,                             additional_commands=additional_commands,component_path=component_path)     blocks.append(block) In\u00a0[\u00a0]: Copied! <pre>flow_url = None\nrunner = BlockRunner(blocks=blocks,json_file_path=json_file_path,\n                     param_name=param_name,param_values=param_values,webhook=flow_url)\n</pre> flow_url = None runner = BlockRunner(blocks=blocks,json_file_path=json_file_path,                      param_name=param_name,param_values=param_values,webhook=flow_url) In\u00a0[\u00a0]: Copied! <pre>runner.submit_blocks_parallel()\n</pre> runner.submit_blocks_parallel()"},{"location":"examples/notebooks/SLURM/using_BlockRunner/#this-is-a-notebook-to-show-how-to-use-the-bmtoolslurm-module","title":"This is a notebook to show how to use the BMTOOL.SLURM module\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#by-gregory-glickert","title":"By Gregory Glickert\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#first-we-will-import-the-packages-we-need","title":"First we will import the packages we need\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#next-we-will-set-up-the-pathing-the-runner-will-use","title":"Next we will set up the pathing the runner will use\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#you-can-also-manually-define-the-target-path-like-this","title":"You can also manually define the target path like this\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#it-is-then-good-practice-to-check-if-that-path-already-exists-and-if-so-delete-it","title":"It is then good practice to check if that path already exists and if so delete it\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#now-we-can-define-out-simulation_cases-this-is-a-dictionary-with-the-key-being-the-name-of-the-case-and-the-value-is-the-command-used-to-run-that-simulation-below-is-an-example-of-what-that-could-look-like","title":"Now we can define out simulation_cases. This is a dictionary with the key being the name of the case and the value is the command used to run that simulation. Below is an example of what that could look like.\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#next-we-define-our-block-parameters-these-are-parameters-that-will-be-used-to-allocate-the-slurm-resources-the-below-example-will-allocate-resources-for-4-hours-on-the-partition-named-batch-the-partition-will-be-different-depending-on-your-system-setup-the-job-will-use-40-cores-and-80g-of-memory-the-output_base_dir-will-be-the-directory-where-the-output-of-the-simulation-will-be-stored","title":"Next we define our block parameters. These are parameters that will be used to allocate the SLURM resources. The below example will allocate resources for 4 hours on the partition named batch. The partition will be different depending on your system setup. The job will use 40 cores and 80G of memory. The output_base_dir will be the directory where the output of the simulation will be stored\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#there-may-also-be-some-additional-commands-you-to-run-in-the-script-before-running-your-model-an-example-could-be-loading-modules-you-can-add-those-like-this-if-your-model-doesnt-need-any-additional-commands-you-can-just-not-include-this","title":"There may also be some additional commands you to run in the script before running your model. An example could be loading modules. You can add those like this. If your model doesn't need any additional commands you can just not include this.\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#next-we-set-up-the-paths-for-changing-our-json-file-the-module-supports-changing-any-key-in-the-json-file-the-component_path-should-be-the-relative-path-to-the-bmtk-components-directory-the-json_file_path-should-be-the-path-from-the-components-folder-to-the-json-file-you-wish-to-edit","title":"Next we set up the paths for changing our json file. The module supports changing any key in the json file. The component_path should be the relative path to the BMTK components directory. The json_file_path should be the path FROM the components folder to the json file you wish to edit.\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#now-we-set-up-the-blocks","title":"Now we set up the blocks.\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#then-we-can-get-the-runner-ready-to-submit-you-can-also-have-the-option-to-put-a-webhook-link-to-send-messages-on-progress-of-the-jobs-currently-only-microsoft-teams-has-been-tested-if-you-want-to-only-do-one-run-of-the-network-then-you-can-just-set-param_values-to-be-none-instead","title":"Then we can get the Runner ready to submit. You can also have the option to put a webhook link to send messages on progress of the jobs. Currently, only Microsoft Teams has been tested. If you want to only do one run of the network then you can just set param_values to be None instead.\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#then-finally-we-submit-the-run","title":"Then finally we submit the run\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#something-to-note-is-if-you-are-using-a-webhook-and-run-this-last-line-the-code-run-until-the-last-job-is-finished-this-can-take-some-time-so-it-may-be-easier-to-run-the-file-on-slurm-so-you-dont-have-to-keep-your-code-running-on-your-end-you-could-also-just-close-the-code-after-all-the-jobs-are-submitted-and-just-not-worry-about-knowing-when-the-jobs-are-done","title":"Something to note is if you are using a webhook and run this last line the code run until the last job is finished. This can take some time, so it may be easier to run the file on SLURM so you don't have to keep your code running on your end. You could also just close the code after all the jobs are submitted and just not worry about knowing when the jobs are done.\u00b6","text":""},{"location":"examples/notebooks/analysis/netcon_report/netcon_example/","title":"Synapse Report","text":"In\u00a0[1]: Copied! <pre>from bmtool.analysis.netcon_reports import load_synapse_report\nimport numpy as np\n\nh5_path = \"/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/lower_gamma_final_1_lower_gap_syn_test/block1/baseline/syn_test2.h5\"\nconfig_path = \"/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json\"\n\n# Load and process the data\nsyn_data = load_synapse_report(h5_path, config_path)\nprint(syn_data)\n\n# Example of how to get data for specific populations\nprint(\"\\nExample queries:\")\n\n# Get mean synapse values for each connection type\nprint(\"\\nMean values by connection type:\")\nfor src_pop in np.unique(syn_data.source_pop.values):\n    for trg_pop in np.unique(syn_data.target_pop.values):\n        conn_data = syn_data.where(\n            (syn_data.source_pop == src_pop) &amp; \n            (syn_data.target_pop == trg_pop),\n            drop=True\n        )\n        if len(conn_data.synapse) &gt; 0:\n            mean_val = conn_data.synapse_value.mean().values\n            print(f\"{src_pop}-&gt;{trg_pop}: {mean_val:.6f}\")\n\n# Count synapses by connection type\nprint(\"\\nSynapse counts by connection type:\")\nfor src_pop in np.unique(syn_data.source_pop.values):\n    for trg_pop in np.unique(syn_data.target_pop.values):\n        count = np.sum((syn_data.source_pop == src_pop) &amp; (syn_data.target_pop == trg_pop))\n        if count.values &gt; 0:\n            print(f\"{src_pop}-&gt;{trg_pop}: {count.values} synapses\")\n</pre> from bmtool.analysis.netcon_reports import load_synapse_report import numpy as np  h5_path = \"/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/lower_gamma_final_1_lower_gap_syn_test/block1/baseline/syn_test2.h5\" config_path = \"/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json\"  # Load and process the data syn_data = load_synapse_report(h5_path, config_path) print(syn_data)  # Example of how to get data for specific populations print(\"\\nExample queries:\")  # Get mean synapse values for each connection type print(\"\\nMean values by connection type:\") for src_pop in np.unique(syn_data.source_pop.values):     for trg_pop in np.unique(syn_data.target_pop.values):         conn_data = syn_data.where(             (syn_data.source_pop == src_pop) &amp;              (syn_data.target_pop == trg_pop),             drop=True         )         if len(conn_data.synapse) &gt; 0:             mean_val = conn_data.synapse_value.mean().values             print(f\"{src_pop}-&gt;{trg_pop}: {mean_val:.6f}\")  # Count synapses by connection type print(\"\\nSynapse counts by connection type:\") for src_pop in np.unique(syn_data.source_pop.values):     for trg_pop in np.unique(syn_data.target_pop.values):         count = np.sum((syn_data.source_pop == src_pop) &amp; (syn_data.target_pop == trg_pop))         if count.values &gt; 0:             print(f\"{src_pop}-&gt;{trg_pop}: {count.values} synapses\") <pre>&lt;xarray.Dataset&gt;\nDimensions:           (time: 100, synapse: 1365)\nCoordinates:\n  * time              (time) float64 0.0 5.0 10.0 15.0 ... 485.0 490.0 495.0\n  * synapse           (synapse) int64 0 1 2 3 4 5 ... 1360 1361 1362 1363 1364\n    source_pop        (synapse) &lt;U10 'CP' 'CP' ... 'unknown_-1' 'unknown_-1'\n    target_pop        (synapse) &lt;U2 'CP' 'CP' 'CP' 'CP' ... 'CP' 'CP' 'CP' 'CP'\n    source_id         (synapse) int64 623 2578 8153 3692 3400 ... -1 -1 -1 -1 -1\n    target_id         (synapse) int64 1 1 1 1 1 1 1 1 1 1 ... 5 5 5 5 5 5 5 5 5\n    sec_id            (synapse) uint64 1 2 1 1 1 1 1 1 1 1 ... 1 1 2 1 1 1 1 1 1\n    sec_x             (synapse) float64 0.5 0.5 0.5 0.5 0.5 ... 0.5 0.5 0.5 0.5\n    connection_label  (synapse) &lt;U14 'CP-&gt;CP' 'CP-&gt;CP' ... 'unknown_-1-&gt;CP'\nData variables:\n    synapse_value     (time, synapse) float64 2.101 2.101 1.834 ... 1.834 1.834\nAttributes:\n    description:  Synapse report data from bmtk simulation\n\nExample queries:\n\nMean values by connection type:\nCP-&gt;CP: 2.101000\nCS-&gt;CP: 1.834000\nunknown_-1-&gt;CP: 1.953314\n\nSynapse counts by connection type:\nCP-&gt;CP: 684 synapses\nCS-&gt;CP: 378 synapses\nunknown_-1-&gt;CP: 303 synapses\n</pre> In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\n\nCP2CP = syn_data.where(\n    (syn_data.source_pop == 'CP') &amp; \n    (syn_data.target_pop == 'CP'),\n    drop=True\n)\n\nplt.plot(CP2CP.time, CP2CP.synapse_value)\nplt.show()\n</pre> import matplotlib.pyplot as plt  CP2CP = syn_data.where(     (syn_data.source_pop == 'CP') &amp;      (syn_data.target_pop == 'CP'),     drop=True )  plt.plot(CP2CP.time, CP2CP.synapse_value) plt.show()"},{"location":"examples/notebooks/analysis/netcon_report/netcon_example/#netcon-report-module","title":"Netcon report module\u00b6","text":"<p>This module was written to help with the analysis of complex netcon reports generated by BMTK. Some example reports could look like</p> <pre>{\n    \"reports\": {\n        \"syn_report\": {\n            \"cells\": \"all\",\n            \"variable_name\": \"tau_r_AMPA\",\n            \"module\": \"netcon_report\",\n            \"sections\": \"all\",\n            \"syn_type\": \"AMPA_NMDA_STP\",\n            \"file_name\": \"syn_test1.h5\",\n            \"start_time\": 0,\n            \"dt\": 5\n        },\n        \"syn_report2\": {\n            \"cells\": {\n                \"node_ids\": [1,2,3,4,5]\n            },\n            \"variable_name\": \"tau_r_AMPA\",\n            \"module\": \"netcon_report\",\n            \"sections\": \"all\",\n            \"syn_type\": \"AMPA_NMDA_STP\",\n            \"file_name\": \"syn_test2.h5\",\n            \"start_time\": 0,\n            \"dt\": 5\n        },\n        \"syn_report3\": {\n            \"cells\": {\n                \"pop_name\": \"CS\"\n            },\n            \"variable_name\": \"tau_r_AMPA\",\n            \"module\": \"netcon_report\",\n            \"sections\": \"all\",\n            \"syn_type\": \"AMPA_NMDA_STP\",\n            \"file_name\": \"syn_test3.h5\",\n            \"start_time\": 0,\n            \"dt\": 5\n        }\n    }\n}\n</pre>"},{"location":"examples/notebooks/analysis/netcon_report/netcon_example/#get-an-example-tone-recording-here","title":"get an example tone recording here\u00b6","text":"<p>plot how the synapse parameter changes over time</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/","title":"Phase Locking","text":"In\u00a0[1]: Copied! <pre>RunningInCOLAB = 'google.colab' in str(get_ipython())\nif RunningInCOLAB:\n    %pip install bmtool &amp;&gt; /dev/null\n</pre> RunningInCOLAB = 'google.colab' in str(get_ipython()) if RunningInCOLAB:     %pip install bmtool &amp;&gt; /dev/null In\u00a0[2]: Copied! <pre>import numpy as np\nfrom scipy import signal as ss\nimport matplotlib.pyplot as plt\nfrom bmtool.analysis.lfp import fit_fooof\nfrom bmtool.analysis.entrainment import calculate_spike_lfp_plv, calculate_ppc, calculate_ppc2\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</pre> import numpy as np from scipy import signal as ss import matplotlib.pyplot as plt from bmtool.analysis.lfp import fit_fooof from bmtool.analysis.entrainment import calculate_spike_lfp_plv, calculate_ppc, calculate_ppc2 import warnings warnings.filterwarnings(\"ignore\")  <pre>/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/bmtool/analysis/lfp.py:11: DeprecationWarning: \nThe `fooof` package is being deprecated and replaced by the `specparam` (spectral parameterization) package.\nThis version of `fooof` (1.1) is fully functional, but will not be further updated.\nNew projects are recommended to update to using `specparam` (see Changelog for details).\n  from fooof import FOOOF\nWarning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> In\u00a0[3]: Copied! <pre>np.random.seed(9) # lucky number 9\nfs = 1000  # 1 kHz sampling rate\nduration = 200  # 200 seconds\nt = np.arange(0, duration, 1/fs)\n\n# Define oscillation frequencies\nbeta_freq = 15  # Hz\ngamma_freq = 40  # Hz\n\n# Create a simulated LFP with multiple frequency components\nlfp = (0.5 * np.sin(2 * np.pi * gamma_freq * t) + 0.2 * np.sin(2 * np.pi * beta_freq * t) +  0.3 * np.random.randn(len(t)))\n\n# Generate phase information for each frequency\nbeta_phase = np.angle(ss.hilbert(np.sin(2 * np.pi * beta_freq * t)))\ngamma_phase = np.angle(ss.hilbert(np.sin(2 * np.pi * gamma_freq * t)))\n\n# Generate beta-synchronized spikes\n# Strong preference for specific beta phase, weak response to gamma\nbeta_spike_probability = (0.8 * (1 + np.cos(beta_phase - np.pi/4)) + \n                            0.1 * (1 + np.cos(gamma_phase - np.pi/3)))\n\nbeta_spike_train = np.random.rand(len(t)) &lt; beta_spike_probability * 0.02\nbeta_spike_times = t[beta_spike_train]\n\n# Generate gamma-synchronized spikes\n# Strong preference for specific gamma phase, weak response to beta\ngamma_spike_probability = (0.1 * (1 + np.cos(beta_phase - np.pi/4)) + \n                            0.8 * (1 + np.cos(gamma_phase - np.pi/3)))\n\ngamma_spike_train = np.random.rand(len(t)) &lt; gamma_spike_probability * 0.02\ngamma_spike_times = t[gamma_spike_train]\n\nprint(f\"Generated {len(beta_spike_times)} beta-synchronized spikes\")\nprint(f\"Generated {len(gamma_spike_times)} gamma-synchronized spikes\")\n</pre> np.random.seed(9) # lucky number 9 fs = 1000  # 1 kHz sampling rate duration = 200  # 200 seconds t = np.arange(0, duration, 1/fs)  # Define oscillation frequencies beta_freq = 15  # Hz gamma_freq = 40  # Hz  # Create a simulated LFP with multiple frequency components lfp = (0.5 * np.sin(2 * np.pi * gamma_freq * t) + 0.2 * np.sin(2 * np.pi * beta_freq * t) +  0.3 * np.random.randn(len(t)))  # Generate phase information for each frequency beta_phase = np.angle(ss.hilbert(np.sin(2 * np.pi * beta_freq * t))) gamma_phase = np.angle(ss.hilbert(np.sin(2 * np.pi * gamma_freq * t)))  # Generate beta-synchronized spikes # Strong preference for specific beta phase, weak response to gamma beta_spike_probability = (0.8 * (1 + np.cos(beta_phase - np.pi/4)) +                              0.1 * (1 + np.cos(gamma_phase - np.pi/3)))  beta_spike_train = np.random.rand(len(t)) &lt; beta_spike_probability * 0.02 beta_spike_times = t[beta_spike_train]  # Generate gamma-synchronized spikes # Strong preference for specific gamma phase, weak response to beta gamma_spike_probability = (0.1 * (1 + np.cos(beta_phase - np.pi/4)) +                              0.8 * (1 + np.cos(gamma_phase - np.pi/3)))  gamma_spike_train = np.random.rand(len(t)) &lt; gamma_spike_probability * 0.02 gamma_spike_times = t[gamma_spike_train]  print(f\"Generated {len(beta_spike_times)} beta-synchronized spikes\") print(f\"Generated {len(gamma_spike_times)} gamma-synchronized spikes\") <pre>Generated 3596 beta-synchronized spikes\nGenerated 3661 gamma-synchronized spikes\n</pre> In\u00a0[4]: Copied! <pre>hz,pxx = ss.welch(lfp,fs)\n_,_ = fit_fooof(hz,pxx,plot=True,report=True,plt_range=(1,100),freq_range=[1,100])\n</pre> hz,pxx = ss.welch(lfp,fs) _,_ = fit_fooof(hz,pxx,plot=True,report=True,plt_range=(1,100),freq_range=[1,100]) <pre>==================================================================================================\n                                                                                                  \n                                   FOOOF - POWER SPECTRUM MODEL                                   \n                                                                                                  \n                        The model was run on the frequency range 3 - 98 Hz                        \n                                 Frequency Resolution is 3.91 Hz                                  \n                                                                                                  \n                            Aperiodic Parameters (offset, exponent):                              \n                                         -3.8986, -0.0807                                         \n                                                                                                  \n                                       2 peaks were found:                                        \n                                CF:  15.00, PW:  1.239, BW:  7.81                                 \n                                CF:  39.85, PW:  2.120, BW:  7.81                                 \n                                                                                                  \n                                     Goodness of fit metrics:                                     \n                                    R^2 of model fit is 0.9808                                    \n                                    Error of the fit is 0.0539                                    \n                                                                                                  \n==================================================================================================\n</pre> In\u00a0[5]: Copied! <pre># Demonstrate phase locking calculations for gamma-entrained spikes at 40 Hz\n# Note: spikes were generated on second time scale, so spike_fs=1\n\nprint(\"=== Phase Locking Results for Gamma-Entrained Spikes (40 Hz) ===\")\n\n# 1. Unbiased PLV (equivalent to square root of PPC)\nplv_result = calculate_spike_lfp_plv(\n    spike_times=gamma_spike_times, \n    lfp_data=lfp, \n    spike_fs=1, \n    lfp_fs=fs, \n    filter_method='wavelet', \n    freq_of_interest=40\n)\nprint(f\"PLV (unbiased): {plv_result:.4f}\")\n\n# 2. PPC with different computational methods (all should give identical results)\nppc_numpy = calculate_ppc(\n    spike_times=gamma_spike_times, \n    lfp_data=lfp, \n    spike_fs=1, \n    lfp_fs=fs, \n    filter_method='wavelet', \n    freq_of_interest=40, \n    ppc_method='numpy'\n)\nprint(f\"\u221aPPC (numpy):   {np.sqrt(ppc_numpy):.4f}\")\n\nppc_numba = calculate_ppc(\n    spike_times=gamma_spike_times, \n    lfp_data=lfp, \n    spike_fs=1, \n    lfp_fs=fs, \n    filter_method='wavelet', \n    freq_of_interest=40, \n    ppc_method='numba'\n)\nprint(f\"\u221aPPC (numba):   {np.sqrt(ppc_numba):.4f}\")\n\n# 3. GPU-accelerated PPC (requires CUDA toolkit and NVIDIA GPU)\ntry:\n    ppc_gpu = calculate_ppc(\n        spike_times=gamma_spike_times, \n        lfp_data=lfp, \n        spike_fs=1, \n        lfp_fs=fs, \n        filter_method='wavelet', \n        freq_of_interest=40, \n        ppc_method='gpu'\n    )\n    print(f\"\u221aPPC (GPU):     {np.sqrt(ppc_gpu):.4f}\")\nexcept:\n    print(\"\u221aPPC (GPU):     Not available (requires NVIDIA GPU + CUDA)\")\n\n# 4. Optimized PPC2 (most efficient for large datasets)\nppc2_result = calculate_ppc2(\n    spike_times=gamma_spike_times, \n    lfp_data=lfp, \n    spike_fs=1, \n    lfp_fs=fs, \n    filter_method='wavelet', \n    freq_of_interest=40\n)\nprint(f\"\u221aPPC2:          {np.sqrt(ppc2_result):.4f}\")\n\nprint(f\"\\nAll methods should produce similar values (~{plv_result:.3f})\")\n</pre> # Demonstrate phase locking calculations for gamma-entrained spikes at 40 Hz # Note: spikes were generated on second time scale, so spike_fs=1  print(\"=== Phase Locking Results for Gamma-Entrained Spikes (40 Hz) ===\")  # 1. Unbiased PLV (equivalent to square root of PPC) plv_result = calculate_spike_lfp_plv(     spike_times=gamma_spike_times,      lfp_data=lfp,      spike_fs=1,      lfp_fs=fs,      filter_method='wavelet',      freq_of_interest=40 ) print(f\"PLV (unbiased): {plv_result:.4f}\")  # 2. PPC with different computational methods (all should give identical results) ppc_numpy = calculate_ppc(     spike_times=gamma_spike_times,      lfp_data=lfp,      spike_fs=1,      lfp_fs=fs,      filter_method='wavelet',      freq_of_interest=40,      ppc_method='numpy' ) print(f\"\u221aPPC (numpy):   {np.sqrt(ppc_numpy):.4f}\")  ppc_numba = calculate_ppc(     spike_times=gamma_spike_times,      lfp_data=lfp,      spike_fs=1,      lfp_fs=fs,      filter_method='wavelet',      freq_of_interest=40,      ppc_method='numba' ) print(f\"\u221aPPC (numba):   {np.sqrt(ppc_numba):.4f}\")  # 3. GPU-accelerated PPC (requires CUDA toolkit and NVIDIA GPU) try:     ppc_gpu = calculate_ppc(         spike_times=gamma_spike_times,          lfp_data=lfp,          spike_fs=1,          lfp_fs=fs,          filter_method='wavelet',          freq_of_interest=40,          ppc_method='gpu'     )     print(f\"\u221aPPC (GPU):     {np.sqrt(ppc_gpu):.4f}\") except:     print(\"\u221aPPC (GPU):     Not available (requires NVIDIA GPU + CUDA)\")  # 4. Optimized PPC2 (most efficient for large datasets) ppc2_result = calculate_ppc2(     spike_times=gamma_spike_times,      lfp_data=lfp,      spike_fs=1,      lfp_fs=fs,      filter_method='wavelet',      freq_of_interest=40 ) print(f\"\u221aPPC2:          {np.sqrt(ppc2_result):.4f}\")  print(f\"\\nAll methods should produce similar values (~{plv_result:.3f})\") <pre>=== Phase Locking Results for Gamma-Entrained Spikes (40 Hz) ===\nPLV (unbiased): 0.4253\n\u221aPPC (numpy):   0.4253\n</pre> <pre>OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n</pre> <pre>\u221aPPC (numba):   0.4253\n\u221aPPC (GPU):     Not available (requires NVIDIA GPU + CUDA)\n\u221aPPC2:          0.4253\n\nAll methods should produce similar values (~0.425)\n</pre> In\u00a0[6]: Copied! <pre># Test entrainment across a range of frequencies\nfreqs = [1, 5, 15, 25, 30, 40, 50, 60, 70, 80]\nppc_gamma = []\nppc_beta = []\n\nprint(\"Calculating PPC across frequencies...\")\nfor freq in freqs:\n    # Calculate PPC for gamma-entrained spikes\n    ppc_gamma.append(calculate_ppc2(\n        spike_times=gamma_spike_times, \n        lfp_data=lfp, \n        spike_fs=1, \n        lfp_fs=fs, \n        filter_method='wavelet', \n        freq_of_interest=freq\n    ))\n    \n    # Calculate PPC for beta-entrained spikes\n    ppc_beta.append(calculate_ppc2(\n        spike_times=beta_spike_times, \n        lfp_data=lfp, \n        spike_fs=1, \n        lfp_fs=fs, \n        filter_method='wavelet', \n        freq_of_interest=freq\n    ))\n\n# Create the frequency entrainment plot\nplt.figure(figsize=(10, 6))\nplt.plot(freqs, ppc_gamma, 'o-', linewidth=2, markersize=8, \n         label=\"Gamma-entrained spikes\", color='red')\nplt.plot(freqs, ppc_beta, 'o-', linewidth=2, markersize=8, \n         label='Beta-entrained spikes', color='blue')\n\n# Add vertical lines to highlight target frequencies\nplt.axvline(x=15, color='blue', linestyle='--', alpha=0.5, label='Beta frequency (15 Hz)')\nplt.axvline(x=40, color='red', linestyle='--', alpha=0.5, label='Gamma frequency (40 Hz)')\n\nplt.xlabel('Frequency (Hz)', fontsize=12)\nplt.ylabel('PPC Value', fontsize=12)\nplt.title('Phase Locking Strength Across Frequencies', fontsize=14, fontweight='bold')\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Print peak frequencies for verification\ngamma_peak_idx = np.argmax(ppc_gamma)\nbeta_peak_idx = np.argmax(ppc_beta)\nprint(f\"\\nPeak entrainment frequencies:\")\nprint(f\"Gamma spikes: {freqs[gamma_peak_idx]} Hz (PPC = {ppc_gamma[gamma_peak_idx]:.4f})\")\nprint(f\"Beta spikes:  {freqs[beta_peak_idx]} Hz (PPC = {ppc_beta[beta_peak_idx]:.4f})\")\n</pre> # Test entrainment across a range of frequencies freqs = [1, 5, 15, 25, 30, 40, 50, 60, 70, 80] ppc_gamma = [] ppc_beta = []  print(\"Calculating PPC across frequencies...\") for freq in freqs:     # Calculate PPC for gamma-entrained spikes     ppc_gamma.append(calculate_ppc2(         spike_times=gamma_spike_times,          lfp_data=lfp,          spike_fs=1,          lfp_fs=fs,          filter_method='wavelet',          freq_of_interest=freq     ))          # Calculate PPC for beta-entrained spikes     ppc_beta.append(calculate_ppc2(         spike_times=beta_spike_times,          lfp_data=lfp,          spike_fs=1,          lfp_fs=fs,          filter_method='wavelet',          freq_of_interest=freq     ))  # Create the frequency entrainment plot plt.figure(figsize=(10, 6)) plt.plot(freqs, ppc_gamma, 'o-', linewidth=2, markersize=8,           label=\"Gamma-entrained spikes\", color='red') plt.plot(freqs, ppc_beta, 'o-', linewidth=2, markersize=8,           label='Beta-entrained spikes', color='blue')  # Add vertical lines to highlight target frequencies plt.axvline(x=15, color='blue', linestyle='--', alpha=0.5, label='Beta frequency (15 Hz)') plt.axvline(x=40, color='red', linestyle='--', alpha=0.5, label='Gamma frequency (40 Hz)')  plt.xlabel('Frequency (Hz)', fontsize=12) plt.ylabel('PPC Value', fontsize=12) plt.title('Phase Locking Strength Across Frequencies', fontsize=14, fontweight='bold') plt.legend(fontsize=10) plt.grid(True, alpha=0.3) plt.tight_layout() plt.show()  # Print peak frequencies for verification gamma_peak_idx = np.argmax(ppc_gamma) beta_peak_idx = np.argmax(ppc_beta) print(f\"\\nPeak entrainment frequencies:\") print(f\"Gamma spikes: {freqs[gamma_peak_idx]} Hz (PPC = {ppc_gamma[gamma_peak_idx]:.4f})\") print(f\"Beta spikes:  {freqs[beta_peak_idx]} Hz (PPC = {ppc_beta[beta_peak_idx]:.4f})\") <pre>Calculating PPC across frequencies...\n</pre> <pre>\nPeak entrainment frequencies:\nGamma spikes: 40 Hz (PPC = 0.1809)\nBeta spikes:  15 Hz (PPC = 0.2066)\n</pre>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#calculating-phase-locking-with-bmtool","title":"Calculating Phase Locking with BMTool\u00b6","text":"<p>By Gregory Glickert</p> <p>This notebook demonstrates how to use BMTool to calculate phase locking between spike times and local field potential (LFP) oscillations. We'll explore three different metrics: Phase-Locking Value (PLV), Pairwise Phase Consistency (PPC), and an optimized version (PPC2).</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#data-generation","title":"Data Generation\u00b6","text":"<p>Let's create synthetic data to demonstrate phase locking analysis. We'll generate:</p> <ol> <li>Simulated LFP with beta (15 Hz) and gamma (40 Hz) oscillations plus noise</li> <li>Beta-entrained spikes that preferentially fire at specific beta phases</li> <li>Gamma-entrained spikes that preferentially fire at specific gamma phases</li> </ol> <p>This controlled setup allows us to verify that our analysis methods correctly identify the known phase relationships.</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#power-spectral-analysis","title":"Power Spectral Analysis\u00b6","text":"<p>We can visualize the frequency content of our simulated LFP to confirm it contains the expected beta (15 Hz) and gamma (40 Hz) oscillations:</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#phase-locking-metrics","title":"Phase Locking Metrics\u00b6","text":"<p>Now we can analyze how our spike times are entrained to the LFP we created. BMTool provides three different metrics for quantifying phase locking:</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#1-phase-locking-value-plv-calculate_spike_lfp_plv","title":"1. Phase-Locking Value (PLV) - <code>calculate_spike_lfp_plv</code>\u00b6","text":"<p>The unbiased Phase Locking Value is calculated through the following steps:</p> <ol> <li><p>Standard PLV calculation: $$PLV = \\left| \\frac{1}{N} \\sum_{j=1}^{N} e^{i\\phi_j} \\right|$$ where $\\phi_j$ is the phase at each spike time.</p> </li> <li><p>Pairwise Phase Consistency (PPC) calculation: $$PPC = \\frac{PLV^2 \\cdot N - 1}{N - 1}$$</p> </li> <li><p>Unbiased PLV calculation: $$PLV_{unbiased} = \\sqrt{\\max(PPC, 0)}$$</p> </li> </ol>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#2-pairwise-phase-consistency-ppc-calculate_ppc","title":"2. Pairwise Phase Consistency (PPC) - <code>calculate_ppc</code>\u00b6","text":"<p>PPC calculates phase consistency through pairwise comparisons of spike-phase differences:</p> <p>$$PPC = \\frac{2}{n(n-1)} \\sum_{j=1}^{n-1} \\sum_{k=j+1}^{n} \\cos(\\phi_j - \\phi_k)$$</p> <p>Where:</p> <ul> <li>$\\phi_j, \\phi_k$ = Phases at spike times $j$ and $k$</li> <li>$n$ = Number of spikes</li> </ul>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#3-optimized-pairwise-phase-consistency-ppc2-calculate_ppc2","title":"3. Optimized Pairwise Phase Consistency (PPC2) - <code>calculate_ppc2</code>\u00b6","text":"<p>PPC2 is an algebraically equivalent but computationally optimized version of PPC.</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#derivation-from-ppc","title":"Derivation from PPC:\u00b6","text":"<ol> <li><p>Original PPC: $$PPC = \\frac{2}{n(n-1)} \\sum_{i &lt; j} \\cos(\\phi_i - \\phi_j)$$</p> </li> <li><p>Rewrite using Euler's formula: $$\\cos(\\phi_i - \\phi_j) = \\text{Re}\\left( e^{i(\\phi_i - \\phi_j)} \\right)$$</p> </li> <li><p>Sum over all pairs: $$\\sum_{i &lt; j} \\cos(\\phi_i - \\phi_j) = \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\cos(\\phi_i - \\phi_j) - \\frac{n}{2}$$</p> </li> <li><p>Express in terms of complex exponentials: $$\\sum_{i &lt; j} \\cos(\\phi_i - \\phi_j) = \\frac{1}{2} \\text{Re} \\left( \\left| \\sum_{j=1}^n e^{i\\phi_j} \\right|^2 \\right) - \\frac{n}{2}$$</p> </li> <li><p>Final PPC2 formula: $$PPC2 = \\frac{\\left|\\sum_{j=1}^{n} e^{i\\phi_j}\\right|^2 - n}{n(n-1)}$$</p> </li> </ol>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#computational-advantages-of-ppc2","title":"Computational advantages of PPC2:\u00b6","text":"<ul> <li>Converts phases to complex unit vectors</li> <li>Uses vector magnitude properties to avoid explicit pairwise comparisons</li> <li>Significantly faster for large spike datasets</li> </ul>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#key-points","title":"Key Points:\u00b6","text":"<ul> <li>All three metrics quantify phase locking between spike times and LFP oscillations</li> <li>With sufficient spike times, all metrics should converge to similar values</li> <li>PPC2 offers the best computational performance for large datasets</li> </ul>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#demonstration-calculating-phase-locking-metrics","title":"Demonstration: Calculating Phase Locking Metrics\u00b6","text":"<p>Let's test all three methods on our gamma-entrained spikes to verify they produce consistent results:</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#frequency-specific-entrainment-analysis","title":"Frequency-Specific Entrainment Analysis\u00b6","text":"<p>We can also examine entrainment strength across different frequencies to see which oscillations our spike trains are most strongly locked to:</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#summary-and-conclusions","title":"Summary and Conclusions\u00b6","text":"<p>This notebook demonstrated how to use BMTool to:</p> <ol> <li>Generate simulated data with known phase relationships between spikes and LFP oscillations</li> <li>Calculate phase locking metrics using three different methods (PLV, PPC, PPC2)</li> <li>Analyze frequency-specific entrainment to identify which oscillations drive neural activity</li> </ol>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#key-results","title":"Key Results:\u00b6","text":"<ul> <li>Gamma-entrained spikes showed strongest phase locking at 40 Hz (gamma frequency)</li> <li>Beta-entrained spikes showed strongest phase locking at 15 Hz (beta frequency)</li> <li>All three metrics (PLV, PPC, PPC2) produced consistent results</li> <li>PPC2 provides the most computationally efficient approach for large datasets</li> </ul>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#when-to-use-each-method","title":"When to Use Each Method:\u00b6","text":"<ul> <li>PLV: Good general-purpose metric, corrected for bias</li> <li>PPC: Gold standard for phase consistency, computationally intensive</li> <li>PPC2: Recommended for large datasets due to computational efficiency</li> </ul> <p>BMTool makes it easy to quantify neural entrainment with just a few function calls!</p>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool/","title":"Plot Spectrogram","text":"In\u00a0[5]: Copied! <pre>from bmtool.analysis.lfp import load_ecp_to_xarray, cwt_spectrogram_xarray, fit_fooof, get_windowed_data, ecp_to_lfp \nfrom bmtool.bmplot.lfp import plot_spectrogram\nimport numpy as np\nimport matplotlib.pyplot as plt\n\necp = load_ecp_to_xarray('/home/gjgpb9/cortex_modeling/bmtool/examples/analysis/network_to_analysis/long/ecp.h5')\nlfp = ecp_to_lfp(ecp)\n</pre> from bmtool.analysis.lfp import load_ecp_to_xarray, cwt_spectrogram_xarray, fit_fooof, get_windowed_data, ecp_to_lfp  from bmtool.bmplot.lfp import plot_spectrogram import numpy as np import matplotlib.pyplot as plt  ecp = load_ecp_to_xarray('/home/gjgpb9/cortex_modeling/bmtool/examples/analysis/network_to_analysis/long/ecp.h5') lfp = ecp_to_lfp(ecp) In\u00a0[6]: Copied! <pre>downsample_freq = 200 # if you want to further down sample\ntseg = 0.5  # time segment length for PSD\naxis = lfp.dims.index('time') # dimension index of time axis in x\n\nlfp_sxx = cwt_spectrogram_xarray(lfp, lfp.fs, axis=axis, downsample_fs = downsample_freq,\n                                        channel_coords={'channel_id': lfp.channel_id}, freq_range=(1 / tseg, np.inf))\n</pre> downsample_freq = 200 # if you want to further down sample tseg = 0.5  # time segment length for PSD axis = lfp.dims.index('time') # dimension index of time axis in x  lfp_sxx = cwt_spectrogram_xarray(lfp, lfp.fs, axis=axis, downsample_fs = downsample_freq,                                         channel_coords={'channel_id': lfp.channel_id}, freq_range=(1 / tseg, np.inf))  In\u00a0[7]: Copied! <pre>fooof_params = dict(aperiodic_mode='fixed', freq_range=(1,100), peak_width_limits=100.)\nplt_range = [2., 100.]\nclr_freq_range = None\nlog_power = 'dB'\n\nfig, ax = plt.subplots(1,1,figsize=(10,6))\nsxx = lfp_sxx.sel(channel_id=0)\nsxx_tot = sxx.PSD.mean(dim='time')\n\nfooof_results, _ = fit_fooof(sxx_tot.frequency.values, sxx_tot.values, **fooof_params,plot=False)\n_ = plot_spectrogram(sxx, remove_aperiodic=fooof_results, log_power=log_power,\n                        plt_range=plt_range, clr_freq_range=clr_freq_range, pad=0.01, ax=ax)\n</pre> fooof_params = dict(aperiodic_mode='fixed', freq_range=(1,100), peak_width_limits=100.) plt_range = [2., 100.] clr_freq_range = None log_power = 'dB'  fig, ax = plt.subplots(1,1,figsize=(10,6)) sxx = lfp_sxx.sel(channel_id=0) sxx_tot = sxx.PSD.mean(dim='time')  fooof_results, _ = fit_fooof(sxx_tot.frequency.values, sxx_tot.values, **fooof_params,plot=False) _ = plot_spectrogram(sxx, remove_aperiodic=fooof_results, log_power=log_power,                         plt_range=plt_range, clr_freq_range=clr_freq_range, pad=0.01, ax=ax) In\u00a0[8]: Copied! <pre># in this example there are 9 pulses given for 1 second with a 500ms break before the next pulse. The pulse starts 1 second into the simulation \non_time = 1 # how long in seconds is our stimulus\noff_time = 0.5 # how long is the break between stim is \nt_start = 1 # when starts in sec\nstimulus_count = 9\n\ntimes_to_avg = []\nfor i in range(stimulus_count):\n    end_time = t_start+on_time+off_time\n    times_to_avg.append([t_start,end_time])\n    t_start = end_time \n\n# make list into np array \ntimes_to_avg = np.array(times_to_avg)\n\n_, _, lfp_sxx_avg = get_windowed_data(lfp_sxx.PSD, times_to_avg, {0: np.arange(times_to_avg.shape[0])})\nlfp_sxx_avg = lfp_sxx_avg[0].mean_.sel(unique_cycle=0).to_dataset(name='PSD')\n\nfig, ax = plt.subplots(1,1,figsize=(10,6))\nsxx = lfp_sxx_avg.sel(channel_id=0)\nsxx_tot = sxx.PSD.mean(dim='time')\nfooof_results, _ = fit_fooof(sxx_tot.frequency.values, sxx_tot.values, **fooof_params,plot=False)\n_ = plot_spectrogram(sxx, remove_aperiodic=fooof_results, log_power=log_power,\n                        plt_range=plt_range, clr_freq_range=clr_freq_range, pad=0.01, ax=ax)\n</pre> # in this example there are 9 pulses given for 1 second with a 500ms break before the next pulse. The pulse starts 1 second into the simulation  on_time = 1 # how long in seconds is our stimulus off_time = 0.5 # how long is the break between stim is  t_start = 1 # when starts in sec stimulus_count = 9  times_to_avg = [] for i in range(stimulus_count):     end_time = t_start+on_time+off_time     times_to_avg.append([t_start,end_time])     t_start = end_time   # make list into np array  times_to_avg = np.array(times_to_avg)  _, _, lfp_sxx_avg = get_windowed_data(lfp_sxx.PSD, times_to_avg, {0: np.arange(times_to_avg.shape[0])}) lfp_sxx_avg = lfp_sxx_avg[0].mean_.sel(unique_cycle=0).to_dataset(name='PSD')  fig, ax = plt.subplots(1,1,figsize=(10,6)) sxx = lfp_sxx_avg.sel(channel_id=0) sxx_tot = sxx.PSD.mean(dim='time') fooof_results, _ = fit_fooof(sxx_tot.frequency.values, sxx_tot.values, **fooof_params,plot=False) _ = plot_spectrogram(sxx, remove_aperiodic=fooof_results, log_power=log_power,                         plt_range=plt_range, clr_freq_range=clr_freq_range, pad=0.01, ax=ax)  <pre>/home/gjgpb9/miniconda3/envs/bmtk/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom &lt;= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n</pre>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool/#example-of-how-to-generate-a-spectrogram-using-bmtool","title":"Example of how to generate a spectrogram using bmtool\u00b6","text":"<p>By Gregory Glickert</p> <p>First we read in our data and process it into an LFP</p>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool/#calculate-spectrogram","title":"calculate spectrogram\u00b6","text":"<p>Then we need to calculate the spectrogram. This will just make an array of the data and then we will process it further to make our plot</p>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool/#plot-spectrogram","title":"Plot spectrogram\u00b6","text":"<p>Whole simulation spectrogram with the aperiodic removed can be done like this. It may be good to check the aperiodic fit of the model before completely trusting the plot. This can be done by setting the plot argument in the fit_fooof function to True.</p>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool/#average-spectrogram-and-plot","title":"Average spectrogram and plot\u00b6","text":"<p>For this simulation be analyzed there were 9 pulses given. It may be easier to see the pulse response if we average the times together</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/","title":"Spikes Module","text":"In\u00a0[41]: Copied! <pre>from bmtool.analysis.spikes import load_spikes_to_df, get_population_spike_rate, compute_firing_rate_stats, average_spike_rate_over_windows\nfrom bmtool.bmplot.spikes import raster, plot_firing_rate_pop_stats, plot_firing_rate_distribution\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n</pre> from bmtool.analysis.spikes import load_spikes_to_df, get_population_spike_rate, compute_firing_rate_stats, average_spike_rate_over_windows from bmtool.bmplot.spikes import raster, plot_firing_rate_pop_stats, plot_firing_rate_distribution import matplotlib.pyplot as plt import numpy as np %matplotlib inline In\u00a0[42]: Copied! <pre>config_path = '/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json'\noutput_path = '/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/final_result_2/block1/long/spikes.h5'\n\ndf = load_spikes_to_df(spike_file=output_path,network_name='cortex',config=config_path)\ndf\n</pre> config_path = '/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json' output_path = '/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/final_result_2/block1/long/spikes.h5'  df = load_spikes_to_df(spike_file=output_path,network_name='cortex',config=config_path) df Out[42]: node_ids timestamps pop_name 0 8622 7.1 FSI 1 9477 7.3 LTS 2 9903 7.7 LTS 3 9300 8.4 FSI 4 3791 9.0 CP ... ... ... ... 464167 9864 14499.6 LTS 464168 9681 14499.6 LTS 464169 1890 14499.6 CP 464170 6613 14499.7 CS 464171 1647 14499.9 CP <p>464172 rows \u00d7 3 columns</p> <p>You can actually have the function label the spikes with any attribute that is there when you build the nodes. By default it is pop_name but you could do anything. So if your build looks like this.</p> <pre>net.add_nodes(N=n_I, pop_name='PV',     # N = number of inhibitory cells\n        model_type='biophysical',\n        model_template='hoc:WBInhCell',  # WBInhCell hoc definition\n        morphology='blank.swc',\n        positions = positions_cuboid(N=n_I,center=[0.,0.,0.],xside_length=10.,yside_length=10.,height=10.,min_dist=2))\n</pre> <p>You could use, N,pop_name,model_type,model_template,morphology,and position to label the cells. For this example N and positions would be poor choices to label due to being unique for every cell.</p> In\u00a0[43]: Copied! <pre>df = load_spikes_to_df(output_path,network_name='cortex',config=config_path,groupby=['model_template','pop_name'])\ndf\n</pre> df = load_spikes_to_df(output_path,network_name='cortex',config=config_path,groupby=['model_template','pop_name']) df Out[43]: node_ids timestamps model_template pop_name 0 8622 7.1 hoc:FSI_Cell FSI 1 9477 7.3 hoc:LTS_Cell LTS 2 9903 7.7 hoc:LTS_Cell LTS 3 9300 8.4 hoc:FSI_Cell FSI 4 3791 9.0 hoc:CP_Cell CP ... ... ... ... ... 464167 9864 14499.6 hoc:LTS_Cell LTS 464168 9681 14499.6 hoc:LTS_Cell LTS 464169 1890 14499.6 hoc:CP_Cell CP 464170 6613 14499.7 hoc:CS_Cell CS 464171 1647 14499.9 hoc:CP_Cell CP <p>464172 rows \u00d7 4 columns</p> <p>This can be helpful if you have nodes with the same pop_name, but different layers so you may want more than one label to tell which cells are spiking.</p> In\u00a0[44]: Copied! <pre>raster(spikes_df=df)\nplt.show()\n</pre> raster(spikes_df=df) plt.show() <p>You can also play with some settings in the raster function if you want to view the data better or in a different way.</p> In\u00a0[45]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n\ncolor_map = {\n    'CS': 'red',\n    'CP': 'red', \n    'FSI': 'blue', \n    'LTS': 'green'\n}\n\nraster(spikes_df=df,ax=ax,\n       groupby='pop_name',\n       color_map=color_map,\n       tstart=0,tstop=1000)\nplt.show()\n</pre> fig, ax = plt.subplots(1, 1, figsize=(8, 5))  color_map = {     'CS': 'red',     'CP': 'red',      'FSI': 'blue',      'LTS': 'green' }  raster(spikes_df=df,ax=ax,        groupby='pop_name',        color_map=color_map,        tstart=0,tstop=1000) plt.show() In\u00a0[46]: Copied! <pre>df = load_spikes_to_df(output_path,network_name='cortex',config=config_path)\nspike_rate = get_population_spike_rate(df,config=config_path,network_name='cortex')\ndisplay(spike_rate)\n</pre> df = load_spikes_to_df(output_path,network_name='cortex',config=config_path) spike_rate = get_population_spike_rate(df,config=config_path,network_name='cortex') display(spike_rate) <pre>&lt;xarray.DataArray (time: 5800, population: 4, type: 2)&gt;\narray([[[0.        , 1.4118588 ],\n        [0.        , 0.88750121],\n        [0.        , 0.78587889],\n        [0.        , 0.72381333]],\n\n       [[0.        , 1.42681938],\n        [0.        , 0.8934122 ],\n        [0.        , 0.79454548],\n        [0.        , 0.73308296]],\n\n       [[0.47058824, 1.45641973],\n        [0.61538462, 0.90502934],\n        [0.        , 0.81154425],\n        [0.        , 0.75131488]],\n\n       ...,\n\n       [[2.35294118, 3.74999671],\n        [4.30769231, 3.12395504],\n        [1.03529412, 1.54630566],\n        [1.78823529, 1.6247682 ]],\n\n       [[2.82352941, 3.73754173],\n        [3.69230769, 3.14184906],\n        [1.69411765, 1.54911239],\n        [1.69411765, 1.62555177]],\n\n       [[4.23529412, 3.73125568],\n        [6.15384615, 3.15105916],\n        [1.97647059, 1.55052619],\n        [1.31764706, 1.62596401]]])\nCoordinates:\n  * time        (time) float64 0.0 2.5 5.0 7.5 ... 1.449e+04 1.45e+04 1.45e+04\n  * population  (population) object 'FSI' 'LTS' 'CP' 'CS'\n  * type        (type) &lt;U8 'raw' 'smoothed'\nAttributes:\n    fs:             400.0\n    normalized:     False\n    smooth_method:  gaussian\n    smooth_window:  50</pre>xarray.DataArray<ul><li>time: 5800</li><li>population: 4</li><li>type: 2</li></ul><ul><li>0.0 1.412 0.0 0.8875 0.0 0.7859 ... 3.151 1.976 1.551 1.318 1.626<pre>array([[[0.        , 1.4118588 ],\n        [0.        , 0.88750121],\n        [0.        , 0.78587889],\n        [0.        , 0.72381333]],\n\n       [[0.        , 1.42681938],\n        [0.        , 0.8934122 ],\n        [0.        , 0.79454548],\n        [0.        , 0.73308296]],\n\n       [[0.47058824, 1.45641973],\n        [0.61538462, 0.90502934],\n        [0.        , 0.81154425],\n        [0.        , 0.75131488]],\n\n       ...,\n\n       [[2.35294118, 3.74999671],\n        [4.30769231, 3.12395504],\n        [1.03529412, 1.54630566],\n        [1.78823529, 1.6247682 ]],\n\n       [[2.82352941, 3.73754173],\n        [3.69230769, 3.14184906],\n        [1.69411765, 1.54911239],\n        [1.69411765, 1.62555177]],\n\n       [[4.23529412, 3.73125568],\n        [6.15384615, 3.15105916],\n        [1.97647059, 1.55052619],\n        [1.31764706, 1.62596401]]])</pre></li><li>Coordinates: (3)<ul><li>time(time)float640.0 2.5 5.0 ... 1.45e+04 1.45e+04<pre>array([0.00000e+00, 2.50000e+00, 5.00000e+00, ..., 1.44925e+04, 1.44950e+04,\n       1.44975e+04])</pre></li><li>population(population)object'FSI' 'LTS' 'CP' 'CS'<pre>array(['FSI', 'LTS', 'CP', 'CS'], dtype=object)</pre></li><li>type(type)&lt;U8'raw' 'smoothed'<pre>array(['raw', 'smoothed'], dtype='&lt;U8')</pre></li></ul></li><li>Indexes: (3)<ul><li>timePandasIndex<pre>PandasIndex(Float64Index([    0.0,     2.5,     5.0,     7.5,    10.0,    12.5,    15.0,\n                 17.5,    20.0,    22.5,\n              ...\n              14475.0, 14477.5, 14480.0, 14482.5, 14485.0, 14487.5, 14490.0,\n              14492.5, 14495.0, 14497.5],\n             dtype='float64', name='time', length=5800))</pre></li><li>populationPandasIndex<pre>PandasIndex(Index(['FSI', 'LTS', 'CP', 'CS'], dtype='object', name='population'))</pre></li><li>typePandasIndex<pre>PandasIndex(Index(['raw', 'smoothed'], dtype='object', name='type'))</pre></li></ul></li><li>Attributes: (4)fs :400.0normalized :Falsesmooth_method :gaussiansmooth_window :50</li></ul> <p>You can plot the data as well. This example just uses the simple plotting built into an xarray, but you ca also plot it with matplotlib.</p> In\u00a0[47]: Copied! <pre>spike_rate.sel(type='raw').plot(hue='population')\nspike_rate.sel(type='smoothed').plot(hue='population')\nplt.title('Population Spike Rate')\nplt.xlim(1000, 5500)\nplt.show()\n</pre> spike_rate.sel(type='raw').plot(hue='population') spike_rate.sel(type='smoothed').plot(hue='population') plt.title('Population Spike Rate') plt.xlim(1000, 5500) plt.show() In\u00a0[48]: Copied! <pre>pulse_times = [(1000,2000), (2500,3500), (4000,5000)]\nspike_rate_avg = average_spike_rate_over_windows(spike_rate, pulse_times)\nspike_rate_avg.sel(type='raw').plot(hue='population')\nspike_rate_avg.sel(type='smoothed').plot(hue='population')\nplt.title('Population Spike Rate avgeraged over windows')\nplt.show()\n</pre> pulse_times = [(1000,2000), (2500,3500), (4000,5000)] spike_rate_avg = average_spike_rate_over_windows(spike_rate, pulse_times) spike_rate_avg.sel(type='raw').plot(hue='population') spike_rate_avg.sel(type='smoothed').plot(hue='population') plt.title('Population Spike Rate avgeraged over windows') plt.show() In\u00a0[49]: Copied! <pre>df = load_spikes_to_df(output_path, network_name=\"cortex\", config=config_path,groupby=['model_template','pop_name'])\npop_stats,indivdual_stats = compute_firing_rate_stats(df, groupby=['model_template','pop_name'])\ndisplay(indivdual_stats)\ndisplay(pop_stats)\n</pre> df = load_spikes_to_df(output_path, network_name=\"cortex\", config=config_path,groupby=['model_template','pop_name']) pop_stats,indivdual_stats = compute_firing_rate_stats(df, groupby=['model_template','pop_name']) display(indivdual_stats) display(pop_stats) node_ids spike_count model_template pop_name firing_rate 0 9848 852 hoc:LTS_Cell LTS 58.787812 1 8541 707 hoc:FSI_Cell FSI 48.782844 2 9430 617 hoc:LTS_Cell LTS 42.572864 3 8527 617 hoc:FSI_Cell FSI 42.572864 4 9214 558 hoc:FSI_Cell FSI 38.501877 ... ... ... ... ... ... 9986 7224 1 hoc:CS_Cell CS 0.069000 9987 8315 1 hoc:CS_Cell CS 0.069000 9988 8980 1 hoc:FSI_Cell FSI 0.069000 9989 9341 1 hoc:FSI_Cell FSI 0.069000 9990 9218 1 hoc:FSI_Cell FSI 0.069000 <p>9991 rows \u00d7 5 columns</p> model_template pop_name firing_rate_mean firing_rate_std 0 hoc:CP_Cell CP 2.210251 1.516544 1 hoc:CS_Cell CS 2.173785 1.428758 2 hoc:FSI_Cell FSI 4.494283 5.867914 3 hoc:LTS_Cell LTS 14.776037 7.553584 In\u00a0[50]: Copied! <pre>plot_firing_rate_distribution(indivdual_stats,groupby='pop_name',plot_type='violin') # type can be box,swarm,violin or a combo \nplt.show()\n</pre> plot_firing_rate_distribution(indivdual_stats,groupby='pop_name',plot_type='violin') # type can be box,swarm,violin or a combo  plt.show() In\u00a0[51]: Copied! <pre>plot_firing_rate_pop_stats(pop_stats, groupby=\"pop_name\")\nplt.show()\n</pre> plot_firing_rate_pop_stats(pop_stats, groupby=\"pop_name\") plt.show() <p>Like the raster function you can also make some changes to the appearance of the plot if you want. Note both plotting functions have the ability only one is shown</p> In\u00a0[52]: Copied! <pre>df = load_spikes_to_df(output_path, network_name=\"cortex\", config=config_path,groupby='pop_name')\npop_stats,indivdual_stats = compute_firing_rate_stats(df, groupby=\"pop_name\")\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5))\nplot_firing_rate_pop_stats(pop_stats, groupby='pop_name',color_map=color_map,ax=ax)\n\nplt.show()\n</pre> df = load_spikes_to_df(output_path, network_name=\"cortex\", config=config_path,groupby='pop_name') pop_stats,indivdual_stats = compute_firing_rate_stats(df, groupby=\"pop_name\")  fig, ax = plt.subplots(1, 1, figsize=(8, 5)) plot_firing_rate_pop_stats(pop_stats, groupby='pop_name',color_map=color_map,ax=ax)  plt.show() In\u00a0[53]: Copied! <pre>from bmtool.analysis.spikes import compare_firing_over_times\n\nspike_df = load_spikes_to_df(\"/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/final_result_2/block1/long/spikes.h5\", \"cortex\",config='/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json')\ncompare_firing_over_times(spike_df,group_by='pop_name',time_window_1=(0,1000),time_window_2=(1000,2000))\n</pre> from bmtool.analysis.spikes import compare_firing_over_times  spike_df = load_spikes_to_df(\"/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/final_result_2/block1/long/spikes.h5\", \"cortex\",config='/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json') compare_firing_over_times(spike_df,group_by='pop_name',time_window_1=(0,1000),time_window_2=(1000,2000)) <pre>Population: FSI\n    Average firing rate in window 1: 4.60 Hz\n    Average firing rate in window 2: 4.69 Hz\n    U-statistic: 366210.50\n    p-value: 0.40101877211121495\n    Significant difference (p&lt;0.05): No\nPopulation: LTS\n    Average firing rate in window 1: 2.14 Hz\n    Average firing rate in window 2: 20.67 Hz\n    U-statistic: 22456.00\n    p-value: 6.613772768519174e-176\n    Significant difference (p&lt;0.05): Yes\nPopulation: CP\n    Average firing rate in window 1: 1.57 Hz\n    Average firing rate in window 2: 2.68 Hz\n    U-statistic: 9692980.50\n    p-value: 1.629101314611588e-10\n    Significant difference (p&lt;0.05): Yes\nPopulation: CS\n    Average firing rate in window 1: 1.57 Hz\n    Average firing rate in window 2: 2.44 Hz\n    U-statistic: 9945849.00\n    p-value: 1.0708612377729946e-18\n    Significant difference (p&lt;0.05): Yes\n</pre>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#using-the-bmtool-spike-analysis-module","title":"Using the bmtool spike analysis module\u00b6","text":"<p>By Gregory Glickert</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#load_spikes_df","title":"load_spikes_df\u00b6","text":"<p>We can use the load_spikes_df to load our network output into a dataframe. It can also be useful to provide the config for the network so we can see which population the cells come from.</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#raster-plot","title":"raster plot\u00b6","text":"<p>You may want a way to look at the spikes to see when they are firing. One way to do this is by a raster plot</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#spike-rate","title":"Spike Rate\u00b6","text":"<p>There is also a population spike rate function. This can turn your dataframe into a time series, which can then be futher analyzed.</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#averaging-the-data","title":"Averaging the data\u00b6","text":"<p>You can also average the data across several windows of time.</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#firing-rate-stats","title":"Firing rate stats\u00b6","text":"<p>You can easily get some firing rate stats and plot them using the below functions</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#compare_firing_over_times","title":"compare_firing_over_times\u00b6","text":"<p>You may want to compare the firing rates during different times during your simulation and compute if the firing is different. This can be useful if you are giving a changing input and want to see if you cells are responding differently.</p>"},{"location":"examples/notebooks/bmplot/bmplot/","title":"BMPlot Tutorial","text":"In\u00a0[1]: Copied! <pre>from bmtk.builder import NetworkBuilder\nfrom bmtk.builder.auxi.node_params import positions_cuboid,positions_list,xiter_random\nfrom bmtool.connectors import ReciprocalConnector, UnidirectionConnector, GapJunction, OneToOneSequentialConnector, syn_section_PN, syn_dist_delay_feng\n\nimport numpy as np\nimport os\n\nif os.path.isfile(\"conn.csv\"):\n    os.remove(\"conn.csv\")\n\n# cell count\nnum_of_PN = 250\nnum_of_FSI = 50\n# generate rand postions\npostions = positions_cuboid(num_of_PN+num_of_FSI,min_dist=10)\n\n# select locations \ninds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_PN, replace=False)\npos = postions[inds, :]\nnet = NetworkBuilder('bio_net')\nnet.add_nodes(N=num_of_PN, \n              pop_name='PN',\n              positions=positions_list(pos),\n              model_type='biophysical',\n              rotation_angle_xaxis = xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),\n              rotation_angle_yaxis = xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),\n              rotation_angle_zaxis= xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),\n              morphology=None)\n# Get rid of coordinates already used\npostions = np.delete(postions, inds, 0)\n\ninds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_FSI, replace=False)\npos = postions[inds, :]\nnet.add_nodes(N=num_of_FSI, \n              pop_name='FSI',\n              positions=positions_list(pos),\n              model_type='biophysical',\n              rotation_angle_xaxis = xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),\n              rotation_angle_yaxis = xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),\n              rotation_angle_zaxis= xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),\n              morphology=None)\n\nbackground = NetworkBuilder('background')\nbackground.add_nodes(N=300,\n                   pop_name='background_nodes',\n                   potential='exc',\n                   model_type='virtual')\n</pre> from bmtk.builder import NetworkBuilder from bmtk.builder.auxi.node_params import positions_cuboid,positions_list,xiter_random from bmtool.connectors import ReciprocalConnector, UnidirectionConnector, GapJunction, OneToOneSequentialConnector, syn_section_PN, syn_dist_delay_feng  import numpy as np import os  if os.path.isfile(\"conn.csv\"):     os.remove(\"conn.csv\")  # cell count num_of_PN = 250 num_of_FSI = 50 # generate rand postions postions = positions_cuboid(num_of_PN+num_of_FSI,min_dist=10)  # select locations  inds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_PN, replace=False) pos = postions[inds, :] net = NetworkBuilder('bio_net') net.add_nodes(N=num_of_PN,                pop_name='PN',               positions=positions_list(pos),               model_type='biophysical',               rotation_angle_xaxis = xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),               rotation_angle_yaxis = xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),               rotation_angle_zaxis= xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),               morphology=None) # Get rid of coordinates already used postions = np.delete(postions, inds, 0)  inds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_FSI, replace=False) pos = postions[inds, :] net.add_nodes(N=num_of_FSI,                pop_name='FSI',               positions=positions_list(pos),               model_type='biophysical',               rotation_angle_xaxis = xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),               rotation_angle_yaxis = xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),               rotation_angle_zaxis= xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),               morphology=None)  background = NetworkBuilder('background') background.add_nodes(N=300,                    pop_name='background_nodes',                    potential='exc',                    model_type='virtual')  <p>Now lets build som example connections using bmtool's connection module</p> In\u00a0[2]: Copied! <pre>connector = UnidirectionConnector(p=0.10,verbose=False,save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\n\nconnector = ReciprocalConnector(p0=0.22,p1=0.36,pr=0.17,verbose=False,save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['FSI']))\nnet.add_edges(**connector.edge_params())\n\nconnector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\n\nconnector = UnidirectionConnector(p=0.30,verbose=False,save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI']))\nnet.add_edges(**connector.edge_params())\n\nconnector = GapJunction(p=0.10,verbose=False,save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI']))\nnet.add_edges(is_gap_junction=True,**connector.edge_params())\n\nconnector = OneToOneSequentialConnector(verbose=False)\nconnector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\nconnector.setup_nodes(target=net.nodes(pop_name = 'FSI'))\nnet.add_edges(**connector.edge_params())\n\nnet.save('network/')\nbackground.save('network/')\n\n\nfrom bmtk.utils.sim_setup import build_env_bionet\n\nbuild_env_bionet(base_dir='./',      \n                network_dir='network/',\n                tstop=3000.0, dt=0.1,\n                spikes_inputs=[('background', \n                                'background.h5')], \n                include_examples=False,    \n                compile_mechanisms=False,   \n                config_file='config.json',\n                overwrite_config=True\n                )\n</pre> connector = UnidirectionConnector(p=0.10,verbose=False,save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params())  connector = ReciprocalConnector(p0=0.22,p1=0.36,pr=0.17,verbose=False,save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['FSI'])) net.add_edges(**connector.edge_params())  connector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params())  connector = UnidirectionConnector(p=0.30,verbose=False,save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI'])) net.add_edges(**connector.edge_params())  connector = GapJunction(p=0.10,verbose=False,save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI'])) net.add_edges(is_gap_junction=True,**connector.edge_params())  connector = OneToOneSequentialConnector(verbose=False) connector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params()) connector.setup_nodes(target=net.nodes(pop_name = 'FSI')) net.add_edges(**connector.edge_params())  net.save('network/') background.save('network/')   from bmtk.utils.sim_setup import build_env_bionet  build_env_bionet(base_dir='./',                       network_dir='network/',                 tstop=3000.0, dt=0.1,                 spikes_inputs=[('background',                                  'background.h5')],                  include_examples=False,                     compile_mechanisms=False,                    config_file='config.json',                 overwrite_config=True                 )   <pre>WARNING:root:No edges have been made for this network, skipping saving of edges file.\n</pre> In\u00a0[3]: Copied! <pre>from bmtool.bmplot import connections as bp\nimport matplotlib.pyplot as plt\n%matplotlib inline\nbp.total_connection_matrix(config='config.json',\n                           sources='bio_net', \n                           targets='bio_net', \n                           tids='pop_name', \n                           sids='pop_name', \n                           no_prepend_pop=True,# if the network source or target should be added to the name or not\n                           include_gap=True # if there are gap junctions in the network should they be included or not in the connection number\n                           )\nplt.show()\n</pre> from bmtool.bmplot import connections as bp import matplotlib.pyplot as plt %matplotlib inline bp.total_connection_matrix(config='config.json',                            sources='bio_net',                             targets='bio_net',                             tids='pop_name',                             sids='pop_name',                             no_prepend_pop=True,# if the network source or target should be added to the name or not                            include_gap=True # if there are gap junctions in the network should they be included or not in the connection number                            ) plt.show() <pre>numprocs=1\n</pre> In\u00a0[4]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.convergence_connection_matrix(**kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.convergence_connection_matrix(**kwargs) plt.show() <p>There are also different methods that can be used. The current methods are 'mean','min','max','stdev' or 'mean+std'. By default the function will use 'mean+std' like above, but you can specify a method like below. In the example below min means display the convergence value that is the smallest in the whole network for those sid and tid pairs.</p> In\u00a0[5]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.convergence_connection_matrix(method='min',\n                                 **kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.convergence_connection_matrix(method='min',                                  **kwargs) plt.show() In\u00a0[6]: Copied! <pre>import pandas as pd\n\nbio_net_convergence = bp.convergence_connection_matrix(return_dict=True,**kwargs)\nplt.show()\n\ndef gen_edge_df(dict):\n    # Get the unique labels (source and target)\n    source_labels = list(dict.keys())\n    target_labels = list(next(iter(dict.values())).keys())\n\n    # Initialize a matrix to store mean and std values\n    matrix_mean = np.full((len(source_labels), len(target_labels)), np.nan)\n    matrix_std = np.full((len(source_labels), len(target_labels)), np.nan)\n\n    # Create a mapping from label to index for source and target\n    source_index = {source_labels[i]: i for i in range(len(source_labels))}\n    target_index = {target_labels[i]: i for i in range(len(target_labels))}\n\n    # Fill the matrices with mean and std values\n    for source in source_labels:\n        for target in target_labels:\n            edge_info = dict.get(source, {}).get(target, 'nan\\nnan')\n            \n            # Check if the edge_info is 'nan\\nnan' or valid\n            if edge_info != 'nan\\nnan':\n                mean, std = map(float, edge_info.split('\\n'))\n                matrix_mean[source_index[source], target_index[target]] = mean\n                matrix_std[source_index[source], target_index[target]] = std\n\n    # Convert the matrices to pandas DataFrame for better visualization\n    df_mean = pd.DataFrame(matrix_mean, index=source_labels, columns=target_labels)\n    df_std = pd.DataFrame(matrix_std, index=source_labels, columns=target_labels)\n\n    return df_mean, df_std\n\nmean_df, std_df = gen_edge_df(bio_net_convergence)\ndisplay(mean_df)\n</pre> import pandas as pd  bio_net_convergence = bp.convergence_connection_matrix(return_dict=True,**kwargs) plt.show()  def gen_edge_df(dict):     # Get the unique labels (source and target)     source_labels = list(dict.keys())     target_labels = list(next(iter(dict.values())).keys())      # Initialize a matrix to store mean and std values     matrix_mean = np.full((len(source_labels), len(target_labels)), np.nan)     matrix_std = np.full((len(source_labels), len(target_labels)), np.nan)      # Create a mapping from label to index for source and target     source_index = {source_labels[i]: i for i in range(len(source_labels))}     target_index = {target_labels[i]: i for i in range(len(target_labels))}      # Fill the matrices with mean and std values     for source in source_labels:         for target in target_labels:             edge_info = dict.get(source, {}).get(target, 'nan\\nnan')                          # Check if the edge_info is 'nan\\nnan' or valid             if edge_info != 'nan\\nnan':                 mean, std = map(float, edge_info.split('\\n'))                 matrix_mean[source_index[source], target_index[target]] = mean                 matrix_std[source_index[source], target_index[target]] = std      # Convert the matrices to pandas DataFrame for better visualization     df_mean = pd.DataFrame(matrix_mean, index=source_labels, columns=target_labels)     df_std = pd.DataFrame(matrix_std, index=source_labels, columns=target_labels)      return df_mean, df_std  mean_df, std_df = gen_edge_df(bio_net_convergence) display(mean_df) PN FSI PN 24.6 54.3 FSI 17.5 18.2 In\u00a0[7]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.divergence_connection_matrix(**kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.divergence_connection_matrix(**kwargs) plt.show() In\u00a0[8]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.gap_junction_matrix(method='convergence',\n                       **kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.gap_junction_matrix(method='convergence',                        **kwargs) plt.show() In\u00a0[9]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.connection_histogram(source_cell='PN', # must be one of the Node attribute\n                        target_cell='FSI',\n                        **kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.connection_histogram(source_cell='PN', # must be one of the Node attribute                         target_cell='FSI',                         **kwargs) plt.show() In\u00a0[10]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.percent_connection_matrix(method='total', # should match the p in the build network above roughly\n                             include_gap=False,\n                             **kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.percent_connection_matrix(method='total', # should match the p in the build network above roughly                              include_gap=False,                              **kwargs) plt.show() In\u00a0[11]: Copied! <pre>bp.connector_percent_matrix(csv_path=\"conn.csv\") # path to csv generated by connectors\nplt.show()\n                            \n</pre> bp.connector_percent_matrix(csv_path=\"conn.csv\") # path to csv generated by connectors plt.show()                              <p>We can see that plot does not look the same as the other percent connectivity or the other plots we have made with the order of the ticks. This can all be adjusted</p> In\u00a0[12]: Copied! <pre>bp.connector_percent_matrix(csv_path=\"conn.csv\", #\n                            exclude_strings=['Gap'], # ignores gap junction can be used to ignore any string \n                            pop_order=['PN','FSI']) # order of x and y ticks\nplt.show()\n</pre> bp.connector_percent_matrix(csv_path=\"conn.csv\", #                             exclude_strings=['Gap'], # ignores gap junction can be used to ignore any string                              pop_order=['PN','FSI']) # order of x and y ticks plt.show() In\u00a0[13]: Copied! <pre>bp.connection_distance(config='config.json',\n                       sources='bio_net',\n                       targets='bio_net',\n                       source_cell_id=1, # node_id \n                       target_id_type='FSI', # pop_name of cells can be left empty for all cells\n                       ignore_z=False) # some connection rules can ignore the z axis\nplt.show()\n</pre> bp.connection_distance(config='config.json',                        sources='bio_net',                        targets='bio_net',                        source_cell_id=1, # node_id                         target_id_type='FSI', # pop_name of cells can be left empty for all cells                        ignore_z=False) # some connection rules can ignore the z axis plt.show() In\u00a0[14]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'background', # note the difference here\n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.convergence_connection_matrix(**kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'background', # note the difference here           'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.convergence_connection_matrix(**kwargs) plt.show() In\u00a0[15]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'model_type',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.convergence_connection_matrix(**kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'model_type',           'tids': 'pop_name',           'no_prepend_pop':True} bp.convergence_connection_matrix(**kwargs) plt.show() In\u00a0[16]: Copied! <pre>kwargs['sids'] = 'pop_name'\nkwargs['sources'] = 'all'\nkwargs['targets'] = 'all'\nkwargs['no_prepend_pop'] = False\nbp.convergence_connection_matrix(**kwargs)\nplt.show()\n</pre> kwargs['sids'] = 'pop_name' kwargs['sources'] = 'all' kwargs['targets'] = 'all' kwargs['no_prepend_pop'] = False bp.convergence_connection_matrix(**kwargs) plt.show() In\u00a0[17]: Copied! <pre>bp.plot_3d_positions(config='config.json', \n                     sources='bio_net', \n                     sid='pop_name', \n                     subset=5 # how many nodes in each pop you want to plot so 5 means take every 5th row and plot it good for extremely large networks\n                     )\nplt.show()\n</pre> bp.plot_3d_positions(config='config.json',                       sources='bio_net',                       sid='pop_name',                       subset=5 # how many nodes in each pop you want to plot so 5 means take every 5th row and plot it good for extremely large networks                      ) plt.show() In\u00a0[18]: Copied! <pre>bp.plot_3d_cell_rotation(config='config.json',\n                         sources='bio_net',\n                         sids='pop_name',\n                         quiver_length=20,\n                         arrow_length_ratio=0.25,\n                         subset=5)\nplt.show()\n</pre> bp.plot_3d_cell_rotation(config='config.json',                          sources='bio_net',                          sids='pop_name',                          quiver_length=20,                          arrow_length_ratio=0.25,                          subset=5) plt.show()"},{"location":"examples/notebooks/bmplot/bmplot/#tutorial-of-how-to-use-bmplot-module","title":"Tutorial of how to use bmplot module\u00b6","text":"<p>By Gregory Glickert</p>"},{"location":"examples/notebooks/bmplot/bmplot/#generate-example-bmtk-network","title":"Generate example bmtk network\u00b6","text":"<p>First we will generate an example BMTK network to then use the functions on</p>"},{"location":"examples/notebooks/bmplot/bmplot/#how-bmplot-works","title":"How bmplot Works\u00b6","text":"<p>In general, bmplot functions take the following inputs:</p> <ul> <li><code>config</code>: Path to a BMTK config file</li> <li><code>sources</code>: Name of the source network</li> <li><code>targets</code>: Name of the target network</li> <li><code>sids</code>: Source IDs</li> <li><code>tids</code>: Target IDs</li> </ul> <p>These inputs are used to generate plots. Additional optional inputs can be provided to refine and clean up the output.</p> <p><code>config</code></p> <ul> <li>A path to the BMTK configuration file.</li> </ul> <p><code>sources</code> and <code>targets</code></p> <ul> <li><p>The name of the source and target network you'd like to analyze.</p> </li> <li><p>In this example generated above a network called <code>bio_net</code> and a network called <code>background</code></p> </li> </ul> <p><code>sids</code>and <code>tids</code></p> <ul> <li><p>The node attribute you want the nodes separted by when generating the plot</p> </li> <li><p>In this example above valid options are <code>pop_name</code>, <code>positions</code>, <code>model_type</code>, <code>rotation_angle_xaxis</code>, <code>rotation_angle_yaxis</code>, <code>rotation_angle_zaxis</code>, and <code>morphology</code>.</p> </li> </ul>"},{"location":"examples/notebooks/bmplot/bmplot/#total_connection_matrix","title":"Total_connection_matrix\u00b6","text":"<p>generates a matrix with the number of total connections between sid and tid pairs.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#convergence_connection_matrix","title":"convergence_connection_matrix\u00b6","text":"<p>generate a matrix with the average and standard deviation of the convergence. Convergence is how many of sid cells connection to a single tid cell.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#connection-dictionary","title":"connection dictionary\u00b6","text":"<p>You can also have the some of the bmplot functions return a dictionary with the connection data. This can be helpful making tables and graphs out of the data. Below is an example function that generates a dataframe from the dictionary and then plots the data. This feature could be helpful for generating custom graphs.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#divergence_connection_matrix","title":"divergence_connection_matrix\u00b6","text":"<p>works the same as the convergence only calculates a different metric. Divergence is a count of how many tids the a given sid cell will go and connect into.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#gap_junction_matrix","title":"gap_junction_matrix\u00b6","text":"<p>generate a matrix only showing the connectivity of gap junctions formed in the network. Method can be either convergence or percent</p>"},{"location":"examples/notebooks/bmplot/bmplot/#connection_histogram","title":"connection_histogram\u00b6","text":"<p>plots the number of connections individual cells in a population receive from another population. The source_cell and target_cell must both be valid strings in sid and tid. In the example below pop_name is used for the sids and tids and \"PN\" and \"FSI\" are valid pop_names in our network</p>"},{"location":"examples/notebooks/bmplot/bmplot/#percent_connection_matrix","title":"percent_connection_matrix\u00b6","text":"<p>generates a matrix of percent connectivity in the network. This percent connectivity does not factor in distance and instead looks at the whole percent connectivity of the population. There are three methods either 'uni', 'bi' or 'total'. 'uni' means unidirectional percent connectivity,'bi' means the bidirectional connectivity and 'total' is both added together.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#connector_percent_matrix","title":"connector_percent_matrix\u00b6","text":"<p>uses the report generated by the bmtool.connectors module to plot the percent connectivity. However this report will show the percent connectivity from possible connections within the connection rule. This makes it the only way to look at percent connectivity of a network while factoring in distance</p>"},{"location":"examples/notebooks/bmplot/bmplot/#connection_distance","title":"connection_distance\u00b6","text":"<p>shows the location of the source and target cells along with the distance between the cells.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#between-network-analysis","title":"Between network analysis\u00b6","text":"<p>Currently the demo has only shown the biophysical connectivity, but bmplot can display any combo of networks connectivity by switching the sources,targets,sids, and tids</p>"},{"location":"examples/notebooks/bmplot/bmplot/#plot_3d_positions","title":"plot_3d_positions\u00b6","text":"<p>takes similar arguments to the previously shown bmplot functions and will generate a 3d plot of where the nodes are located in the network</p>"},{"location":"examples/notebooks/bmplot/bmplot/#plot_3d_cell_rotation","title":"plot_3d_cell_rotation\u00b6","text":"<p>will plot the rotation of the neuron in 3d space.</p>"},{"location":"examples/notebooks/bmplot/run_bionet/","title":"Run bionet","text":"In\u00a0[\u00a0]: Copied! <pre>import os, sys\nfrom bmtk.simulator import bionet\n</pre> import os, sys from bmtk.simulator import bionet In\u00a0[\u00a0]: Copied! <pre>def run(config_file):\n    conf = bionet.Config.from_json(config_file, validate=True)\n    conf.build_env()\n\n    graph = bionet.BioNetwork.from_config(conf)\n    sim = bionet.BioSimulator.from_config(conf, network=graph)\n    sim.run()\n    bionet.nrn.quit_execution()\n</pre> def run(config_file):     conf = bionet.Config.from_json(config_file, validate=True)     conf.build_env()      graph = bionet.BioNetwork.from_config(conf)     sim = bionet.BioSimulator.from_config(conf, network=graph)     sim.run()     bionet.nrn.quit_execution() In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n    if __file__ != sys.argv[-1]:\n        run(sys.argv[-1])\n    else:\n        run('${CONFIG}')\n</pre> if __name__ == '__main__':     if __file__ != sys.argv[-1]:         run(sys.argv[-1])     else:         run('${CONFIG}')"},{"location":"examples/notebooks/connectors/connectors/","title":"Connectors Tutorial","text":"In\u00a0[1]: Copied! <pre>from bmtk.builder import NetworkBuilder\nfrom bmtk.builder.auxi.node_params import positions_cuboid,positions_list,xiter_random\nimport numpy as np\nimport os\n\n# this will be used later\nif os.path.isfile(\"conn.csv\"):\n    os.remove(\"conn.csv\")\n\n# cell count\nnum_of_PN = 250\nnum_of_FSI = 50\n# generate rand postions\npostions = positions_cuboid(num_of_PN+num_of_FSI,min_dist=10)\n\n# select locations \ninds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_PN, replace=False)\npos = postions[inds, :]\nnet = NetworkBuilder('bio_net')\nnet.add_nodes(N=num_of_PN, \n              pop_name='PN',\n              positions=positions_list(pos),\n              model_type='biophysical',\n              morphology=None)\n# Get rid of coordinates already used\npostions = np.delete(postions, inds, 0)\n\ninds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_FSI, replace=False)\npos = postions[inds, :]\nnet.add_nodes(N=num_of_FSI, \n              pop_name='FSI',\n              positions=positions_list(pos),\n              model_type='biophysical',\n              morphology=None)\n\nbackground = NetworkBuilder('background')\nbackground.add_nodes(N=num_of_FSI+num_of_PN,\n                   pop_name='background_nodes',\n                   potential='exc',\n                   model_type='virtual')\n</pre> from bmtk.builder import NetworkBuilder from bmtk.builder.auxi.node_params import positions_cuboid,positions_list,xiter_random import numpy as np import os  # this will be used later if os.path.isfile(\"conn.csv\"):     os.remove(\"conn.csv\")  # cell count num_of_PN = 250 num_of_FSI = 50 # generate rand postions postions = positions_cuboid(num_of_PN+num_of_FSI,min_dist=10)  # select locations  inds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_PN, replace=False) pos = postions[inds, :] net = NetworkBuilder('bio_net') net.add_nodes(N=num_of_PN,                pop_name='PN',               positions=positions_list(pos),               model_type='biophysical',               morphology=None) # Get rid of coordinates already used postions = np.delete(postions, inds, 0)  inds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_FSI, replace=False) pos = postions[inds, :] net.add_nodes(N=num_of_FSI,                pop_name='FSI',               positions=positions_list(pos),               model_type='biophysical',               morphology=None)  background = NetworkBuilder('background') background.add_nodes(N=num_of_FSI+num_of_PN,                    pop_name='background_nodes',                    potential='exc',                    model_type='virtual')  <p>Now that we have our nodes we can start building some edges with the bmtool.connectors module. There are two main types of network models either homogenous or distance depended. A homogenous network will form connections using a constant probability, while a distance depended model will vary its probability based off how far away the cell pairs are from each other.</p> In\u00a0[2]: Copied! <pre>from bmtool.connectors import GaussianDropoff\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n\ndrop_off = GaussianDropoff(stdev=126.77, min_dist=0, max_dist=300,\n                           ptotal=0.1538, ptotal_dist_range=(0, 50.),\n                           dist_type='spherical')\n\ndistances = np.linspace(0, 500, 1000)\nprobabilities_for_dropoff = drop_off.probability(distances)\nplt.plot(distances, probabilities_for_dropoff,label=f'Pmax={drop_off.pmax:.2f} Sigma={drop_off.stdev}')\nplt.title(\"Distance depended connector function\")\nplt.xlabel(\"Distance (um)\")\nplt.ylabel(\"probability of connection\")\nplt.grid(True)\nplt.show()\n</pre> from bmtool.connectors import GaussianDropoff import matplotlib.pyplot as plt import numpy as np %matplotlib inline  drop_off = GaussianDropoff(stdev=126.77, min_dist=0, max_dist=300,                            ptotal=0.1538, ptotal_dist_range=(0, 50.),                            dist_type='spherical')  distances = np.linspace(0, 500, 1000) probabilities_for_dropoff = drop_off.probability(distances) plt.plot(distances, probabilities_for_dropoff,label=f'Pmax={drop_off.pmax:.2f} Sigma={drop_off.stdev}') plt.title(\"Distance depended connector function\") plt.xlabel(\"Distance (um)\") plt.ylabel(\"probability of connection\") plt.grid(True) plt.show() In\u00a0[3]: Copied! <pre>from bmtool.connectors import UnidirectionConnector, spherical_dist\n\n# distance depeneded example \nconnector = UnidirectionConnector(p=drop_off, # function from above cell\n                                  p_arg=spherical_dist, # spherical distance meaning check x,y,z for distance calculation\n                                  verbose=False,\n                                  save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\n\n# constant p example\nconnector = UnidirectionConnector(p=0.10,verbose=False,save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\n</pre> from bmtool.connectors import UnidirectionConnector, spherical_dist  # distance depeneded example  connector = UnidirectionConnector(p=drop_off, # function from above cell                                   p_arg=spherical_dist, # spherical distance meaning check x,y,z for distance calculation                                   verbose=False,                                   save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params())  # constant p example connector = UnidirectionConnector(p=0.10,verbose=False,save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params()) Out[3]: <pre>&lt;bmtk.builder.connection_map.ConnectionMap at 0x177170690&gt;</pre> In\u00a0[4]: Copied! <pre>from bmtool.connectors import ReciprocalConnector\n\n# sets up a homogenous network forward prob of 22%, backwards prob of 36% with a reciprocal probs of 17% for both connections \nconnector = ReciprocalConnector(p0=0.22,p1=0.36,pr=0.17,verbose=False,save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['FSI']))\nnet.add_edges(**connector.edge_params())\n# have to run setup_nodes for both directions since we have forwards and backwards probs.\nconnector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\n</pre> from bmtool.connectors import ReciprocalConnector  # sets up a homogenous network forward prob of 22%, backwards prob of 36% with a reciprocal probs of 17% for both connections  connector = ReciprocalConnector(p0=0.22,p1=0.36,pr=0.17,verbose=False,save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['FSI'])) net.add_edges(**connector.edge_params()) # have to run setup_nodes for both directions since we have forwards and backwards probs. connector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params()) Out[4]: <pre>&lt;bmtk.builder.connection_map.ConnectionMap at 0x177170890&gt;</pre> In\u00a0[5]: Copied! <pre>from bmtool.connectors import GapJunction\n\nconnector = GapJunction(p=0.10,verbose=False,save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI']))\nnet.add_edges(is_gap_junction=True,**connector.edge_params())\n</pre> from bmtool.connectors import GapJunction  connector = GapJunction(p=0.10,verbose=False,save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI'])) net.add_edges(is_gap_junction=True,**connector.edge_params()) Out[5]: <pre>&lt;bmtk.builder.connection_map.ConnectionMap at 0x177188b10&gt;</pre> In\u00a0[6]: Copied! <pre>from bmtool.connectors import OneToOneSequentialConnector\n\nconnector = OneToOneSequentialConnector(verbose=False)\nconnector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\nconnector.setup_nodes(target=net.nodes(pop_name = 'FSI'))\nnet.add_edges(**connector.edge_params())\n</pre> from bmtool.connectors import OneToOneSequentialConnector  connector = OneToOneSequentialConnector(verbose=False) connector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params()) connector.setup_nodes(target=net.nodes(pop_name = 'FSI')) net.add_edges(**connector.edge_params()) Out[6]: <pre>&lt;bmtk.builder.connection_map.ConnectionMap at 0x1771b0550&gt;</pre> In\u00a0[7]: Copied! <pre>net.save('network/')\nbackground.save('network/')\n\n\nfrom bmtk.utils.sim_setup import build_env_bionet\n\nbuild_env_bionet(base_dir='./',      \n                network_dir='network/',\n                tstop=3000.0, dt=0.1,\n                spikes_inputs=[('background', \n                                'background.h5')], \n                include_examples=False,    \n                compile_mechanisms=False,   \n                config_file='config.json',\n                overwrite_config=True\n                )\n</pre> net.save('network/') background.save('network/')   from bmtk.utils.sim_setup import build_env_bionet  build_env_bionet(base_dir='./',                       network_dir='network/',                 tstop=3000.0, dt=0.1,                 spikes_inputs=[('background',                                  'background.h5')],                  include_examples=False,                     compile_mechanisms=False,                    config_file='config.json',                 overwrite_config=True                 ) <pre>WARNING:root:No edges have been made for this network, skipping saving of edges file.\n</pre>"},{"location":"examples/notebooks/connectors/connectors/#example-use-case-for-using-connectors-module","title":"Example use case for using connectors module\u00b6","text":"<p>By Gregory Glickert</p>"},{"location":"examples/notebooks/connectors/connectors/#first-we-will-start-with-an-example-network","title":"First we will start with an example network\u00b6","text":""},{"location":"examples/notebooks/connectors/connectors/#unidirectionalconnector","title":"UnidirectionalConnector\u00b6","text":"<p>The UnidirectionalConnector will make one way or forward connection from the source population to the target population. p is the probability of a connection forming. P can be constant or a deterministic function whose value must be within the range 0 to 1. When p is a constant this will form a homogenous network and the deterministic function can be used to make a distance depended network. There is an optional argument p_arg which are input argument(s) for the p function when using a deterministic function. The example below will use the built in GaussianDropoff.</p>"},{"location":"examples/notebooks/connectors/connectors/#reciprocalconnector","title":"ReciprocalConnector\u00b6","text":"<p>The ReciprocalConnector takes in similar arguments but this time we have p0,p1 and pr. p0 is the probability of a forward connections, p1 is the probability of a backwards connection or a connection from target to source. pr is the probability of reciprocal connection. This connections can get very complex and the Pydocs here has more details on how to use the function.</p> <p>An important thing to note here is how the ReciprocalConnector is called twice. Once for the forward connection and once for the backwards connection. The connector will factor in the reciprocal connections while doing the forward and backwards connections.</p>"},{"location":"examples/notebooks/connectors/connectors/#gapjunction","title":"GapJunction\u00b6","text":"<p>The GapJunction connector is similar to the UnidirectionConnector, but will only form electrical synapses between pairs.</p>"},{"location":"examples/notebooks/connectors/connectors/#correlatedgapjunction","title":"CorrelatedGapJunction\u00b6","text":"<p>NEED TO ADD THIS</p>"},{"location":"examples/notebooks/connectors/connectors/#onetoonesequentialconnector","title":"OneToOneSequentialConnector\u00b6","text":"<p>The OneToOneSequentialConnector is a way to connection two populations one to one. Meaning that only one node from pop A will form a synapses onto one node from pop B. This is normally how background synapses can be modeled. In the example below a one background node is connected to one PN cell in our network.</p>"},{"location":"examples/notebooks/connectors/run_bionet/","title":"Run bionet","text":"In\u00a0[\u00a0]: Copied! <pre>import os, sys\nfrom bmtk.simulator import bionet\n</pre> import os, sys from bmtk.simulator import bionet In\u00a0[\u00a0]: Copied! <pre>def run(config_file):\n    conf = bionet.Config.from_json(config_file, validate=True)\n    conf.build_env()\n\n    graph = bionet.BioNetwork.from_config(conf)\n    sim = bionet.BioSimulator.from_config(conf, network=graph)\n    sim.run()\n    bionet.nrn.quit_execution()\n</pre> def run(config_file):     conf = bionet.Config.from_json(config_file, validate=True)     conf.build_env()      graph = bionet.BioNetwork.from_config(conf)     sim = bionet.BioSimulator.from_config(conf, network=graph)     sim.run()     bionet.nrn.quit_execution() In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n    if __file__ != sys.argv[-1]:\n        run(sys.argv[-1])\n    else:\n        run('${CONFIG}')\n</pre> if __name__ == '__main__':     if __file__ != sys.argv[-1]:         run(sys.argv[-1])     else:         run('${CONFIG}')"},{"location":"examples/notebooks/single_cell/Allen_tutorial/singleCellTutorial/","title":"Allen Database Tutorial","text":"In\u00a0[3]: Copied! <pre>import glob\nimport os\n\ndownload_from_allen = True\nif download_from_allen:\n    from allensdk.api.queries.biophysical_api import BiophysicalApi\n\n    bp = BiophysicalApi()\n    bp.cache_stimulus = False\n    neuronal_model_id = 472451419 \n    bp.cache_data(neuronal_model_id)\n    \nworking_dir = os.getcwd()\njson_files = glob.glob(os.path.join(working_dir, \"*fit*.json\"))\nswc_files = glob.glob(os.path.join(working_dir, \"*.swc\"))\ndynamic_params = json_files[0]\nmorphology = swc_files[0]\nprint(dynamic_params)\nprint(morphology)\n</pre> import glob import os  download_from_allen = True if download_from_allen:     from allensdk.api.queries.biophysical_api import BiophysicalApi      bp = BiophysicalApi()     bp.cache_stimulus = False     neuronal_model_id = 472451419      bp.cache_data(neuronal_model_id)      working_dir = os.getcwd() json_files = glob.glob(os.path.join(working_dir, \"*fit*.json\")) swc_files = glob.glob(os.path.join(working_dir, \"*.swc\")) dynamic_params = json_files[0] morphology = swc_files[0] print(dynamic_params) print(morphology) <pre>2025-09-24 15:18:42,872 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/491459173\n2025-09-24 15:18:47,032 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/496607103\n2025-09-24 15:18:47,285 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337293\n2025-09-24 15:18:47,474 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337054\n2025-09-24 15:18:47,649 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337225\n2025-09-24 15:18:47,783 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337019\n2025-09-24 15:18:47,966 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337003\n2025-09-24 15:18:48,186 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337050\n2025-09-24 15:18:48,327 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337042\n2025-09-24 15:18:48,508 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337011\n2025-09-24 15:18:48,649 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337046\n2025-09-24 15:18:48,788 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337015\n2025-09-24 15:18:49,008 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337066\n2025-09-24 15:18:49,206 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/464138096\n2025-09-24 15:18:49,386 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337007\n2025-09-24 15:18:49,566 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337062\n2025-09-24 15:18:49,732 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/491113425\n2025-09-24 15:18:49,866 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/395337070\n2025-09-24 15:18:50,004 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/497235805\n</pre> <pre>/home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/386049446_fit.json\n/home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/Nr5a1-Cre_Ai14-177334.05.01.01_491459171_m.swc\n</pre> In\u00a0[4]: Copied! <pre># if already compiled then lets delete the folder and force a recompile\nif os.path.isdir('x86_64'):\n    os.system(\"rm -rf x86_64\")\n# compile the mod files\nif not os.path.isdir(\"x86_64\"):\n    os.system(\"nrnivmodl modfiles\")\n</pre> # if already compiled then lets delete the folder and force a recompile if os.path.isdir('x86_64'):     os.system(\"rm -rf x86_64\") # compile the mod files if not os.path.isdir(\"x86_64\"):     os.system(\"nrnivmodl modfiles\") <pre>/home/gjgpb9/miniconda3/envs/bmtk/bin/nrnivmodl:10: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import working_set\nTranslating CaDynamics.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/CaDynamics.c\nTranslating Ca_HVA.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/Ca_HVA.c\nTranslating Ca_LVA.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/Ca_LVA.c\nThread Safe\nThread Safe\nThread Safe\n</pre> <pre>/home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial\nMod files: \"modfiles/modfiles/CaDynamics.mod\" \"modfiles/modfiles/Ca_HVA.mod\" \"modfiles/modfiles/Ca_LVA.mod\" \"modfiles/modfiles/Ih.mod\" \"modfiles/modfiles/Im.mod\" \"modfiles/modfiles/Im_v2.mod\" \"modfiles/modfiles/Kd.mod\" \"modfiles/modfiles/K_P.mod\" \"modfiles/modfiles/K_T.mod\" \"modfiles/modfiles/Kv2like.mod\" \"modfiles/modfiles/Kv3_1.mod\" \"modfiles/modfiles/Nap.mod\" \"modfiles/modfiles/NaTa.mod\" \"modfiles/modfiles/NaTs.mod\" \"modfiles/modfiles/NaV.mod\" \"modfiles/modfiles/SK.mod\"\n\nCreating 'x86_64' directory for .o files.\n\n -&gt; Compiling mod_func.cpp\n -&gt; NMODL ../modfiles/CaDynamics.mod\n -&gt; NMODL ../modfiles/Ca_LVA.mod\n -&gt; NMODL ../modfiles/Ca_HVA.mod\n -&gt; NMODL ../modfiles/Ih.mod\n -&gt; NMODL ../modfiles/Im.mod\n -&gt; NMODL ../modfiles/Im_v2.mod\n -&gt; NMODL ../modfiles/Kd.mod\n -&gt; NMODL ../modfiles/K_P.mod\n -&gt; NMODL ../modfiles/K_T.mod\n -&gt; NMODL ../modfiles/Kv2like.mod\n -&gt; NMODL ../modfiles/Kv3_1.mod\n -&gt; NMODL ../modfiles/Nap.mod\n -&gt; NMODL ../modfiles/NaTa.mod\n -&gt; NMODL ../modfiles/NaTs.mod\n -&gt; NMODL ../modfiles/NaV.mod\n -&gt; NMODL ../modfiles/SK.mod\n -&gt; Compiling CaDynamics.c\n -&gt; Compiling Ca_HVA.c\n -&gt; Compiling Ca_LVA.c\n -&gt; Compiling Ih.c\n -&gt; Compiling Im.c\n -&gt; Compiling Im_v2.c\n -&gt; Compiling Kd.c\n</pre> <pre>Translating Ih.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/Ih.c\nThread Safe\nTranslating Im.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/Im.c\nTranslating Im_v2.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/Im_v2.c\nThread Safe\nThread Safe\nTranslating Kd.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/Kd.c\nThread Safe\nTranslating K_P.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/K_P.c\nTranslating K_T.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/K_T.c\nThread Safe\nThread Safe\nTranslating Kv2like.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/Kv2like.c\nThread Safe\nTranslating Kv3_1.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/Kv3_1.c\nTranslating Nap.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/Nap.c\nThread Safe\nThread Safe\nTranslating NaTa.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/NaTa.c\nTranslating NaTs.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/NaTs.c\nThread Safe\nTranslating NaV.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/NaV.c\nThread Safe\nNEURON's CVode method ignores conservation\nNotice: LINEAR is not thread safe.\nTranslating SK.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Allen_tutorial/x86_64/SK.c\nThread Safe\n</pre> <pre> -&gt; Compiling K_P.c\n -&gt; Compiling K_T.c\n -&gt; Compiling Kv2like.c\n -&gt; Compiling Kv3_1.c\n -&gt; Compiling Nap.c\n -&gt; Compiling NaTa.c\n -&gt; Compiling NaTs.c\n -&gt; Compiling NaV.c\n -&gt; Compiling SK.c\n =&gt; LINKING shared library ./libnrnmech.so\n =&gt; LINKING executable ./special LDFLAGS are:    -pthread\nSuccessfully created x86_64/special\n</pre> <p>There may be some warnings from the logger that I don't want to see so i will just turn them off. Comment this line out if you want to see the warnings</p> In\u00a0[5]: Copied! <pre>import logging\n\nlogging.disable(logging.WARNING)\n</pre> import logging  logging.disable(logging.WARNING) In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\nfrom bmtool.singlecell import CurrentClamp,run_and_plot,Profiler,load_allen_database_cells\n\ncell_creater = load_allen_database_cells(morphology=morphology,\n                                 dynamic_params=dynamic_params,\n                                 model_processing='aibs_allactive')\n</pre> import matplotlib.pyplot as plt from bmtool.singlecell import CurrentClamp,run_and_plot,Profiler,load_allen_database_cells  cell_creater = load_allen_database_cells(morphology=morphology,                                  dynamic_params=dynamic_params,                                  model_processing='aibs_allactive') <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> <p>Now we can use this cell_creater in a similar way to the cell template name in the Neuron_hoc tutorial.</p> In\u00a0[7]: Copied! <pre>profiler = Profiler(template_dir='.', mechanism_dir = 'modfiles', dt=0.1)\n\nsim = CurrentClamp(cell_creater, inj_amp=500., inj_delay=1500., inj_dur=1000., tstop=3000., threshold=-15.)\n\nX, Y = run_and_plot(sim, title='Current Injection', xlabel='Time (ms)',\n                    ylabel='Membrane Potential (mV)', plot_injection_only=True)\nplt.show()\n</pre>  profiler = Profiler(template_dir='.', mechanism_dir = 'modfiles', dt=0.1)  sim = CurrentClamp(cell_creater, inj_amp=500., inj_delay=1500., inj_dur=1000., tstop=3000., threshold=-15.)  X, Y = run_and_plot(sim, title='Current Injection', xlabel='Time (ms)',                     ylabel='Membrane Potential (mV)', plot_injection_only=True) plt.show() <pre>NEURON mechanisms not found in modfiles.\nInjection location: Biophys1[0].soma[0](0.5)\nRecording: Biophys1[0].soma[0](0.5)._ref_v\nCurrent clamp simulation running...\n\nNumber of spikes: 15\n\n</pre> In\u00a0[8]: Copied! <pre>from bmtool.singlecell import FI\n\nsim = FI(cell_creater,tdur=1000,i_increment=50)\nX, Y = run_and_plot(sim,xlabel='Current',ylabel=\"Spikes\")\nplt.show()\n</pre> from bmtool.singlecell import FI  sim = FI(cell_creater,tdur=1000,i_increment=50) X, Y = run_and_plot(sim,xlabel='Current',ylabel=\"Spikes\") plt.show() <pre>Injection location: Biophys1[21].soma[0](0.5)\nRecording: Biophys1[21].soma[0](0.5)._ref_v\nRunning simulations for FI curve...\n\nResults\n    Injection (nA):  number of spikes\n0              0.00                 0\n1              0.05                 0\n2              0.10                 2\n3              0.15                 6\n4              0.20                 9\n5              0.25                11\n6              0.30                12\n7              0.35                13\n8              0.40                14\n9              0.45                14\n10             0.50                15\n11             0.55                15\n12             0.60                15\n13             0.65                15\n14             0.70                 5\n15             0.75                 4\n16             0.80                 3\n17             0.85                 3\n18             0.90                 2\n19             0.95                 2\n20             1.00                 2\n\n</pre>"},{"location":"examples/notebooks/single_cell/Allen_tutorial/singleCellTutorial/#example-of-how-to-load-an-allen-database-cell-into-the-single-cell-module","title":"Example of how to load an Allen Database cell into the single cell module\u00b6","text":"<p>By Gregory Glickert</p>"},{"location":"examples/notebooks/single_cell/Allen_tutorial/singleCellTutorial/#first-download-the-cell-from-allen-database","title":"First download the cell from Allen Database\u00b6","text":""},{"location":"examples/notebooks/single_cell/Allen_tutorial/singleCellTutorial/#load-the-allen-database-cell","title":"Load the Allen Database cell\u00b6","text":"<p>To use the Allen cell with bmtool we need to use the load_allen_database_cells function. This will load the cell into a format that bmtool can use</p>"},{"location":"examples/notebooks/single_cell/Neuron_hoc/Single-Cell-Tuning/","title":"Neuron Hoc Tutorial","text":"In\u00a0[1]: Copied! <pre>import os\n\n# if already compiled then lets delete the folder and force a recompile\nif os.path.isdir('modfiles/x86_64'):\n    os.system(\"rm -rf modfiles/x86_64 \")\n# compile the mod files\nif not os.path.isdir(\"modfiles/x86_64\"):\n    os.chdir('modfiles')\n    os.system(\"nrnivmodl\")\n    os.chdir(\"..\")\n</pre> import os  # if already compiled then lets delete the folder and force a recompile if os.path.isdir('modfiles/x86_64'):     os.system(\"rm -rf modfiles/x86_64 \") # compile the mod files if not os.path.isdir(\"modfiles/x86_64\"):     os.chdir('modfiles')     os.system(\"nrnivmodl\")     os.chdir(\"..\") <pre>/home/gjgpb9/miniconda3/envs/bmtk/bin/nrnivmodl:10: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import working_set\n</pre> <pre>/home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles\nMod files: \"./AMPA_NMDA_STP.mod\" \"./cadad.mod\" \"./cal2.mod\" \"./can_mig.mod\" \"./exp2syn_stp.mod\" \"./GABA_A_STP.mod\" \"./gap.mod\" \"./Gfluct.mod\" \"./h_kole.mod\" \"./imCA3.mod\" \"./kap_BS.mod\" \"./kBK.mod\" \"./kdmc_BS.mod\" \"./kdr_BS.mod\" \"./kdrCA3.mod\" \"./kdrinter.mod\" \"./leak.mod\" \"./nainter.mod\" \"./napCA3.mod\" \"./natCA3.mod\" \"./nax_BS.mod\" \"./vecevent_coreneuron.mod\"\n\nCreating 'x86_64' directory for .o files.\n\n -&gt; Compiling mod_func.cpp\n -&gt; NMODL ../AMPA_NMDA_STP.mod\n -&gt; NMODL ../cal2.mod\n -&gt; NMODL ../cadad.mod\nWarning: Default 2 of PARAMETER cao will be ignored and set by NEURON.\nWarning: Default 5e-05 of PARAMETER cai will be ignored and set by NEURON.\n -&gt; NMODL ../can_mig.mod\n -&gt; NMODL ../exp2syn_stp.mod\n -&gt; NMODL ../GABA_A_STP.mod\nWarning: Default 2 of PARAMETER cao will be ignored and set by NEURON.\nWarning: Default 5e-05 of PARAMETER cai will be ignored and set by NEURON.\n -&gt; NMODL ../gap.mod\n -&gt; NMODL ../Gfluct.mod\n -&gt; NMODL ../h_kole.mod\n -&gt; NMODL ../imCA3.mod\n -&gt; NMODL ../kap_BS.mod\n -&gt; NMODL ../kBK.mod\n -&gt; NMODL ../kdmc_BS.mod\n -&gt; NMODL ../kdr_BS.mod\n -&gt; NMODL ../kdrCA3.mod\n -&gt; NMODL ../kdrinter.mod\n -&gt; NMODL ../leak.mod\n -&gt; NMODL ../nainter.mod\nWarning: Default -80 of PARAMETER ek will be ignored and set by NEURON.\n -&gt; NMODL ../napCA3.mod\nWarning: Default 45 of PARAMETER ena will be ignored and set by NEURON.\n -&gt; NMODL ../natCA3.mod\n -&gt; NMODL ../nax_BS.mod\n -&gt; NMODL ../vecevent_coreneuron.mod\nNotice: ARTIFICIAL_CELL is a synonym for POINT_PROCESS which hints that it\nonly affects and is affected by discrete events. As such it is not\nlocated in a section and is not associated with an integrator\n -&gt; Compiling AMPA_NMDA_STP.c\n -&gt; Compiling cadad.c\n -&gt; Compiling cal2.c\n -&gt; Compiling can_mig.c\n</pre> <pre>Translating cal2.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/cal2.c\nTranslating AMPA_NMDA_STP.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/AMPA_NMDA_STP.c\nTranslating cadad.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/cadad.c\nThread Safe\nThread Safe\nThread Safe\nTranslating can_mig.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/can_mig.c\nTranslating exp2syn_stp.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/exp2syn_stp.c\nTranslating GABA_A_STP.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/GABA_A_STP.c\nThread Safe\nThread Safe\nThread Safe\nTranslating gap.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/gap.c\nThread Safe\nTranslating h_kole.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/h_kole.c\nTranslating Gfluct.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/Gfluct.c\nNotice: This mechanism cannot be used with CVODE\nThread Safe\nThread Safe\nTranslating imCA3.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/imCA3.c\nThread Safe\nTranslating kap_BS.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/kap_BS.c\nTranslating kBK.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/kBK.c\nThread Safe\nThread Safe\nTranslating kdmc_BS.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/kdmc_BS.c\nThread Safe\nTranslating kdr_BS.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/kdr_BS.c\nTranslating kdrCA3.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/kdrCA3.c\nThread Safe\nThread Safe\nTranslating kdrinter.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/kdrinter.c\nThread Safe\nTranslating leak.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/leak.c\nTranslating nainter.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/nainter.c\nThread Safe\nThread Safe\nTranslating napCA3.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/napCA3.c\nThread Safe\nTranslating natCA3.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/natCA3.c\nTranslating nax_BS.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/nax_BS.c\nThread Safe\nTranslating vecevent_coreneuron.mod into /home/gjgpb9/cortex_modeling/bmtool/docs/examples/notebooks/single_cell/Neuron_hoc/modfiles/x86_64/vecevent_coreneuron.c\nThread Safe\nThread Safe\n</pre> <pre> -&gt; Compiling exp2syn_stp.c\n -&gt; Compiling GABA_A_STP.c\n -&gt; Compiling gap.c\n -&gt; Compiling Gfluct.c\n -&gt; Compiling h_kole.c\n -&gt; Compiling imCA3.c\n -&gt; Compiling kap_BS.c\n -&gt; Compiling kBK.c\n -&gt; Compiling kdmc_BS.c\n -&gt; Compiling kdr_BS.c\n -&gt; Compiling kdrCA3.c\n -&gt; Compiling kdrinter.c\n -&gt; Compiling leak.c\n -&gt; Compiling nainter.c\n -&gt; Compiling napCA3.c\n -&gt; Compiling natCA3.c\n -&gt; Compiling nax_BS.c\n -&gt; Compiling vecevent_coreneuron.c\n =&gt; LINKING shared library ./libnrnmech.so\n =&gt; LINKING executable ./special LDFLAGS are:    -pthread\nSuccessfully created x86_64/special\n</pre> In\u00a0[2]: Copied! <pre>import numpy as np\nfrom neuron import h\nimport matplotlib.pyplot as plt\nfrom bmtool.singlecell import Profiler, Passive, CurrentClamp, FI, ZAP, run_and_plot\n%matplotlib inline\n\ntemplate_dir = '.' #templates are in templates.hoc in this working dir\nmechanism_dir = 'modfiles'\n\nprofiler = Profiler(template_dir=template_dir, mechanism_dir=mechanism_dir, dt = 0.05)\n</pre> import numpy as np from neuron import h import matplotlib.pyplot as plt from bmtool.singlecell import Profiler, Passive, CurrentClamp, FI, ZAP, run_and_plot %matplotlib inline  template_dir = '.' #templates are in templates.hoc in this working dir mechanism_dir = 'modfiles'  profiler = Profiler(template_dir=template_dir, mechanism_dir=mechanism_dir, dt = 0.05) <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> In\u00a0[3]: Copied! <pre>noise = False\npost_init_function = 'insert_mechs(0)' if noise else None\n\nbasic_settings = {\n    'Passive': {\n        'celsius': 26.,\n        'kwargs': {\n            'inj_amp': -50.,\n            'inj_delay': 1500.,\n            'inj_dur': 1000.,\n            'tstop': 2500.,\n            'method': 'exp2' # can be exp2, exp or simple\n        }\n    },\n    'CurrentClamp': {\n        'celsius': 34.,\n        'kwargs': {\n            'post_init_function': post_init_function,\n            'inj_amp': 350.,\n            'inj_delay': 1500.,\n            'inj_dur': 1000.,\n            'tstop': 3000.,\n            'threshold': 0.\n        }\n    },\n    'ZAP': {\n        'celsius': 34.,\n        'kwargs': {\n            'post_init_function': post_init_function,\n            'inj_amp': 100.,\n            'inj_delay': 1000.,\n            'inj_dur': 15000.,\n            'tstop': 15500.,\n            'fstart': 0.,\n            'fend': 15.,\n            'chirp_type': 'linear'\n        }\n    },\n    'FI': {\n        'celsius': 34.,\n        'kwargs': {\n            'post_init_function': post_init_function,\n            'i_start': 0.,\n            'i_stop': 2000.,\n            'i_increment': 100.,\n            'tstart': 1500.\n        }\n    }\n}\n\nCell = 'CP_Cell'\n</pre> noise = False post_init_function = 'insert_mechs(0)' if noise else None  basic_settings = {     'Passive': {         'celsius': 26.,         'kwargs': {             'inj_amp': -50.,             'inj_delay': 1500.,             'inj_dur': 1000.,             'tstop': 2500.,             'method': 'exp2' # can be exp2, exp or simple         }     },     'CurrentClamp': {         'celsius': 34.,         'kwargs': {             'post_init_function': post_init_function,             'inj_amp': 350.,             'inj_delay': 1500.,             'inj_dur': 1000.,             'tstop': 3000.,             'threshold': 0.         }     },     'ZAP': {         'celsius': 34.,         'kwargs': {             'post_init_function': post_init_function,             'inj_amp': 100.,             'inj_delay': 1000.,             'inj_dur': 15000.,             'tstop': 15500.,             'fstart': 0.,             'fend': 15.,             'chirp_type': 'linear'         }     },     'FI': {         'celsius': 34.,         'kwargs': {             'post_init_function': post_init_function,             'i_start': 0.,             'i_stop': 2000.,             'i_increment': 100.,             'tstart': 1500.         }     } }  Cell = 'CP_Cell' In\u00a0[4]: Copied! <pre>proc = basic_settings['Passive']\nh.celsius = proc['celsius']\nsim = Passive(Cell, **proc['kwargs'])\ntitle = 'Passive Cell Current Injection'\nxlabel = 'Time (ms)'\nylabel = 'Membrane Potential (mV)'\n</pre> proc = basic_settings['Passive'] h.celsius = proc['celsius'] sim = Passive(Cell, **proc['kwargs']) title = 'Passive Cell Current Injection' xlabel = 'Time (ms)' ylabel = 'Membrane Potential (mV)' <pre>Injection location: CP_Cell[0].soma[0](0.5)\nRecording: CP_Cell[0].soma[0](0.5)._ref_v\n</pre> <p>we will have different plots for the different methods</p> In\u00a0[5]: Copied! <pre>if sim.method =='exp2':\n    X, Y = run_and_plot(sim, title, xlabel, ylabel, plot_injection_only=True)\n    plt.gca().plot(*sim.double_exponential_fit(), 'r:', label='double exponential fit')\n    plt.legend()\nelif sim.method =='exp':\n    X, Y = run_and_plot(sim, title, xlabel, ylabel, plot_injection_only=True)\n    plt.gca().plot(*sim.single_exponential_fit(), 'r:', label='single exponential fit')\n    plt.legend()\nelse:\n    X, Y = run_and_plot(sim, title, xlabel, ylabel, plot_injection_only=True)\nplt.show()\n</pre> if sim.method =='exp2':     X, Y = run_and_plot(sim, title, xlabel, ylabel, plot_injection_only=True)     plt.gca().plot(*sim.double_exponential_fit(), 'r:', label='double exponential fit')     plt.legend() elif sim.method =='exp':     X, Y = run_and_plot(sim, title, xlabel, ylabel, plot_injection_only=True)     plt.gca().plot(*sim.single_exponential_fit(), 'r:', label='single exponential fit')     plt.legend() else:     X, Y = run_and_plot(sim, title, xlabel, ylabel, plot_injection_only=True) plt.show() <pre>Running simulation for passive properties...\n\nV Rest: -67.74 (mV)\nResistance: 86.03 (MOhms)\nMembrane time constant: 39.95 (ms)\n\nV_rest Calculation: Voltage taken at time 1500.0 (ms) is\n-67.74 (mV)\n\nR_in Calculation: dV/dI = (v_final-v_rest)/(i_final-i_start)\n(-72.04 - (-67.74)) / (-0.05 - 0)\n4.30 (mV) / 0.05 (nA) = 86.03 (MOhms)\n\nTau Calculation: Fit a double exponential curve to the membrane potential response\nf(t) = a0 + a1*exp(-t/tau1) + a2*exp(-t/tau2)\nConstrained by initial value: f(0) = a0 + a1 + a2 = v_rest\nFit parameters: (a0, a1, a2, tau1, tau2) = (-72.03, -0.48, 4.77, 39.95, 11.90)\nMembrane time constant is determined from the slowest exponential term: 39.95 (ms)\n\nSag potential: v_sag = v_peak - v_final = -0.08 (mV)\nNormalized sag potential: v_sag / (v_peak - v_rest) = 0.019\n\n</pre> In\u00a0[6]: Copied! <pre>proc = basic_settings['CurrentClamp']\nh.celsius = proc['celsius']\nsim2 = CurrentClamp(Cell, **proc['kwargs'])\ntitle = 'Current Injection'\nxlabel = 'Time (ms)'\nylabel = 'Membrane Potential (mV)'\n</pre> proc = basic_settings['CurrentClamp'] h.celsius = proc['celsius'] sim2 = CurrentClamp(Cell, **proc['kwargs']) title = 'Current Injection' xlabel = 'Time (ms)' ylabel = 'Membrane Potential (mV)' <pre>Injection location: CP_Cell[1].soma[0](0.5)\nRecording: CP_Cell[1].soma[0](0.5)._ref_v\n</pre> In\u00a0[7]: Copied! <pre>X, Y = run_and_plot(sim2, title, xlabel, ylabel, plot_injection_only=True)\nplt.show()\n</pre> X, Y = run_and_plot(sim2, title, xlabel, ylabel, plot_injection_only=True) plt.show() <pre>Current clamp simulation running...\n\nNumber of spikes: 27\n\n</pre> In\u00a0[8]: Copied! <pre>proc = basic_settings['ZAP']\nh.celsius = proc['celsius']\nsim3 = ZAP(Cell, **proc['kwargs'])\ntitle = 'ZAP Response'\nxlabel = 'Time (ms)'\nylabel = 'Membrane Potential (mV)'\n</pre> proc = basic_settings['ZAP'] h.celsius = proc['celsius'] sim3 = ZAP(Cell, **proc['kwargs']) title = 'ZAP Response' xlabel = 'Time (ms)' ylabel = 'Membrane Potential (mV)' <pre>Injection location: CP_Cell[2].soma[0](0.5)\nRecording: CP_Cell[2].soma[0](0.5)._ref_v\n</pre> In\u00a0[9]: Copied! <pre>X, Y = run_and_plot(sim3, title, xlabel, ylabel, plot_injection_only=True)\n\nplt.figure()\nplt.plot(X, sim3.zap_vec)\nplt.title('ZAP Current')\nplt.xlabel('Time (ms)')\nplt.ylabel('Current Injection (nA)')\n\nplt.figure()\nplt.plot(*sim3.get_impedance(smooth=9))\nplt.title('Impedance Amplitude Profile')\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Impedance (MOhms)')\nplt.show()\n</pre> X, Y = run_and_plot(sim3, title, xlabel, ylabel, plot_injection_only=True)  plt.figure() plt.plot(X, sim3.zap_vec) plt.title('ZAP Current') plt.xlabel('Time (ms)') plt.ylabel('Current Injection (nA)')  plt.figure() plt.plot(*sim3.get_impedance(smooth=9)) plt.title('Impedance Amplitude Profile') plt.xlabel('Frequency (Hz)') plt.ylabel('Impedance (MOhms)') plt.show() <pre>ZAP current simulation running...\n\nChirp current injection with frequency changing from 0 to 15 Hz over 15 seconds\nImpedance is calculated as the ratio of FFT amplitude of membrane voltage to FFT amplitude of chirp current\n\nResonant Peak Frequency: 0.333 (Hz)\n</pre> In\u00a0[10]: Copied! <pre>proc = basic_settings['FI']\nh.celsius = proc['celsius']\nsim4 = FI(Cell, **proc['kwargs'])\ntitle = 'FI Curve'\nxlabel = 'Injection (nA)'\nylabel = '# Spikes'\n</pre> proc = basic_settings['FI'] h.celsius = proc['celsius'] sim4 = FI(Cell, **proc['kwargs']) title = 'FI Curve' xlabel = 'Injection (nA)' ylabel = '# Spikes' <pre>Injection location: CP_Cell[22].soma[0](0.5)\nRecording: CP_Cell[22].soma[0](0.5)._ref_v\n</pre> In\u00a0[11]: Copied! <pre>X, Y = run_and_plot(sim4, title, xlabel, ylabel)\nplt.show()\n</pre> X, Y = run_and_plot(sim4, title, xlabel, ylabel) plt.show() <pre>Running simulations for FI curve...\n\nResults\n    Injection (nA):  number of spikes\n0               0.0                 0\n1               0.1                 0\n2               0.2                15\n3               0.3                23\n4               0.4                31\n5               0.5                40\n6               0.6                49\n7               0.7                59\n8               0.8                70\n9               0.9                 4\n10              1.0                 2\n11              1.1                 2\n12              1.2                 2\n13              1.3                 2\n14              1.4                 2\n15              1.5                 1\n16              1.6                 1\n17              1.7                 1\n18              1.8                 1\n19              1.9                 1\n\n</pre> <p>You can then take the output of the FI curve and find some simple properties of it</p> In\u00a0[12]: Copied! <pre>def find_slope(X, Y):\n    \"\"\"Finds the slope and intercept using least squares\n    \n    \"\"\"\n    # Create the A matrix with a column of X and a column of ones (for the intercept term)\n    A = np.vstack([X, np.ones_like(X)]).T\n\n    # Perform least-squares fitting to find the slope (m) and intercept (c)\n    m, c = np.linalg.lstsq(A, Y, rcond=None)[0]\n\n    # Return the slope (m) and intercept (c)\n    return m, c\n\ndef fit_slope_to_max_y(X, Y):\n    \"\"\"Fits a line between Y &gt; 0 and the point where Y reaches its maximum\"\"\"\n    # Find where Y &gt; 0\n    positive_data = Y &gt; 0\n    X_positive = X[positive_data]\n    Y_positive = Y[positive_data]\n\n    # Find the index of the maximum Y value\n    max_idx = np.argmax(Y_positive)\n\n    # Only use data from where Y is increasing up to the max value\n    X_fit = X_positive[:max_idx + 1]\n    Y_fit = Y_positive[:max_idx + 1]\n\n    # Find the slope and intercept for this range\n    m, c = find_slope(X_fit, Y_fit)\n\n    return m, c, X_fit, Y_fit\n\ndef find_rheobase(X,Y):\n    \"\"\"Rheobase is min current to cause a spike\"\"\"\n    non_zero_indices = np.nonzero(Y)[0]\n    rheobase = X[non_zero_indices[0]]\n    print(\"Rheobase: \" + str(rheobase) + 'pA')\n\nX = X * 1000 #nA to pA\nm, c, X_fit, Y_fit = fit_slope_to_max_y(X, Y)\nplt.scatter(X,Y)\nplt.plot(X_fit, m*X_fit + c, label=f'Fit Line: y = {m:.2f}x + {c:.2f}', color='red')\nplt.title(\"Fitted slope to FI curve\")\nplt.legend()\nplt.show()\n\nfind_rheobase(X,Y)\n</pre> def find_slope(X, Y):     \"\"\"Finds the slope and intercept using least squares          \"\"\"     # Create the A matrix with a column of X and a column of ones (for the intercept term)     A = np.vstack([X, np.ones_like(X)]).T      # Perform least-squares fitting to find the slope (m) and intercept (c)     m, c = np.linalg.lstsq(A, Y, rcond=None)[0]      # Return the slope (m) and intercept (c)     return m, c  def fit_slope_to_max_y(X, Y):     \"\"\"Fits a line between Y &gt; 0 and the point where Y reaches its maximum\"\"\"     # Find where Y &gt; 0     positive_data = Y &gt; 0     X_positive = X[positive_data]     Y_positive = Y[positive_data]      # Find the index of the maximum Y value     max_idx = np.argmax(Y_positive)      # Only use data from where Y is increasing up to the max value     X_fit = X_positive[:max_idx + 1]     Y_fit = Y_positive[:max_idx + 1]      # Find the slope and intercept for this range     m, c = find_slope(X_fit, Y_fit)      return m, c, X_fit, Y_fit  def find_rheobase(X,Y):     \"\"\"Rheobase is min current to cause a spike\"\"\"     non_zero_indices = np.nonzero(Y)[0]     rheobase = X[non_zero_indices[0]]     print(\"Rheobase: \" + str(rheobase) + 'pA')  X = X * 1000 #nA to pA m, c, X_fit, Y_fit = fit_slope_to_max_y(X, Y) plt.scatter(X,Y) plt.plot(X_fit, m*X_fit + c, label=f'Fit Line: y = {m:.2f}x + {c:.2f}', color='red') plt.title(\"Fitted slope to FI curve\") plt.legend() plt.show()  find_rheobase(X,Y)  <pre>Rheobase: 200.0pA\n</pre>"},{"location":"examples/notebooks/single_cell/Neuron_hoc/Single-Cell-Tuning/#example-of-how-to-use-bmtool-single-cell-analysis-with-neuron-hoc-files","title":"Example of how to use BMTOOL single cell analysis with neuron hoc files\u00b6","text":"<p>By Gregory Glickert</p> <p>First we make sure our modfiles are compiled</p>"},{"location":"examples/notebooks/single_cell/Neuron_hoc/Single-Cell-Tuning/#init-our-profiler","title":"Init our profiler\u00b6","text":"<p>we need to init our profiler this will make sure the templates and mechanisms are loaded</p>"},{"location":"examples/notebooks/single_cell/Neuron_hoc/Single-Cell-Tuning/#then-we-will-setup-our-settings-for-each-function-in-the-singlecell-module-these-setting-may-change-depending-on-cell-type-and-the-biology-data-you-have-to-constain-off-of-for-now-some-basic-ones-are-used-for-looking-at-principle-cells","title":"Then we will setup our settings for each function in the singlecell module. These setting may change depending on cell type and the biology data you have to constain off of. For now some basic ones are used for looking at principle cells\u00b6","text":""},{"location":"examples/notebooks/single_cell/Neuron_hoc/Single-Cell-Tuning/#passive-properties","title":"Passive Properties\u00b6","text":""},{"location":"examples/notebooks/single_cell/Neuron_hoc/Single-Cell-Tuning/#current-injection","title":"Current Injection\u00b6","text":""},{"location":"examples/notebooks/single_cell/Neuron_hoc/Single-Cell-Tuning/#impedance-amplitude-profile-zap","title":"Impedance Amplitude Profile (ZAP)\u00b6","text":""},{"location":"examples/notebooks/single_cell/Neuron_hoc/Single-Cell-Tuning/#f-i-curve","title":"F-I Curve\u00b6","text":""},{"location":"examples/notebooks/single_cell/Neuron_hoc/optimizer_test/","title":"Optimizer test","text":"In\u00a0[1]: Copied! <pre>from bmtool.singlecell import Profiler, Passive, CurrentClamp, FI, ZAP, run_and_plot\nfrom neuron import h\nimport neuron\nfrom scipy.optimize import minimize\n\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple, Callable, Optional\nimport numpy as np\n\n# Load the cell template\nneuron.load_mechanisms(\"modfiles\")\nh.load_file(\"stdrun.hoc\")\nh.load_file(\"templates.hoc\")  # Replace with your HOC file\n</pre> from bmtool.singlecell import Profiler, Passive, CurrentClamp, FI, ZAP, run_and_plot from neuron import h import neuron from scipy.optimize import minimize  from dataclasses import dataclass from typing import Dict, List, Tuple, Callable, Optional import numpy as np  # Load the cell template neuron.load_mechanisms(\"modfiles\") h.load_file(\"stdrun.hoc\") h.load_file(\"templates.hoc\")  # Replace with your HOC file  <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> Out[1]: <pre>1.0</pre> In\u00a0[2]: Copied! <pre>@dataclass\nclass PassiveOptimizationResult:\n    \"\"\"Container for passive parameter optimization results\"\"\"\n    optimal_params: Dict[str, float]  # Optimal parameter values\n    achieved_metrics: Dict[str, float]  # Achieved metrics (e.g., R_in, Tau)\n    target_metrics: Dict[str, float]  # Target metrics\n    error: float  # Final error value\n    optimization_path: List[Dict[str, float]]  # History of parameter values and errors\n</pre> @dataclass class PassiveOptimizationResult:     \"\"\"Container for passive parameter optimization results\"\"\"     optimal_params: Dict[str, float]  # Optimal parameter values     achieved_metrics: Dict[str, float]  # Achieved metrics (e.g., R_in, Tau)     target_metrics: Dict[str, float]  # Target metrics     error: float  # Final error value     optimization_path: List[Dict[str, float]]  # History of parameter values and errors  In\u00a0[3]: Copied! <pre>class PassiveOptimizer:\n    def __init__(self, cell):\n        \"\"\"\n        Initialize the passive properties optimizer.\n        \n        Parameters:\n        -----------\n        cell : Callable\n            A callable that returns a new instance of the NEURON cell object.\n        \"\"\"\n        self.cell = cell\n        self.optimization_history = []\n        self.param_scales = {}\n\n    def _normalize_params(self, params: np.ndarray, param_names: List[str]) -&gt; np.ndarray:\n        \"\"\"Normalize parameters to similar scales\"\"\"\n        return np.array([params[i] / self.param_scales[name] for i, name in enumerate(param_names)])\n\n    def _denormalize_params(self, normalized_params: np.ndarray, param_names: List[str]) -&gt; np.ndarray:\n        \"\"\"Convert normalized parameters back to original scale\"\"\"\n        return np.array([normalized_params[i] * self.param_scales[name] for i, name in enumerate(param_names)])\n\n    def _calculate_metrics(self, cell_instance) -&gt; Dict[str, float]:\n        \"\"\"Calculate passive properties (R_in and Tau) from the current simulation.\"\"\"\n        passive = Passive(cell_instance, inj_amp=-100, inj_delay=200, inj_dur=1000, tstop=1200, method='exp2')\n        time, voltage = passive.execute()\n        return {\n            'R_in': passive.r_in,  # Input resistance (M\u03a9)\n            'Tau': passive.tau  # Membrane time constant (ms)\n        }\n\n    def _default_cost_function(self, metrics: Dict[str, float], target_metrics: Dict[str, float]) -&gt; float:\n        \"\"\"Default cost function that targets R_in and Tau.\"\"\"\n        error_r_in = (metrics['R_in'] - target_metrics['R_in']) ** 2\n        error_tau = (metrics['Tau'] - target_metrics['Tau']) ** 2\n        return error_r_in + error_tau\n\n    def _objective_function(self, normalized_params: np.ndarray, param_names: List[str], cost_function: Callable, target_metrics: Dict[str, float], tolerance: Optional[Dict[str, float]] = None) -&gt; float:\n        \"\"\"\n        Calculate error using provided cost function.\n        \n        Parameters:\n        -----------\n        tolerance : Optional[Dict[str, float]]\n            Tolerance for each metric. If the achieved metric is within the tolerance of the target, the optimization stops.\n        \"\"\"\n        # Denormalize parameters\n        params = self._denormalize_params(normalized_params, param_names)\n\n        # Create a new instance of the cell\n        cell_instance = self.cell()\n\n        # Update passive properties\n        for name, value in zip(param_names, params):\n            if name == 'RM_scale':\n                for sec in cell_instance.all:\n                    sec.g_pas = 1.0 / (sec.g_pas * value)\n            elif name == 'Cap_scale':\n                for sec in cell_instance.all:\n                    sec.cm *= value\n\n        # Calculate metrics and error\n        metrics = self._calculate_metrics(cell_instance)\n        error = cost_function(metrics, target_metrics)\n\n        # Store history with denormalized values\n        history_entry = {\n            'params': dict(zip(param_names, params)),\n            'metrics': metrics,\n            'error': error\n        }\n        self.optimization_history.append(history_entry)\n\n        # Check if metrics are within tolerance\n        if tolerance is not None:\n            within_tolerance = all(\n                abs(metrics[metric] - target_metrics[metric]) &lt;= tolerance[metric]\n                for metric in target_metrics\n            )\n            if within_tolerance:\n                # Return a very small error to signal the optimizer to stop\n                return 1e-10\n\n        return error\n\n    def optimize_parameters(self, target_metrics: Dict[str, float], param_bounds: Dict[str, Tuple[float, float]], cost_function: Optional[Callable] = None, method: str = 'L-BFGS-B', init_guess: str = 'middle_guess', tolerance: Optional[Dict[str, float]] = None) -&gt; PassiveOptimizationResult:\n        \"\"\"\n        Optimize passive properties to match target values.\n        \n        Parameters:\n        -----------\n        target_metrics : Dict[str, float]\n            Target values for passive properties (e.g., {'R_in': 150.0, 'Tau': 20.0}).\n        param_bounds : Dict[str, Tuple[float, float]]\n            Bounds for each parameter to optimize (e.g., {'RM_scale': (0.1, 10.0), 'Cap_scale': (0.1, 10.0)}).\n        cost_function : Optional[Callable]\n            Custom cost function for optimization. If None, the default cost function is used.\n        method : str, optional\n            Optimization method to use (default: 'L-BFGS-B').\n        init_guess : str, optional\n            Method for initial parameter guess ('random' or 'middle_guess').\n        tolerance : Optional[Dict[str, float]]\n            Tolerance for each metric. If the achieved metric is within the tolerance of the target, the optimization stops.\n            \n        Returns:\n        --------\n        PassiveOptimizationResult\n            Results of the optimization.\n        \"\"\"\n        self.optimization_history = []\n\n        param_names = list(param_bounds.keys())\n        bounds = [param_bounds[name] for name in param_names]\n\n        if cost_function is None:\n            cost_function = self._default_cost_function\n\n        # Calculate scaling factors\n        self.param_scales = {\n            name: max(abs(bounds[i][0]), abs(bounds[i][1]))\n            for i, name in enumerate(param_names)\n        }\n\n        # Normalize bounds\n        normalized_bounds = [\n            (b[0] / self.param_scales[name], b[1] / self.param_scales[name])\n            for name, b in zip(param_names, bounds)\n        ]\n\n        # Initial guess\n        if init_guess == 'random':\n            x0 = np.array([np.random.uniform(b[0], b[1]) for b in bounds])\n        elif init_guess == 'middle_guess':\n            x0 = [(b[0] + b[1]) / 2 for b in bounds]\n        else:\n            raise ValueError(\"Invalid init_guess method. Use 'random' or 'middle_guess'.\")\n        normalized_x0 = self._normalize_params(x0, param_names)\n\n        # Run optimization\n        result = minimize(\n            self._objective_function,\n            normalized_x0,\n            args=(param_names, cost_function, target_metrics, tolerance),\n            method=method,\n            bounds=normalized_bounds\n        )\n\n        # Get final parameters and metrics\n        final_params = dict(zip(param_names, self._denormalize_params(result.x, param_names)))\n        final_metrics = self._calculate_metrics(self.cell())\n\n        return PassiveOptimizationResult(\n            optimal_params=final_params,\n            achieved_metrics=final_metrics,\n            target_metrics=target_metrics,\n            error=result.fun,\n            optimization_path=self.optimization_history\n        )\n    def plot_optimization_results(self, result: PassiveOptimizationResult):\n        \"\"\"\n        Plot optimization results including convergence and final traces.\n        \n        Parameters:\n        -----------\n        result : PassiveOptimizationResult\n            The result of the optimization.\n        \"\"\"\n        # Ensure errors are properly shaped for plotting\n        iterations = range(len(result.optimization_path))\n        errors = np.array([float(h['error']) for h in result.optimization_path]).flatten()\n        \n        # Plot error convergence\n        fig1, ax1 = plt.subplots(figsize=(8, 5))\n        ax1.plot(iterations, errors, label='Error')\n        ax1.set_xlabel('Iteration')\n        ax1.set_ylabel('Error')\n        ax1.set_title('Error Convergence')\n        ax1.set_yscale('log')\n        ax1.legend()\n        plt.tight_layout()\n        plt.show()\n        \n        # Plot parameter convergence\n        param_names = list(result.optimal_params.keys())\n        num_params = len(param_names)\n        fig2, axs = plt.subplots(nrows=num_params, ncols=1, figsize=(8, 5 * num_params))\n        \n        if num_params == 1:\n            axs = [axs]\n            \n        for ax, param in zip(axs, param_names):\n            values = [float(h['params'][param]) for h in result.optimization_path]\n            ax.plot(iterations, values, label=f'{param}')\n            ax.set_xlabel('Iteration')\n            ax.set_ylabel('Parameter Value')\n            ax.set_title(f'Convergence of {param}')\n            ax.legend()\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Print final results\n        print(\"Optimization Results:\")\n        print(f\"Final Error: {float(result.error):.2e}\\n\")\n        print(\"Target Metrics:\")\n        for metric, value in result.target_metrics.items():\n            achieved = result.achieved_metrics.get(metric)\n            if achieved is not None:\n                print(f\"{metric}: {float(achieved):.3f} (target: {float(value):.3f})\")\n        \n        print(\"\\nOptimal Parameters:\")\n        for param, value in result.optimal_params.items():\n            print(f\"{param}: {float(value):.3f}\")\n</pre> class PassiveOptimizer:     def __init__(self, cell):         \"\"\"         Initialize the passive properties optimizer.                  Parameters:         -----------         cell : Callable             A callable that returns a new instance of the NEURON cell object.         \"\"\"         self.cell = cell         self.optimization_history = []         self.param_scales = {}      def _normalize_params(self, params: np.ndarray, param_names: List[str]) -&gt; np.ndarray:         \"\"\"Normalize parameters to similar scales\"\"\"         return np.array([params[i] / self.param_scales[name] for i, name in enumerate(param_names)])      def _denormalize_params(self, normalized_params: np.ndarray, param_names: List[str]) -&gt; np.ndarray:         \"\"\"Convert normalized parameters back to original scale\"\"\"         return np.array([normalized_params[i] * self.param_scales[name] for i, name in enumerate(param_names)])      def _calculate_metrics(self, cell_instance) -&gt; Dict[str, float]:         \"\"\"Calculate passive properties (R_in and Tau) from the current simulation.\"\"\"         passive = Passive(cell_instance, inj_amp=-100, inj_delay=200, inj_dur=1000, tstop=1200, method='exp2')         time, voltage = passive.execute()         return {             'R_in': passive.r_in,  # Input resistance (M\u03a9)             'Tau': passive.tau  # Membrane time constant (ms)         }      def _default_cost_function(self, metrics: Dict[str, float], target_metrics: Dict[str, float]) -&gt; float:         \"\"\"Default cost function that targets R_in and Tau.\"\"\"         error_r_in = (metrics['R_in'] - target_metrics['R_in']) ** 2         error_tau = (metrics['Tau'] - target_metrics['Tau']) ** 2         return error_r_in + error_tau      def _objective_function(self, normalized_params: np.ndarray, param_names: List[str], cost_function: Callable, target_metrics: Dict[str, float], tolerance: Optional[Dict[str, float]] = None) -&gt; float:         \"\"\"         Calculate error using provided cost function.                  Parameters:         -----------         tolerance : Optional[Dict[str, float]]             Tolerance for each metric. If the achieved metric is within the tolerance of the target, the optimization stops.         \"\"\"         # Denormalize parameters         params = self._denormalize_params(normalized_params, param_names)          # Create a new instance of the cell         cell_instance = self.cell()          # Update passive properties         for name, value in zip(param_names, params):             if name == 'RM_scale':                 for sec in cell_instance.all:                     sec.g_pas = 1.0 / (sec.g_pas * value)             elif name == 'Cap_scale':                 for sec in cell_instance.all:                     sec.cm *= value          # Calculate metrics and error         metrics = self._calculate_metrics(cell_instance)         error = cost_function(metrics, target_metrics)          # Store history with denormalized values         history_entry = {             'params': dict(zip(param_names, params)),             'metrics': metrics,             'error': error         }         self.optimization_history.append(history_entry)          # Check if metrics are within tolerance         if tolerance is not None:             within_tolerance = all(                 abs(metrics[metric] - target_metrics[metric]) &lt;= tolerance[metric]                 for metric in target_metrics             )             if within_tolerance:                 # Return a very small error to signal the optimizer to stop                 return 1e-10          return error      def optimize_parameters(self, target_metrics: Dict[str, float], param_bounds: Dict[str, Tuple[float, float]], cost_function: Optional[Callable] = None, method: str = 'L-BFGS-B', init_guess: str = 'middle_guess', tolerance: Optional[Dict[str, float]] = None) -&gt; PassiveOptimizationResult:         \"\"\"         Optimize passive properties to match target values.                  Parameters:         -----------         target_metrics : Dict[str, float]             Target values for passive properties (e.g., {'R_in': 150.0, 'Tau': 20.0}).         param_bounds : Dict[str, Tuple[float, float]]             Bounds for each parameter to optimize (e.g., {'RM_scale': (0.1, 10.0), 'Cap_scale': (0.1, 10.0)}).         cost_function : Optional[Callable]             Custom cost function for optimization. If None, the default cost function is used.         method : str, optional             Optimization method to use (default: 'L-BFGS-B').         init_guess : str, optional             Method for initial parameter guess ('random' or 'middle_guess').         tolerance : Optional[Dict[str, float]]             Tolerance for each metric. If the achieved metric is within the tolerance of the target, the optimization stops.                      Returns:         --------         PassiveOptimizationResult             Results of the optimization.         \"\"\"         self.optimization_history = []          param_names = list(param_bounds.keys())         bounds = [param_bounds[name] for name in param_names]          if cost_function is None:             cost_function = self._default_cost_function          # Calculate scaling factors         self.param_scales = {             name: max(abs(bounds[i][0]), abs(bounds[i][1]))             for i, name in enumerate(param_names)         }          # Normalize bounds         normalized_bounds = [             (b[0] / self.param_scales[name], b[1] / self.param_scales[name])             for name, b in zip(param_names, bounds)         ]          # Initial guess         if init_guess == 'random':             x0 = np.array([np.random.uniform(b[0], b[1]) for b in bounds])         elif init_guess == 'middle_guess':             x0 = [(b[0] + b[1]) / 2 for b in bounds]         else:             raise ValueError(\"Invalid init_guess method. Use 'random' or 'middle_guess'.\")         normalized_x0 = self._normalize_params(x0, param_names)          # Run optimization         result = minimize(             self._objective_function,             normalized_x0,             args=(param_names, cost_function, target_metrics, tolerance),             method=method,             bounds=normalized_bounds         )          # Get final parameters and metrics         final_params = dict(zip(param_names, self._denormalize_params(result.x, param_names)))         final_metrics = self._calculate_metrics(self.cell())          return PassiveOptimizationResult(             optimal_params=final_params,             achieved_metrics=final_metrics,             target_metrics=target_metrics,             error=result.fun,             optimization_path=self.optimization_history         )     def plot_optimization_results(self, result: PassiveOptimizationResult):         \"\"\"         Plot optimization results including convergence and final traces.                  Parameters:         -----------         result : PassiveOptimizationResult             The result of the optimization.         \"\"\"         # Ensure errors are properly shaped for plotting         iterations = range(len(result.optimization_path))         errors = np.array([float(h['error']) for h in result.optimization_path]).flatten()                  # Plot error convergence         fig1, ax1 = plt.subplots(figsize=(8, 5))         ax1.plot(iterations, errors, label='Error')         ax1.set_xlabel('Iteration')         ax1.set_ylabel('Error')         ax1.set_title('Error Convergence')         ax1.set_yscale('log')         ax1.legend()         plt.tight_layout()         plt.show()                  # Plot parameter convergence         param_names = list(result.optimal_params.keys())         num_params = len(param_names)         fig2, axs = plt.subplots(nrows=num_params, ncols=1, figsize=(8, 5 * num_params))                  if num_params == 1:             axs = [axs]                      for ax, param in zip(axs, param_names):             values = [float(h['params'][param]) for h in result.optimization_path]             ax.plot(iterations, values, label=f'{param}')             ax.set_xlabel('Iteration')             ax.set_ylabel('Parameter Value')             ax.set_title(f'Convergence of {param}')             ax.legend()                  plt.tight_layout()         plt.show()                  # Print final results         print(\"Optimization Results:\")         print(f\"Final Error: {float(result.error):.2e}\\n\")         print(\"Target Metrics:\")         for metric, value in result.target_metrics.items():             achieved = result.achieved_metrics.get(metric)             if achieved is not None:                 print(f\"{metric}: {float(achieved):.3f} (target: {float(value):.3f})\")                  print(\"\\nOptimal Parameters:\")         for param, value in result.optimal_params.items():             print(f\"{param}: {float(value):.3f}\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[4]: Copied! <pre># Create the cell (as a callable)\ncell = h.CP_Cell\n\n# Define target metrics\ntarget_metrics = {'R_in': 91.98, 'Tau': 37.58}\n\n# Define parameter bounds\nparam_bounds = {'Cap_scale': (0.1, 2.0)}\n\n# Define tolerance for each metric\ntolerance = {'R_in': 5.0, 'Tau': 2.0}  # Stop if R_in is within \u00b11.0 and Tau is within \u00b12.0 of the target\n\n# Create the optimizer\noptimizer = PassiveOptimizer(cell)\n\n# Run the optimization\nresult = optimizer.optimize_parameters(target_metrics, param_bounds, tolerance=tolerance)\n\n# Print the results\nprint(\"Optimization Results:\")\nprint(f\"Optimal Parameters: {result.optimal_params}\")\nprint(f\"Achieved Metrics: {result.achieved_metrics}\")\nprint(f\"Error: {result.error}\")\n</pre> # Create the cell (as a callable) cell = h.CP_Cell  # Define target metrics target_metrics = {'R_in': 91.98, 'Tau': 37.58}  # Define parameter bounds param_bounds = {'Cap_scale': (0.1, 2.0)}  # Define tolerance for each metric tolerance = {'R_in': 5.0, 'Tau': 2.0}  # Stop if R_in is within \u00b11.0 and Tau is within \u00b12.0 of the target  # Create the optimizer optimizer = PassiveOptimizer(cell)  # Run the optimization result = optimizer.optimize_parameters(target_metrics, param_bounds, tolerance=tolerance)  # Print the results print(\"Optimization Results:\") print(f\"Optimal Parameters: {result.optimal_params}\") print(f\"Achieved Metrics: {result.achieved_metrics}\") print(f\"Error: {result.error}\") <pre>Injection location: CP_Cell[0].soma[0](0.5)\nRecording: CP_Cell[0].soma[0](0.5)._ref_v\nRunning simulation for passive properties...\n\nV Rest: -66.45 (mV)\nResistance: 91.98 (MOhms)\nMembrane time constant: 51.17 (ms)\n\nV_rest Calculation: Voltage taken at time 200.0 (ms) is\n-66.45 (mV)\n\nR_in Calculation: dV/dI = (v_final-v_rest)/(i_final-i_start)\n(-75.64 - (-66.45)) / (-0.1 - 0)\n9.20 (mV) / 0.1 (nA) = 91.98 (MOhms)\n\nTau Calculation: Fit a double exponential curve to the membrane potential response\nf(t) = a0 + a1*exp(-t/tau1) + a2*exp(-t/tau2)\nConstrained by initial value: f(0) = a0 + a1 + a2 = v_rest\nFit parameters: (a0, a1, a2, tau1, tau2) = (-75.64, -1.20, 10.40, 51.17, 13.56)\nMembrane time constant is determined from the slowest exponential term: 51.17 (ms)\n\nSag potential: v_sag = v_peak - v_final = -0.28 (mV)\nNormalized sag potential: v_sag / (v_peak - v_rest) = 0.030\n\nInjection location: CP_Cell[1].soma[0](0.5)\nRecording: CP_Cell[1].soma[0](0.5)._ref_v\nRunning simulation for passive properties...\n\nV Rest: -66.45 (mV)\nResistance: 91.98 (MOhms)\nMembrane time constant: 51.17 (ms)\n\nV_rest Calculation: Voltage taken at time 200.0 (ms) is\n-66.45 (mV)\n\nR_in Calculation: dV/dI = (v_final-v_rest)/(i_final-i_start)\n(-75.64 - (-66.45)) / (-0.1 - 0)\n9.20 (mV) / 0.1 (nA) = 91.98 (MOhms)\n\nTau Calculation: Fit a double exponential curve to the membrane potential response\nf(t) = a0 + a1*exp(-t/tau1) + a2*exp(-t/tau2)\nConstrained by initial value: f(0) = a0 + a1 + a2 = v_rest\nFit parameters: (a0, a1, a2, tau1, tau2) = (-75.64, -1.20, 10.40, 51.17, 13.56)\nMembrane time constant is determined from the slowest exponential term: 51.17 (ms)\n\nSag potential: v_sag = v_peak - v_final = -0.28 (mV)\nNormalized sag potential: v_sag / (v_peak - v_rest) = 0.030\n\nInjection location: CP_Cell[2].soma[0](0.5)\nRecording: CP_Cell[2].soma[0](0.5)._ref_v\nRunning simulation for passive properties...\n\nV Rest: -66.45 (mV)\nResistance: 91.98 (MOhms)\nMembrane time constant: 36.51 (ms)\n\nV_rest Calculation: Voltage taken at time 200.0 (ms) is\n-66.45 (mV)\n\nR_in Calculation: dV/dI = (v_final-v_rest)/(i_final-i_start)\n(-75.64 - (-66.45)) / (-0.1 - 0)\n9.20 (mV) / 0.1 (nA) = 91.98 (MOhms)\n\nTau Calculation: Fit a double exponential curve to the membrane potential response\nf(t) = a0 + a1*exp(-t/tau1) + a2*exp(-t/tau2)\nConstrained by initial value: f(0) = a0 + a1 + a2 = v_rest\nFit parameters: (a0, a1, a2, tau1, tau2) = (-75.64, -1.06, 10.26, 36.51, 1.31)\nMembrane time constant is determined from the slowest exponential term: 36.51 (ms)\n\nSag potential: v_sag = v_peak - v_final = -0.88 (mV)\nNormalized sag potential: v_sag / (v_peak - v_rest) = 0.087\n\nInjection location: CP_Cell[3].soma[0](0.5)\nRecording: CP_Cell[3].soma[0](0.5)._ref_v\nRunning simulation for passive properties...\n\nV Rest: -66.45 (mV)\nResistance: 91.98 (MOhms)\nMembrane time constant: 36.51 (ms)\n\nV_rest Calculation: Voltage taken at time 200.0 (ms) is\n-66.45 (mV)\n\nR_in Calculation: dV/dI = (v_final-v_rest)/(i_final-i_start)\n(-75.64 - (-66.45)) / (-0.1 - 0)\n9.20 (mV) / 0.1 (nA) = 91.98 (MOhms)\n\nTau Calculation: Fit a double exponential curve to the membrane potential response\nf(t) = a0 + a1*exp(-t/tau1) + a2*exp(-t/tau2)\nConstrained by initial value: f(0) = a0 + a1 + a2 = v_rest\nFit parameters: (a0, a1, a2, tau1, tau2) = (-75.64, -1.06, 10.26, 36.51, 1.31)\nMembrane time constant is determined from the slowest exponential term: 36.51 (ms)\n\nSag potential: v_sag = v_peak - v_final = -0.88 (mV)\nNormalized sag potential: v_sag / (v_peak - v_rest) = 0.087\n\nInjection location: CP_Cell[4].soma[0](0.5)\nRecording: CP_Cell[4].soma[0](0.5)._ref_v\nRunning simulation for passive properties...\n\nV Rest: -66.45 (mV)\nResistance: 91.98 (MOhms)\nMembrane time constant: 50.37 (ms)\n\nV_rest Calculation: Voltage taken at time 200.0 (ms) is\n-66.45 (mV)\n\nR_in Calculation: dV/dI = (v_final-v_rest)/(i_final-i_start)\n(-75.64 - (-66.45)) / (-0.1 - 0)\n9.20 (mV) / 0.1 (nA) = 91.98 (MOhms)\n\nTau Calculation: Fit a double exponential curve to the membrane potential response\nf(t) = a0 + a1*exp(-t/tau1) + a2*exp(-t/tau2)\nConstrained by initial value: f(0) = a0 + a1 + a2 = v_rest\nFit parameters: (a0, a1, a2, tau1, tau2) = (-75.64, -1.21, 10.40, 50.37, 12.94)\nMembrane time constant is determined from the slowest exponential term: 50.37 (ms)\n\nSag potential: v_sag = v_peak - v_final = -0.29 (mV)\nNormalized sag potential: v_sag / (v_peak - v_rest) = 0.031\n\nOptimization Results:\nOptimal Parameters: {'Cap_scale': 0.1}\nAchieved Metrics: {'R_in': 91.97862677319023, 'Tau': 50.36569160422358}\nError: 1e-10\n</pre> In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\noptimizer.plot_optimization_results(result)\n</pre> import matplotlib.pyplot as plt optimizer.plot_optimization_results(result) <pre>Optimization Results:\nFinal Error: 1.00e-10\n\nTarget Metrics:\nR_in: 91.979 (target: 91.980)\nTau: 50.366 (target: 37.580)\n\nOptimal Parameters:\nCap_scale: 0.100\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/notebooks/single_cell/Neuron_hoc/test/","title":"Test","text":"In\u00a0[1]: Copied! <pre>from bmtool.singlecell import Profiler, Passive, CurrentClamp, FI, ZAP, run_and_plot\nfrom neuron import h\n</pre> from bmtool.singlecell import Profiler, Passive, CurrentClamp, FI, ZAP, run_and_plot from neuron import h <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> In\u00a0[2]: Copied! <pre>template_dir = '.' #templates are in templates.hoc in this working dir\nmechanism_dir = 'modfiles'\n\nprofiler = Profiler(template_dir=template_dir, mechanism_dir=mechanism_dir, dt = 0.05)\n</pre> template_dir = '.' #templates are in templates.hoc in this working dir mechanism_dir = 'modfiles'  profiler = Profiler(template_dir=template_dir, mechanism_dir=mechanism_dir, dt = 0.05)  In\u00a0[6]: Copied! <pre>cell = h.CP_Cell()\n\nall_sections = [sec for sec in h.allsec()]\n\nfor sec in all_sections:\n    print(sec.name())\n    print(sec.g_pas)\n    print(sec.diam)\n    print(sec.ek)\n</pre> cell = h.CP_Cell()  all_sections = [sec for sec in h.allsec()]  for sec in all_sections:     print(sec.name())     print(sec.g_pas)     print(sec.diam)     print(sec.ek) <pre>CP_Cell[3].soma[0]\n5.404892936362239e-05\n28.214910507202145\n-104.0\nCP_Cell[3].axon[0]\n0.00025347188561059\n14.096628189086916\n-104.0\nCP_Cell[3].dend[0]\n7.620207783345154e-05\n2.2799248695373535\n-104.0\nCP_Cell[3].apic[0]\n9.30129299683914e-05\n1.5831890106201174\n-104.0\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/","title":"Gap Junction Tuner","text":"<p>First we will compile the modfiles</p> In\u00a0[1]: Copied! <pre>import os\n\n# if already compiled then lets delete the folder and force a recompile\nif os.path.isdir('modfiles/x86_64'):\n    os.system(\"rm -rf modfiles/x86_64 \")\n# compile the mod files\nif not os.path.isdir(\"modfiles/x86_64\"):\n    os.chdir('modfiles')\n    os.system(\"nrnivmodl\")\n    os.chdir(\"..\")\n</pre> import os  # if already compiled then lets delete the folder and force a recompile if os.path.isdir('modfiles/x86_64'):     os.system(\"rm -rf modfiles/x86_64 \") # compile the mod files if not os.path.isdir(\"modfiles/x86_64\"):     os.chdir('modfiles')     os.system(\"nrnivmodl\")     os.chdir(\"..\") <pre>/home/gjgpb9/miniconda3/envs/bmtk/bin/nrnivmodl:10: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import working_set\nTranslating AMPA_NMDA_STP.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/AMPA_NMDA_STP.c\n</pre> <pre>/home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles\nMod files: \"./AMPA_NMDA_STP.mod\" \"./cadad.mod\" \"./cal2.mod\" \"./can_mig.mod\" \"./exp2syn_stp.mod\" \"./GABA_A_STP.mod\" \"./gap.mod\" \"./Gfluct.mod\" \"./h_kole.mod\" \"./imCA3.mod\" \"./kap_BS.mod\" \"./kBK.mod\" \"./kdmc_BS.mod\" \"./kdr_BS.mod\" \"./kdrCA3.mod\" \"./kdrinter.mod\" \"./leak.mod\" \"./nainter.mod\" \"./napCA3.mod\" \"./natCA3.mod\" \"./nax_BS.mod\" \"./vecevent_coreneuron.mod\"\n\nCreating 'x86_64' directory for .o files.\n\n -&gt; Compiling mod_func.cpp\n -&gt; NMODL ../AMPA_NMDA_STP.mod\n -&gt; NMODL ../cadad.mod\n -&gt; NMODL ../cal2.mod\n -&gt; NMODL ../can_mig.mod\nWarning: Default 2 of PARAMETER cao will be ignored and set by NEURON.\nWarning: Default 5e-05 of PARAMETER cai will be ignored and set by NEURON.\n -&gt; NMODL ../exp2syn_stp.mod\n -&gt; NMODL ../GABA_A_STP.mod\n -&gt; NMODL ../gap.mod\nWarning: Default 2 of PARAMETER cao will be ignored and set by NEURON.\nWarning: Default 5e-05 of PARAMETER cai will be ignored and set by NEURON.\n -&gt; NMODL ../Gfluct.mod\n -&gt; NMODL ../h_kole.mod\n -&gt; NMODL ../imCA3.mod\n -&gt; NMODL ../kap_BS.mod\n -&gt; NMODL ../kBK.mod\n -&gt; NMODL ../kdmc_BS.mod\n -&gt; NMODL ../kdr_BS.mod\n -&gt; NMODL ../kdrCA3.mod\n -&gt; NMODL ../kdrinter.mod\n -&gt; NMODL ../leak.mod\n -&gt; NMODL ../nainter.mod\n -&gt; NMODL ../napCA3.mod\nWarning: Default -80 of PARAMETER ek will be ignored and set by NEURON.\n -&gt; NMODL ../natCA3.mod\nWarning: Default 45 of PARAMETER ena will be ignored and set by NEURON.\n -&gt; NMODL ../nax_BS.mod\n -&gt; NMODL ../vecevent_coreneuron.mod\n -&gt; Compiling AMPA_NMDA_STP.c\n -&gt; Compiling cadad.c\nNotice: ARTIFICIAL_CELL is a synonym for POINT_PROCESS which hints that it\nonly affects and is affected by discrete events. As such it is not\nlocated in a section and is not associated with an integrator\n -&gt; Compiling cal2.c\n -&gt; Compiling can_mig.c\n -&gt; Compiling exp2syn_stp.c\n -&gt; Compiling GABA_A_STP.c\n -&gt; Compiling gap.c\n -&gt; Compiling Gfluct.c\n -&gt; Compiling h_kole.c\n -&gt; Compiling imCA3.c\n -&gt; Compiling kap_BS.c\n -&gt; Compiling kBK.c\n -&gt; Compiling kdmc_BS.c\n -&gt; Compiling kdr_BS.c\n -&gt; Compiling kdrCA3.c\n -&gt; Compiling kdrinter.c\n -&gt; Compiling leak.c\n -&gt; Compiling nainter.c\n -&gt; Compiling napCA3.c\n -&gt; Compiling natCA3.c\n -&gt; Compiling nax_BS.c\n -&gt; Compiling vecevent_coreneuron.c\n =&gt; LINKING shared library ./libnrnmech.so\n =&gt; LINKING executable ./special LDFLAGS are:    -pthread\n</pre> <pre>Translating cal2.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/cal2.c\nTranslating cadad.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/cadad.c\nThread Safe\nThread Safe\nThread Safe\nTranslating can_mig.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/can_mig.c\nThread Safe\nTranslating exp2syn_stp.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/exp2syn_stp.c\nTranslating GABA_A_STP.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/GABA_A_STP.c\nThread Safe\nTranslating gap.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/gap.c\nThread Safe\nThread Safe\nTranslating Gfluct.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/Gfluct.c\nNotice: This mechanism cannot be used with CVODE\nThread Safe\nTranslating h_kole.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/h_kole.c\nTranslating imCA3.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/imCA3.c\nTranslating kap_BS.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/kap_BS.c\nThread Safe\nThread Safe\nThread Safe\nTranslating kBK.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/kBK.c\nThread Safe\nTranslating kdmc_BS.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/kdmc_BS.c\nTranslating kdr_BS.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/kdr_BS.c\nTranslating kdrCA3.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/kdrCA3.c\nThread Safe\nThread Safe\nThread Safe\nTranslating kdrinter.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/kdrinter.c\nThread Safe\nTranslating leak.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/leak.c\nTranslating nainter.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/nainter.c\nTranslating napCA3.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/napCA3.c\nThread Safe\nThread Safe\nThread Safe\nTranslating natCA3.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/natCA3.c\nThread Safe\nTranslating nax_BS.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/nax_BS.c\nTranslating vecevent_coreneuron.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles/x86_64/vecevent_coreneuron.c\nThread Safe\nThread Safe\n</pre> <pre>Successfully created x86_64/special\n</pre> In\u00a0[2]: Copied! <pre>from bmtool.synapses import GapJunctionTuner\n%matplotlib inline\n\nmechanisms_dir = 'modfiles'\ntemplates_file = 'templates.hoc'\n\ngeneral_settings = {\n    'tstart': 500., # when the current clamp starts\n    'tdur': 500.,    # Dur of the current clamp\n    'dt': 0.025, # simulation dt\n    'celsius': 20 # temp of sim\n}\n\nconn_type_settings = {\n    'FSI_gap': {\n        'cell': 'FSI_Cell', #template name \n        'iclamp_amp' : -0.01, # nA what amp to stim cell at\n        'sec_x': 0.5, # location of syn\n        'sec_id': 0,  # location of syn\n        \"level_of_detail\": \"gap\", # which synaptic mech to use\n    }\n}\n\n# Pass the full conn_type_settings dictionary instead of just the FSI_gap part\ntuner = GapJunctionTuner(mechanisms_dir=mechanisms_dir,templates_dir=templates_file,\n                         general_settings=general_settings,conn_type_settings=conn_type_settings)\n</pre> from bmtool.synapses import GapJunctionTuner %matplotlib inline  mechanisms_dir = 'modfiles' templates_file = 'templates.hoc'  general_settings = {     'tstart': 500., # when the current clamp starts     'tdur': 500.,    # Dur of the current clamp     'dt': 0.025, # simulation dt     'celsius': 20 # temp of sim }  conn_type_settings = {     'FSI_gap': {         'cell': 'FSI_Cell', #template name          'iclamp_amp' : -0.01, # nA what amp to stim cell at         'sec_x': 0.5, # location of syn         'sec_id': 0,  # location of syn         \"level_of_detail\": \"gap\", # which synaptic mech to use     } }  # Pass the full conn_type_settings dictionary instead of just the FSI_gap part tuner = GapJunctionTuner(mechanisms_dir=mechanisms_dir,templates_dir=templates_file,                          general_settings=general_settings,conn_type_settings=conn_type_settings) <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> <pre>{'cell': 'FSI_Cell', 'iclamp_amp': -0.01, 'sec_x': 0.5, 'sec_id': 0, 'level_of_detail': 'gap'}\nFSI_Cell\n</pre> <p>We can then manually tune using the run.model method</p> In\u00a0[3]: Copied! <pre>tuner.InteractiveTuner()\n</pre> tuner.InteractiveTuner() <pre>VBox(children=(Button(button_style='primary', description='Run', icon='history', style=ButtonStyle()), FloatLo\u2026</pre> <pre>Output()</pre> In\u00a0[4]: Copied! <pre>from bmtool.synapses import GapJunctionOptimizer\n\n# Create optimizer\noptimizer = GapJunctionOptimizer(tuner)\n\n# Optimize for a target coupling coefficient\ntarget_cc = 0.37  # Example target coupling coefficient\nresult = optimizer.optimize_resistance(\n    target_cc=target_cc,\n    resistance_bounds=(1e-4, 1e-2)  # Min and max resistance values to try\n)\n\n# Plot optimization results\noptimizer.plot_optimization_results(result)\n</pre> from bmtool.synapses import GapJunctionOptimizer  # Create optimizer optimizer = GapJunctionOptimizer(tuner)  # Optimize for a target coupling coefficient target_cc = 0.37  # Example target coupling coefficient result = optimizer.optimize_resistance(     target_cc=target_cc,     resistance_bounds=(1e-4, 1e-2)  # Min and max resistance values to try )  # Plot optimization results optimizer.plot_optimization_results(result) In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nresistance_range = np.logspace(-4, -2, 20)  # 20 points between 1e-4 and 1e-2\nsweep_results = optimizer.parameter_sweep(resistance_range)\n\n# Plot sweep results\nplt.figure(figsize=(8, 6))\nplt.semilogx(sweep_results['resistance'], sweep_results['coupling_coefficient'], 'o-')\nplt.xlabel('Resistance')\nplt.ylabel('Coupling Coefficient')\nplt.title('Resistance vs Coupling Coefficient')\nplt.grid(True)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  resistance_range = np.logspace(-4, -2, 20)  # 20 points between 1e-4 and 1e-2 sweep_results = optimizer.parameter_sweep(resistance_range)  # Plot sweep results plt.figure(figsize=(8, 6)) plt.semilogx(sweep_results['resistance'], sweep_results['coupling_coefficient'], 'o-') plt.xlabel('Resistance') plt.ylabel('Coupling Coefficient') plt.title('Resistance vs Coupling Coefficient') plt.grid(True) plt.show() <pre>Sweeping resistance values:   0%|          | 0/20 [00:00&lt;?, ?it/s]</pre>"},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/#example-of-how-to-use-bmtool-gap-junction-tuner","title":"Example of how to use BMTOOL gap junction tuner.\u00b6","text":"<p>By Gregory Glickert</p> <p>We first need to set up the tuner. We have general settings and connection specific parameters. We also need to say where our mechanisms and cell templates are located</p>"},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/#initialize-tuner","title":"initialize tuner\u00b6","text":""},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/#interactivetuner","title":"InteractiveTuner()\u00b6","text":""},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/#gapjunctionoptimizer","title":"GapJunctionOptimizer\u00b6","text":"<p>While we can manually tune the coupling coefficient since there is only one parameter that we are tuning we can optimize and find the best resistance!</p>"},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/#parameter-sweep","title":"parameter sweep\u00b6","text":"<p>We can also perform a parameter sweep to see the relationship between resistance and coupling coefficient</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/bmtk_chem_syn_tuner/","title":"BMTK Chemical Synapse Tuner","text":"In\u00a0[1]: Copied! <pre>from bmtool.synapses import SynapseTuner\n\ntuner = SynapseTuner(\n    config='bmtk_files/simulation_config.json',  # Path to BMTK config\n    current_name='i',                            # Synaptic current to record\n    slider_vars=['initW','Dep','Fac','Use','tau1','tau2']  # Parameters for sliders\n)\n</pre> from bmtool.synapses import SynapseTuner  tuner = SynapseTuner(     config='bmtk_files/simulation_config.json',  # Path to BMTK config     current_name='i',                            # Synaptic current to record     slider_vars=['initW','Dep','Fac','Use','tau1','tau2']  # Parameters for sliders )  <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> <pre>loading /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/templates/cell_templates.hoc\nloading /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/templates/templates_V1.hoc\nBuilding conn_type_settings from BMTK config files...\nFound 8 connection types: ['Exc2PV', 'SOM2PV', 'PV2PV', 'Exc2SOM', 'Exc2Exc', 'SOM2Exc', 'PV2Exc', 'background_syn']\nNo connection specified, using first available: Exc2PV\nWarning: erev cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\nWarning: initW cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\nError accessing 'Dep' in syn Exp2Syn[0]: 'hoc.HocObject' object has no attribute 'Dep'\nWarning: erev cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\nWarning: initW cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\nError accessing 'Fac' in syn Exp2Syn[1]: 'hoc.HocObject' object has no attribute 'Fac'\nWarning: erev cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\nWarning: initW cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\nError accessing 'Use' in syn Exp2Syn[2]: 'hoc.HocObject' object has no attribute 'Use'\nWarning: erev cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\nWarning: initW cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\n</pre> <p>When using the tuner with multiple networks you can switch between your different bmtk networks to test every single connection in your network. Note the synaptic weight (sometimes called initW) is the bmtk netconn weight. If you are tuning a synapse for its strenth make sure to keep this in mind. You should be able to convert that to a scale factor for you weight inside the mod file if needed</p> In\u00a0[2]: Copied! <pre>tuner.InteractiveTuner()\n</pre> tuner.InteractiveTuner() <pre>VBox(children=(HBox(children=(Dropdown(description='Network:', options=('network_to_network', 'extnet_to_netwo\u2026</pre> <pre>Output()</pre> <p>If using the optimizer with a bmtk network just make sure you specifiy which connection you want to use. You can switch it in the Interactive tuner or using the switch_connection method</p> In\u00a0[3]: Copied! <pre>tuner._switch_connection('PV2Exc')\n</pre> tuner._switch_connection('PV2Exc') <pre>Successfully switched to connection: PV2Exc\n</pre> <p>Now we tune that PV2Exc synapse</p> In\u00a0[4]: Copied! <pre>from bmtool.synapses import SynapseOptimizer\n# Create the optimizer\noptimizer = SynapseOptimizer(tuner)\n\n# Define parameter bounds these can be any range variable you wish to tune\nparam_bounds = {\n    'Dep': (0, 200.0),\n    'Fac': (0, 400.0),\n    'Use': (0.1, 1.0),\n    'tau1': (1,4), # tau r needs to be less than tau d so be careful\n    'tau2': (5,20)\n}\n\n# Define target metrics these are the metrics that the tuner will try to automatic get the synapse to respond with\n# max amps is an absolute value\ntarget_metrics = {\n    'induction': -0.75,\n    'ppr': 0.8,\n    'recovery': 0.0,\n    'rise_time': 2,          # This wont always be the case, but for this synapse Use controls STP and max amps so it can sometimes struggle to fit.\n    'decay_time': 9\n}\n\n# currently the only metrics in the SynapseOptimizer are \n#            - induction: measure of synaptic facilitation/depression\n#            - ppr: paired-pulse ratio\n#            - recovery: recovery from facilitation/depression\n#            - max_amplitude: maximum synaptic response amplitude\n#            - rise_time: time for synaptic response to rise from 20% to 80% of peak\n#            - decay_time: time constant of synaptic response decay\n#            - latency: synaptic response latency\n#            - half_width: synaptic response half-width\n#            - baseline: baseline current\n#            - amp: peak amplitude from syn_props\ndef custom_cost(metrics, targets):\n    # equal zero unless using train input\n    induction_error = (metrics['induction'] - targets['induction']) ** 2\n    ppr_error = (metrics['ppr'] - targets['ppr']) ** 2\n    recovery_error = (metrics['recovery'] - targets['recovery']) ** 2\n    max_amp_errror = (metrics['max_amplitude'] - targets['max_amplitude']) ** 2\n    # equal zero unless using SingleEvent\n    rise_time_error = (metrics['rise_time'] - targets['rise_time']) ** 2\n    decay_time_error = (metrics['decay_time'] - targets['decay_time']) ** 2 \n\n    #return rise_time_error + decay_time_error\n    return induction_error + 3 * ppr_error + recovery_error + rise_time_error + decay_time_error #+ 0.5*max_amp_errror\n\n# Run optimization with custom cost function\nresult = optimizer.optimize_parameters(\n    target_metrics=target_metrics,\n    param_bounds=param_bounds,\n    run_single_event=True,  # Run and use parameters from SingleEvent\n    run_train_input=True,   # Run and use parameters from train input \n    train_frequency=50,     # Freq in Hz of train input\n    train_delay=250,        # delay in ms of second train\n    init_guess='random',    # either random or middle_guess. Random will start the synapse witha random value in the param_bound. Middle guess will pick the middle value in the param_bounds\n    cost_function=custom_cost,\n    method='SLSQP'          # I believe this will be the fastest method, but you may try others check out https://docs.scipy.org/doc/scipy-1.15.0/reference/generated/scipy.optimize.minimize.html\n                            # SLSQP is a gradient based method while nelder-mead is simplex (whatever that means)\n)\n\n# Plot results\noptimizer.plot_optimization_results(result)\n</pre> from bmtool.synapses import SynapseOptimizer # Create the optimizer optimizer = SynapseOptimizer(tuner)  # Define parameter bounds these can be any range variable you wish to tune param_bounds = {     'Dep': (0, 200.0),     'Fac': (0, 400.0),     'Use': (0.1, 1.0),     'tau1': (1,4), # tau r needs to be less than tau d so be careful     'tau2': (5,20) }  # Define target metrics these are the metrics that the tuner will try to automatic get the synapse to respond with # max amps is an absolute value target_metrics = {     'induction': -0.75,     'ppr': 0.8,     'recovery': 0.0,     'rise_time': 2,          # This wont always be the case, but for this synapse Use controls STP and max amps so it can sometimes struggle to fit.     'decay_time': 9 }  # currently the only metrics in the SynapseOptimizer are  #            - induction: measure of synaptic facilitation/depression #            - ppr: paired-pulse ratio #            - recovery: recovery from facilitation/depression #            - max_amplitude: maximum synaptic response amplitude #            - rise_time: time for synaptic response to rise from 20% to 80% of peak #            - decay_time: time constant of synaptic response decay #            - latency: synaptic response latency #            - half_width: synaptic response half-width #            - baseline: baseline current #            - amp: peak amplitude from syn_props def custom_cost(metrics, targets):     # equal zero unless using train input     induction_error = (metrics['induction'] - targets['induction']) ** 2     ppr_error = (metrics['ppr'] - targets['ppr']) ** 2     recovery_error = (metrics['recovery'] - targets['recovery']) ** 2     max_amp_errror = (metrics['max_amplitude'] - targets['max_amplitude']) ** 2     # equal zero unless using SingleEvent     rise_time_error = (metrics['rise_time'] - targets['rise_time']) ** 2     decay_time_error = (metrics['decay_time'] - targets['decay_time']) ** 2       #return rise_time_error + decay_time_error     return induction_error + 3 * ppr_error + recovery_error + rise_time_error + decay_time_error #+ 0.5*max_amp_errror  # Run optimization with custom cost function result = optimizer.optimize_parameters(     target_metrics=target_metrics,     param_bounds=param_bounds,     run_single_event=True,  # Run and use parameters from SingleEvent     run_train_input=True,   # Run and use parameters from train input      train_frequency=50,     # Freq in Hz of train input     train_delay=250,        # delay in ms of second train     init_guess='random',    # either random or middle_guess. Random will start the synapse witha random value in the param_bound. Middle guess will pick the middle value in the param_bounds     cost_function=custom_cost,     method='SLSQP'          # I believe this will be the fastest method, but you may try others check out https://docs.scipy.org/doc/scipy-1.15.0/reference/generated/scipy.optimize.minimize.html                             # SLSQP is a gradient based method while nelder-mead is simplex (whatever that means) )  # Plot results optimizer.plot_optimization_results(result) <pre>Optimization Results:\nFinal Error: 9.74e-03\n\nTarget Metrics:\ninduction: -0.700 (target: -0.750)\nppr: 0.795 (target: 0.800)\nrecovery: -0.068 (target: 0.000)\nmax_amplitude: 0.243 (target: 25.000)\nrise_time: 2.050 (target: 2.000)\ndecay_time: 9.000 (target: 9.000)\n\nOptimal Parameters:\nDep: 132.061\nFac: 18.686\nUse: 0.486\ntau1: 2.028\ntau2: 7.913\n</pre> <pre>\n========================================\nShort Term Plasticity Results for 50Hz with 250 Delay\n========================================\nPPR: Above 1 is facilitating, below 1 is depressing.\nInduction: Above 0 is facilitating, below 0 is depressing.\nRecovery: A measure of how fast STP decays.\n\nPaired Pulse Response (PPR)\nCalculation: 2nd pulse / 1st pulse\nValues: ([[0.19]]) / ([[0.24]]) = [[0.79]]\n\nInduction\nCalculation: (avg(6th, 7th, 8th pulses) - 1st pulse) / max amps\nValues: avg([[0.07, 0.07, 0.07]]) - [[0.24]] / [0.24]\n([0.07]) - ([0.24]) / [0.24] = -0.700\n\nRecovery\nCalculation: (avg(9th, 10th, 11th, 12th pulses) - avg(1st to 4th pulses)) / max amps\nValues: avg([[0.21, 0.17, 0.12, 0.09]]) - avg([[0.24, 0.19, 0.12, 0.09]]) / [0.24]\n([0.15]) - ([0.16]) / [0.24] = -0.068\n\n========================================\n\n</pre> <pre>('baseline', 0.0)\n('sign', 1.0)\n('latency', 1.35)\n('amp', 0.0002428663192964911)\n('rise_time', 2.0500000000000003)\n('decay_time', 8.999801595557695)\n('half_width', 11.925)\nCurrent Integral in pA*ms: 3.07\n</pre>"},{"location":"examples/notebooks/synapses/synaptic_tuner/bmtk_chem_syn_tuner/#demo-of-bmtool-chemical-synaptic-tuner-using-bmtk-network","title":"Demo of bmtool chemical synaptic tuner using BMTK network\u00b6","text":"<p>by Gregory Glickert</p> <p>For a bit more context in using the tuner you can check out the notebook called neuron_chem_syn_tuner which goes over more details about using the tuner with pure NEURON models.</p> <p>When running this tuner you will see some warnings. That is on purpose, some of the synapses in this bmtk testing network do not have the Dep, Fac or Use parameters. That is ok and the tuner will still work for those synapses, but you can't tune those parameteres since they dont exist in the mod file.</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/","title":"Neuron Chemical Synapse Tuner","text":"In\u00a0[1]: Copied! <pre>general_settings = {\n    'vclamp': True, # if vclamp should start on or off used mostly for singleEventv\n    'rise_interval': (0.1, 0.9), #10-90%\n    'tstart': 500., # when the singleEvent should start\n    'tdur': 100.,    # Dur of sim after single synaptic event has occured\n    'threshold': -15., #threshold for spike in mV\n    'delay': 1.3, # netcon delay\n    'weight': 1., # netcon weight\n    'dt': 0.025, # simulation dt\n    'celsius': 20 # temp of sim\n}\n\nconn_type_settings = {\n    'Fac2FSI': { # facilitating synapse \n        'spec_settings': {\n            'post_cell': 'FSI_Cell', \n            'vclamp_amp' : -70., # voltage clamp amps\n            'sec_x': 0.5, # location of synapse\n            'sec_id': 1,  # location of synapse \n            \"level_of_detail\": \"AMPA_NMDA_STP\", # name of mechanism from the modfile\n        },\n        'spec_syn_param': { # synaptic parameters from modfile \n            'initW': 0.76,\n            'tau_r_AMPA': 0.45,\n            'tau_d_AMPA': 7.5,\n            'Use': 0.13,\n            'Dep': 0.,\n            'Fac': 200.\n        },\n    },\n    'Dep2FSI': { # depressing synapse\n        'spec_settings': {\n            'post_cell': 'FSI_Cell',\n            'vclamp_amp': -55,\n            'sec_x': 0.5,\n            'sec_id':0,\n            \"level_of_detail\": \"GABA_A_STP\",\n        },\n        'spec_syn_param': {\n            'initW': 20,\n            'tau_r_GABAA': 0.9,\n            'tau_d_GABAA': 15,\n            'e_GABAA':-75,\n            'Use': 0.4,\n            'Dep': 190.,\n            'Fac': 0.\n        },\n    },\n\n}\n</pre> general_settings = {     'vclamp': True, # if vclamp should start on or off used mostly for singleEventv     'rise_interval': (0.1, 0.9), #10-90%     'tstart': 500., # when the singleEvent should start     'tdur': 100.,    # Dur of sim after single synaptic event has occured     'threshold': -15., #threshold for spike in mV     'delay': 1.3, # netcon delay     'weight': 1., # netcon weight     'dt': 0.025, # simulation dt     'celsius': 20 # temp of sim }  conn_type_settings = {     'Fac2FSI': { # facilitating synapse          'spec_settings': {             'post_cell': 'FSI_Cell',              'vclamp_amp' : -70., # voltage clamp amps             'sec_x': 0.5, # location of synapse             'sec_id': 1,  # location of synapse              \"level_of_detail\": \"AMPA_NMDA_STP\", # name of mechanism from the modfile         },         'spec_syn_param': { # synaptic parameters from modfile              'initW': 0.76,             'tau_r_AMPA': 0.45,             'tau_d_AMPA': 7.5,             'Use': 0.13,             'Dep': 0.,             'Fac': 200.         },     },     'Dep2FSI': { # depressing synapse         'spec_settings': {             'post_cell': 'FSI_Cell',             'vclamp_amp': -55,             'sec_x': 0.5,             'sec_id':0,             \"level_of_detail\": \"GABA_A_STP\",         },         'spec_syn_param': {             'initW': 20,             'tau_r_GABAA': 0.9,             'tau_d_GABAA': 15,             'e_GABAA':-75,             'Use': 0.4,             'Dep': 190.,             'Fac': 0.         },     },  } <p>Then the modfiles must be compiled in order for the tuner to work properly</p> In\u00a0[2]: Copied! <pre>import os\n\n# if already compiled then lets delete the folder and force a recompile\nif os.path.isdir('modfiles/x86_64'):\n    os.system(\"rm -rf modfiles/x86_64 \")\n# compile the mod files\nif not os.path.isdir(\"modfiles/x86_64\"):\n    os.chdir('modfiles')\n    os.system(\"nrnivmodl\")\n    os.chdir(\"..\")\n</pre> import os  # if already compiled then lets delete the folder and force a recompile if os.path.isdir('modfiles/x86_64'):     os.system(\"rm -rf modfiles/x86_64 \") # compile the mod files if not os.path.isdir(\"modfiles/x86_64\"):     os.chdir('modfiles')     os.system(\"nrnivmodl\")     os.chdir(\"..\") <pre>/home/gjgpb9/miniconda3/envs/bmtk/bin/nrnivmodl:10: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import working_set\n</pre> <pre>/home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles\nMod files: \"./AMPA_NMDA_STP.mod\" \"./cadad.mod\" \"./cal2.mod\" \"./can_mig.mod\" \"./exp2syn_stp.mod\" \"./GABA_A_STP.mod\" \"./gap.mod\" \"./Gfluct.mod\" \"./h_kole.mod\" \"./imCA3.mod\" \"./kap_BS.mod\" \"./kBK.mod\" \"./kdmc_BS.mod\" \"./kdr_BS.mod\" \"./kdrCA3.mod\" \"./kdrinter.mod\" \"./leak.mod\" \"./nainter.mod\" \"./napCA3.mod\" \"./natCA3.mod\" \"./nax_BS.mod\" \"./vecevent_coreneuron.mod\"\n\nCreating 'x86_64' directory for .o files.\n\n -&gt; Compiling mod_func.cpp\n -&gt; NMODL ../AMPA_NMDA_STP.mod\n -&gt; NMODL ../cadad.mod\n -&gt; NMODL ../cal2.mod\n -&gt; NMODL ../can_mig.mod\nWarning: Default 2 of PARAMETER cao will be ignored and set by NEURON.\nWarning: Default 5e-05 of PARAMETER cai will be ignored and set by NEURON.\n -&gt; NMODL ../exp2syn_stp.mod\n -&gt; NMODL ../GABA_A_STP.mod\nWarning: Default 2 of PARAMETER cao will be ignored and set by NEURON.\nWarning: Default 5e-05 of PARAMETER cai will be ignored and set by NEURON.\n -&gt; NMODL ../gap.mod\n -&gt; NMODL ../Gfluct.mod\n -&gt; NMODL ../h_kole.mod\n -&gt; NMODL ../imCA3.mod\n -&gt; NMODL ../kap_BS.mod\n -&gt; NMODL ../kBK.mod\n -&gt; NMODL ../kdmc_BS.mod\n -&gt; NMODL ../kdr_BS.mod\n -&gt; NMODL ../kdrCA3.mod\n -&gt; NMODL ../kdrinter.mod\n -&gt; NMODL ../leak.mod\nWarning: Default -80 of PARAMETER ek will be ignored and set by NEURON.\n -&gt; NMODL ../nainter.mod\n -&gt; NMODL ../napCA3.mod\n -&gt; NMODL ../natCA3.mod\nWarning: Default 45 of PARAMETER ena will be ignored and set by NEURON.\n -&gt; NMODL ../nax_BS.mod\n -&gt; NMODL ../vecevent_coreneuron.mod\n -&gt; Compiling AMPA_NMDA_STP.c\n -&gt; Compiling cadad.c\nNotice: ARTIFICIAL_CELL is a synonym for POINT_PROCESS which hints that it\nonly affects and is affected by discrete events. As such it is not\nlocated in a section and is not associated with an integrator\n -&gt; Compiling cal2.c\n -&gt; Compiling can_mig.c\n -&gt; Compiling exp2syn_stp.c\n -&gt; Compiling GABA_A_STP.c\n -&gt; Compiling gap.c\n -&gt; Compiling Gfluct.c\n -&gt; Compiling h_kole.c\n -&gt; Compiling imCA3.c\n -&gt; Compiling kap_BS.c\n -&gt; Compiling kBK.c\n -&gt; Compiling kdmc_BS.c\n -&gt; Compiling kdr_BS.c\n -&gt; Compiling kdrCA3.c\n -&gt; Compiling kdrinter.c\n -&gt; Compiling leak.c\n -&gt; Compiling nainter.c\n -&gt; Compiling napCA3.c\n -&gt; Compiling natCA3.c\n -&gt; Compiling nax_BS.c\n -&gt; Compiling vecevent_coreneuron.c\n =&gt; LINKING shared library ./libnrnmech.so\n =&gt; LINKING executable ./special LDFLAGS are:    -pthread\n</pre> <pre>Translating AMPA_NMDA_STP.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/AMPA_NMDA_STP.c\nTranslating cadad.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/cadad.c\nTranslating cal2.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/cal2.c\nThread Safe\nThread Safe\nThread Safe\nTranslating can_mig.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/can_mig.c\nThread Safe\nTranslating exp2syn_stp.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/exp2syn_stp.c\nTranslating GABA_A_STP.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/GABA_A_STP.c\nTranslating gap.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/gap.c\nThread Safe\nThread Safe\nThread Safe\nTranslating Gfluct.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/Gfluct.c\nNotice: This mechanism cannot be used with CVODE\nThread Safe\nTranslating h_kole.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/h_kole.c\nTranslating imCA3.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/imCA3.c\nTranslating kap_BS.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/kap_BS.c\nThread Safe\nThread Safe\nThread Safe\nTranslating kBK.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/kBK.c\nThread Safe\nTranslating kdmc_BS.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/kdmc_BS.c\nTranslating kdr_BS.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/kdr_BS.c\nTranslating kdrCA3.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/kdrCA3.c\nThread Safe\nThread Safe\nThread Safe\nTranslating kdrinter.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/kdrinter.c\nThread Safe\nTranslating leak.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/leak.c\nTranslating napCA3.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/napCA3.c\nTranslating nainter.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/nainter.c\nThread Safe\nThread Safe\nThread Safe\nTranslating natCA3.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/natCA3.c\nThread Safe\nTranslating nax_BS.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/nax_BS.c\nTranslating vecevent_coreneuron.mod into /home/gjgpb9/BMTK-test-net/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles/x86_64/vecevent_coreneuron.c\nThread Safe\nThread Safe\n</pre> <pre>Successfully created x86_64/special\n</pre> In\u00a0[3]: Copied! <pre>mechanisms_dir = 'modfiles'\ntemplates_file = 'templates.hoc'\n\n# Initialize our tuner\nfrom bmtool.synapses import SynapseTuner\ntuner = SynapseTuner(mechanisms_dir=mechanisms_dir, # where x86_64 is located\n                    templates_dir=templates_file, # where the neuron templates are located\n                    conn_type_settings=conn_type_settings, # dict of connection settings\n                    general_settings = general_settings, # dict of general settings\n                    connection = 'Fac2FSI', # key in connection settings for which connection you want to tune\n                    current_name = 'i', # name of current variable in synapase\n                    other_vars_to_record = ['record_Pr', 'record_use'], # Other synaptic variables you wish to record besides the normal ones\n                    slider_vars=['initW','Dep','Fac','Use','tau_r_AMPA','tau_d_AMPA']) # Range variables you want to tune to adjust synaptic response.\n</pre> mechanisms_dir = 'modfiles' templates_file = 'templates.hoc'  # Initialize our tuner from bmtool.synapses import SynapseTuner tuner = SynapseTuner(mechanisms_dir=mechanisms_dir, # where x86_64 is located                     templates_dir=templates_file, # where the neuron templates are located                     conn_type_settings=conn_type_settings, # dict of connection settings                     general_settings = general_settings, # dict of general settings                     connection = 'Fac2FSI', # key in connection settings for which connection you want to tune                     current_name = 'i', # name of current variable in synapase                     other_vars_to_record = ['record_Pr', 'record_use'], # Other synaptic variables you wish to record besides the normal ones                     slider_vars=['initW','Dep','Fac','Use','tau_r_AMPA','tau_d_AMPA']) # Range variables you want to tune to adjust synaptic response.  <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> In\u00a0[4]: Copied! <pre>tuner.SingleEvent()\n</pre> tuner.SingleEvent() <pre>('baseline', -0.0002407732493736603)\n('sign', -1.0)\n('latency', 1.35)\n('amp', 0.007084197529025005)\n('rise_time', 0.7000000000000001)\n('decay_time', 8.155594566100724)\n('half_width', 8.275)\nCurrent Integral in pA*ms: 73.67\n</pre> In\u00a0[5]: Copied! <pre>tuner.InteractiveTuner()\n</pre> tuner.InteractiveTuner() <pre>VBox(children=(HBox(children=(Dropdown(description='Connection:', options=('Fac2FSI', 'Dep2FSI'), style=Descri\u2026</pre> <pre>Output()</pre> In\u00a0[6]: Copied! <pre>results = tuner.stp_frequency_response(log_plot=False)\n</pre> results = tuner.stp_frequency_response(log_plot=False) <pre>Analyzing frequencies:   0%|          | 0/16 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre>from bmtool.synapses import SynapseOptimizer\n# Create the optimizer\noptimizer = SynapseOptimizer(tuner)\n\n# Define parameter bounds these can be any range variable you wish to tune\nparam_bounds = {\n    'Dep': (0, 200.0),\n    'Fac': (0, 400.0),\n    'Use': (0.1, 1.0),\n    'tau_r_AMPA': (1,4), # tau r needs to be less than tau d so be careful\n    'tau_d_AMPA': (5,20)\n}\n\n# Define target metrics these are the metrics that the tuner will try to automatic get the synapse to respond with\n# max amps is an absolute value\ntarget_metrics = {\n    'induction': -0.75,\n    'ppr': 0.8,\n    'recovery': 0.0,\n    'max_amplitude': 25,     # note if you get rid of the max amps in the cost function the fit will normally be better. Then you could scale the max amps with the initW\n    'rise_time': 2,          # This wont always be the case, but for this synapse Use controls STP and max amps so it can sometimes struggle to fit.\n    'decay_time': 9\n}\n\n# currently the only metrics in the SynapseOptimizer are \n#            - induction: measure of synaptic facilitation/depression\n#            - ppr: paired-pulse ratio\n#            - recovery: recovery from facilitation/depression\n#            - max_amplitude: maximum synaptic response amplitude\n#            - rise_time: time for synaptic response to rise from 20% to 80% of peak\n#            - decay_time: time constant of synaptic response decay\n#            - latency: synaptic response latency\n#            - half_width: synaptic response half-width\n#            - baseline: baseline current\n#            - amp: peak amplitude from syn_props\ndef custom_cost(metrics, targets):\n    # equal zero unless using train input\n    induction_error = (metrics['induction'] - targets['induction']) ** 2\n    ppr_error = (metrics['ppr'] - targets['ppr']) ** 2\n    recovery_error = (metrics['recovery'] - targets['recovery']) ** 2\n    max_amp_errror = (metrics['max_amplitude'] - targets['max_amplitude']) ** 2\n    # equal zero unless using SingleEvent\n    rise_time_error = (metrics['rise_time'] - targets['rise_time']) ** 2\n    decay_time_error = (metrics['decay_time'] - targets['decay_time']) ** 2 \n\n    #return rise_time_error + decay_time_error\n    return induction_error + 3 * ppr_error + recovery_error + rise_time_error + decay_time_error #+ 0.5*max_amp_errror\n\n# Run optimization with custom cost function\nresult = optimizer.optimize_parameters(\n    target_metrics=target_metrics,\n    param_bounds=param_bounds,\n    run_single_event=True,  # Run and use parameters from SingleEvent\n    run_train_input=True,   # Run and use parameters from train input \n    train_frequency=50,     # Freq in Hz of train input\n    train_delay=250,        # delay in ms of second train\n    init_guess='random',    # either random or middle_guess. Random will start the synapse witha random value in the param_bound. Middle guess will pick the middle value in the param_bounds\n    cost_function=custom_cost,\n    method='SLSQP'          # I believe this will be the fastest method, but you may try others check out https://docs.scipy.org/doc/scipy-1.15.0/reference/generated/scipy.optimize.minimize.html\n                            # SLSQP is a gradient based method while nelder-mead is simplex (whatever that means)\n)\n\n# Plot results\noptimizer.plot_optimization_results(result)\n</pre> from bmtool.synapses import SynapseOptimizer # Create the optimizer optimizer = SynapseOptimizer(tuner)  # Define parameter bounds these can be any range variable you wish to tune param_bounds = {     'Dep': (0, 200.0),     'Fac': (0, 400.0),     'Use': (0.1, 1.0),     'tau_r_AMPA': (1,4), # tau r needs to be less than tau d so be careful     'tau_d_AMPA': (5,20) }  # Define target metrics these are the metrics that the tuner will try to automatic get the synapse to respond with # max amps is an absolute value target_metrics = {     'induction': -0.75,     'ppr': 0.8,     'recovery': 0.0,     'max_amplitude': 25,     # note if you get rid of the max amps in the cost function the fit will normally be better. Then you could scale the max amps with the initW     'rise_time': 2,          # This wont always be the case, but for this synapse Use controls STP and max amps so it can sometimes struggle to fit.     'decay_time': 9 }  # currently the only metrics in the SynapseOptimizer are  #            - induction: measure of synaptic facilitation/depression #            - ppr: paired-pulse ratio #            - recovery: recovery from facilitation/depression #            - max_amplitude: maximum synaptic response amplitude #            - rise_time: time for synaptic response to rise from 20% to 80% of peak #            - decay_time: time constant of synaptic response decay #            - latency: synaptic response latency #            - half_width: synaptic response half-width #            - baseline: baseline current #            - amp: peak amplitude from syn_props def custom_cost(metrics, targets):     # equal zero unless using train input     induction_error = (metrics['induction'] - targets['induction']) ** 2     ppr_error = (metrics['ppr'] - targets['ppr']) ** 2     recovery_error = (metrics['recovery'] - targets['recovery']) ** 2     max_amp_errror = (metrics['max_amplitude'] - targets['max_amplitude']) ** 2     # equal zero unless using SingleEvent     rise_time_error = (metrics['rise_time'] - targets['rise_time']) ** 2     decay_time_error = (metrics['decay_time'] - targets['decay_time']) ** 2       #return rise_time_error + decay_time_error     return induction_error + 3 * ppr_error + recovery_error + rise_time_error + decay_time_error #+ 0.5*max_amp_errror  # Run optimization with custom cost function result = optimizer.optimize_parameters(     target_metrics=target_metrics,     param_bounds=param_bounds,     run_single_event=True,  # Run and use parameters from SingleEvent     run_train_input=True,   # Run and use parameters from train input      train_frequency=50,     # Freq in Hz of train input     train_delay=250,        # delay in ms of second train     init_guess='random',    # either random or middle_guess. Random will start the synapse witha random value in the param_bound. Middle guess will pick the middle value in the param_bounds     cost_function=custom_cost,     method='SLSQP'          # I believe this will be the fastest method, but you may try others check out https://docs.scipy.org/doc/scipy-1.15.0/reference/generated/scipy.optimize.minimize.html                             # SLSQP is a gradient based method while nelder-mead is simplex (whatever that means) )  # Plot results optimizer.plot_optimization_results(result) <pre>/home/gjgpb9/miniconda3/envs/bmtk/lib/python3.8/site-packages/scipy/optimize/_optimize.py:353: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n  warnings.warn(\"Values in x were outside bounds during a \"\n</pre> <pre>Optimization Results:\nFinal Error: 1.65e-01\n\nTarget Metrics:\ninduction: -0.701 (target: -0.750)\nppr: 0.792 (target: 0.800)\nrecovery: 0.000 (target: 0.000)\nmax_amplitude: 19.099 (target: 25.000)\nrise_time: 2.400 (target: 2.000)\ndecay_time: 8.948 (target: 9.000)\n\nOptimal Parameters:\nDep: 199.769\nFac: 5.485\nUse: 0.353\ntau_r_AMPA: 2.969\ntau_d_AMPA: 6.530\n</pre> <pre>\n========================================\nShort Term Plasticity Results for 50Hz with 250 Delay\n========================================\nPPR: Above 1 is facilitating, below 1 is depressing.\nInduction: Above 0 is facilitating, below 0 is depressing.\nRecovery: A measure of how fast STP decays.\n\nPaired Pulse Response (PPR)\nCalculation: 2nd pulse / 1st pulse\nValues: ([[15.13]]) / ([[19.1]]) = [[0.79]]\n\nInduction\nCalculation: (avg(6th, 7th, 8th pulses) - 1st pulse) / max amps\nValues: avg([[6.17, 5.63, 5.31]]) - [[19.1]] / [19.1]\n([5.7]) - ([19.1]) / [19.1] = -0.701\n\nRecovery\nCalculation: (avg(9th, 10th, 11th, 12th pulses) - avg(1st to 4th pulses)) / max amps\nValues: avg([[19.1 , 15.14, 11.08,  8.56]]) - avg([[19.1 , 15.13, 11.08,  8.56]]) / [19.1]\n([13.47]) - ([13.47]) / [19.1] = 0.000\n\n========================================\n\n</pre> <pre>('baseline', -0.0002407732493736603)\n('sign', -1.0)\n('latency', 1.35)\n('amp', 0.019223754106732827)\n('rise_time', 2.4000000000000004)\n('decay_time', 8.948361102783672)\n('half_width', 12.55)\nCurrent Integral in pA*ms: -115.87\n</pre>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#demo-of-bmtool-chemical-synaptic-tuner-using-only-neuron","title":"Demo of bmtool chemical synaptic tuner using only Neuron\u00b6","text":"<p>By Gregory Glickert</p> <p>First we must define some general settings and the settings for the connection we would like to tune. Below is an example of what this could look like for excitatory and inhibitory connections. Currently all of these settings, but the ones in the spec_syn_param are needed in order to use the tuner. If you dont include the general settting when initalizing the tuner then by default it will use these settings. The spec_settings are going to depend on your exact use case and connection type.</p> <p>If you are using the tuner for a BMTK network you can look at this notebook, but make sure to also check out the bmtk_tuner notebook in this same directory.</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#initialize-tuner","title":"initialize tuner\u00b6","text":"<p>Now we can initialize the synaptic tuner. When initializing you will have to change a few arguments depending on your use case. other_vars_to_record can be any variable in your synaptic mechanism, while slider_vars can be any range variable you wish to tune in the synapse. If the variable is not defined in the spec_syn_param than the tuner will get the value from the mechanism and try to set up some sliders to tune it.</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#singleevent","title":"SingleEvent\u00b6","text":"<p>The SingleEvent method will run a short pulse and then print out the synaptic properties for the synapse.</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#interactivetuner","title":"InteractiveTuner\u00b6","text":"<p>The InteractiveTuner will deliver an input to the cell at a desired weight and frequency. The frequency by default will be 8 spikes then a 250ms delay and then 4 more spikes.</p> <p>Paired-pulse ratio is (Avg 2nd pulse - Avg 1st pulse) \u00f7 90th percentile amplitude.</p> <p>Induction is (Avg (6th, 7th, 8th pulses) - Avg 1st pulse) \u00f7 90th percentile amplitude.</p> <p>Recovery is (Avg (9th, 10th, 11th, 12th pulses) - Avg (1st, 2nd, 3rd, 4th pulses)) \u00f7 90th percentile amplitude</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#frequency-reponse","title":"Frequency reponse\u00b6","text":"<p>We can also see how the STP parameters vary with different train frequencies</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#synapseoptimizer","title":"SynapseOptimizer\u00b6","text":"<p>If we don't feel like tuner by hand we can also try to optimize an output of our model. In this example we will optimize and find the best STP parameters that give the induction and paired pulse response we want. Something to note is that the optimizer does not know what the trace should look like and only knows the features. So it might get some wild trace that happens to work. Also if you are using the random init_guess and don't like the voltage trace then run it again. The seed is different each time so the optimization will be different and can result in a better fit.</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#extra-notes","title":"Extra notes\u00b6","text":"<p>If there is a need for faster optimization with more parameters or more complex metrics the SynapseOptimize could be modifed to use jax and the scipy method. I did not look into this much as my use case currently only takes around 1 min to optimize. Also for the example provided it is important to note that we are using bounded optimizing methods since the Use parameter can not go above one. If there is a case where the parameters have no upper or lower bounds one could look into other optimizing methods</p>"},{"location":"modules/analysis/","title":"Analysis Module","text":"<p>The Analysis module provides a comprehensive suite of tools for processing and analyzing simulation results from BMTK models. It is organized into several submodules, each focusing on specific aspects of neural data analysis.</p>"},{"location":"modules/analysis/#overview","title":"Overview","text":""},{"location":"modules/analysis/#spike-analysis","title":"Spike Analysis","text":"<ul> <li>Loading and processing spike data</li> <li>Computing firing rate statistics</li> <li>Population spike rate analysis</li> <li>Spike train analysis tools</li> </ul>"},{"location":"modules/analysis/#lfpecp-analysis","title":"LFP/ECP Analysis","text":"<ul> <li>Loading and processing LFP/ECP data</li> <li>Spectral analysis and filtering</li> <li>Time-frequency analysis</li> <li>Signal quality metrics</li> </ul>"},{"location":"modules/analysis/#entrainment-analysis","title":"Entrainment Analysis","text":"<ul> <li>Phase-locking analysis</li> <li>Spike-field coherence</li> <li>Population entrainment metrics</li> <li>Cross-frequency coupling</li> </ul>"},{"location":"modules/analysis/#network-connectivity","title":"Network Connectivity","text":"<ul> <li>Connection statistics</li> <li>Network topology analysis</li> <li>Synaptic weight distributions</li> <li>Connectivity visualization</li> </ul>"},{"location":"modules/analysis/#spike-analysis_1","title":"Spike Analysis","text":"<p>Load and analyze spike data from simulation output:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom bmtool.analysis.spikes import load_spikes_to_df, compute_firing_rate_stats, get_population_spike_rate\nfrom bmtool.bmplot import raster, plot_firing_rate_pop_stats, plot_firing_rate_distribution\n\n# Load spike data from a simulation\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network',\n    config='config.json',  # Optional: to label cell types\n    groupby='pop_name'\n)\n\n# Get basic spike statistics by population\npop_stats, individual_stats = compute_firing_rate_stats(\n    df=spikes_df,\n    groupby='pop_name',\n    start_time=500,\n    stop_time=1500\n)\n\nprint(\"Population firing rate statistics:\")\nprint(pop_stats)\n\n# Calculate population spike rates over time\npopulation_rates = get_population_spike_rate(\n    spikes=spikes_df,\n    fs=400.0,               # Sampling frequency in Hz\n    t_start=0,\n    t_stop=2000,\n    config='config.json',   # Optional\n    network_name='network'  # Optional\n)\n\n# Plot population rates\nfor pop_name, rates in population_rates.items():\n    plt.plot(rates, label=pop_name)\nplt.xlabel('Time (ms)')\nplt.ylabel('Firing Rate (Hz)')\nplt.legend()\nplt.title('Population Firing Rates')\nplt.show()\n</code></pre>"},{"location":"modules/analysis/#raster-plots","title":"Raster Plots","text":"<p>Create raster plots to visualize spike patterns using the BMPlot module:</p> <pre><code>import matplotlib.pyplot as plt\nfrom bmtool.analysis.spikes import load_spikes_to_df\nfrom bmtool.bmplot import raster\n\n# Load spike data\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network',\n    config='config.json'\n)\n\n# Create a basic raster plot\nfig, ax = plt.subplots(figsize=(10, 6))\nraster(\n    spikes_df=spikes_df,\n    groupby='pop_name',\n    time_range=(0, 2000),\n    ax=ax\n)\nplt.show()\n\n# Plot firing rate statistics\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_pop_stats(\n    firing_stats=pop_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n\n# Plot firing rate distributions\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_distribution(\n    individual_stats=individual_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/analysis/#lfpecp-analysis_1","title":"LFP/ECP Analysis","text":"<p>Analyze LFP (Local Field Potential) and ECP (Extracellular Potential) data:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport xarray as xr\nfrom bmtool.analysis.lfp import (\n    load_ecp_to_xarray,\n    ecp_to_lfp,\n    slice_time_series,\n    cwt_spectrogram_xarray,\n    plot_spectrogram,\n    butter_bandpass_filter,\n    fit_fooof\n)\n\n# Load ECP data\necp_data = load_ecp_to_xarray('output/ecp.h5', demean=True)\n\n# Convert ECP to LFP with filtering\nlfp_data = ecp_to_lfp(\n    ecp_data=ecp_data,\n    cutoff=250,        # Cutoff frequency in Hz\n    fs=10000           # Sampling frequency in Hz\n)\n\n# Slice data to specific time range\nlfp_slice = slice_time_series(lfp_data, time_ranges=(500, 1500))\n\n# Calculate spectrogram\nspectrogram = cwt_spectrogram_xarray(\n    x=lfp_slice.sel(channel=0).data,\n    fs=10000,\n    freq_range=(1, 100),\n    nNotes=8\n)\n\n# Plot spectrogram\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_spectrogram(\n    sxx_xarray=spectrogram,\n    log_power=True,\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/analysis/#frequency-analysis-and-phase-locking","title":"Frequency Analysis and Phase Locking","text":"<p>Analyze frequency content and phase locking between signals:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom bmtool.analysis.lfp import (\n    butter_bandpass_filter,\n    calculate_spike_lfp_plv,\n    calculate_signal_signal_plv,\n    calculate_ppc\n)\nfrom bmtool.analysis.spikes import load_spikes_to_df\n\n# Load spike data and LFP data\nspikes_df = load_spikes_to_df('output/spikes.h5', network_name='network')\nlfp_data = load_ecp_to_xarray('output/ecp.h5', demean=True)\n\n# Filter LFP to specific frequency band (e.g., theta: 4-8 Hz)\nlfp_signal = lfp_data.sel(channel=0).data\nfs = 10000  # Hz\nfiltered_lfp = butter_bandpass_filter(\n    data=lfp_signal,\n    lowcut=4,\n    highcut=8,\n    fs=fs\n)\n\n# Extract spike times for a specific population\npopulation_spikes = spikes_df[spikes_df['pop_name'] == 'Pyramidal']\nspike_times = population_spikes['timestamps'].to_numpy()\n\n# Calculate phase-locking value between spikes and LFP\nplv = calculate_spike_lfp_plv(\n    spike_times=spike_times,\n    lfp_signal=filtered_lfp,\n    spike_fs=1000,  # Spike time unit in milliseconds\n    lfp_fs=fs,\n    fmin=4,\n    fmax=8\n)\nprint(f\"Phase-locking value: {plv}\")\n\n# Calculate pairwise phase consistency\nppc = calculate_ppc(\n    spike_times=spike_times,\n    lfp_signal=filtered_lfp,\n    spike_fs=1000,\n    lfp_fs=fs,\n    fmin=4,\n    fmax=8\n)\nprint(f\"Pairwise phase consistency: {ppc}\")\n</code></pre>"},{"location":"modules/analysis/#signal-processing","title":"Signal Processing","text":"<p>Apply filters and transformations to time series data:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom bmtool.analysis.lfp import (\n    butter_bandpass_filter,\n    wavelet_filter,\n    fit_fooof,\n    generate_resd_from_fooof,\n    calculate_SNR\n)\n\n# Apply band-pass filter\nfiltered_signal = butter_bandpass_filter(\n    data=lfp_signal,\n    lowcut=30,\n    highcut=80,\n    fs=10000\n)\n\n# Apply wavelet filter centered at a specific frequency\ngamma_filtered = wavelet_filter(\n    x=lfp_signal,\n    freq=40,        # Center frequency in Hz\n    fs=10000,       # Sampling rate\n    bandwidth=10    # Bandwidth in Hz\n)\n\n# Calculate power spectrum and fit FOOOF model\nfrom scipy import signal\n\n# Calculate power spectrum\nfreqs, pxx = signal.welch(lfp_signal, fs=10000, nperseg=4096)\n\n# Fit FOOOF model to extract oscillatory and aperiodic components\nfooof_model = fit_fooof(\n    f=freqs,\n    pxx=pxx,\n    freq_range=[1, 100],\n    peak_width_limits=[1, 8],\n    max_n_peaks=6\n)\n\n# Get the residuals between the original spectrum and the aperiodic fit\nresid_spectra, idx_freqs = generate_resd_from_fooof(fooof_model)\n\n# Calculate signal-to-noise ratio in a specific frequency band\nsnr = calculate_SNR(fooof_model, freq_band=(30, 80))\nprint(f\"Signal-to-noise ratio in gamma band: {snr}\")\n</code></pre>"},{"location":"modules/bmplot/","title":"BMPlot Module","text":"<p>The BMPlot module provides visualization tools for BMTK networks, allowing you to analyze and plot connectivity patterns, cell positions, and network properties.</p>"},{"location":"modules/bmplot/#features","title":"Features","text":"<ul> <li>Connection Matrices: Generate matrices showing connectivity between populations</li> <li>Position Plots: Visualize 3D positions of cells in the network</li> <li>Rotation Plots: Visualize cell orientation in 3D space</li> <li>Connection Analysis: Analyze connection properties and distributions</li> <li>Raster Plots: Visualize spike data from simulations</li> </ul>"},{"location":"modules/bmplot/#connection-matrices","title":"Connection Matrices","text":""},{"location":"modules/bmplot/#total-connection-matrix","title":"Total Connection Matrix","text":"<pre><code>import bmtool.bmplot.connections as connections\n\n# Default - all connections\nconnections.total_connection_matrix(\n    config='config.json',\n    title='Total Connection Matrix',\n    sources=None,  # Use all sources (default)\n    targets=None,  # Use all targets (default)\n    save_file=None  # Optional path to save the figure\n)\n\n# Specific source/target populations\nconnections.total_connection_matrix(\n    config='config.json',\n    sources='LA',\n    targets='LA'\n)\n</code></pre>"},{"location":"modules/bmplot/#percent-connection-matrix","title":"Percent Connection Matrix","text":"<p>Generate a matrix showing the percent connectivity between neuron populations:</p> <pre><code># Default - all connections\nconnections.percent_connection_matrix(\n    config='config.json',\n    method='total'  # Default method\n)\n\n# Only unidirectional connections\nconnections.percent_connection_matrix(\n    config='config.json',\n    method='unidirectional'\n)\n\n# Only bidirectional connections\nconnections.percent_connection_matrix(\n    config='config.json',\n    method='bidirectional'\n)\n</code></pre>"},{"location":"modules/bmplot/#convergence-connection-matrix","title":"Convergence Connection Matrix","text":"<p>Generate a matrix showing the convergence of connections between neuron populations:</p> <pre><code># Mean convergence (default)\nconnections.convergence_connection_matrix(\n    config='config.json',\n    method='mean+std'  # Default method\n)\n\n# Maximum convergence\nconnections.convergence_connection_matrix(\n    config='config.json',\n    method='max'\n)\n\n# Minimum convergence\nconnections.convergence_connection_matrix(\n    config='config.json',\n    method='min'\n)\n\n# Standard deviation of convergence\nconnections.convergence_connection_matrix(\n    config='config.json',\n    method='std'\n)\n</code></pre>"},{"location":"modules/bmplot/#divergence-connection-matrix","title":"Divergence Connection Matrix","text":"<p>Generate a matrix showing the divergence of connections between neuron populations:</p> <pre><code># Mean divergence (default)\nconnections.divergence_connection_matrix(\n    config='config.json',\n    method='mean+std'  # Default method\n)\n\n# Maximum divergence\nconnections.divergence_connection_matrix(\n    config='config.json',\n    method='max'\n)\n\n# Minimum divergence\nconnections.divergence_connection_matrix(\n    config='config.json',\n    method='min'\n)\n\n# Standard deviation of divergence\nconnections.divergence_connection_matrix(\n    config='config.json',\n    method='std'\n)\n</code></pre>"},{"location":"modules/bmplot/#gap-junction-matrix","title":"Gap Junction Matrix","text":"<p>Generate a matrix specifically for gap junctions:</p> <pre><code>connections.gap_junction_matrix(config='config.json', method='percent')\n</code></pre>"},{"location":"modules/bmplot/#connector-percent-matrix","title":"Connector Percent Matrix","text":"<p>Generate a percentage connectivity matrix from a CSV file produced by BMTool connectors:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.connector_percent_matrix(\n    csv_path='connections.csv',\n    title='Percent Connection Matrix',\n    exclude_strings=None  # Optional strings to exclude\n)\n</code></pre>"},{"location":"modules/bmplot/#connection-distance","title":"Connection Distance","text":"<p>Generate a 3D plot with source and target cell locations and connection distance analysis:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.connection_distance(\n    config='config.json',\n    sources='PopA',\n    targets='PopB',\n    source_cell_id=1,  # Node ID of source cell\n    target_id_type='PopB',  # Target population to analyze\n    ignore_z=False  # Whether to ignore z-axis in distance calculations\n)\n</code></pre>"},{"location":"modules/bmplot/#connection-histogram","title":"Connection Histogram","text":"<p>Generate a histogram showing the distribution of connections:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.connection_histogram(\n    config='config.json',\n    sources='PopA',\n    targets='PopB',\n    source_cell='PopA',  # Source cell type\n    target_cell='PopB'   # Target cell type\n)\n</code></pre>"},{"location":"modules/bmplot/#3d-visualization","title":"3D Visualization","text":""},{"location":"modules/bmplot/#3d-cell-positions","title":"3D Cell Positions","text":"<p>Generate a plot of cell positions in 3D space:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.plot_3d_positions(\n    config='config.json',\n    sources=['PopA', 'PopB'],\n    title='3D Cell Positions',\n    save_file=None  # Optional path to save the figure\n)\n</code></pre>"},{"location":"modules/bmplot/#3d-cell-orientation","title":"3D Cell Orientation","text":"<p>Generate a plot showing cell locations and orientation in 3D space:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.plot_3d_cell_rotation(\n    config='config.json',\n    sources=['PopA'],\n    title='3D Cell Orientation',\n    save_file=None  # Optional path to save the figure\n)\n</code></pre>"},{"location":"modules/bmplot/#network-visualization","title":"Network Visualization","text":""},{"location":"modules/bmplot/#network-graph","title":"Network Graph","text":"<p>Plot a network connection diagram:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.plot_network_graph(\n    config='config.json',\n    sources='LA',\n    targets='LA',\n    tids='pop_name',\n    sids='pop_name',\n    no_prepend_pop=True  # Whether to prepend population name to node labels\n)\n</code></pre>"},{"location":"modules/bmplot/#spike-analysis","title":"Spike Analysis","text":""},{"location":"modules/bmplot/#raster-plot","title":"Raster Plot","text":"<p>Generate a raster plot of spike times:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom bmtool.bmplot.spikes import raster\n\n# Load spike data\nspikes_df = pd.read_csv('spikes.csv')\n\n# Create raster plot\nraster(\n    spikes_df=spikes_df,\n    config='config.json',  # Optional, to load node population data\n    network_name='network',  # Optional, specific network to use\n    groupby='pop_name'  # Column to group spikes by\n)\n\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/#firing-rate-statistics","title":"Firing Rate Statistics","text":"<p>Plot firing rate statistics for different populations:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom bmtool.bmplot.spikes import plot_firing_rate_pop_stats, plot_firing_rate_distribution\n\n# Assuming you already have firing rate statistics dataframes\n# from bmtool.analysis.spikes.compute_firing_rate_stats()\n\n# Plot mean firing rates with error bars\nplot_firing_rate_pop_stats(\n    firing_stats=firing_stats_df,\n    groupby='pop_name'\n)\n\n# Plot distribution of individual firing rates\nplot_firing_rate_distribution(\n    individual_stats=individual_stats_df,\n    groupby='pop_name',\n    plot_type=['box', 'swarm']  # Can use 'box', 'violin', 'swarm' or combinations\n)\n\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/#entrainment-analysis","title":"Entrainment Analysis","text":""},{"location":"modules/bmplot/#spike-power-correlation","title":"Spike-Power Correlation","text":"<p>Plot the correlation between population spike rates and LFP power:</p> <pre><code>import bmtool.bmplot.entrainment as entrainment\n\n# Assuming you have correlation results from bmtool.analysis.entrainment\nentrainment.plot_spike_power_correlation(\n    correlation_results=correlation_results,\n    frequencies=frequencies,\n    pop_names=pop_names\n)\n</code></pre>"},{"location":"modules/bmplot/#lfp-analysis","title":"LFP Analysis","text":""},{"location":"modules/bmplot/#spectrogram","title":"Spectrogram","text":"<p>Plot a spectrogram from LFP data:</p> <p>```python import bmtool.bmplot.lfp as lfp</p>"},{"location":"modules/bmplot/#assuming-you-have-an-xarray-dataset-with-spectrogram-data","title":"Assuming you have an xarray dataset with spectrogram data","text":"<p>lfp.plot_spectrogram(     sxx_xarray=spectrogram_data,     remove_aperiodic=None,  # Optional aperiodic component to remove     log_power=True,  # Whether to use log scale for power     plt_range=[0, 100]  # Frequency range to plot )</p>"},{"location":"modules/connectors/","title":"Connectors Module","text":"<p>The Connectors module provides helper functions and classes that work with BMTK's NetworkBuilder module to facilitate building complex network connectivity patterns. It supports creating reciprocal connections, distance-dependent connections, gap junctions, and more.</p>"},{"location":"modules/connectors/#features","title":"Features","text":"<ul> <li>Unidirectional Connector: Build connections with given probability between populations</li> <li>Reciprocal Connector: Build connections with reciprocal probability between populations</li> <li>Correlated Gap Junction: Create gap junctions correlated with chemical synapses</li> <li>One-to-One Sequential Connector: Create one-to-one mappings between populations</li> </ul>"},{"location":"modules/connectors/#basic-setup","title":"Basic Setup","text":"<p>All connector examples use the following network node structure:</p> <pre><code>from bmtk.builder import NetworkBuilder\n\n# Create main network\nnet = NetworkBuilder('example_net')\nnet.add_nodes(N=100, pop_name='PopA', model_type='biophysical')\nnet.add_nodes(N=100, pop_name='PopB', model_type='biophysical')\n\n# Create background inputs\nbackground = NetworkBuilder('background')\nbackground.add_nodes(N=300, pop_name='tON', potential='exc', model_type='virtual')\n</code></pre>"},{"location":"modules/connectors/#unidirectional-connector","title":"Unidirectional Connector","text":"<p>Build unidirectional connections in a BMTK network model with a given probability within a single population or between two populations.</p> <pre><code>from bmtool.connectors import UnidirectionConnector\n\n# Create connector with 15% connection probability and 1 synapse per connection\nconnector = UnidirectionConnector(p=0.15, n_syn=1)\n\n# Set up source and target nodes\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopB'))\n\n# Add the edges to the network\nnet.add_edges(**connector.edge_params())\n</code></pre>"},{"location":"modules/connectors/#reciprocal-connector","title":"Reciprocal Connector","text":"<p>Build connections with reciprocal probability within a single population or between two populations.</p> <pre><code>from bmtool.connectors import ReciprocalConnector\n\n# Create connector with 15% base probability and 6.7% reciprocal probability\nconnector = ReciprocalConnector(p0=0.15, pr=0.06767705087, n_syn0=1, n_syn1=1, estimate_rho=False)\n\n# Setup for recurrent connections within PopA\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\n\n# Add the edges to the network\nnet.add_edges(**connector.edge_params())\n</code></pre>"},{"location":"modules/connectors/#correlated-gap-junction","title":"Correlated Gap Junction","text":"<p>Build gap junction connections that can be correlated with chemical synapses.</p> <pre><code>from bmtool.connectors import ReciprocalConnector, CorrelatedGapJunction\n\n# First create a chemical synapse connectivity pattern\nconnector = ReciprocalConnector(p0=0.15, pr=0.06, n_syn0=1, n_syn1=1, estimate_rho=False)\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\nnet.add_edges(**connector.edge_params())\n\n# Then create gap junctions that are correlated with chemical synapses\ngap_junc = CorrelatedGapJunction(p_non=0.1228, p_uni=0.56, p_rec=1, connector=connector)\ngap_junc.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\n\n# Add gap junction edges\nconn = net.add_edges(\n    is_gap_junction=True,\n    syn_weight=0.0000495,\n    target_sections=None,\n    afferent_section_id=0,\n    afferent_section_pos=0.5,\n    **gap_junc.edge_params()\n)\n</code></pre>"},{"location":"modules/connectors/#one-to-one-sequential-connector","title":"One-to-One Sequential Connector","text":"<p>Build one-to-one correspondence connections between two populations.</p> <pre><code>from bmtool.connectors import OneToOneSequentialConnector\n\n# Create the connector\nconnector = OneToOneSequentialConnector()\n\n# Connect background to PopA\nconnector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name='PopA'))\nnet.add_edges(**connector.edge_params())\n\n# Connect background to PopB\nconnector.setup_nodes(target=net.nodes(pop_name='PopB'))\nnet.add_edges(**connector.edge_params())\n</code></pre>"},{"location":"modules/graphs/","title":"Graphs Module","text":"<p>The Graphs module provides functions for analyzing BMTK network connectivity as graph structures using the NetworkX library. It allows you to convert BMTK networks into graph representations for analysis and visualization.</p>"},{"location":"modules/graphs/#features","title":"Features","text":"<ul> <li>Graph Generation: Convert BMTK network connectivity to NetworkX graphs</li> <li>Data Export: Export connection data for further analysis with other tools</li> </ul>"},{"location":"modules/graphs/#generate-graph","title":"Generate Graph","text":"<p>Convert a BMTK network into a NetworkX graph for analysis:</p> <pre><code>import networkx as nx\nfrom bmtool.graphs import generate_graph\n\n# Generate a graph from a BMTK network model\ngraph = generate_graph(config='config.json', source='LA', target='LA')\n\n# Get basic graph statistics\nprint(\"Number of nodes:\", graph.number_of_nodes())\nprint(\"Number of edges:\", graph.number_of_edges())\n\n# Examine node attributes\nnode_attrs = graph.nodes(data=True)\nprint(\"Sample node:\", list(node_attrs)[0])\n\n# Examine edge attributes\nedge_attrs = graph.edges(data=True)\nprint(\"Sample edge:\", list(edge_attrs)[0])\n</code></pre>"},{"location":"modules/graphs/#export-node-connections","title":"Export Node Connections","text":"<p>Export connection data to CSV format for analysis in other tools:</p> <pre><code>from bmtool.graphs import generate_graph, export_node_connections_to_csv\nimport pandas as pd\n\n# Generate a graph from a BMTK network model\ngraph = generate_graph(config='config.json', source='LA', target='LA')\n\n# Export connection data to CSV\nexport_node_connections_to_csv(graph, 'node_connections.csv')\n\n# Load and view the connection data\ndf = pd.read_csv('node_connections.csv')\nprint(df.head())\n</code></pre>"},{"location":"modules/graphs/#advanced-analysis-with-networkx","title":"Advanced Analysis with NetworkX","text":"<p>Use NetworkX's built-in functions for graph analysis:</p> <pre><code>import networkx as nx\nimport matplotlib.pyplot as plt\nfrom bmtool.graphs import generate_graph\n\n# Generate a graph from a BMTK network model\ngraph = generate_graph(config='config.json', source='LA', target='LA')\n\n# Calculate node degree distribution\ndegrees = [d for n, d in graph.degree()]\nplt.figure(figsize=(8, 6))\nplt.hist(degrees, bins=20)\nplt.xlabel('Degree')\nplt.ylabel('Count')\nplt.title('Node Degree Distribution')\nplt.show()\n\n# Find connected components\nif nx.is_directed(graph):\n    components = list(nx.weakly_connected_components(graph))\nelse:\n    components = list(nx.connected_components(graph))\nprint(f\"Number of connected components: {len(components)}\")\nprint(f\"Size of largest component: {len(max(components, key=len))}\")\n\n# Calculate centrality measures\ncentrality = nx.degree_centrality(graph)\nsorted_centrality = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 nodes by degree centrality:\")\nfor node, cent in sorted_centrality[:5]:\n    print(f\"Node {node}: {cent:.4f}\")\n</code></pre>"},{"location":"modules/graphs/#working-with-networkx-attributes","title":"Working with NetworkX Attributes","text":"<p>Access and manipulate node and edge attributes:</p> <pre><code>import networkx as nx\nfrom bmtool.graphs import generate_graph\n\n# Generate a graph from a BMTK network model\ngraph = generate_graph(config='config.json', source='LA', target='LA')\n\n# Get all unique node labels (e.g., cell types)\nnode_labels = set(nx.get_node_attributes(graph, 'label').values())\nprint(\"Node labels:\", node_labels)\n\n# Count nodes by label\nlabel_counts = {}\nfor node, attrs in graph.nodes(data=True):\n    label = attrs.get('label', 'unknown')\n    label_counts[label] = label_counts.get(label, 0) + 1\nprint(\"Nodes per label:\", label_counts)\n\n# Find all edges with a specific property\nedge_types = {}\nfor u, v, attrs in graph.edges(data=True):\n    edge_type = attrs.get('edge_type', 'unknown')\n    edge_types[edge_type] = edge_types.get(edge_type, 0) + 1\nprint(\"Edge types:\", edge_types)\n</code></pre>"},{"location":"modules/singlecell/","title":"Single Cell Module","text":"<p>The Single Cell module provides tools for analyzing and tuning biophysical cell models. It works with any neuron HOC object and can also turn Allen Institute database SWC and JSON files into HOC objects for analysis.</p>"},{"location":"modules/singlecell/#features","title":"Features","text":"<ul> <li>Passive Properties: Calculate resting membrane potential, input resistance, and membrane time constant</li> <li>Current Injection: Run current clamp simulations to observe spiking behavior</li> <li>FI Curves: Generate frequency-current curves to characterize neuronal excitability</li> <li>ZAP Protocol: Analyze frequency response characteristics using chirp current injections</li> <li>Cell Tuning: Interactive interface for tuning cell parameters</li> <li>VHalf Segregation: Simplify channel tuning by separating channel activation based on Alturki et al. (2016)</li> </ul>"},{"location":"modules/singlecell/#getting-started","title":"Getting Started","text":"<p>First, initialize the Profiler with paths to your templates and mechanisms:</p> <pre><code>from bmtool.singlecell import Profiler\nprofiler = Profiler(template_dir='templates', mechanism_dir='mechanisms', dt=0.1)\n</code></pre> <p>For Allen Institute cell models, load them using:</p> <pre><code>from bmtool.singlecell import load_allen_database_cells\ncell = load_allen_database_cells(path_to_SWC_file, path_to_json_file)\n</code></pre>"},{"location":"modules/singlecell/#passive-properties","title":"Passive Properties","text":"<p>Calculate passive membrane properties (V-rest, input resistance, and time constant):</p> <pre><code>from bmtool.singlecell import Passive, run_and_plot\nimport matplotlib.pyplot as plt\n\nsim = Passive('Cell_Cf', inj_amp=-100., inj_delay=1500., inj_dur=1000.,\n              tstop=2500., method='exp2')\ntitle = 'Passive Cell Current Injection'\nxlabel = 'Time (ms)'\nylabel = 'Membrane Potential (mV)'\nX, Y = run_and_plot(sim, title, xlabel, ylabel, plot_injection_only=True)\nplt.gca().plot(*sim.double_exponential_fit(), 'r:', label='double exponential fit')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"modules/singlecell/#current-clamp","title":"Current Clamp","text":"<p>Run a current clamp simulation:</p> <pre><code>from bmtool.singlecell import CurrentClamp\nsim = CurrentClamp('Cell_Cf', inj_amp=350., inj_delay=1500., inj_dur=1000.,\n                   tstop=3000., threshold=-15.)\nX, Y = run_and_plot(sim, title='Current Injection', xlabel='Time (ms)',\n                    ylabel='Membrane Potential (mV)', plot_injection_only=True)\nplt.show()\n</code></pre>"},{"location":"modules/singlecell/#fi-curve","title":"FI Curve","text":"<p>Generate a frequency-current (FI) curve:</p> <pre><code>from bmtool.singlecell import FI\nsim = FI('Cell_Cf', i_start=0., i_stop=1000., i_increment=50.,\n          tstart=1500., threshold=-15.)\nX, Y = run_and_plot(sim, title='FI Curve', xlabel='Injection (nA)',\n                    ylabel='# Spikes')\nplt.show()\n</code></pre>"},{"location":"modules/singlecell/#zap-protocol","title":"ZAP Protocol","text":"<p>Analyze frequency response using a chirp current (ZAP):</p> <pre><code>from bmtool.singlecell import ZAP\nsim = ZAP('Cell_Cf')\nX, Y = run_and_plot(sim)\nplt.show()\n</code></pre>"},{"location":"modules/singlecell/#cell-tuning","title":"Cell Tuning","text":"<p>The cell tuning interface can be accessed via the command line:</p> <pre><code># For BMTK models with a simulation_config.json file\nbmtool util cell tune --builder\n\n# For non-BMTK cell tuning\nbmtool util cell --template TemplateFile.hoc --mod-folder ./ tune --builder\n</code></pre>"},{"location":"modules/singlecell/#vhalf-segregation","title":"VHalf Segregation","text":"<p>The VHalf Segregation module helps simplify channel tuning:</p> <pre><code># Interactive wizard mode\nbmtool util cell vhseg\n\n# Command mode\nbmtool util cell --template CA3PyramidalCell vhseg --othersec dend[0],dend[1] \\\n  --infvars inf_im --segvars gbar_im --gleak gl_ichan2CA3 --eleak el_ichan2CA3\n</code></pre> <p>For building simple models:</p> <pre><code>bmtool util cell --hoc cell_template.hoc vhsegbuild --build\nbmtool util cell --hoc segmented_template.hoc vhsegbuild\n</code></pre>"},{"location":"modules/slurm/","title":"SLURM Module","text":"<p>The SLURM module provides utilities for running BMTK simulations on SLURM-based high-performance computing clusters. It simplifies the process of submitting jobs, running parameter sweeps, and managing simulation workflows.</p>"},{"location":"modules/slurm/#features","title":"Features","text":"<ul> <li>Job Submission: Simplify SLURM job submission for BMTK models</li> <li>Parameter Sweeps: Vary model parameters systematically across simulations</li> <li>Job Management: Monitor and manage running jobs</li> <li>Result Collection: Gather results from multiple simulations</li> </ul>"},{"location":"modules/slurm/#block-runner","title":"Block Runner","text":"<p>The BlockRunner is a core component that manages simulation workflows:</p> <pre><code>from bmtool.SLURM import BlockRunner\n\n# Initialize a block runner for a BMTK model\nrunner = BlockRunner(\n    model_dir='/path/to/model',\n    config='simulation_config.json',\n    steps_per_block=10,  # Number of simulation steps per SLURM job\n    total_steps=100      # Total simulation steps\n)\n\n# Submit the jobs to SLURM\nrunner.run()\n\n# Check the status of submitted jobs\nstatus = runner.check_status()\nprint(status)\n\n# Collect results from completed jobs\nresults = runner.collect_results()\n</code></pre>"},{"location":"modules/slurm/#parameter-sweeps","title":"Parameter Sweeps","text":"<p>The SLURM module supports parameter sweeps to explore model behavior:</p> <pre><code>from bmtool.SLURM import ParameterSweep\n\n# Create a parameter sweep\nsweep = ParameterSweep(\n    base_config='simulation_config.json',\n    model_dir='/path/to/model',\n    parameter_specs={\n        'syn_weight': [0.001, 0.002, 0.003, 0.004],\n        'conn_prob': [0.1, 0.2, 0.3],\n        'input_rate': [10, 20, 30, 40, 50]\n    }\n)\n\n# Generate configurations\nconfigs = sweep.generate_configs()\n\n# Run the parameter sweep\nsweep.run(time_limit='2:00:00', memory='16G')\n</code></pre>"},{"location":"modules/slurm/#job-customization","title":"Job Customization","text":"<p>Customize SLURM job parameters for specific requirements:</p> <pre><code>from bmtool.SLURM import SLURMRunner\n\n# Create a custom SLURM runner\nrunner = SLURMRunner(\n    job_name='bmtk_simulation',\n    partition='normal',\n    nodes=1,\n    cores_per_node=16,\n    memory_gb=32,\n    time_limit='08:00:00',\n    email='user@example.com',\n    email_options=['END', 'FAIL']\n)\n\n# Submit a BMTK simulation\nrunner.submit(\n    model_dir='/path/to/model',\n    config='simulation_config.json',\n    modules_to_load=['neuron', 'python']\n)\n</code></pre>"},{"location":"modules/slurm/#advanced-features","title":"Advanced Features","text":""},{"location":"modules/slurm/#custom-job-arrays","title":"Custom Job Arrays","text":"<p>Create job arrays for parameter variations:</p> <pre><code>from bmtool.SLURM import JobArray\n\narray = JobArray(\n    base_config='simulation_config.json',\n    model_dir='/path/to/model',\n    parameter_variations=[\n        {'input_rate': 10, 'conn_prob': 0.1},\n        {'input_rate': 20, 'conn_prob': 0.1},\n        {'input_rate': 30, 'conn_prob': 0.2}\n    ],\n    array_size=3\n)\n\narray.submit()\n</code></pre>"},{"location":"modules/slurm/#result-analysis","title":"Result Analysis","text":"<p>Analyze results from parameter sweeps:</p> <pre><code>from bmtool.SLURM import SweepAnalyzer\n\nanalyzer = SweepAnalyzer(sweep_dir='/path/to/sweep/results')\nsummary = analyzer.summarize()\nanalyzer.plot_parameter_effects('input_rate', 'mean_firing_rate')\n</code></pre>"},{"location":"modules/slurm/#command-line-interface","title":"Command Line Interface","text":"<p>The SLURM module can also be accessed through the command line:</p> <pre><code># Create a new parameter sweep\nbmtool util slurm sweep-create --config simulation_config.json --param syn_weight 0.001 0.002 0.003\n\n# Submit a sweep to SLURM\nbmtool util slurm sweep-run --sweep-dir sweeps/sweep_001\n\n# Check status of running jobs\nbmtool util slurm status\n</code></pre>"},{"location":"modules/synapses/","title":"Synapses Module","text":"<p>The Synapses module provides tools for configuring and tuning synaptic connections in NEURON models, including both chemical synapses and electrical synapses (gap junctions).</p>"},{"location":"modules/synapses/#features","title":"Features","text":"<ul> <li>Synaptic Tuner: Interactive tuning of synaptic properties via Jupyter notebooks</li> <li>Gap Junction Tuner: Tools for adjusting gap junction properties with coupling coefficient optimization</li> </ul>"},{"location":"modules/synapses/#synaptic-tuner","title":"Synaptic Tuner","text":"<p>The SynapseTuner provides two main usage modes: one for BMTK networks and one for pure NEURON models. It offers an interactive interface with sliders in a Jupyter notebook to adjust synaptic parameters and view the effects in real-time.</p>"},{"location":"modules/synapses/#key-features","title":"Key Features","text":"<ul> <li>Interactive sliders for all synapse parameters</li> <li>Visualization of postsynaptic responses</li> <li>Support for BMTK network configurations</li> <li>Support for pure NEURON model tuning</li> <li>Parameter optimization algorithms</li> <li>Support for various synapse types (Exp2Syn, AMPA, NMDA, STP mechanisms, etc.)</li> </ul>"},{"location":"modules/synapses/#example-usage-with-bmtk","title":"Example Usage with BMTK","text":"<pre><code>from bmtool.synapses import SynapseTuner\n\n# Create a tuner for BMTK networks\ntuner = SynapseTuner(\n    config='simulation_config.json',  # Path to BMTK config\n    current_name='i',                 # Synaptic current to record\n    slider_vars=['initW','Dep','Fac','Use','tau1','tau2']  # Parameters for sliders\n)\n\n# Display the interactive tuner\ntuner.InteractiveTuner()\n\n# Switch between different connections in your network\ntuner._switch_connection('PV2Exc')\n</code></pre>"},{"location":"modules/synapses/#example-usage-with-pure-neuron","title":"Example Usage with Pure NEURON","text":"<pre><code>from bmtool.synapses import SynapseTuner\n\n# Define general and connection-specific settings\ngeneral_settings = {\n    'vclamp': True,\n    'rise_interval': (0.1, 0.9),\n    'tstart': 500.,\n    'tdur': 100.,\n    'threshold': -15.,\n    'delay': 1.3,\n    'weight': 1.,\n    'dt': 0.025,\n    'celsius': 20\n}\n\nconn_type_settings = {\n    'Exc2FSI': {\n        'spec_settings': {\n            'post_cell': 'FSI_Cell',\n            'vclamp_amp': -70.,\n            'sec_x': 0.5,\n            'sec_id': 1,\n            \"level_of_detail\": \"AMPA_NMDA_STP\",\n        },\n        'spec_syn_param': {\n            'initW': 0.76,\n            'tau_r_AMPA': 0.45,\n            'tau_d_AMPA': 7.5,\n            'Use': 0.13,\n            'Dep': 0.,\n            'Fac': 200.\n        },\n    }\n}\n\n# Create tuner with custom settings\ntuner = SynapseTuner(\n    general_settings=general_settings,\n    conn_type_settings=conn_type_settings\n)\n\n# Display the interactive tuner\ntuner.InteractiveTuner()\n</code></pre>"},{"location":"modules/synapses/#gap-junction-tuner","title":"Gap Junction Tuner","text":"<p>The GapJunctionTuner provides tools for tuning electrical synapses (gap junctions) to achieve desired coupling coefficients.</p>"},{"location":"modules/synapses/#key-features_1","title":"Key Features","text":"<ul> <li>Interactive sliders for gap junction resistance</li> <li>Calculation of coupling coefficient</li> <li>Optimization algorithm to automatically find resistance values for desired coupling coefficients</li> <li>Visualization of voltage changes in coupled cells</li> </ul>"},{"location":"modules/synapses/#example-usage","title":"Example Usage","text":"<pre><code>from bmtool.synapses import GapJunctionTuner\n\n# Create a tuner for gap junctions\ntuner = GapJunctionTuner(\n    cell1_template='Interneuron',\n    cell2_template='Interneuron',\n    template_dir='path/to/templates',\n    mod_dir='path/to/mechanisms'\n)\n\n# Display the interactive tuner\ntuner.show()\n\n# Use the optimizer to find resistance for a target coupling coefficient\noptimal_resistance = tuner.optimize(target_cc=0.05)\nprint(f\"Optimal gap junction resistance: {optimal_resistance} MOhm\")\n</code></pre>"},{"location":"modules/synapses/#advanced-features","title":"Advanced Features","text":""},{"location":"modules/synapses/#synapse-optimization","title":"Synapse Optimization","text":"<p>Use the SynapseOptimizer to automatically tune synapse parameters:</p> <pre><code>from bmtool.synapses import SynapseOptimizer\n\n# Create the optimizer\noptimizer = SynapseOptimizer(tuner)\n\n# Define parameter bounds\nparam_bounds = {\n    'Dep': (0, 200.0),\n    'Fac': (0, 400.0),\n    'Use': (0.1, 1.0),\n    'tau1': (1, 4),\n    'tau2': (5, 20)\n}\n\n# Define target metrics\ntarget_metrics = {\n    'max_amp': 5.0,  # Target maximum amplitude (mV)\n    'half_width': 10.0,  # Target half-width (ms)\n    'rise_time': 2.0  # Target rise time (ms)\n}\n\n# Run optimization\nresult = optimizer.optimize_parameters(param_bounds, target_metrics)\nprint(result)\n</code></pre>"},{"location":"modules/synapses/#short-term-plasticity-analysis","title":"Short-Term Plasticity Analysis","text":"<p>Analyze frequency response characteristics of synapses with short-term plasticity:</p> <pre><code># Analyze STP frequency response\nfrequencies, responses = tuner.stp_frequency_response(\n    frequencies=[1, 5, 10, 20, 50, 100],  # Hz\n    duration=1000  # ms\n)\n\n# Plot the results\nimport matplotlib.pyplot as plt\nplt.plot(frequencies, responses)\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Steady-state Response')\nplt.title('STP Frequency Response')\nplt.show()\n</code></pre>"},{"location":"modules/analysis/entrainment/","title":"Entrainment Analysis","text":"<p>This module provides tools for analyzing the relationship between spike timing and LFP oscillations, including phase-locking and spike-field coherence metrics.</p>"},{"location":"modules/analysis/entrainment/#phase-locking-analysis","title":"Phase-Locking Analysis","text":"<p>Calculate phase-locking between spikes and LFP:</p> <pre><code>import numpy as np\nfrom bmtool.analysis.entrainment import (\n    calculate_spike_lfp_plv,\n    calculate_signal_signal_plv,\n    calculate_ppc,\n    calculate_ppc2\n)\nfrom bmtool.analysis.spikes import load_spikes_to_df\nfrom bmtool.analysis.lfp import load_ecp_to_xarray, butter_bandpass_filter\n\n# Load data\nspikes_df = load_spikes_to_df('output/spikes.h5', network_name='network')\nlfp_data = load_ecp_to_xarray('output/ecp.h5', demean=True)\n\n# Filter LFP to specific frequency band (e.g., theta: 4-8 Hz)\nlfp_signal = lfp_data.sel(channel=0).data\nfs = 10000  # Hz\nfiltered_lfp = butter_bandpass_filter(\n    data=lfp_signal,\n    lowcut=4,\n    highcut=8,\n    fs=fs\n)\n\n# Get spike times for a specific population\npopulation_spikes = spikes_df[spikes_df['pop_name'] == 'Pyramidal']\nspike_times = population_spikes['timestamps'].values\n\n# Calculate phase-locking value (PLV)\nplv = calculate_spike_lfp_plv(\n    spike_times=spike_times,\n    lfp_data=filtered_lfp,\n    spike_fs=1000,  # Spike times in milliseconds\n    lfp_fs=fs,\n    filter_method='butter',\n    lowcut=4,\n    highcut=8\n)\nprint(f\"Phase-locking value: {plv}\")\n\n# Calculate pairwise phase consistency (PPC)\nppc = calculate_ppc(\n    spike_times=spike_times,\n    lfp_data=filtered_lfp,\n    spike_fs=1000,\n    lfp_fs=fs,\n    filter_method='butter',\n    lowcut=4,\n    highcut=8\n)\nprint(f\"Pairwise phase consistency: {ppc}\")\n\n# Calculate PPC2 (alternate method)\nppc2 = calculate_ppc2(\n    spike_times=spike_times,\n    lfp_data=filtered_lfp,\n    spike_fs=1000,\n    lfp_fs=fs,\n    filter_method='butter',\n    lowcut=4,\n    highcut=8\n)\nprint(f\"PPC2 value: {ppc2}\")\n</code></pre>"},{"location":"modules/analysis/entrainment/#population-entrainment-analysis","title":"Population Entrainment Analysis","text":"<p>Analyze entrainment across multiple cells or populations:</p> <pre><code>from bmtool.analysis.entrainment import calculate_entrainment_per_cell\n\n# Calculate entrainment metrics for all cells\nentrainment_dict = calculate_entrainment_per_cell(\n    spike_df=spikes_df,\n    lfp_data=lfp_signal,\n    filter_method='wavelet',\n    pop_names=['Pyramidal', 'Basket'],\n    entrainment_method='plv',  # or 'ppc', 'ppc2'\n    spike_fs=1000,\n    lfp_fs=fs,\n    freqs=[4, 8, 20, 40, 80]  # Frequencies of interest\n)\n\n# Print results for each population\nfor pop, cell_dict in entrainment_dict.items():\n    print(f\"\\nPopulation: {pop}\")\n    for cell_id, freq_dict in cell_dict.items():\n        print(f\"Cell {cell_id}:\")\n        for freq, value in freq_dict.items():\n            print(f\"  {freq} Hz: {value:.3f}\")\n</code></pre>"},{"location":"modules/analysis/entrainment/#spike-lfp-power-correlation","title":"Spike-LFP Power Correlation","text":"<p>Analyze correlation between spike rates and LFP power:</p> <pre><code>from bmtool.analysis.entrainment import calculate_spike_rate_power_correlation\n\n# Calculate correlations across frequency bands\ncorrelation_results, frequencies = calculate_spike_rate_power_correlation(\n    spike_rate=population_rates,\n    lfp_data=lfp_signal,\n    fs=fs,\n    pop_names=['Pyramidal', 'Basket'],\n    filter_method='wavelet',\n    freq_range=(4, 100),\n    freq_step=4\n)\n\n# Plot results\nfor pop in correlation_results:\n    corr_values = [correlation_results[pop][f]['correlation'] for f in frequencies]\n    plt.plot(frequencies, corr_values, label=pop)\n\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Correlation Coefficient')\nplt.legend()\nplt.title('Spike Rate-LFP Power Correlation')\nplt.show()\n</code></pre>"},{"location":"modules/analysis/lfp/","title":"LFP/ECP Analysis","text":"<p>This module provides tools for analyzing Local Field Potentials (LFP) and Extracellular Potentials (ECP) from BMTK simulations.</p>"},{"location":"modules/analysis/lfp/#loading-and-processing-lfpecp-data","title":"Loading and Processing LFP/ECP Data","text":"<pre><code>import numpy as np\nimport xarray as xr\nfrom bmtool.analysis.lfp import load_ecp_to_xarray, ecp_to_lfp, slice_time_series\n\n# Load ECP data\necp_data = load_ecp_to_xarray('output/ecp.h5', demean=True)\n\n# Convert ECP to LFP with filtering\nlfp_data = ecp_to_lfp(\n    ecp_data=ecp_data,\n    cutoff=250,        # Cutoff frequency in Hz\n    fs=10000           # Sampling frequency in Hz\n)\n\n# Slice data to specific time range\nlfp_slice = slice_time_series(lfp_data, time_ranges=(500, 1500))\n</code></pre>"},{"location":"modules/analysis/lfp/#spectral-analysis","title":"Spectral Analysis","text":"<p>Analyze frequency content using wavelets and FOOOF:</p> <pre><code>from bmtool.analysis.lfp import (\n    cwt_spectrogram_xarray,\n    fit_fooof,\n    generate_resd_from_fooof\n)\nimport matplotlib.pyplot as plt\n\n# Calculate wavelet spectrogram\nspectrogram = cwt_spectrogram_xarray(\n    x=lfp_slice.sel(channel=0).data,\n    fs=10000,\n    freq_range=(1, 100),\n    nNotes=8\n)\n\n# Calculate power spectrum and fit FOOOF model\nfrom scipy import signal\n\n# Calculate power spectrum\nfreqs, pxx = signal.welch(lfp_data.sel(channel=0).data, fs=10000, nperseg=4096)\n\n# Fit FOOOF model\nfooof_model = fit_fooof(\n    f=freqs,\n    pxx=pxx,\n    freq_range=[1, 100],\n    peak_width_limits=[1, 8],\n    max_n_peaks=6\n)\n\n# Get residuals between original spectrum and aperiodic fit\nresid_spectra, idx_freqs = generate_resd_from_fooof(fooof_model)\n</code></pre>"},{"location":"modules/analysis/lfp/#filtering-and-signal-processing","title":"Filtering and Signal Processing","text":"<p>Apply various filters to LFP/ECP data:</p> <pre><code>from bmtool.analysis.lfp import butter_bandpass_filter, wavelet_filter, calculate_SNR\n\n# Band-pass filter\nfiltered_signal = butter_bandpass_filter(\n    data=lfp_data.sel(channel=0).data,\n    lowcut=30,\n    highcut=80,\n    fs=10000\n)\n\n# Wavelet filter centered at specific frequency\ngamma_filtered = wavelet_filter(\n    x=lfp_data.sel(channel=0).data,\n    freq=40,        # Center frequency in Hz\n    fs=10000,       # Sampling rate\n    bandwidth=10    # Bandwidth in Hz\n)\n\n# Calculate signal-to-noise ratio\nsnr = calculate_SNR(fooof_model, freq_band=(30, 80))\nprint(f\"Signal-to-noise ratio in gamma band: {snr}\")\n</code></pre>"},{"location":"modules/analysis/netcon_reports/","title":"Network Connectivity Analysis","text":"<p>This module provides tools for analyzing and visualizing network connectivity patterns in BMTK simulations.</p>"},{"location":"modules/analysis/netcon_reports/#overview","title":"Overview","text":"<p>The <code>netcon_reports</code> module helps analyze: - Connection statistics between populations - Synaptic weight distributions - Network topology metrics - Connectivity visualization</p>"},{"location":"modules/analysis/netcon_reports/#example-usage","title":"Example Usage","text":"<p>Coming soon. This module is currently under development.</p>"},{"location":"modules/analysis/spikes/","title":"Spike Analysis","text":"<p>This module provides tools for analyzing spike data from BMTK simulations.</p>"},{"location":"modules/analysis/spikes/#loading-and-processing-spike-data","title":"Loading and Processing Spike Data","text":"<pre><code>import pandas as pd\nfrom bmtool.analysis.spikes import load_spikes_to_df, compute_firing_rate_stats\n\n# Load spike data from a simulation\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network',\n    config='config.json',  # Optional: to label cell types\n    groupby='pop_name'\n)\n\n# Get basic spike statistics by population\npop_stats, individual_stats = compute_firing_rate_stats(\n    df=spikes_df,\n    groupby='pop_name',\n    start_time=500,\n    stop_time=1500\n)\n\nprint(\"Population firing rate statistics:\")\nprint(pop_stats)\n</code></pre>"},{"location":"modules/analysis/spikes/#population-spike-rates","title":"Population Spike Rates","text":"<p>Calculate and visualize population spike rates:</p> <pre><code>from bmtool.analysis.spikes import get_population_spike_rate\nimport matplotlib.pyplot as plt\n\n# Calculate population spike rates over time\npopulation_rates = get_population_spike_rate(\n    spikes=spikes_df,\n    fs=400.0,               # Sampling frequency in Hz\n    t_start=0,\n    t_stop=2000,\n    config='config.json',   # Optional\n    network_name='network'  # Optional\n)\n\n# Plot population rates\nfor pop_name, rates in population_rates.items():\n    plt.plot(rates, label=pop_name)\nplt.xlabel('Time (ms)')\nplt.ylabel('Firing Rate (Hz)')\nplt.legend()\nplt.title('Population Firing Rates')\nplt.show()\n</code></pre>"},{"location":"modules/analysis/spikes/#visualizing-spike-data","title":"Visualizing Spike Data","text":"<p>Use BMPlot module for spike data visualization:</p> <pre><code>from bmtool.bmplot import raster, plot_firing_rate_pop_stats, plot_firing_rate_distribution\n\n# Create a raster plot\nfig, ax = plt.subplots(figsize=(10, 6))\nraster(\n    spikes_df=spikes_df,\n    groupby='pop_name',\n    time_range=(0, 2000),\n    ax=ax\n)\nplt.show()\n\n# Plot firing rate statistics\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_pop_stats(\n    firing_stats=pop_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n\n# Plot firing rate distributions\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_distribution(\n    individual_stats=individual_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/connections/","title":"Network Connections Plotting","text":"<p>The <code>bmplot.connections</code> module provides functions for visualizing network connectivity patterns and connection statistics.</p>"},{"location":"modules/bmplot/connections/#connection-matrix","title":"Connection Matrix","text":"<pre><code>from bmtool.bmplot.connections import plot_connection_matrix\nimport matplotlib.pyplot as plt\n\n# Plot connection matrix\nfig, ax = plt.subplots(figsize=(8, 8))\nplot_connection_matrix(\n    connection_data=connection_matrix,\n    source_pops=source_populations,\n    target_pops=target_populations,\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/connections/#connection-statistics","title":"Connection Statistics","text":"<pre><code>from bmtool.bmplot.connections import plot_connection_statistics\n\n# Plot connection statistics\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_connection_statistics(\n    stats_df=connection_stats,\n    metric='convergence',  # or 'divergence', 'probability'\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/connections/#weight-distributions","title":"Weight Distributions","text":"<pre><code>from bmtool.bmplot.connections import plot_weight_distribution\n\n# Plot synaptic weight distributions\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_weight_distribution(\n    weights=synapse_weights,\n    by_population=True,\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/entrainment/","title":"Entrainment Plotting","text":"<p>The <code>bmplot.entrainment</code> module provides functions for visualizing phase-locking, coherence, and other entrainment metrics.</p>"},{"location":"modules/bmplot/entrainment/#phase-locking-plots","title":"Phase-Locking Plots","text":"<pre><code>from bmtool.bmplot.entrainment import plot_phase_distribution\nimport matplotlib.pyplot as plt\n\n# Plot spike phase distribution\nfig, ax = plt.subplots(figsize=(8, 8), subplot_kw={'projection': 'polar'})\nplot_phase_distribution(\n    phases=spike_phases,\n    population='Pyramidal',\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/entrainment/#population-entrainment","title":"Population Entrainment","text":"<pre><code>from bmtool.bmplot.entrainment import plot_entrainment_by_frequency\n\n# Plot entrainment metrics across frequencies\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_entrainment_by_frequency(\n    entrainment_dict=entrainment_results,\n    metric='plv',  # or 'ppc', 'ppc2'\n    populations=['Pyramidal', 'Basket'],\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/entrainment/#spike-lfp-coherence","title":"Spike-LFP Coherence","text":"<pre><code>from bmtool.bmplot.entrainment import plot_spike_lfp_coherence\n\n# Plot spike-LFP coherence\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_spike_lfp_coherence(\n    coherence_data=coherence_results,\n    populations=['Pyramidal', 'Basket'],\n    frequencies=freq_range,\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/lfp/","title":"LFP/ECP Plotting","text":"<p>The <code>bmplot.lfp</code> module provides functions for visualizing LFP/ECP data, spectrograms, and power spectra.</p>"},{"location":"modules/bmplot/lfp/#spectrograms","title":"Spectrograms","text":"<pre><code>from bmtool.bmplot.lfp import plot_spectrogram\nimport matplotlib.pyplot as plt\n\n# Plot spectrogram\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_spectrogram(\n    sxx_xarray=spectrogram,\n    log_power=True,\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/lfp/#power-spectra","title":"Power Spectra","text":"<pre><code>from bmtool.bmplot.lfp import plot_power_spectrum\n\n# Plot power spectrum with FOOOF fit\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_power_spectrum(\n    freqs=freqs,\n    pxx=pxx,\n    fooof_model=fooof_model,\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/lfp/#lfp-time-series","title":"LFP Time Series","text":"<pre><code>from bmtool.bmplot.lfp import plot_lfp_timeseries\n\n# Plot LFP time series\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_lfp_timeseries(\n    lfp_data=lfp_data,\n    channels=[0, 1, 2],  # Optional: specify channels to plot\n    time_range=(0, 1000),  # Optional: specify time range\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/spikes/","title":"Spike Plotting","text":"<p>The <code>bmplot.spikes</code> module provides functions for visualizing spike data and firing rate statistics.</p>"},{"location":"modules/bmplot/spikes/#raster-plots","title":"Raster Plots","text":"<pre><code>from bmtool.bmplot import raster\nimport matplotlib.pyplot as plt\n\n# Create a raster plot\nfig, ax = plt.subplots(figsize=(10, 6))\nraster(\n    spikes_df=spikes_df,\n    groupby='pop_name',\n    time_range=(0, 2000),\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/spikes/#firing-rate-statistics","title":"Firing Rate Statistics","text":"<pre><code>from bmtool.bmplot import plot_firing_rate_pop_stats, plot_firing_rate_distribution\n\n# Plot population firing rate statistics\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_pop_stats(\n    firing_stats=pop_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n\n# Plot firing rate distributions\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_distribution(\n    individual_stats=individual_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n</code></pre>"}]}