{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"BMTool","text":"<p>A collection of modules to make developing Neuron and BMTK models easier.</p> <p></p>"},{"location":"#overview","title":"Overview","text":"<p>BMTool provides several modules to simplify the development of computational neuroscience models with NEURON and the Brain Modeling Toolkit (BMTK). It offers functionality for:</p> <ul> <li>Single Cell Modeling: Analyze passive properties, current injection, FI curves, and impedance profiles</li> <li>Synapse Development: Tools for tuning synaptic properties and gap junctions</li> <li>Network Construction: Connectors for building complex network structures</li> <li>Visualization: Plot connection matrices, network positions, and more</li> <li>Simulation Management: Run simulations on SLURM clusters with parameter sweeps</li> <li>Analysis: Process simulation results efficiently</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation\npip install bmtool\n\n# For development installation\ngit clone https://github.com/cyneuro/bmtool.git\ncd bmtool\npython setup.py develop\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<p>BMTool provides multiple modules to assist with different aspects of neural modeling:</p> <ul> <li>Single Cell Module: Analyze and tune biophysical cell models</li> <li>Synapses Module: Configure and tune synaptic connections</li> <li>Connectors Module: Build complex network connectivity patterns</li> <li>BMPlot Module: Visualize network connectivity and simulation results</li> <li>Analysis Module: Process spike and report data from simulations</li> <li>SLURM Module: Manage simulation workflows on HPC clusters</li> <li>Graphs Module: Analyze network properties and connectivity patterns</li> </ul>"},{"location":"#command-line-interface","title":"Command Line Interface","text":"<p>BMTool provides a CLI for accessing functionality directly from the command line:</p> <pre><code>bmtool --help\n</code></pre> <p>See the CLI documentation for more details.</p>"},{"location":"cli/","title":"Command Line Interface","text":"<p>BMTool provides a command-line interface (CLI) that makes many of its features accessible without writing Python code. This page documents the available commands and their usage.</p>"},{"location":"cli/#basic-usage","title":"Basic Usage","text":"<p>To see all available commands:</p> <pre><code>bmtool --help\n</code></pre> <pre><code>Usage: bmtool [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --verbose  Verbose printing\n  --help     Show this message and exit.\n\nCommands:\n  debug\n  plot\n  util\n</code></pre>"},{"location":"cli/#plot-commands","title":"Plot Commands","text":"<p>The <code>plot</code> command provides access to visualization features:</p> <pre><code>bmtool plot --help\n</code></pre> <pre><code>Usage: bmtool plot [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --config PATH  Configuration file to use, default: \"simulation_config.json\"\n  --no-display   When set there will be no plot displayed, useful for saving\n                 plots\n  --help         Show this message and exit.\n\nCommands:\n  connection  Display information related to neuron connections\n  positions   Plot cell positions for a given set of populations\n  raster      Plot the spike raster for a given population\n  report      Plot the specified report using BMTK's default report plotter\n</code></pre>"},{"location":"cli/#examples","title":"Examples","text":"<pre><code># Plot raster from simulation output\nbmtool plot raster\n\n# Plot cell positions\nbmtool plot positions\n\n# Plot connection information\nbmtool plot connection\n</code></pre>"},{"location":"cli/#utility-commands","title":"Utility Commands","text":"<p>The <code>util</code> command provides access to various utilities:</p> <pre><code>bmtool util --help\n</code></pre>"},{"location":"cli/#cell-utilities","title":"Cell Utilities","text":"<pre><code>bmtool util cell --help\n</code></pre>"},{"location":"cli/#cell-tuning","title":"Cell Tuning","text":"<pre><code># For BMTK models with a simulation_config.json file\nbmtool util cell tune --builder\n\n# For non-BMTK cell tuning\nbmtool util cell --template TemplateFile.hoc --mod-folder ./ tune --builder\n</code></pre>"},{"location":"cli/#vhalf-segregation","title":"VHalf Segregation","text":"<pre><code># Interactive wizard mode\nbmtool util cell vhseg\n\n# Command mode\nbmtool util cell --template CA3PyramidalCell vhseg --othersec dend[0],dend[1] \\\n  --infvars inf_im --segvars gbar_im --gleak gl_ichan2CA3 --eleak el_ichan2CA3\n</code></pre> <pre><code># For building simple models\nbmtool util cell --hoc cell_template.hoc vhsegbuild --build\nbmtool util cell --hoc segmented_template.hoc vhsegbuild\n</code></pre>"},{"location":"cli/#debug-commands","title":"Debug Commands","text":"<p>The <code>debug</code> command provides debug utilities:</p> <pre><code>bmtool debug --help\n</code></pre>"},{"location":"contributing/","title":"Contributing to BMTool","text":"<p>Thank you for your interest in contributing to BMTool. This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"contributing/#development-installation","title":"Development Installation","text":"<p>For development, install BMTool in development mode:</p> <pre><code>git clone https://github.com/cyneuro/bmtool.git\ncd bmtool\npython setup.py develop\n</code></pre>"},{"location":"contributing/#package-management","title":"Package Management","text":""},{"location":"contributing/#uploading-to-pypi","title":"Uploading to PyPI","text":"<p>To upload a new version to PyPI, follow these steps:</p> <ol> <li>Install required tools:</li> </ol> <pre><code># Install setuptools and wheel\npython -m pip install --user --upgrade setuptools wheel\n</code></pre> <ol> <li>Build the distribution packages:</li> </ol> <pre><code># Run from setup.py directory\npython setup.py sdist bdist_wheel\n</code></pre> <p>This will generate files in the <code>dist</code> directory: <pre><code>dist/\n  bmtool-X.Y.Z-py3-none-any.whl\n  bmtool-X.Y.Z.tar.gz\n</code></pre></p> <ol> <li>Upload to PyPI:</li> </ol> <pre><code># Install Twine\npython -m pip install --user --upgrade twine\n\n# Upload to Test PyPI (optional)\npython -m twine upload --repository-url https://test.pypi.org/legacy/ dist/*\n\n# Upload to PyPI\npython -m twine upload dist/*\n</code></pre>"},{"location":"contributing/#combined-commands","title":"Combined commands","text":"<p>For convenience, here are all the commands together:</p> <pre><code>python -m pip install --upgrade setuptools wheel\npython setup.py sdist bdist_wheel\npython -m pip install --upgrade twine\npython -m twine upload dist/*\n</code></pre>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Update documentation if needed</li> <li>Ensure all tests pass</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#documentation-contributions","title":"Documentation Contributions","text":"<p>To contribute to the documentation:</p> <ol> <li>Install MkDocs and required extensions:</li> </ol> <pre><code>pip install mkdocs mkdocs-material pymdown-extensions mkdocstrings mkdocstrings-python\n</code></pre> <ol> <li> <p>Make changes to the Markdown files in the <code>docs/</code> directory</p> </li> <li> <p>Preview locally:</p> </li> </ol> <pre><code>mkdocs serve\n</code></pre> <ol> <li>Build the documentation:</li> </ol> <pre><code>mkdocs build\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Tests for BMTool are a work in progress. When contributing, please ensure your changes don't break existing functionality.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>Please follow these guidelines for code style: - Use 4 spaces for indentation (not tabs) - Follow PEP 8 style guidelines where possible - Use meaningful variable and function names - Add docstrings for functions and classes</p>"},{"location":"getting-started/","title":"Getting Started with BMTool","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>BMTool can be installed directly from PyPI:</p> <pre><code>pip install bmtool\n</code></pre>"},{"location":"getting-started/#development-installation","title":"Development Installation","text":"<p>For developers who will be contributing to BMTool or need the latest features:</p> <pre><code>git clone https://github.com/cyneuro/bmtool.git\ncd bmtool\npython setup.py develop\n</code></pre> <p>Update the repository (from the bmtool directory) with:</p> <pre><code>git pull\n</code></pre>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>BMTool requires:</p> <ul> <li>Python 3.6 or later</li> <li>NEURON 7.7 or later (for cell modeling functionality)</li> <li>BMTK (Brain Modeling Toolkit)</li> </ul> <p>Additional dependencies are automatically installed with the package.</p>"},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/#command-line-interface","title":"Command Line Interface","text":"<p>BMTool provides a command-line interface for easy access to many features:</p> <pre><code># View available commands\nbmtool --help\n\n# Access plotting functionality\nbmtool plot --help\n\n# Access utility functions\nbmtool util --help\n</code></pre>"},{"location":"getting-started/#python-module-usage","title":"Python Module Usage","text":"<p>BMTool can be imported as a Python module to access its functionality:</p> <pre><code># Import specific modules\nfrom bmtool.singlecell import Profiler, Passive, CurrentClamp, FI, ZAP\nfrom bmtool.bmplot import total_connection_matrix, plot_3d_positions\nfrom bmtool.connectors import UnidirectionConnector, ReciprocalConnector\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Check out the module documentation for details on specific modules</li> <li>Explore the examples to learn how to use BMTool features</li> <li>Read the API reference for detailed function and class documentation</li> </ul>"},{"location":"api/analysis/","title":"Analysis API Reference","text":"<p>This page provides an overview of the Analysis module, which contains several submodules for different types of neural data analysis.</p> <p>The Analysis module provides tools for processing and analyzing simulation results from BMTK models, including:</p>"},{"location":"api/analysis/#spike-analysis","title":"Spike Analysis","text":"<p>The <code>spikes</code> module provides functions for loading and analyzing spike data from simulations, including: - Loading spike data into pandas DataFrames - Computing firing rate statistics - Calculating population spike rates</p>"},{"location":"api/analysis/#lfpecp-analysis","title":"LFP/ECP Analysis","text":"<p>The <code>lfp</code> module provides tools for analyzing local field potentials (LFP) and extracellular potentials (ECP), including: - Loading and processing ECP/LFP data - Time series analysis and filtering - Spectral analysis and wavelet transforms - Signal-to-noise ratio calculations</p>"},{"location":"api/analysis/#entrainment-analysis","title":"Entrainment Analysis","text":"<p>The <code>entrainment</code> module provides tools for analyzing the relationship between spikes and LFP signals, including: - Phase-locking value (PLV) calculations - Pairwise phase consistency (PPC) analysis - Spike-LFP entrainment metrics - Spike rate and LFP power correlations</p>"},{"location":"api/analysis/#network-connectivity-analysis","title":"Network Connectivity Analysis","text":"<p>The <code>netcon_reports</code> module provides tools for analyzing and reporting network connectivity statistics.</p>"},{"location":"api/bmplot/","title":"BMPlot API Reference","text":"<p>This page provides API reference documentation for the BMPlot module which contains functions for plotting and visualizing BMTK network models and simulation results.</p>"},{"location":"api/bmplot/#connections-module","title":"Connections Module","text":""},{"location":"api/bmplot/#bmtool.bmplot.connections.is_notebook","title":"<code>bmtool.bmplot.connections.is_notebook()</code>","text":"<p>Detect if code is running in a Jupyter notebook environment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if running in a Jupyter notebook, False otherwise.</p> Notes <p>This is used to determine whether to call plt.show() explicitly or rely on Jupyter's automatic display functionality.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; if is_notebook():\n...     plt.show()\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def is_notebook() -&gt; bool:\n    \"\"\"\n    Detect if code is running in a Jupyter notebook environment.\n\n    Returns\n    -------\n    bool\n        True if running in a Jupyter notebook, False otherwise.\n\n    Notes\n    -----\n    This is used to determine whether to call plt.show() explicitly or\n    rely on Jupyter's automatic display functionality.\n\n    Examples\n    --------\n    &gt;&gt;&gt; if is_notebook():\n    ...     plt.show()\n    \"\"\"\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == \"ZMQInteractiveShell\":\n            return True  # Jupyter notebook or qtconsole\n        elif shell == \"TerminalInteractiveShell\":\n            return False  # Terminal running IPython\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False  # Probably standard Python interpreter\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.total_connection_matrix","title":"<code>bmtool.bmplot.connections.total_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, synaptic_info='0', include_gap=True)</code>","text":"<p>Generate a plot displaying total connections or other synaptic statistics.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network names to use as sources.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network names to use as targets.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifiers to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifiers to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, don't display population name before sid or tid in the plot. Default is False.</p> <code>False</code> <code>synaptic_info</code> <code>str</code> <p>Type of information to display. Options: - '0': Total connections (default) - '1': Mean and standard deviation of connections - '2': All synapse .mod files used - '3': All synapse .json files used</p> <code>'0'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions and chemical synapses in the analysis. If False, only include chemical synapses. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple of (Figure, Axes)</code> <p>The matplotlib Figure and Axes objects for further customization or saving.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; total_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     title='PN to LN Connections'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def total_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    synaptic_info: str = \"0\",\n    include_gap: bool = True,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generate a plot displaying total connections or other synaptic statistics.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network names to use as sources.\n    targets : str, optional\n        Comma-separated string of network names to use as targets.\n    sids : str, optional\n        Comma-separated string of source node identifiers to filter.\n    tids : str, optional\n        Comma-separated string of target node identifiers to filter.\n    no_prepend_pop : bool, optional\n        If True, don't display population name before sid or tid in the plot. Default is False.\n    synaptic_info : str, optional\n        Type of information to display. Options:\n        - '0': Total connections (default)\n        - '1': Mean and standard deviation of connections\n        - '2': All synapse .mod files used\n        - '3': All synapse .json files used\n    include_gap : bool, optional\n        If True, include gap junctions and chemical synapses in the analysis.\n        If False, only include chemical synapses. Default is True.\n\n    Returns\n    -------\n    tuple of (Figure, Axes)\n        The matplotlib Figure and Axes objects for further customization or saving.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; total_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     title='PN to LN Connections'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    text, num, source_labels, target_labels = util.connection_totals(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        synaptic_info=synaptic_info,\n        include_gap=include_gap,\n    )\n\n    if title is None or title == \"\":\n        title = \"Total Connections\"\n    if synaptic_info == \"1\":\n        title = \"Mean and Stdev # of Conn on Target\"\n    if synaptic_info == \"2\":\n        title = \"All Synapse .mod Files Used\"\n    if synaptic_info == \"3\":\n        title = \"All Synapse .json Files Used\"\n\n    return plot_connection_info(\n        text, num, source_labels, target_labels, title, syn_info=synaptic_info\n    )\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.percent_connection_matrix","title":"<code>bmtool.bmplot.connections.percent_connection_matrix(config, nodes=None, edges=None, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, method='total', include_gap=True, return_dict=False)</code>","text":"<p>Generates a plot showing the percent connectivity of a network.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid in the plot. Default is False.</p> <code>False</code> <code>method</code> <code>str</code> <p>Method for calculating percent connectivity. Options: 'total', 'uni', 'bi'. Default is 'total'.</p> <code>'total'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. If False, only include chemical synapses. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tuple[Figure, Axes], Dict]</code> <p>If return_dict=True, returns a dictionary of connection information. Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = percent_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     return_dict=True\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def percent_connection_matrix(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    method: str = \"total\",\n    include_gap: bool = True,\n    return_dict: bool = False,\n) -&gt; Union[Tuple[Any, Any], Dict]:\n    \"\"\"\n    Generates a plot showing the percent connectivity of a network.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid in the plot. Default is False.\n    method : str, optional\n        Method for calculating percent connectivity. Options: 'total', 'uni', 'bi'.\n        Default is 'total'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. If False, only include chemical synapses.\n        Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is False.\n\n    Returns\n    -------\n    Union[Tuple[Figure, Axes], Dict]\n        If return_dict=True, returns a dictionary of connection information.\n        Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = percent_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     return_dict=True\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    text, num, source_labels, target_labels = util.percent_connections(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        method=method,\n        include_gap=include_gap,\n    )\n    if title is None or title == \"\":\n        title = \"Percent Connectivity\"\n\n    if return_dict:\n        result_dict = plot_connection_info(\n            text, num, source_labels, target_labels, title, return_dict=return_dict\n        )\n        return result_dict\n    else:\n        return plot_connection_info(text, num, source_labels, target_labels, title)\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.probability_connection_matrix","title":"<code>bmtool.bmplot.connections.probability_connection_matrix(config, nodes=None, edges=None, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, dist_X=True, dist_Y=True, dist_Z=True, bins=8, line_plot=False, verbose=False, include_gap=True)</code>","text":"<p>Generates probability graphs showing connectivity as a function of distance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> required <code>dist_X</code> <code>bool</code> <p>If True, include X distance in calculations. Default is True.</p> <code>True</code> <code>dist_Y</code> <code>bool</code> <p>If True, include Y distance in calculations. Default is True.</p> <code>True</code> <code>dist_Z</code> <code>bool</code> <p>If True, include Z distance in calculations. Default is True.</p> <code>True</code> <code>bins</code> <code>int</code> <p>Number of distance bins for the probability calculation. Default is 8.</p> <code>8</code> <code>line_plot</code> <code>bool</code> <p>If True, plot lines instead of bars. Default is False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, print debugging information. Default is False.</p> <code>False</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> Notes <p>This function needs model_template to be defined to work properly.</p> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def probability_connection_matrix(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    dist_X: bool = True,\n    dist_Y: bool = True,\n    dist_Z: bool = True,\n    bins: int = 8,\n    line_plot: bool = False,\n    verbose: bool = False,\n    include_gap: bool = True,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generates probability graphs showing connectivity as a function of distance.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    dist_X : bool, optional\n        If True, include X distance in calculations. Default is True.\n    dist_Y : bool, optional\n        If True, include Y distance in calculations. Default is True.\n    dist_Z : bool, optional\n        If True, include Z distance in calculations. Default is True.\n    bins : int, optional\n        Number of distance bins for the probability calculation. Default is 8.\n    line_plot : bool, optional\n        If True, plot lines instead of bars. Default is False.\n    verbose : bool, optional\n        If True, print debugging information. Default is False.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Notes\n    -----\n    This function needs model_template to be defined to work properly.\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    throwaway, data, source_labels, target_labels = util.connection_probabilities(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        dist_X=dist_X,\n        dist_Y=dist_Y,\n        dist_Z=dist_Z,\n        num_bins=bins,\n        include_gap=include_gap,\n    )\n    if not data.any():\n        return\n    if data[0][0] == -1:\n        return\n    # plot_connection_info(data,source_labels,target_labels,title, save_file=save_file)\n\n    # plt.clf()# clears previous plots\n    np.seterr(divide=\"ignore\", invalid=\"ignore\")\n    num_src, num_tar = data.shape\n    fig, axes = plt.subplots(nrows=num_src, ncols=num_tar, figsize=(12, 12))\n    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n\n    for x in range(num_src):\n        for y in range(num_tar):\n            ns = data[x][y][\"ns\"]\n            bins_data = data[x][y][\"bins\"]\n\n            XX = bins_data[:-1]\n            YY = ns[0] / ns[1]\n\n            if line_plot:\n                axes[x, y].plot(XX, YY)\n            else:\n                axes[x, y].bar(XX, YY)\n\n            if x == num_src - 1:\n                axes[x, y].set_xlabel(target_labels[y])\n            if y == 0:\n                axes[x, y].set_ylabel(source_labels[x])\n\n            if verbose:\n                print(\"Source: [\" + source_labels[x] + \"] | Target: [\" + target_labels[y] + \"]\")\n                print(\"X:\")\n                print(XX)\n                print(\"Y:\")\n                print(YY)\n\n    tt = \"Distance Probability Matrix\"\n    if title:\n        tt = title\n    st = fig.suptitle(tt, fontsize=14)\n    fig.text(0.5, 0.04, \"Target\", ha=\"center\")\n    fig.text(0.04, 0.5, \"Source\", va=\"center\", rotation=\"vertical\")\n\n    return fig, axes\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.convergence_connection_matrix","title":"<code>bmtool.bmplot.connections.convergence_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, convergence=True, method='mean+std', include_gap=True, return_dict=None)</code>","text":"<p>Generates connection plot displaying synaptic convergence data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> required <code>convergence</code> <code>bool</code> <p>If True, compute convergence; if False, compute divergence. Default is True.</p> <code>True</code> <code>method</code> <code>str</code> <p>Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'. Default is 'mean+std'.</p> <code>'mean+std'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Figure, Axes], Dict, None]</code> <p>If return_dict=True, returns a dictionary of connection information. Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = convergence_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='mean+std'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def convergence_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    convergence: bool = True,\n    method: str = \"mean+std\",\n    include_gap: bool = True,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[Tuple[Any, Any], Dict, None]:\n    \"\"\"\n    Generates connection plot displaying synaptic convergence data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    convergence : bool, optional\n        If True, compute convergence; if False, compute divergence. Default is True.\n    method : str, optional\n        Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'.\n        Default is 'mean+std'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is None.\n\n    Returns\n    -------\n    Union[Tuple[Figure, Axes], Dict, None]\n        If return_dict=True, returns a dictionary of connection information.\n        Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = convergence_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='mean+std'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    return divergence_connection_matrix(\n        config,\n        title,\n        sources,\n        targets,\n        sids,\n        tids,\n        no_prepend_pop,\n        convergence,\n        method,\n        include_gap=include_gap,\n        return_dict=return_dict,\n    )\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.divergence_connection_matrix","title":"<code>bmtool.bmplot.connections.divergence_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, convergence=False, method='mean+std', include_gap=True, return_dict=None)</code>","text":"<p>Generates connection plot displaying synaptic divergence data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> required <code>convergence</code> <code>bool</code> <p>If True, compute convergence; if False, compute divergence. Default is False.</p> <code>False</code> <code>method</code> <code>str</code> <p>Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'. Default is 'mean+std'.</p> <code>'mean+std'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Figure, Axes], Dict, None]</code> <p>If return_dict=True, returns a dictionary of connection information. Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = divergence_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='mean+std'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def divergence_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    convergence: bool = False,\n    method: str = \"mean+std\",\n    include_gap: bool = True,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[Tuple[Any, Any], Dict, None]:\n    \"\"\"\n    Generates connection plot displaying synaptic divergence data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    convergence : bool, optional\n        If True, compute convergence; if False, compute divergence. Default is False.\n    method : str, optional\n        Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'.\n        Default is 'mean+std'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is None.\n\n    Returns\n    -------\n    Union[Tuple[Figure, Axes], Dict, None]\n        If return_dict=True, returns a dictionary of connection information.\n        Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = divergence_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='mean+std'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    syn_info, data, source_labels, target_labels = util.connection_divergence(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        convergence=convergence,\n        method=method,\n        include_gap=include_gap,\n    )\n\n    # data, labels = util.connection_divergence_average(config=config,nodes=nodes,edges=edges,populations=populations)\n\n    if title is None or title == \"\":\n        if method == \"min\":\n            title = \"Minimum \"\n        elif method == \"max\":\n            title = \"Maximum \"\n        elif method == \"std\":\n            title = \"Standard Deviation \"\n        elif method == \"mean\":\n            title = \"Mean \"\n        else:\n            title = \"Mean + Std \"\n\n        if convergence:\n            title = title + \"Synaptic Convergence\"\n        else:\n            title = title + \"Synaptic Divergence\"\n    if return_dict:\n        result_dict = plot_connection_info(\n            syn_info,\n            data,\n            source_labels,\n            target_labels,\n            title,\n            return_dict=return_dict,\n        )\n        return result_dict\n    else:\n        return plot_connection_info(\n            syn_info, data, source_labels, target_labels, title\n        )\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.gap_junction_matrix","title":"<code>bmtool.bmplot.connections.gap_junction_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, method='convergence')</code>","text":"<p>Generates connection plot displaying gap junction data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> required <code>method</code> <code>str</code> <p>Method for computing gap junction statistics. Options: 'convergence', 'percent'. Default is 'convergence'.</p> <code>'convergence'</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined, sources/targets are not defined, or method is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gap_junction_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='convergence'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def gap_junction_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    method: str = \"convergence\",\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generates connection plot displaying gap junction data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    method : str, optional\n        Method for computing gap junction statistics. Options: 'convergence', 'percent'.\n        Default is 'convergence'.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If config is not defined, sources/targets are not defined, or method is invalid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; gap_junction_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='convergence'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    if method != \"convergence\" and method != \"percent\":\n        raise Exception(\"type must be 'convergence' or 'percent'\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    syn_info, data, source_labels, target_labels = util.gap_junction_connections(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        method=method,\n    )\n\n    def filter_rows(\n        syn_info: np.ndarray,\n        data: np.ndarray,\n        source_labels: List,\n        target_labels: List,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, List]:\n        \"\"\"\n        Filters out rows in a connectivity matrix that contain only NaN or zero values.\n\n        This function is used to clean up connection matrices by removing rows that have\n        no meaningful data, which helps create more informative visualizations of network connectivity.\n\n        Parameters\n        ----------\n        syn_info : np.ndarray\n            Array containing synaptic information corresponding to the data matrix.\n        data : np.ndarray\n            2D matrix containing connectivity data with rows representing sources\n            and columns representing targets.\n        source_labels : list\n            List of labels for the source populations corresponding to rows in the data matrix.\n        target_labels : list\n            List of labels for the target populations corresponding to columns in the data matrix.\n\n        Returns\n        -------\n        tuple\n            A tuple containing (syn_info, data, source_labels, target_labels) with invalid rows removed.\n        \"\"\"\n        # Identify rows with all NaN or all zeros\n        valid_rows = ~np.all(np.isnan(data), axis=1) &amp; ~np.all(data == 0, axis=1)\n\n        # Filter rows based on valid_rows mask\n        new_syn_info = syn_info[valid_rows]\n        new_data = data[valid_rows]\n        new_source_labels = np.array(source_labels)[valid_rows]\n\n        return new_syn_info, new_data, new_source_labels, target_labels\n\n    def filter_rows_and_columns(\n        syn_info: np.ndarray,\n        data: np.ndarray,\n        source_labels: List,\n        target_labels: List,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, List, List]:\n        \"\"\"\n        Filters out both rows and columns in a connectivity matrix that contain only NaN or zero values.\n\n        This function performs a two-step filtering process: first removing rows with no data,\n        then transposing the matrix and removing columns with no data (by treating them as rows).\n        This creates a cleaner, more informative connectivity matrix visualization.\n\n        Parameters\n        ----------\n        syn_info : np.ndarray\n            Array containing synaptic information corresponding to the data matrix.\n        data : np.ndarray\n            2D matrix containing connectivity data with rows representing sources\n            and columns representing targets.\n        source_labels : list\n            List of labels for the source populations corresponding to rows in the data matrix.\n        target_labels : list\n            List of labels for the target populations corresponding to columns in the data matrix.\n\n        Returns\n        -------\n        tuple\n            A tuple containing (syn_info, data, source_labels, target_labels) with both\n            invalid rows and columns removed.\n        \"\"\"\n        # Filter rows first\n        syn_info, data, source_labels, target_labels = filter_rows(\n            syn_info, data, source_labels, target_labels\n        )\n\n        # Transpose data to filter columns\n        transposed_syn_info = np.transpose(syn_info)\n        transposed_data = np.transpose(data)\n        transposed_source_labels = target_labels\n        transposed_target_labels = source_labels\n\n        # Filter columns (by treating them as rows in transposed data)\n        (\n            transposed_syn_info,\n            transposed_data,\n            transposed_source_labels,\n            transposed_target_labels,\n        ) = filter_rows(\n            transposed_syn_info, transposed_data, transposed_source_labels, transposed_target_labels\n        )\n\n        # Transpose back to original orientation\n        filtered_syn_info = np.transpose(transposed_syn_info)\n        filtered_data = np.transpose(transposed_data)\n        filtered_source_labels = transposed_target_labels  # Back to original source_labels\n        filtered_target_labels = transposed_source_labels  # Back to original target_labels\n\n        return filtered_syn_info, filtered_data, filtered_source_labels, filtered_target_labels\n\n    syn_info, data, source_labels, target_labels = filter_rows_and_columns(\n        syn_info, data, source_labels, target_labels\n    )\n\n    if title is None or title == \"\":\n        title = \"Gap Junction\"\n        if method == \"convergence\":\n            title += \" Syn Convergence\"\n        elif method == \"percent\":\n            title += \" Percent Connectivity\"\n    return plot_connection_info(syn_info, data, source_labels, target_labels, title)\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.connection_histogram","title":"<code>bmtool.bmplot.connections.connection_histogram(config, nodes=None, edges=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=True, synaptic_info='0', source_cell=None, target_cell=None, include_gap=True)</code>","text":"<p>Generates histogram of the number of connections individual cells receive from another population.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot as sources.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot as targets.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter by.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter by.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not prepended to sid or tid. Default is True.</p> <code>True</code> <code>synaptic_info</code> <code>str</code> <p>Type of synaptic information to display. Default is '0'.</p> <code>'0'</code> <code>source_cell</code> <code>str</code> <p>Specific source cell type to plot connections from.</p> <code>None</code> <code>target_cell</code> <code>str</code> <p>Specific target cell type to plot connections onto.</p> <code>None</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(matplotlib.figure.Figure, matplotlib.axes.Axes) containing the histogram.</p> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def connection_histogram(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = True,\n    synaptic_info: str = \"0\",\n    source_cell: Optional[str] = None,\n    target_cell: Optional[str] = None,\n    include_gap: bool = True,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generates histogram of the number of connections individual cells receive from another population.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot as sources.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot as targets.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter by.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter by.\n    no_prepend_pop : bool, optional\n        If True, population name is not prepended to sid or tid. Default is True.\n    synaptic_info : str, optional\n        Type of synaptic information to display. Default is '0'.\n    source_cell : str, optional\n        Specific source cell type to plot connections from.\n    target_cell : str, optional\n        Specific target cell type to plot connections onto.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n\n    Returns\n    -------\n    tuple\n        (matplotlib.figure.Figure, matplotlib.axes.Axes) containing the histogram.\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    sources_list = sources.split(\",\") if sources else []\n    targets_list = targets.split(\",\") if targets else []\n    if sids:\n        sids_list = sids.split(\",\")\n    else:\n        sids_list = []\n    if tids:\n        tids_list = tids.split(\",\")\n    else:\n        tids_list = []\n\n    def connection_pair_histogram(ax=None, **kwargs: Dict) -&gt; None:\n        \"\"\"\n        Creates a histogram showing the distribution of connection counts between specific cell types.\n\n        This function is designed to be used with the relation_matrix utility and will only\n        create histograms for the specified source and target cell types.\n\n        Parameters\n        ----------\n        ax : matplotlib.axes.Axes, optional\n            The axes object on which to create the histogram. If None, uses current axes.\n        kwargs : dict\n            Dictionary containing edge data and filtering information.\n            - edges: DataFrame containing edge information\n            - sid: Column name for source ID type in the edges DataFrame\n            - tid: Column name for target ID type in the edges DataFrame\n            - source_id: Value to filter edges by source ID type\n            - target_id: Value to filter edges by target ID type\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if ax is None:\n            ax = plt.gca()\n        edges_data = kwargs[\"edges\"]\n        source_id_type = kwargs[\"sid\"]\n        target_id_type = kwargs[\"tid\"]\n        source_id = kwargs[\"source_id\"]\n        target_id = kwargs[\"target_id\"]\n        if source_id == source_cell and target_id == target_cell:\n            temp = edges_data[\n                (edges_data[source_id_type] == source_id) &amp; (edges_data[target_id_type] == target_id)\n            ]\n            if not include_gap:\n                gap_col = temp[\"is_gap_junction\"].fillna(False).astype(bool)\n                temp = temp[~gap_col]\n            node_pairs = temp.groupby(\"target_node_id\")[\"source_node_id\"].count()\n            try:\n                conn_mean = statistics.mean(node_pairs.values)\n                conn_std = statistics.stdev(node_pairs.values)\n                conn_median = statistics.median(node_pairs.values)\n                label = \"mean {:.2f} std {:.2f} median {:.2f}\".format(\n                    conn_mean, conn_std, conn_median\n                )\n            except (statistics.StatisticsError, ValueError):  # lazy fix for std not calculated with 1 node\n                conn_mean = statistics.mean(node_pairs.values)\n                conn_median = statistics.median(node_pairs.values)\n                label = \"mean {:.2f} median {:.2f}\".format(conn_mean, conn_median)\n            ax.hist(node_pairs.values, density=False, bins=\"auto\", stacked=True, label=label)\n            ax.legend()\n            ax.set_xlabel(\"# of conns from {} to {}\".format(source_cell, target_cell))\n            ax.set_ylabel(\"# of cells\")\n        else:  # dont care about other cell pairs so pass\n            pass\n\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    # Create figure for the histogram\n    fig, ax = plt.subplots()\n\n    # Wrapper to pass ax to the connection_pair_histogram function\n    def relation_func_wrapper(**kwargs):\n        return connection_pair_histogram(ax=ax, **kwargs)\n\n    util.relation_matrix(\n        config,\n        nodes,\n        edges,\n        sources_list,\n        targets_list,\n        sids_list,\n        tids_list,\n        not no_prepend_pop,\n        relation_func=relation_func_wrapper,\n        synaptic_info=synaptic_info,\n    )\n\n    return fig, ax\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.connection_distance","title":"<code>bmtool.bmplot.connections.connection_distance(config, sources, targets, source_cell_id, target_id_type, ignore_z=False)</code>","text":"<p>Plots the 3D spatial distribution of target nodes relative to a source node and a histogram of distances from the source node to each target node.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>sources</code> <code>str</code> <p>Network name(s) to plot as sources.</p> required <code>targets</code> <code>str</code> <p>Network name(s) to plot as targets.</p> required <code>source_cell_id</code> <code>int</code> <p>ID of the source cell for calculating distances to target nodes.</p> required <code>target_id_type</code> <code>str</code> <p>String to filter target nodes based off the target_query.</p> required <code>ignore_z</code> <code>bool</code> <p>If True, ignore Z axis when calculating distance. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Two tuples, each containing (matplotlib.figure.Figure, matplotlib.axes.Axes): - First tuple: 3D/2D scatter plot of node positions - Second tuple: Histogram of distances</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; (fig1, ax1), (fig2, ax2) = connection_distance(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     source_cell_id=0,\n...     target_id_type='LN',\n...     ignore_z=False\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def connection_distance(\n    config: str,\n    sources: str,\n    targets: str,\n    source_cell_id: int,\n    target_id_type: str,\n    ignore_z: bool = False,\n) -&gt; Tuple[Tuple[Any, Any], Tuple[Any, Any]]:\n    \"\"\"\n    Plots the 3D spatial distribution of target nodes relative to a source node\n    and a histogram of distances from the source node to each target node.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    sources : str\n        Network name(s) to plot as sources.\n    targets : str\n        Network name(s) to plot as targets.\n    source_cell_id : int\n        ID of the source cell for calculating distances to target nodes.\n    target_id_type : str\n        String to filter target nodes based off the target_query.\n    ignore_z : bool, optional\n        If True, ignore Z axis when calculating distance. Default is False.\n\n    Returns\n    -------\n    tuple\n        Two tuples, each containing (matplotlib.figure.Figure, matplotlib.axes.Axes):\n        - First tuple: 3D/2D scatter plot of node positions\n        - Second tuple: Histogram of distances\n\n    Examples\n    --------\n    &gt;&gt;&gt; (fig1, ax1), (fig2, ax2) = connection_distance(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     source_cell_id=0,\n    ...     target_id_type='LN',\n    ...     ignore_z=False\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    # if source != target:\n    # raise Exception(\"Code is setup for source and target to be the same! Look at source code for function to add feature\")\n\n    # Load nodes and edges based on config file\n    nodes, edges = util.load_nodes_edges_from_config(config)\n\n    edge_network = sources + \"_to_\" + targets\n    node_network = sources\n\n    # Filter edges to obtain connections originating from the source node\n    edge = edges[edge_network]\n    edge = edge[edge[\"source_node_id\"] == source_cell_id]\n    if target_id_type:\n        edge = edge[edge[\"target_query\"].str.contains(target_id_type, na=False)]\n\n    target_node_ids = edge[\"target_node_id\"]\n\n    # Filter nodes to obtain only the target and source nodes\n    node = nodes[node_network]\n    target_nodes = node.loc[node.index.isin(target_node_ids)]\n    source_node = node.loc[node.index == source_cell_id]\n\n    # Calculate distances between source node and each target node\n    if ignore_z:\n        target_positions = target_nodes[[\"pos_x\", \"pos_y\"]].values\n        source_position = np.array(\n            [source_node[\"pos_x\"], source_node[\"pos_y\"]]\n        ).ravel()  # Ensure 1D shape\n    else:\n        target_positions = target_nodes[[\"pos_x\", \"pos_y\", \"pos_z\"]].values\n        source_position = np.array(\n            [source_node[\"pos_x\"], source_node[\"pos_y\"], source_node[\"pos_z\"]]\n        ).ravel()  # Ensure 1D shape\n    distances = np.linalg.norm(target_positions - source_position, axis=1)\n\n    # Plot positions of source and target nodes in 3D space or 2D\n    if ignore_z:\n        fig = plt.figure(figsize=(8, 6))\n        ax = fig.add_subplot(111)\n        ax.scatter(target_nodes[\"pos_x\"], target_nodes[\"pos_y\"], c=\"blue\", label=\"target cells\")\n        ax.scatter(source_node[\"pos_x\"], source_node[\"pos_y\"], c=\"red\", label=\"source cell\")\n    else:\n        fig = plt.figure(figsize=(8, 6))\n        ax = fig.add_subplot(111, projection=\"3d\")\n        ax.scatter(\n            target_nodes[\"pos_x\"],\n            target_nodes[\"pos_y\"],\n            target_nodes[\"pos_z\"],\n            c=\"blue\",\n            label=\"target cells\",\n        )\n        ax.scatter(\n            source_node[\"pos_x\"],\n            source_node[\"pos_y\"],\n            source_node[\"pos_z\"],\n            c=\"red\",\n            label=\"source cell\",\n        )\n\n    # Optional: Add text annotations for distances\n    # for i, distance in enumerate(distances):\n    #     ax.text(target_nodes['pos_x'].iloc[i], target_nodes['pos_y'].iloc[i], target_nodes['pos_z'].iloc[i],\n    #             f'{distance:.2f}', color='black', fontsize=8, ha='center')\n\n    plt.legend()\n\n    # Plot distances in a separate 2D plot\n    fig2, ax2 = plt.subplots(figsize=(8, 6))\n    ax2.hist(distances, bins=20, color=\"blue\", edgecolor=\"black\")\n    ax2.set_xlabel(\"Distance\")\n    ax2.set_ylabel(\"Count\")\n    ax2.set_title(\"Distance from Source Node to Each Target Node\")\n    ax2.grid(True)\n\n    return (fig, ax), (fig2, ax2)\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.edge_histogram_matrix","title":"<code>bmtool.bmplot.connections.edge_histogram_matrix(config, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=None, edge_property=None, time=None, time_compare=None, report=None, title=None)</code>","text":"<p>Generates a matrix of histograms showing the distribution of edge properties between populations.</p> <p>This function creates a grid of histograms where each cell represents the distribution of a specific edge property between source and target populations.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>sources</code> <code>str</code> <p>Comma-separated list of source network names.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated list of target network names.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated list of source node identifiers to filter by.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated list of target node identifiers to filter by.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population names are not prepended to node identifiers.</p> <code>None</code> <code>edge_property</code> <code>str</code> <p>The edge property to analyze (e.g., 'syn_weight', 'delay').</p> <code>None</code> <code>time</code> <code>int</code> <p>Time point to analyze from a time series report.</p> <code>None</code> <code>time_compare</code> <code>int</code> <p>Second time point for comparison with time.</p> <code>None</code> <code>report</code> <code>str</code> <p>Name of the report to analyze.</p> <code>None</code> <code>title</code> <code>str</code> <p>Custom title for the plot.</p> <code>None</code> <code>save_file</code> <code>str</code> <p>Path to save the generated plot.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(matplotlib.figure.Figure, matplotlib.axes.Axes) containing the histogram matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, axes = edge_histogram_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     edge_property='syn_weight'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def edge_histogram_matrix(\n    config: str,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: Optional[bool] = None,\n    edge_property: Optional[str] = None,\n    time: Optional[int] = None,\n    time_compare: Optional[int] = None,\n    report: Optional[str] = None,\n    title: Optional[str] = None,\n\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generates a matrix of histograms showing the distribution of edge properties between populations.\n\n    This function creates a grid of histograms where each cell represents the distribution\n    of a specific edge property between source and target populations.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    sources : str, optional\n        Comma-separated list of source network names.\n    targets : str, optional\n        Comma-separated list of target network names.\n    sids : str, optional\n        Comma-separated list of source node identifiers to filter by.\n    tids : str, optional\n        Comma-separated list of target node identifiers to filter by.\n    no_prepend_pop : bool, optional\n        If True, population names are not prepended to node identifiers.\n    edge_property : str, optional\n        The edge property to analyze (e.g., 'syn_weight', 'delay').\n    time : int, optional\n        Time point to analyze from a time series report.\n    time_compare : int, optional\n        Second time point for comparison with time.\n    report : str, optional\n        Name of the report to analyze.\n    title : str, optional\n        Custom title for the plot.\n    save_file : str, optional\n        Path to save the generated plot.\n\n    Returns\n    -------\n    tuple\n        (matplotlib.figure.Figure, matplotlib.axes.Axes) containing the histogram matrix.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, axes = edge_histogram_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     edge_property='syn_weight'\n    ... )\n    \"\"\"\n\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    if time_compare:\n        time_compare = int(time_compare)\n\n    data, source_labels, target_labels = util.edge_property_matrix(\n        edge_property,\n        nodes=None,\n        edges=None,\n        config=config,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        report=report,\n        time=time,\n        time_compare=time_compare,\n    )\n\n    # Fantastic resource\n    # https://stackoverflow.com/questions/7941207/is-there-a-function-to-make-scatterplot-matrices-in-matplotlib\n    num_src, num_tar = data.shape\n    fig, axes = plt.subplots(nrows=num_src, ncols=num_tar, figsize=(12, 12))\n    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n\n    for x in range(num_src):\n        for y in range(num_tar):\n            axes[x, y].hist(data[x][y])\n\n            if x == num_src - 1:\n                axes[x, y].set_xlabel(target_labels[y])\n            if y == 0:\n                axes[x, y].set_ylabel(source_labels[x])\n\n    tt = edge_property + \" Histogram Matrix\"\n    if title:\n        tt = title\n    st = fig.suptitle(tt, fontsize=14)\n    fig.text(0.5, 0.04, \"Target\", ha=\"center\")\n    fig.text(0.04, 0.5, \"Source\", va=\"center\", rotation=\"vertical\")\n    plt.draw()\n\n    return fig, axes\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.plot_connection_info","title":"<code>bmtool.bmplot.connections.plot_connection_info(text, num, source_labels, target_labels, title, syn_info='0', return_dict=None)</code>","text":"<p>Plot connection information as a heatmap with text annotations.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>ndarray</code> <p>2D array of text annotations for each cell.</p> required <code>num</code> <code>ndarray</code> <p>2D array of numerical values for the heatmap colors.</p> required <code>source_labels</code> <code>list of str</code> <p>Labels for source populations (rows).</p> required <code>target_labels</code> <code>list of str</code> <p>Labels for target populations (columns).</p> required <code>title</code> <code>str</code> <p>Title for the plot.</p> required <code>syn_info</code> <code>str</code> <p>Type of synaptic information being displayed. Options: '0', '1', '2', '3'. Default is '0'.</p> <code>'0'</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> required <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple, Dict, None]</code> <p>If return_dict=True, returns a dictionary of connection information. Otherwise, returns a tuple of (Figure, Axes), or None if just displaying.</p> Notes <p>Handles missing source and target values by setting them to 0.</p> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def plot_connection_info(\n    text: np.ndarray,\n    num: np.ndarray,\n    source_labels: List[str],\n    target_labels: List[str],\n    title: str,\n    syn_info: str = \"0\",\n\n    return_dict: Optional[bool] = None,\n) -&gt; Union[Tuple, Dict, None]:\n    \"\"\"\n    Plot connection information as a heatmap with text annotations.\n\n    Parameters\n    ----------\n    text : np.ndarray\n        2D array of text annotations for each cell.\n    num : np.ndarray\n        2D array of numerical values for the heatmap colors.\n    source_labels : list of str\n        Labels for source populations (rows).\n    target_labels : list of str\n        Labels for target populations (columns).\n    title : str\n        Title for the plot.\n    syn_info : str, optional\n        Type of synaptic information being displayed. Options: '0', '1', '2', '3'.\n        Default is '0'.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is None.\n\n    Returns\n    -------\n    Union[Tuple, Dict, None]\n        If return_dict=True, returns a dictionary of connection information.\n        Otherwise, returns a tuple of (Figure, Axes), or None if just displaying.\n\n    Notes\n    -----\n    Handles missing source and target values by setting them to 0.\n    \"\"\"\n    # Ensure text dimensions match num dimensions\n    num_source = len(source_labels)\n    num_target = len(target_labels)\n\n    # Set color map\n    matplotlib.rc(\"image\", cmap=\"viridis\")\n\n    # Calculate square cell size to ensure proper aspect ratio\n    base_cell_size = 0.6  # Base size per cell\n\n    # Calculate figure dimensions with proper aspect ratio\n    # Make sure width and height are proportional to the matrix dimensions\n    fig_width = max(8, num_target * base_cell_size + 4)  # Width based on columns\n    fig_height = max(6, num_source * base_cell_size + 3)  # Height based on rows\n\n    # Ensure minimum readable size\n    min_fig_size = 8\n    if fig_width &lt; min_fig_size or fig_height &lt; min_fig_size:\n        scale_factor = min_fig_size / min(fig_width, fig_height)\n        fig_width *= scale_factor\n        fig_height *= scale_factor\n\n    # Create figure and axis\n    fig1, ax1 = plt.subplots(figsize=(fig_width, fig_height))\n\n    # Replace NaN with 0 and create heatmap\n    num_clean = np.nan_to_num(num, nan=0)\n    # if string is nan\\nnan make it 0\n\n    # Use 'auto' aspect ratio to let matplotlib handle it properly\n    # This prevents the stretching issue\n    im1 = ax1.imshow(num_clean, aspect=\"auto\", interpolation=\"nearest\")\n\n    # Set ticks and labels\n    ax1.set_xticks(list(np.arange(len(target_labels))))\n    ax1.set_yticks(list(np.arange(len(source_labels))))\n    ax1.set_xticklabels(target_labels)\n    ax1.set_yticklabels(source_labels)\n\n    # Improved font sizing based on matrix size\n    label_font_size = max(8, min(14, 120 / max(num_source, num_target)))\n\n    # Style the tick labels\n    ax1.tick_params(axis=\"y\", labelsize=label_font_size, pad=5)\n    plt.setp(\n        ax1.get_xticklabels(),\n        rotation=45,\n        ha=\"right\",\n        rotation_mode=\"anchor\",\n        fontsize=label_font_size,\n    )\n\n    # Dictionary to store connection information\n    graph_dict = {}\n\n    # Improved text size calculation - more readable for larger matrices\n    text_size = max(6, min(12, 80 / max(num_source, num_target)))\n\n    # Loop over data dimensions and create text annotations\n    for i in range(num_source):\n        for j in range(num_target):\n            edge_info = text[i, j] if text[i, j] is not None else \"0\\n0\"\n\n            if source_labels[i] not in graph_dict:\n                graph_dict[source_labels[i]] = {}\n            graph_dict[source_labels[i]][target_labels[j]] = edge_info\n\n            # Skip displaying text for NaN values to reduce clutter\n            if edge_info == \"nan\\nnan\":\n                edge_info = \"0\\n\u00b10\"\n\n            # Format the text display\n            if isinstance(edge_info, str) and \"\\n\" in edge_info:\n                # For mean/std format (e.g. \"15.5\\n4.0\")\n                parts = edge_info.split(\"\\n\")\n                if len(parts) == 2:\n                    try:\n                        mean_val = float(parts[0])\n                        std_val = float(parts[1])\n                        display_text = f\"{mean_val:.1f}\\n\u00b1{std_val:.1f}\"\n                    except ValueError:\n                        display_text = edge_info\n                else:\n                    display_text = edge_info\n            else:\n                display_text = str(edge_info)\n\n            # Add text to plot with better contrast\n            text_color = \"white\" if num_clean[i, j] &lt; (np.nanmax(num_clean) * 0.9) else \"black\"\n\n            if syn_info == \"2\" or syn_info == \"3\":\n                ax1.text(\n                    j,\n                    i,\n                    display_text,\n                    ha=\"center\",\n                    va=\"center\",\n                    color=text_color,\n                    rotation=37.5,\n                    fontsize=text_size,\n                    weight=\"bold\",\n                )\n            else:\n                ax1.text(\n                    j,\n                    i,\n                    display_text,\n                    ha=\"center\",\n                    va=\"center\",\n                    color=text_color,\n                    fontsize=text_size,\n                    weight=\"bold\",\n                )\n\n    # Set labels and title\n    title_font_size = max(12, min(18, label_font_size + 4))\n    ax1.set_ylabel(\"Source\", fontsize=title_font_size, weight=\"bold\", labelpad=10)\n    ax1.set_xlabel(\"Target\", fontsize=title_font_size, weight=\"bold\", labelpad=10)\n    ax1.set_title(title, fontsize=title_font_size + 2, weight=\"bold\", pad=20)\n\n    # Add colorbar\n    cbar = plt.colorbar(im1, shrink=0.8)\n    cbar.ax.tick_params(labelsize=label_font_size)\n\n    # Adjust layout to minimize whitespace and prevent stretching\n    plt.tight_layout(pad=1.5)\n\n    # Force square cells by setting equal axis limits if needed\n    ax1.set_xlim(-0.5, num_target - 0.5)\n    ax1.set_ylim(num_source - 0.5, -0.5)  # Inverted for proper matrix orientation\n\n    # Display or save the plot\n    try:\n        # Check if running in notebook\n        from IPython import get_ipython\n\n        notebook = get_ipython() is not None\n    except ImportError:\n        notebook = False\n\n    if not notebook:\n        plt.show()\n\n    if return_dict:\n        return graph_dict\n    else:\n        return fig1, ax1\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.connector_percent_matrix","title":"<code>bmtool.bmplot.connections.connector_percent_matrix(csv_path=None, exclude_strings=None, assemb_key=None, title='Percent connection matrix', pop_order=None)</code>","text":"<p>Generates and plots a connection matrix based on connection probabilities from a CSV file.</p> <p>This function visualizes percent connectivity while factoring in population distance and other parameters. It processes connection data by filtering 'Source' and 'Target' columns in the CSV and displays the percentage of connected pairs for each population combination in a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>str</code> <p>Path to the CSV file containing connection data. The CSV should be an output from the bmtool.connector classes, specifically generated by the <code>save_connection_report()</code> function.</p> <code>None</code> <code>exclude_strings</code> <code>list of str</code> <p>List of strings to exclude rows where 'Source' or 'Target' contain these strings.</p> <code>None</code> <code>assemb_key</code> <code>str</code> <p>Key to identify and process assembly connections.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the generated plot. Default is 'Percent connection matrix'.</p> <code>'Percent connection matrix'</code> <code>pop_order</code> <code>list of str</code> <p>List of population labels to specify the order for x- and y-ticks in the plot.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(matplotlib.figure.Figure, matplotlib.axes.Axes) containing the heatmap.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, ax = connector_percent_matrix(\n...     csv_path='connections.csv',\n...     exclude_strings=['Gap'],\n...     title='Network Connectivity'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def connector_percent_matrix(\n    csv_path: Optional[str] = None,\n    exclude_strings: Optional[List[str]] = None,\n    assemb_key: Optional[str] = None,\n    title: str = \"Percent connection matrix\",\n    pop_order: Optional[List[str]] = None,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generates and plots a connection matrix based on connection probabilities from a CSV file.\n\n    This function visualizes percent connectivity while factoring in population distance and other parameters.\n    It processes connection data by filtering 'Source' and 'Target' columns in the CSV and displays the\n    percentage of connected pairs for each population combination in a matrix.\n\n    Parameters\n    ----------\n    csv_path : str, optional\n        Path to the CSV file containing connection data. The CSV should be an output from the\n        bmtool.connector classes, specifically generated by the `save_connection_report()` function.\n    exclude_strings : list of str, optional\n        List of strings to exclude rows where 'Source' or 'Target' contain these strings.\n    assemb_key : str, optional\n        Key to identify and process assembly connections.\n    title : str, optional\n        Title for the generated plot. Default is 'Percent connection matrix'.\n    pop_order : list of str, optional\n        List of population labels to specify the order for x- and y-ticks in the plot.\n\n    Returns\n    -------\n    tuple\n        (matplotlib.figure.Figure, matplotlib.axes.Axes) containing the heatmap.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = connector_percent_matrix(\n    ...     csv_path='connections.csv',\n    ...     exclude_strings=['Gap'],\n    ...     title='Network Connectivity'\n    ... )\n    \"\"\"\n    # Read the CSV data\n    df = pd.read_csv(csv_path)\n\n    # Choose the column to display\n    selected_column = \"Percent connectionivity within possible connections\"\n\n    # Filter the DataFrame based on exclude_strings\n    def filter_dataframe(df, column_name, exclude_strings):\n        def process_string(string):\n            match = re.search(r\"\\[\\'(.*?)\\'\\]\", string)\n            if exclude_strings and any(ex_string in string for ex_string in exclude_strings):\n                return None\n            elif match:\n                filtered_string = match.group(1)\n                if \"Gap\" in string:\n                    filtered_string = filtered_string + \"-Gap\"\n\n                if assemb_key:\n                    if assemb_key in string:\n                        filtered_string = filtered_string + assemb_key\n\n                return filtered_string  # Return matched string\n\n            return string  # If no match, return the original string\n\n        df[column_name] = df[column_name].apply(process_string)\n        df = df.dropna(subset=[column_name])\n\n        return df\n\n    df = filter_dataframe(df, \"Source\", exclude_strings)\n    df = filter_dataframe(df, \"Target\", exclude_strings)\n\n    # process assem rows and combine them into one prob per assem type\n    if assemb_key:\n        assems = df[df[\"Source\"].str.contains(assemb_key)]\n        unique_sources = assems[\"Source\"].unique()\n\n        for source in unique_sources:\n            source_assems = assems[assems[\"Source\"] == source]\n            unique_targets = source_assems[\n                \"Target\"\n            ].unique()  # Filter targets for the current source\n\n            for target in unique_targets:\n                # Filter the assemblies with the current source and target\n                unique_assems = source_assems[source_assems[\"Target\"] == target]\n\n                # find the prob of a conn\n                forward_probs = []\n                for _, row in unique_assems.iterrows():\n                    selected_percentage = row[selected_column]\n                    selected_percentage = [\n                        float(p) for p in selected_percentage.strip(\"[]\").split()\n                    ]\n                    if len(selected_percentage) == 1 or len(selected_percentage) == 2:\n                        forward_probs.append(selected_percentage[0])\n                    if len(selected_percentage) == 3:\n                        forward_probs.append(selected_percentage[0])\n                        forward_probs.append(selected_percentage[1])\n\n                mean_probs = np.mean(forward_probs)\n                source = source.replace(assemb_key, \"\")\n                target = target.replace(assemb_key, \"\")\n                new_row = pd.DataFrame(\n                    {\n                        \"Source\": [source],\n                        \"Target\": [target],\n                        \"Percent connectionivity within possible connections\": [mean_probs],\n                        \"Percent connectionivity within all connections\": [0],\n                    }\n                )\n\n                df = pd.concat([df, new_row], ignore_index=False)\n\n    # Prepare connection data\n    connection_data = {}\n    for _, row in df.iterrows():\n        source, target, selected_percentage = row[\"Source\"], row[\"Target\"], row[selected_column]\n        if isinstance(selected_percentage, str):\n            selected_percentage = [float(p) for p in selected_percentage.strip(\"[]\").split()]\n        connection_data[(source, target)] = selected_percentage\n\n    # Determine population order\n    populations = sorted(list(set(df[\"Source\"].unique()) | set(df[\"Target\"].unique())))\n    if pop_order:\n        populations = [\n            pop for pop in pop_order if pop in populations\n        ]  # Order according to pop_order, if provided\n    num_populations = len(populations)\n\n    # Create an empty matrix and populate it\n    connection_matrix = np.zeros((num_populations, num_populations), dtype=float)\n    for (source, target), probabilities in connection_data.items():\n        if source in populations and target in populations:\n            source_idx = populations.index(source)\n            target_idx = populations.index(target)\n\n            if isinstance(probabilities, float):\n                connection_matrix[source_idx][target_idx] = probabilities\n            elif len(probabilities) == 1:\n                connection_matrix[source_idx][target_idx] = probabilities[0]\n            elif len(probabilities) == 2:\n                connection_matrix[source_idx][target_idx] = probabilities[0]\n            elif len(probabilities) == 3:\n                connection_matrix[source_idx][target_idx] = probabilities[0]\n                connection_matrix[target_idx][source_idx] = probabilities[1]\n            else:\n                raise Exception(\"unsupported format\")\n\n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 8))\n    im = ax.imshow(connection_matrix, cmap=\"viridis\", interpolation=\"nearest\")\n\n    # Add annotations\n    for i in range(num_populations):\n        for j in range(num_populations):\n            text = ax.text(\n                j,\n                i,\n                f\"{connection_matrix[i, j]:.2f}%\",\n                ha=\"center\",\n                va=\"center\",\n                color=\"w\",\n                size=10,\n                weight=\"semibold\",\n            )\n\n    # Add colorbar\n    plt.colorbar(im, label=f\"{selected_column}\")\n\n    # Set title and axis labels\n    ax.set_title(title)\n    ax.set_xlabel(\"Target Population\")\n    ax.set_ylabel(\"Source Population\")\n\n    # Set ticks and labels based on populations in specified order\n    ax.set_xticks(np.arange(num_populations))\n    ax.set_yticks(np.arange(num_populations))\n    ax.set_xticklabels(populations, rotation=45, ha=\"right\", size=12, weight=\"semibold\")\n    ax.set_yticklabels(populations, size=12, weight=\"semibold\")\n\n    plt.tight_layout()\n\n    return fig, ax\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.plot_3d_positions","title":"<code>bmtool.bmplot.connections.plot_3d_positions(config=None, sources=None, sid=None, title=None, subset=None)</code>","text":"<p>Plots a 3D graph of all cells with x, y, z location.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Which network(s) to plot. If None or 'all', plots all networks.</p> <code>None</code> <code>sid</code> <code>str</code> <p>Column name to group cell types (node grouping criteria).</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title. Default is '3D positions'.</p> <code>None</code> <code>subset</code> <code>int</code> <p>Take every Nth row. This makes plotting large networks easier to visualize.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(matplotlib.figure.Figure, matplotlib.axes.Axes) containing the 3D plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, ax = plot_3d_positions(\n...     config='config.json',\n...     sources='cortex',\n...     sid='node_type_id',\n...     title='3D Neuron Positions'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def plot_3d_positions(\n    config: Optional[str] = None,\n    sources: Optional[str] = None,\n    sid: Optional[str] = None,\n    title: Optional[str] = None,\n\n    subset: Optional[int] = None,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Plots a 3D graph of all cells with x, y, z location.\n\n    Parameters\n    ----------\n    config : str, optional\n        Path to a BMTK simulation config file.\n    sources : str, optional\n        Which network(s) to plot. If None or 'all', plots all networks.\n    sid : str, optional\n        Column name to group cell types (node grouping criteria).\n    title : str, optional\n        Plot title. Default is '3D positions'.\n    subset : int, optional\n        Take every Nth row. This makes plotting large networks easier to visualize.\n\n    Returns\n    -------\n    tuple\n        (matplotlib.figure.Figure, matplotlib.axes.Axes) containing the 3D plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = plot_3d_positions(\n    ...     config='config.json',\n    ...     sources='cortex',\n    ...     sid='node_type_id',\n    ...     title='3D Neuron Positions'\n    ... )\n    \"\"\"\n\n    if not config:\n        raise Exception(\"config not defined\")\n\n    if sources is None:\n        sources = \"all\"\n\n    # Set group keys (e.g., node types)\n    group_keys = sid\n    if title is None:\n        title = \"3D positions\"\n\n    # Load nodes from the configuration\n    nodes = util.load_nodes_from_config(config)\n\n    # Get the list of populations to plot\n    if \"all\" in sources:\n        populations = list(nodes)\n    else:\n        populations = sources.split(\",\")\n\n    # Split group_by into list\n    group_keys = group_keys.split(\",\")\n    group_keys += (len(populations) - len(group_keys)) * [\n        \"node_type_id\"\n    ]  # Extend the array to default values if not enough given\n    if len(group_keys) &gt; 1:\n        raise Exception(\"Only one group by is supported currently!\")\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(projection=\"3d\")\n    handles = []\n\n    for pop in list(nodes):\n        if \"all\" not in populations and pop not in populations:\n            continue\n\n        nodes_df = nodes[pop]\n        group_key = group_keys[0]\n\n        # If group_key is provided, ensure the column exists in the dataframe\n        if group_key is not None:\n            if group_key not in nodes_df:\n                raise Exception(f\"Could not find column '{group_key}' in {pop}\")\n\n            groupings = nodes_df.groupby(group_key)\n            n_colors = nodes_df[group_key].nunique()\n            color_norm = colors.Normalize(vmin=0, vmax=(n_colors - 1))\n            scalar_map = cmx.ScalarMappable(norm=color_norm, cmap=\"hsv\")\n            color_map = [scalar_map.to_rgba(i) for i in range(n_colors)]\n        else:\n            groupings = [(None, nodes_df)]\n            color_map = [\"blue\"]\n\n        # Loop over groupings and plot\n        for color, (group_name, group_df) in zip(color_map, groupings):\n            if \"pos_x\" not in group_df or \"pos_y\" not in group_df or \"pos_z\" not in group_df:\n                print(\n                    f\"Warning: Missing position columns in group '{group_name}' for {pop}. Skipping this group.\"\n                )\n                continue  # Skip if position columns are missing\n\n            # Subset the dataframe by taking every Nth row if subset is provided\n            if subset is not None:\n                group_df = group_df.iloc[::subset]\n\n            h = ax.scatter(\n                group_df[\"pos_x\"],\n                group_df[\"pos_y\"],\n                group_df[\"pos_z\"],\n                color=color,\n                label=group_name,\n            )\n            handles.append(h)\n\n    if not handles:\n        print(\"No data to plot.\")\n        return fig, ax\n\n    # Set plot title and legend\n    plt.title(title)\n    plt.legend(handles=handles)\n\n    # Add axis labels\n    ax.set_xlabel(\"X Position (\u03bcm)\")\n    ax.set_ylabel(\"Y Position (\u03bcm)\")\n    ax.set_zlabel(\"Z Position (\u03bcm)\")\n\n    # Draw the plot\n    plt.draw()\n    plt.tight_layout()\n\n    return fig, ax\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.connections.plot_3d_cell_rotation","title":"<code>bmtool.bmplot.connections.plot_3d_cell_rotation(config=None, sources=None, sids=None, title=None, quiver_length=None, arrow_length_ratio=None, group=None, subset=None)</code>","text":"<p>Plot 3D visualization of cell rotations with quiver arrows showing rotation orientations.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> <code>None</code> <code>sources</code> <code>list of str</code> <p>Network names to plot. If None or contains 'all', plots all networks.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated column names to group cell types.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title. Default is 'Cell rotations'.</p> <code>None</code> <code>quiver_length</code> <code>float</code> <p>Length of the quiver arrows. If None, use matplotlib default.</p> <code>None</code> <code>arrow_length_ratio</code> <code>float</code> <p>Ratio of arrow head size to quiver length.</p> <code>None</code> <code>group</code> <code>str</code> <p>Comma-separated group names to include. If None, include all groups.</p> <code>None</code> <code>subset</code> <code>int</code> <p>Take every Nth row. Useful for visualizing large networks.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(matplotlib.figure.Figure, matplotlib.axes.Axes) containing the 3D plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, ax = plot_3d_cell_rotation(\n...     config='config.json',\n...     sources=['cortex'],\n...     sids='node_type_id',\n...     title='Cell Rotation Vectors'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def plot_3d_cell_rotation(\n    config: Optional[str] = None,\n    sources: Optional[List[str]] = None,\n    sids: Optional[str] = None,\n    title: Optional[str] = None,\n\n    quiver_length: Optional[float] = None,\n    arrow_length_ratio: Optional[float] = None,\n    group: Optional[str] = None,\n    subset: Optional[int] = None,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Plot 3D visualization of cell rotations with quiver arrows showing rotation orientations.\n\n    Parameters\n    ----------\n    config : str, optional\n        Path to a BMTK simulation config file.\n    sources : list of str, optional\n        Network names to plot. If None or contains 'all', plots all networks.\n    sids : str, optional\n        Comma-separated column names to group cell types.\n    title : str, optional\n        Plot title. Default is 'Cell rotations'.\n    quiver_length : float, optional\n        Length of the quiver arrows. If None, use matplotlib default.\n    arrow_length_ratio : float, optional\n        Ratio of arrow head size to quiver length.\n    group : str, optional\n        Comma-separated group names to include. If None, include all groups.\n    subset : int, optional\n        Take every Nth row. Useful for visualizing large networks.\n\n    Returns\n    -------\n    tuple\n        (matplotlib.figure.Figure, matplotlib.axes.Axes) containing the 3D plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = plot_3d_cell_rotation(\n    ...     config='config.json',\n    ...     sources=['cortex'],\n    ...     sids='node_type_id',\n    ...     title='Cell Rotation Vectors'\n    ... )\n    \"\"\"\n    from scipy.spatial.transform import Rotation as R\n\n    if not config:\n        raise Exception(\"config not defined\")\n\n    if sources is None:\n        sources = [\"all\"]\n\n    group_keys = sids.split(\",\") if sids else []\n\n    if title is None:\n        title = \"Cell rotations\"\n\n    nodes = util.load_nodes_from_config(config)\n\n    if \"all\" in sources:\n        populations = list(nodes)\n    else:\n        populations = sources\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111, projection=\"3d\")\n    handles = []\n\n    for nodes_key, group_key in zip(list(nodes), group_keys):\n        if \"all\" not in populations and nodes_key not in populations:\n            continue\n\n        nodes_df = nodes[nodes_key]\n\n        if group_key is not None:\n            if group_key not in nodes_df.columns:\n                raise Exception(f\"Could not find column {group_key}\")\n            groupings = nodes_df.groupby(group_key)\n\n            n_colors = nodes_df[group_key].nunique()\n            color_norm = colors.Normalize(vmin=0, vmax=(n_colors - 1))\n            scalar_map = cmx.ScalarMappable(norm=color_norm, cmap=\"hsv\")\n            color_map = [scalar_map.to_rgba(i) for i in range(n_colors)]\n        else:\n            groupings = [(None, nodes_df)]\n            color_map = [\"blue\"]\n\n        for color, (group_name, group_df) in zip(color_map, groupings):\n            if subset is not None:\n                group_df = group_df.iloc[::subset]\n\n            if group and group_name not in group.split(\",\"):\n                continue\n\n            if \"pos_x\" not in group_df or \"rotation_angle_xaxis\" not in group_df:\n                continue\n\n            X = group_df[\"pos_x\"]\n            Y = group_df[\"pos_y\"]\n            Z = group_df[\"pos_z\"]\n            U = group_df[\"rotation_angle_xaxis\"].values\n            V = group_df[\"rotation_angle_yaxis\"].values\n            W = group_df[\"rotation_angle_zaxis\"].values\n\n            if U is None:\n                U = np.zeros(len(X))\n            if V is None:\n                V = np.zeros(len(Y))\n            if W is None:\n                W = np.zeros(len(Z))\n\n            # Create rotation matrices from Euler angles\n            rotations = R.from_euler(\"xyz\", np.column_stack((U, V, W)), degrees=False)\n\n            # Define initial vectors\n            init_vectors = np.column_stack((np.ones(len(X)), np.zeros(len(Y)), np.zeros(len(Z))))\n\n            # Apply rotations to initial vectors\n            rots = np.dot(rotations.as_matrix(), init_vectors.T).T\n\n            # Extract x, y, and z components of the rotated vectors\n            rot_x = rots[:, 0]\n            rot_y = rots[:, 1]\n            rot_z = rots[:, 2]\n\n            h = ax.quiver(\n                X,\n                Y,\n                Z,\n                rot_x,\n                rot_y,\n                rot_z,\n                color=color,\n                label=group_name,\n                arrow_length_ratio=arrow_length_ratio,\n                length=quiver_length,\n            )\n            ax.scatter(X, Y, Z, color=color, label=group_name)\n            ax.set_xlim([min(X), max(X)])\n            ax.set_ylim([min(Y), max(Y)])\n            ax.set_zlim([min(Z), max(Z)])\n            handles.append(h)\n\n    if not handles:\n        return fig, ax\n\n    plt.title(title)\n    plt.legend(handles=handles)\n    plt.draw()\n\n    return fig, ax\n</code></pre>"},{"location":"api/bmplot/#spikes-module","title":"Spikes Module","text":""},{"location":"api/bmplot/#bmtool.bmplot.spikes.raster","title":"<code>bmtool.bmplot.spikes.raster(spikes_df=None, config=None, network_name=None, groupby='pop_name', sortby=None, ax=None, tstart=None, tstop=None, color_map=None, dot_size=0.3)</code>","text":"<p>Plots a raster plot of neural spikes, with different colors for each population.</p> Parameters: <p>spikes_df : pd.DataFrame, optional     DataFrame containing spike data with columns 'timestamps', 'node_ids', and optional 'pop_name'. config : str, optional     Path to the configuration file used to load node data. network_name : str, optional     Specific network name to select from the configuration; if not provided, uses the first network. groupby : str, optional     Column name to group spikes by for coloring. Default is 'pop_name'. sortby : str, optional     Column name to sort node_ids within each group. If provided, nodes within each population will be sorted by this column. ax : matplotlib.axes.Axes, optional     Axes on which to plot the raster; if None, a new figure and axes are created. tstart : float, optional     Start time for filtering spikes; only spikes with timestamps greater than <code>tstart</code> will be plotted. tstop : float, optional     Stop time for filtering spikes; only spikes with timestamps less than <code>tstop</code> will be plotted. color_map : dict, optional     Dictionary specifying colors for each population. Keys should be population names, and values should be color values. dot_size: float, optional     Size of the dot to display on the scatterplot</p> Returns: <p>matplotlib.axes.Axes     Axes with the raster plot.</p> Notes: <ul> <li>If <code>config</code> is provided, the function merges population names from the node data with <code>spikes_df</code>.</li> <li>Each unique population from groupby in <code>spikes_df</code> will be represented by a different color if <code>color_map</code> is not specified.</li> <li>If <code>color_map</code> is provided, it should contain colors for all unique <code>pop_name</code> values in <code>spikes_df</code>.</li> </ul> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def raster(\n    spikes_df: Optional[pd.DataFrame] = None,\n    config: Optional[str] = None,\n    network_name: Optional[str] = None,\n    groupby: str = \"pop_name\",\n    sortby: Optional[str] = None,\n    ax: Optional[Axes] = None,\n    tstart: Optional[float] = None,\n    tstop: Optional[float] = None,\n    color_map: Optional[Dict[str, str]] = None,\n    dot_size: float = 0.3,\n) -&gt; Axes:\n    \"\"\"\n    Plots a raster plot of neural spikes, with different colors for each population.\n\n    Parameters:\n    ----------\n    spikes_df : pd.DataFrame, optional\n        DataFrame containing spike data with columns 'timestamps', 'node_ids', and optional 'pop_name'.\n    config : str, optional\n        Path to the configuration file used to load node data.\n    network_name : str, optional\n        Specific network name to select from the configuration; if not provided, uses the first network.\n    groupby : str, optional\n        Column name to group spikes by for coloring. Default is 'pop_name'.\n    sortby : str, optional\n        Column name to sort node_ids within each group. If provided, nodes within each population will be sorted by this column.\n    ax : matplotlib.axes.Axes, optional\n        Axes on which to plot the raster; if None, a new figure and axes are created.\n    tstart : float, optional\n        Start time for filtering spikes; only spikes with timestamps greater than `tstart` will be plotted.\n    tstop : float, optional\n        Stop time for filtering spikes; only spikes with timestamps less than `tstop` will be plotted.\n    color_map : dict, optional\n        Dictionary specifying colors for each population. Keys should be population names, and values should be color values.\n    dot_size: float, optional\n        Size of the dot to display on the scatterplot\n\n    Returns:\n    -------\n    matplotlib.axes.Axes\n        Axes with the raster plot.\n\n    Notes:\n    -----\n    - If `config` is provided, the function merges population names from the node data with `spikes_df`.\n    - Each unique population from groupby in `spikes_df` will be represented by a different color if `color_map` is not specified.\n    - If `color_map` is provided, it should contain colors for all unique `pop_name` values in `spikes_df`.\n    \"\"\"\n    # Initialize axes if none provided\n    sns.set_style(\"whitegrid\")\n    if ax is None:\n        _, ax = plt.subplots(1, 1)\n\n    # Filter spikes by time range if specified\n    if tstart is not None:\n        spikes_df = spikes_df[spikes_df[\"timestamps\"] &gt; tstart]\n    if tstop is not None:\n        spikes_df = spikes_df[spikes_df[\"timestamps\"] &lt; tstop]\n\n    # Load and merge node population data if config is provided\n    if config:\n        nodes = load_nodes_from_config(config)\n        if network_name:\n            nodes = nodes.get(network_name, {})\n        else:\n            nodes = list(nodes.values())[0] if nodes else {}\n            print(\n                \"Grabbing first network; specify a network name to ensure correct node population is selected.\"\n            )\n\n        # Find common columns, but exclude the join key from the list\n        common_columns = spikes_df.columns.intersection(nodes.columns).tolist()\n        common_columns = [\n            col for col in common_columns if col != \"node_ids\"\n        ]  # Remove our join key from the common list\n\n        # Drop all intersecting columns except the join key column from df2\n        spikes_df = spikes_df.drop(columns=common_columns)\n        # merge nodes and spikes df\n        spikes_df = spikes_df.merge(\n            nodes[groupby], left_on=\"node_ids\", right_index=True, how=\"left\"\n        )\n\n    # Get unique population names\n    unique_pop_names = spikes_df[groupby].unique()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"tab10\")  # Default colormap\n        color_map = {\n            pop_name: cmap(i / len(unique_pop_names)) for i, pop_name in enumerate(unique_pop_names)\n        }\n    else:\n        # Ensure color_map contains all population names\n        missing_colors = [pop for pop in unique_pop_names if pop not in color_map]\n        if missing_colors:\n            raise ValueError(f\"color_map is missing colors for populations: {missing_colors}\")\n\n    # Plot each population with its specified or generated color\n    legend_handles = []\n    y_offset = 0  # Track y-position offset for stacking populations\n\n    for pop_name, group in spikes_df.groupby(groupby):\n        if sortby:\n            # Sort by the specified column, putting NaN values at the end\n            group_sorted = group.sort_values(by=sortby, na_position='last')\n            # Create a mapping from node_ids to consecutive y-positions based on sorted order\n            # Use the sorted order to maintain the same sequence for all spikes from same node\n            unique_nodes_sorted = group_sorted['node_ids'].drop_duplicates()\n            node_to_y = {node_id: y_offset + i for i, node_id in enumerate(unique_nodes_sorted)}\n            # Map node_ids to new y-positions for ALL spikes (not just the sorted group)\n            y_positions = group['node_ids'].map(node_to_y)\n            # Verify no data was lost\n            assert len(y_positions) == len(group), f\"Data loss detected in population {pop_name}\"\n            assert y_positions.isna().sum() == 0, f\"Unmapped node_ids found in population {pop_name}\"\n        else:\n            y_positions = group['node_ids']\n\n        ax.scatter(group[\"timestamps\"], y_positions, color=color_map[pop_name], s=dot_size)\n        # Dummy scatter for consistent legend appearance\n        handle = ax.scatter([], [], color=color_map[pop_name], label=pop_name, s=20)\n        legend_handles.append(handle)\n\n        # Update y_offset for next population if sortby is used\n        if sortby:\n            y_offset += len(unique_nodes_sorted)\n\n    # Label axes\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Node ID\")\n    ax.legend(handles=legend_handles, title=\"Population\", loc=\"upper right\", framealpha=0.9)\n\n    return ax\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.spikes.plot_firing_rate_pop_stats","title":"<code>bmtool.bmplot.spikes.plot_firing_rate_pop_stats(firing_stats, groupby, ax=None, color_map=None)</code>","text":"<p>Plots a bar graph of mean firing rates with error bars (standard deviation).</p> Parameters: <p>firing_stats : pd.DataFrame     Dataframe containing 'firing_rate_mean' and 'firing_rate_std'. groupby : str or list of str     Column(s) used for grouping. ax : matplotlib.axes.Axes, optional     Axes on which to plot the bar chart; if None, a new figure and axes are created. color_map : dict, optional     Dictionary specifying colors for each group. Keys should be group names, and values should be color values.</p> Returns: <p>matplotlib.axes.Axes     Axes with the bar plot.</p> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def plot_firing_rate_pop_stats(\n    firing_stats: pd.DataFrame,\n    groupby: Union[str, List[str]],\n    ax: Optional[Axes] = None,\n    color_map: Optional[Dict[str, str]] = None,\n) -&gt; Axes:\n    \"\"\"\n    Plots a bar graph of mean firing rates with error bars (standard deviation).\n\n    Parameters:\n    ----------\n    firing_stats : pd.DataFrame\n        Dataframe containing 'firing_rate_mean' and 'firing_rate_std'.\n    groupby : str or list of str\n        Column(s) used for grouping.\n    ax : matplotlib.axes.Axes, optional\n        Axes on which to plot the bar chart; if None, a new figure and axes are created.\n    color_map : dict, optional\n        Dictionary specifying colors for each group. Keys should be group names, and values should be color values.\n\n    Returns:\n    -------\n    matplotlib.axes.Axes\n        Axes with the bar plot.\n    \"\"\"\n    # Ensure groupby is a list for consistent handling\n    sns.set_style(\"whitegrid\")\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    # Create a categorical column for grouping\n    firing_stats[\"group\"] = firing_stats[groupby].astype(str).agg(\"_\".join, axis=1)\n\n    # Get unique group names\n    unique_groups = firing_stats[\"group\"].unique()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"viridis\")\n        color_map = {group: cmap(i / len(unique_groups)) for i, group in enumerate(unique_groups)}\n    else:\n        # Ensure color_map contains all groups\n        missing_colors = [group for group in unique_groups if group not in color_map]\n        if missing_colors:\n            raise ValueError(f\"color_map is missing colors for groups: {missing_colors}\")\n\n    # Create new figure and axes if ax is not provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Sort data for consistent plotting\n    firing_stats = firing_stats.sort_values(by=\"group\")\n\n    # Extract values for plotting\n    x_labels = firing_stats[\"group\"]\n    means = firing_stats[\"firing_rate_mean\"]\n    std_devs = firing_stats[\"firing_rate_std\"]\n\n    # Get colors for each group\n    colors = [color_map[group] for group in x_labels]\n\n    # Create bar plot\n    bars = ax.bar(x_labels, means, yerr=std_devs, capsize=5, color=colors, edgecolor=\"black\")\n\n    # Add error bars manually with caps\n    _, caps, _ = ax.errorbar(\n        x=np.arange(len(x_labels)),\n        y=means,\n        yerr=std_devs,\n        fmt=\"none\",\n        capsize=5,\n        capthick=2,\n        color=\"black\",\n    )\n\n    # Formatting\n    ax.set_xticks(np.arange(len(x_labels)))\n    ax.set_xticklabels(x_labels, rotation=45, ha=\"right\")\n    ax.set_xlabel(\"Population Group\")\n    ax.set_ylabel(\"Mean Firing Rate (spikes/s)\")\n    ax.set_title(\"Firing Rate Statistics by Population\")\n    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n    return ax\n</code></pre>"},{"location":"api/bmplot/#bmtool.bmplot.spikes.plot_firing_rate_distribution","title":"<code>bmtool.bmplot.spikes.plot_firing_rate_distribution(individual_stats, groupby, ax=None, color_map=None, plot_type='box', swarm_alpha=0.6, logscale=False)</code>","text":"<p>Plots a distribution of individual firing rates using one or more plot types (box plot, violin plot, or swarm plot), overlaying them on top of each other.</p> Parameters: <p>individual_stats : pd.DataFrame     Dataframe containing individual firing rates and corresponding group labels. groupby : str or list of str     Column(s) used for grouping. ax : matplotlib.axes.Axes, optional     Axes on which to plot the graph; if None, a new figure and axes are created. color_map : dict, optional     Dictionary specifying colors for each group. Keys should be group names, and values should be color values. plot_type : str or list of str, optional     List of plot types to generate. Options: \"box\", \"violin\", \"swarm\". Default is \"box\". swarm_alpha : float, optional     Transparency of swarm plot points. Default is 0.6. logscale : bool, optional     If True, use logarithmic scale for the y-axis (default is False).</p> Returns: <p>matplotlib.axes.Axes     Axes with the selected plot type(s) overlayed.</p> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def plot_firing_rate_distribution(\n    individual_stats: pd.DataFrame,\n    groupby: Union[str, List[str]],\n    ax: Optional[Axes] = None,\n    color_map: Optional[Dict[str, str]] = None,\n    plot_type: Union[str, List[str]] = \"box\",\n    swarm_alpha: float = 0.6,\n    logscale: bool = False,\n) -&gt; Axes:\n    \"\"\"\n    Plots a distribution of individual firing rates using one or more plot types\n    (box plot, violin plot, or swarm plot), overlaying them on top of each other.\n\n    Parameters:\n    ----------\n    individual_stats : pd.DataFrame\n        Dataframe containing individual firing rates and corresponding group labels.\n    groupby : str or list of str\n        Column(s) used for grouping.\n    ax : matplotlib.axes.Axes, optional\n        Axes on which to plot the graph; if None, a new figure and axes are created.\n    color_map : dict, optional\n        Dictionary specifying colors for each group. Keys should be group names, and values should be color values.\n    plot_type : str or list of str, optional\n        List of plot types to generate. Options: \"box\", \"violin\", \"swarm\". Default is \"box\".\n    swarm_alpha : float, optional\n        Transparency of swarm plot points. Default is 0.6.\n    logscale : bool, optional\n        If True, use logarithmic scale for the y-axis (default is False).\n\n    Returns:\n    -------\n    matplotlib.axes.Axes\n        Axes with the selected plot type(s) overlayed.\n    \"\"\"\n    sns.set_style(\"whitegrid\")\n    # Ensure groupby is a list for consistent handling\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    # Create a categorical column for grouping\n    individual_stats[\"group\"] = individual_stats[groupby].astype(str).agg(\"_\".join, axis=1)\n\n    # Validate plot_type (it can be a list or a single type)\n    if isinstance(plot_type, str):\n        plot_type = [plot_type]\n\n    for pt in plot_type:\n        if pt not in [\"box\", \"violin\", \"swarm\"]:\n            raise ValueError(\"plot_type must be one of: 'box', 'violin', 'swarm'.\")\n\n    # Get unique groups for coloring\n    unique_groups = individual_stats[\"group\"].unique()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"viridis\")\n        color_map = {group: cmap(i / len(unique_groups)) for i, group in enumerate(unique_groups)}\n\n    # Ensure color_map contains all groups\n    missing_colors = [group for group in unique_groups if group not in color_map]\n    if missing_colors:\n        raise ValueError(f\"color_map is missing colors for groups: {missing_colors}\")\n\n    # Create new figure and axes if ax is not provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Sort data for consistent plotting\n    individual_stats = individual_stats.sort_values(by=\"group\")\n\n    # Loop over each plot type and overlay them\n    for pt in plot_type:\n        if pt == \"box\":\n            sns.boxplot(\n                data=individual_stats,\n                x=\"group\",\n                y=\"firing_rate\",\n                ax=ax,\n                palette=color_map,\n                width=0.5,\n            )\n        elif pt == \"violin\":\n            sns.violinplot(\n                data=individual_stats,\n                x=\"group\",\n                y=\"firing_rate\",\n                ax=ax,\n                palette=color_map,\n                inner=\"box\",\n                alpha=0.4,\n                cut=0,  # This prevents the KDE from extending beyond the data range\n            )\n        elif pt == \"swarm\":\n            sns.swarmplot(\n                data=individual_stats,\n                x=\"group\",\n                y=\"firing_rate\",\n                ax=ax,\n                palette=color_map,\n                alpha=swarm_alpha,\n            )\n\n    # Formatting\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_xlabel(\"Population Group\")\n    ax.set_ylabel(\"Firing Rate (spikes/s)\")\n    ax.set_title(\"Firing Rate Distribution for individual cells\")\n    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n    if logscale:\n        ax.set_yscale('log')\n\n    return ax\n</code></pre>"},{"location":"api/bmplot/#entrainment-module","title":"Entrainment Module","text":""},{"location":"api/bmplot/#bmtool.bmplot.entrainment.plot_spike_power_correlation","title":"<code>bmtool.bmplot.entrainment.plot_spike_power_correlation(spike_df, lfp_data, fs, pop_names, filter_method='wavelet', bandwidth=2.0, lowcut=None, highcut=None, freq_range=(10, 100), freq_step=5, type_name='raw', figsize=(12, 8))</code>","text":"<p>Calculate and plot spike rate-LFP power correlation across frequencies for full signal.</p> <p>Analyzes the relationship between population spike rates and LFP power across a range of frequencies, using Spearman correlation for the entire signal duration.</p> <p>Parameters:</p> Name Type Description Default <code>spike_df</code> <code>DataFrame</code> <p>DataFrame containing spike data with columns 'timestamps', 'node_ids', and 'pop_name'.</p> required <code>lfp_data</code> <code>DataArray</code> <p>LFP data with time dimension.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> required <code>pop_names</code> <code>List[str]</code> <p>List of population names to analyze.</p> required <code>filter_method</code> <code>str</code> <p>Filtering method: 'wavelet' or 'butter' (default: 'wavelet').</p> <code>'wavelet'</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter (default: 2.0).</p> <code>2.0</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth filter. Required if filter_method='butter'.</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth filter. Required if filter_method='butter'.</p> <code>None</code> <code>freq_range</code> <code>Tuple[float, float]</code> <p>Min and max frequency to analyze in Hz (default: (10, 100)).</p> <code>(10, 100)</code> <code>freq_step</code> <code>float</code> <p>Step size for frequency analysis in Hz (default: 5).</p> <code>5</code> <code>type_name</code> <code>str</code> <p>Which type of spike rate to use (default: 'raw').</p> <code>'raw'</code> <code>figsize</code> <code>Tuple[float, float]</code> <p>Figure size (width, height) in inches (default: (12, 8)).</p> <code>(12, 8)</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Figure containing the correlation plot.</p> Notes <ul> <li>Uses Spearman correlation (rank-based, robust to outliers).</li> <li>Pre-computes LFP power at all frequencies for efficiency.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig = plot_spike_power_correlation(\n...     spike_df=spike_df,\n...     lfp_data=lfp,\n...     fs=400,\n...     pop_names=['PV', 'SST'],\n...     freq_range=(10, 100),\n...     freq_step=5\n... )\n</code></pre> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def plot_spike_power_correlation(\n    spike_df: pd.DataFrame,\n    lfp_data: xr.DataArray,\n    fs: float,\n    pop_names: List[str],\n    filter_method: str = \"wavelet\",\n    bandwidth: float = 2.0,\n    lowcut: Optional[float] = None,\n    highcut: Optional[float] = None,\n    freq_range: Tuple[float, float] = (10, 100),\n    freq_step: float = 5,\n    type_name: str = \"raw\",\n    figsize: Tuple[float, float] = (12, 8),\n) -&gt; Figure:\n    \"\"\"\n    Calculate and plot spike rate-LFP power correlation across frequencies for full signal.\n\n    Analyzes the relationship between population spike rates and LFP power across a range\n    of frequencies, using Spearman correlation for the entire signal duration.\n\n    Parameters\n    ----------\n    spike_df : pd.DataFrame\n        DataFrame containing spike data with columns 'timestamps', 'node_ids', and 'pop_name'.\n    lfp_data : xr.DataArray\n        LFP data with time dimension.\n    fs : float\n        Sampling frequency in Hz.\n    pop_names : List[str]\n        List of population names to analyze.\n    filter_method : str, optional\n        Filtering method: 'wavelet' or 'butter' (default: 'wavelet').\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter (default: 2.0).\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth filter. Required if filter_method='butter'.\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth filter. Required if filter_method='butter'.\n    freq_range : Tuple[float, float], optional\n        Min and max frequency to analyze in Hz (default: (10, 100)).\n    freq_step : float, optional\n        Step size for frequency analysis in Hz (default: 5).\n    type_name : str, optional\n        Which type of spike rate to use (default: 'raw').\n    figsize : Tuple[float, float], optional\n        Figure size (width, height) in inches (default: (12, 8)).\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure containing the correlation plot.\n\n    Notes\n    -----\n    - Uses Spearman correlation (rank-based, robust to outliers).\n    - Pre-computes LFP power at all frequencies for efficiency.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = plot_spike_power_correlation(\n    ...     spike_df=spike_df,\n    ...     lfp_data=lfp,\n    ...     fs=400,\n    ...     pop_names=['PV', 'SST'],\n    ...     freq_range=(10, 100),\n    ...     freq_step=5\n    ... )\n    \"\"\"\n    # Compute spike rate for all spikes\n    spike_rate = bmspikes.get_population_spike_rate(spike_df, fs=fs)\n\n    # Setup frequencies for analysis\n    frequencies = np.arange(freq_range[0], freq_range[1] + 1, freq_step)\n\n    # Pre-calculate LFP power for all frequencies\n    power_by_freq = {}\n    for freq in frequencies:\n        power_by_freq[freq] = get_lfp_power(\n            lfp_data, freq, fs, filter_method, lowcut=lowcut, highcut=highcut, bandwidth=bandwidth\n        )\n\n    # Calculate correlations for each population and frequency\n    results = {}\n    for pop in pop_names:\n        results[pop] = {}\n        pop_spike_rate = spike_rate.sel(population=pop, type=type_name)\n\n        for freq in frequencies:\n            lfp_power = power_by_freq[freq]\n\n            if len(pop_spike_rate) != len(lfp_power):\n                print(f\"Warning: Length mismatch for {pop} at {freq} Hz\")\n                print(f\"{len(pop_spike_rate)} {len(lfp_power)}\")\n                continue\n\n            corr, p_val = stats.spearmanr(pop_spike_rate.values, lfp_power.values)\n            results[pop][freq] = {\"correlation\": corr, \"p_value\": p_val}\n\n    # Create plot\n    sns.set_style(\"whitegrid\")\n    fig = plt.figure(figsize=figsize)\n\n    colors = plt.get_cmap(\"tab10\")\n    for i, pop in enumerate(pop_names):\n        plot_freqs = []\n        plot_corrs = []\n\n        for freq in frequencies:\n            if freq in results[pop] and not np.isnan(results[pop][freq][\"correlation\"]):\n                plot_freqs.append(freq)\n                plot_corrs.append(results[pop][freq][\"correlation\"])\n\n        if len(plot_freqs) == 0:\n            continue\n\n        plot_freqs = np.array(plot_freqs)\n        plot_corrs = np.array(plot_corrs)\n        color = colors(i)\n\n        plt.plot(\n            plot_freqs, plot_corrs, marker=\"o\", label=pop, linewidth=2, markersize=6, color=color\n        )\n\n    # Formatting\n    plt.xlabel(\"Frequency (Hz)\", fontsize=12)\n    plt.ylabel(\"Spike Rate-Power Correlation\", fontsize=12)\n\n    plt.title(\n        \"Spike Rate-LFP Power Correlation\",\n        fontsize=14,\n    )\n    plt.grid(True, alpha=0.3)\n    plt.axhline(y=0, color=\"gray\", linestyle=\"-\", alpha=0.5)\n\n    # Setup legend\n    from matplotlib.lines import Line2D\n\n    legend_elements = [\n        Line2D([0], [0], color=colors(i), marker=\"o\", linestyle=\"-\", label=pop)\n        for i, pop in enumerate(pop_names)\n    ]\n    plt.legend(handles=legend_elements, fontsize=10, loc=\"best\")\n\n    # Axis formatting\n    if len(frequencies) &gt; 10:\n        plt.xticks(frequencies[::2])\n    else:\n        plt.xticks(frequencies)\n    plt.xlim(frequencies[0], frequencies[-1])\n\n    y_min, y_max = plt.ylim()\n    plt.ylim(min(y_min, -0.1), max(y_max, 0.1))\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/#lfp-module","title":"LFP Module","text":""},{"location":"api/bmplot/#bmtool.bmplot.lfp.plot_spectrogram","title":"<code>bmtool.bmplot.lfp.plot_spectrogram(sxx_xarray, remove_aperiodic=None, log_power=False, plt_range=None, clr_freq_range=None, pad=0.03, ax=None, vmin=None, vmax=None)</code>","text":"<p>Plot a power spectrogram with optional aperiodic removal and frequency-based coloring.</p> <p>Parameters:</p> Name Type Description Default <code>sxx_xarray</code> <code>array - like</code> <p>Spectrogram data as an xarray DataArray with PSD values.</p> required <code>remove_aperiodic</code> <code>optional</code> <p>FOOOF model object for aperiodic subtraction. If None, raw spectrum is displayed.</p> <code>None</code> <code>log_power</code> <code>bool or str</code> <p>If True or 'dB', convert power to log scale. Default is False.</p> <code>False</code> <code>plt_range</code> <code>tuple of float</code> <p>Frequency range to display as (f_min, f_max). If None, displays full range.</p> <code>None</code> <code>clr_freq_range</code> <code>tuple of float</code> <p>Frequency range to use for determining color limits. If None, uses full range.</p> <code>None</code> <code>pad</code> <code>float</code> <p>Padding for colorbar. Default is 0.03.</p> <code>0.03</code> <code>ax</code> <code>Axes</code> <p>Axes to plot on. If None, creates a new figure and axes.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>Minimum value for colorbar scaling. If None, computed from data.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>Maximum value for colorbar scaling. If None, computed from data.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The figure object containing the spectrogram.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig = plot_spectrogram(\n...     sxx_xarray, log_power='dB',\n...     plt_range=(10, 100), clr_freq_range=(20, 50)\n... )\n</code></pre> Source code in <code>bmtool/bmplot/lfp.py</code> <pre><code>def plot_spectrogram(\n    sxx_xarray: Any,\n    remove_aperiodic: Optional[Any] = None,\n    log_power: bool = False,\n    plt_range: Optional[Tuple[float, float]] = None,\n    clr_freq_range: Optional[Tuple[float, float]] = None,\n    pad: float = 0.03,\n    ax: Optional[plt.Axes] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n) -&gt; Figure:\n    \"\"\"\n    Plot a power spectrogram with optional aperiodic removal and frequency-based coloring.\n\n    Parameters\n    ----------\n    sxx_xarray : array-like\n        Spectrogram data as an xarray DataArray with PSD values.\n    remove_aperiodic : optional\n        FOOOF model object for aperiodic subtraction. If None, raw spectrum is displayed.\n    log_power : bool or str, optional\n        If True or 'dB', convert power to log scale. Default is False.\n    plt_range : tuple of float, optional\n        Frequency range to display as (f_min, f_max). If None, displays full range.\n    clr_freq_range : tuple of float, optional\n        Frequency range to use for determining color limits. If None, uses full range.\n    pad : float, optional\n        Padding for colorbar. Default is 0.03.\n    ax : matplotlib.axes.Axes, optional\n        Axes to plot on. If None, creates a new figure and axes.\n    vmin : float, optional\n        Minimum value for colorbar scaling. If None, computed from data.\n    vmax : float, optional\n        Maximum value for colorbar scaling. If None, computed from data.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The figure object containing the spectrogram.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = plot_spectrogram(\n    ...     sxx_xarray, log_power='dB',\n    ...     plt_range=(10, 100), clr_freq_range=(20, 50)\n    ... )\n    \"\"\"\n    sxx = sxx_xarray.PSD.values.copy()\n    t = sxx_xarray.time.values.copy()\n    f = sxx_xarray.frequency.values.copy()\n\n    cbar_label = \"PSD\" if remove_aperiodic is None else \"PSD Residual\"\n    if log_power:\n        with np.errstate(divide=\"ignore\"):\n            sxx = np.log10(sxx)\n        cbar_label += \" dB\" if log_power == \"dB\" else \" log(power)\"\n\n    if remove_aperiodic is not None:\n        f1_idx = 0 if f[0] else 1\n        ap_fit = gen_aperiodic(f[f1_idx:], remove_aperiodic.aperiodic_params)\n        sxx[f1_idx:, :] -= (ap_fit if log_power else 10**ap_fit)[:, None]\n        sxx[:f1_idx, :] = 0.0\n\n    if log_power == \"dB\":\n        sxx *= 10\n\n    if ax is None:\n        _, ax = plt.subplots(1, 1)\n    plt_range = np.array(f[-1]) if plt_range is None else np.array(plt_range)\n    if plt_range.size == 1:\n        plt_range = [f[0 if f[0] else 1] if log_power else 0.0, plt_range.item()]\n    f_idx = (f &gt;= plt_range[0]) &amp; (f &lt;= plt_range[1])\n\n    # Determine vmin and vmax: explicit parameters take precedence, then clr_freq_range, then None\n    if vmin is None:\n        if clr_freq_range is not None:\n            c_idx = (f &gt;= clr_freq_range[0]) &amp; (f &lt;= clr_freq_range[1])\n            vmin = sxx[c_idx, :].min()\n\n    if vmax is None:\n        if clr_freq_range is not None:\n            c_idx = (f &gt;= clr_freq_range[0]) &amp; (f &lt;= clr_freq_range[1])\n            vmax = sxx[c_idx, :].max()\n\n    f = f[f_idx]\n    pcm = ax.pcolormesh(t, f, sxx[f_idx, :], shading=\"gouraud\", vmin=vmin, vmax=vmax, rasterized=True,cmap='viridis')\n    if \"cone_of_influence_frequency\" in sxx_xarray:\n        coif = sxx_xarray.cone_of_influence_frequency\n        ax.plot(t, coif)\n        ax.fill_between(t, coif, step=\"mid\", alpha=0.2)\n    ax.set_xlim(t[0], t[-1])\n    # ax.set_xlim(t[0],0.2)\n    ax.set_ylim(f[0], f[-1])\n    plt.colorbar(mappable=pcm, ax=ax, label=cbar_label, pad=pad)\n    ax.set_xlabel(\"Time (sec)\")\n    ax.set_ylabel(\"Frequency (Hz)\")\n    return ax.figure\n</code></pre>"},{"location":"api/connectors/","title":"Connectors API Reference","text":"<p>This page provides API reference documentation for the Connectors module, which contains classes and functions for creating complex connectivity patterns in BMTK networks.</p>"},{"location":"api/connectors/#utility-functions","title":"Utility Functions","text":""},{"location":"api/connectors/#bmtool.connectors.num_prop","title":"<code>bmtool.connectors.num_prop(ratio, N)</code>","text":"<p>Calculate numbers of total N in proportion to ratio.</p> Parameters: <p>ratio : array-like     Proportions to distribute N across. N : int     Total number to distribute.</p> Returns: <p>numpy.ndarray     Array of integers that sum to N, proportionally distributed according to ratio.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def num_prop(ratio, N):\n    \"\"\"\n    Calculate numbers of total N in proportion to ratio.\n\n    Parameters:\n    -----------\n    ratio : array-like\n        Proportions to distribute N across.\n    N : int\n        Total number to distribute.\n\n    Returns:\n    --------\n    numpy.ndarray\n        Array of integers that sum to N, proportionally distributed according to ratio.\n    \"\"\"\n    ratio = np.asarray(ratio)\n    p = np.cumsum(np.insert(ratio.ravel(), 0, 0))  # cumulative proportion\n    return np.diff(np.round(N / p[-1] * p).astype(int)).reshape(ratio.shape)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.decision","title":"<code>bmtool.connectors.decision(prob, size=None)</code>","text":"<p>Make random decision(s) based on input probability.</p> Parameters: <p>prob : float     Probability threshold between 0 and 1. size : int or tuple, optional     Size of the output array. If None, a single decision is returned.</p> Returns: <p>bool or numpy.ndarray     Boolean result(s) of the random decision(s). True if the random number     is less than prob, False otherwise.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def decision(prob, size=None):\n    \"\"\"\n    Make random decision(s) based on input probability.\n\n    Parameters:\n    -----------\n    prob : float\n        Probability threshold between 0 and 1.\n    size : int or tuple, optional\n        Size of the output array. If None, a single decision is returned.\n\n    Returns:\n    --------\n    bool or numpy.ndarray\n        Boolean result(s) of the random decision(s). True if the random number\n        is less than prob, False otherwise.\n    \"\"\"\n    return rng.random(size) &lt; prob\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.decisions","title":"<code>bmtool.connectors.decisions(prob)</code>","text":"<p>Make multiple random decisions based on input probabilities.</p> Parameters: <p>prob : array-like     Array of probability thresholds between 0 and 1.</p> Returns: <p>numpy.ndarray     Boolean array with the same shape as prob, containing results of     the random decisions.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def decisions(prob):\n    \"\"\"\n    Make multiple random decisions based on input probabilities.\n\n    Parameters:\n    -----------\n    prob : array-like\n        Array of probability thresholds between 0 and 1.\n\n    Returns:\n    --------\n    numpy.ndarray\n        Boolean array with the same shape as prob, containing results of\n        the random decisions.\n    \"\"\"\n    prob = np.asarray(prob)\n    return rng.random(prob.shape) &lt; prob\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.euclid_dist","title":"<code>bmtool.connectors.euclid_dist(p1, p2)</code>","text":"<p>Euclidean distance between two points p1, p2: Coordinates in numpy array</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def euclid_dist(p1, p2):\n    \"\"\"\n    Euclidean distance between two points\n    p1, p2: Coordinates in numpy array\n    \"\"\"\n    dvec = np.asarray(p1) - np.asarray(p2)\n    return (dvec @ dvec) ** 0.5\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.spherical_dist","title":"<code>bmtool.connectors.spherical_dist(node1, node2)</code>","text":"<p>Spherical distance between two input nodes</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def spherical_dist(node1, node2):\n    \"\"\"Spherical distance between two input nodes\"\"\"\n    return euclid_dist(node1[\"positions\"], node2[\"positions\"]).item()\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.cylindrical_dist_z","title":"<code>bmtool.connectors.cylindrical_dist_z(node1, node2)</code>","text":"<p>Cylindircal distance between two input nodes (ignoring z-axis)</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def cylindrical_dist_z(node1, node2):\n    \"\"\"Cylindircal distance between two input nodes (ignoring z-axis)\"\"\"\n    return euclid_dist(node1[\"positions\"][:2], node2[\"positions\"][:2]).item()\n</code></pre>"},{"location":"api/connectors/#probability-functions","title":"Probability Functions","text":""},{"location":"api/connectors/#bmtool.connectors.ProbabilityFunction","title":"<code>bmtool.connectors.ProbabilityFunction</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for connection probability function</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class ProbabilityFunction(ABC):\n    \"\"\"Abstract base class for connection probability function\"\"\"\n\n    @abstractmethod\n    def probability(self, *arg, **kwargs):\n        \"\"\"Allow numpy array input and return probability in numpy array\"\"\"\n        return NotImplemented\n\n    @abstractmethod\n    def __call__(self, *arg, **kwargs):\n        \"\"\"Return probability within [0, 1] for single input\"\"\"\n        return NotImplemented\n\n    @abstractmethod\n    def decisions(self, *arg, **kwargs):\n        \"\"\"Return bool array of decisions according probability\"\"\"\n        return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ProbabilityFunction.__call__","title":"<code>__call__(*arg, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Return probability within [0, 1] for single input</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@abstractmethod\ndef __call__(self, *arg, **kwargs):\n    \"\"\"Return probability within [0, 1] for single input\"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ProbabilityFunction.decisions","title":"<code>decisions(*arg, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Return bool array of decisions according probability</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@abstractmethod\ndef decisions(self, *arg, **kwargs):\n    \"\"\"Return bool array of decisions according probability\"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ProbabilityFunction.probability","title":"<code>probability(*arg, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Allow numpy array input and return probability in numpy array</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@abstractmethod\ndef probability(self, *arg, **kwargs):\n    \"\"\"Allow numpy array input and return probability in numpy array\"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.DistantDependentProbability","title":"<code>bmtool.connectors.DistantDependentProbability</code>","text":"<p>               Bases: <code>ProbabilityFunction</code></p> <p>Base class for distance dependent probability</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class DistantDependentProbability(ProbabilityFunction):\n    \"\"\"Base class for distance dependent probability\"\"\"\n\n    def __init__(self, min_dist=0.0, max_dist=np.inf):\n        assert min_dist &gt;= 0 and min_dist &lt; max_dist\n        self.min_dist, self.max_dist = min_dist, max_dist\n\n    def __call__(self, dist, *arg, **kwargs):\n        \"\"\"Return probability for single distance input\"\"\"\n        if dist &gt;= self.min_dist and dist &lt;= self.max_dist:\n            return self.probability(dist)\n        else:\n            return 0.0\n\n    def decisions(self, dist):\n        \"\"\"Return bool array of decisions given distance array\"\"\"\n        dist = np.asarray(dist)\n        dec = np.zeros(dist.shape, dtype=bool)\n        mask = (dist &gt;= self.min_dist) &amp; (dist &lt;= self.max_dist)\n        dist = dist[mask]\n        prob = np.empty(dist.shape)\n        prob[:] = self.probability(dist)\n        dec[mask] = decisions(prob)\n        return dec\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.DistantDependentProbability.__call__","title":"<code>__call__(dist, *arg, **kwargs)</code>","text":"<p>Return probability for single distance input</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def __call__(self, dist, *arg, **kwargs):\n    \"\"\"Return probability for single distance input\"\"\"\n    if dist &gt;= self.min_dist and dist &lt;= self.max_dist:\n        return self.probability(dist)\n    else:\n        return 0.0\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.DistantDependentProbability.decisions","title":"<code>decisions(dist)</code>","text":"<p>Return bool array of decisions given distance array</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def decisions(self, dist):\n    \"\"\"Return bool array of decisions given distance array\"\"\"\n    dist = np.asarray(dist)\n    dec = np.zeros(dist.shape, dtype=bool)\n    mask = (dist &gt;= self.min_dist) &amp; (dist &lt;= self.max_dist)\n    dist = dist[mask]\n    prob = np.empty(dist.shape)\n    prob[:] = self.probability(dist)\n    dec[mask] = decisions(prob)\n    return dec\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UniformInRange","title":"<code>bmtool.connectors.UniformInRange</code>","text":"<p>               Bases: <code>DistantDependentProbability</code></p> <p>Constant probability within a distance range</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class UniformInRange(DistantDependentProbability):\n    \"\"\"Constant probability within a distance range\"\"\"\n\n    def __init__(self, p=0.0, min_dist=0.0, max_dist=np.inf):\n        super().__init__(min_dist=min_dist, max_dist=max_dist)\n        self.p = np.array(p)\n        assert self.p.size == 1\n        assert p &gt;= 0.0 and p &lt;= 1.0\n\n    def probability(self, dist):\n        return self.p\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.gaussian","title":"<code>bmtool.connectors.gaussian(x, mean=0.0, stdev=1.0, pmax=NORM_COEF)</code>","text":"<p>Gaussian function. Default is the PDF of standard normal distribution</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def gaussian(x, mean=0.0, stdev=1.0, pmax=NORM_COEF):\n    \"\"\"Gaussian function. Default is the PDF of standard normal distribution\"\"\"\n    x = (x - mean) / stdev\n    return pmax * np.exp(-x * x / 2)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GaussianDropoff","title":"<code>bmtool.connectors.GaussianDropoff</code>","text":"<p>               Bases: <code>DistantDependentProbability</code></p> <p>Connection probability class that follows a Gaussian function of distance.</p> <p>This class calculates connection probabilities using a Gaussian function of the distance between cells, with options for spherical or cylindrical metrics.</p> Parameters: <p>mean : float, optional     Mean parameter of the Gaussian function, typically 0 for peak at origin. stdev : float, optional     Standard deviation parameter controlling the width of the Gaussian. min_dist : float, optional     Minimum distance for connections. Below this distance, probability is zero. max_dist : float, optional     Maximum distance for connections. Above this distance, probability is zero. pmax : float, optional     Maximum probability value at the peak of the Gaussian function. ptotal : float, optional     Overall connection probability within the specified distance range.     If provided, pmax is calculated to achieve this overall probability. ptotal_dist_range : tuple, optional     Distance range (min_dist, max_dist) for calculating pmax when ptotal is provided. dist_type : str, optional     Distance metric to use, either 'spherical' (default) or 'cylindrical'.</p> Notes: <p>When ptotal is specified, the maximum probability (pmax) is calculated to achieve the desired overall connection probability within the specified distance range, assuming homogeneous cell density.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class GaussianDropoff(DistantDependentProbability):\n    \"\"\"\n    Connection probability class that follows a Gaussian function of distance.\n\n    This class calculates connection probabilities using a Gaussian function\n    of the distance between cells, with options for spherical or cylindrical metrics.\n\n    Parameters:\n    -----------\n    mean : float, optional\n        Mean parameter of the Gaussian function, typically 0 for peak at origin.\n    stdev : float, optional\n        Standard deviation parameter controlling the width of the Gaussian.\n    min_dist : float, optional\n        Minimum distance for connections. Below this distance, probability is zero.\n    max_dist : float, optional\n        Maximum distance for connections. Above this distance, probability is zero.\n    pmax : float, optional\n        Maximum probability value at the peak of the Gaussian function.\n    ptotal : float, optional\n        Overall connection probability within the specified distance range.\n        If provided, pmax is calculated to achieve this overall probability.\n    ptotal_dist_range : tuple, optional\n        Distance range (min_dist, max_dist) for calculating pmax when ptotal is provided.\n    dist_type : str, optional\n        Distance metric to use, either 'spherical' (default) or 'cylindrical'.\n\n    Notes:\n    ------\n    When ptotal is specified, the maximum probability (pmax) is calculated to achieve\n    the desired overall connection probability within the specified distance range,\n    assuming homogeneous cell density.\n    \"\"\"\n\n    def __init__(\n        self,\n        mean=0.0,\n        stdev=1.0,\n        min_dist=0.0,\n        max_dist=np.inf,\n        pmax=1,\n        ptotal=None,\n        ptotal_dist_range=None,\n        dist_type=\"spherical\",\n    ):\n        super().__init__(min_dist=min_dist, max_dist=max_dist)\n        self.mean, self.stdev = mean, stdev\n        self.ptotal = ptotal\n        self.ptotal_dist_range = (\n            (min_dist, max_dist) if ptotal_dist_range is None else ptotal_dist_range\n        )\n        self.dist_type = dist_type if dist_type in [\"cylindrical\"] else \"spherical\"\n        self.pmax = pmax if ptotal is None else self.calc_pmax_from_ptotal()\n        self.set_probability_func()\n\n    def calc_pmax_from_ptotal(self):\n        \"\"\"\n        Calculate the pmax value such that the expected overall connection\n        probability to all possible targets within the distance range [r1, r2]=\n        `ptotal_dist_range` equals ptotal, assuming homogeneous cell density.\n        That is, integral_r1^r2 {g(r)p(r)dr} = ptotal, where g is the Gaussian\n        function with pmax, p(r) is the cell density per unit distance at r\n        normalized by total cell number within the distance range.\n        For cylindrical distance, p(r) = 2 * r / (r2^2 - r1^2)\n        For spherical distance, p(r) = 3 * r^2 / (r2^3 - r1^3)\n        The solution has a closed form except that te error function erf is in\n        the expression, but only when resulting pmax &lt;= 1.\n\n        Caveat: When the calculated pmax &gt; 1, the actual overall probability\n        will be lower than expected and all cells within certain distance will\n        be always connected. This usually happens when the distance range is\n        set too wide. Because a large population will be included for\n        evaluating ptotal, and there will be a significant drop in the Gaussian\n        function as distance gets further. So, a large pmax will be required to\n        achieve the desired ptotal.\n        \"\"\"\n        mu, sig = self.mean, self.stdev\n        r1, r2 = self.ptotal_dist_range[:2]\n        x1, x2 = (r1 - mu) / sig, (r2 - mu) / sig  # normalized distance\n        if self.dist_type == \"cylindrical\":\n            dr = r2**2 - r1**2\n\n            def F(x):\n                f1 = sig * mu / NORM_COEF * erf(x / 2**0.5)\n                f2 = -2 * sig * sig * gaussian(x, pmax=1.0)\n                return f1 + f2\n        else:\n            dr = r2**3 - r1**3\n\n            def F(x):\n                f1 = 1.5 * sig * (sig**2 + mu**2) / NORM_COEF * erf(x / 2**0.5)\n                f2 = -3 * sig * sig * (2 * mu + sig * x) * gaussian(x, pmax=1.0)\n                return f1 + f2\n\n        return self.ptotal * dr / (F(x2) - F(x1))\n\n    def probability(self):\n        pass  # to be set up in set_probability_func()\n\n    def compute_ptotal_integral(self, num_points=1000):\n        \"\"\"\n        Compute the total integrated probability by numerically integrating\n        the Gaussian probability function over the ptotal_dist_range.\n\n        This is useful when ptotal was not explicitly set and needs to be\n        calculated from the distribution parameters (pmax, stdev, etc.).\n\n        Parameters:\n        -----------\n        num_points : int, optional\n            Number of points to use for numerical integration (default: 1000).\n\n        Returns:\n        --------\n        float\n            The integrated total probability over the ptotal_dist_range.\n\n        Notes:\n        ------\n        For cylindrical distance: integrates p(r) * 2\u03c0r over the range,\n        normalized by the total cylindrical area.\n        For spherical distance: integrates p(r) * 4\u03c0r\u00b2 over the range,\n        normalized by the total spherical volume.\n        \"\"\"\n        from scipy.integrate import trapz\n\n        # Handle both range objects and tuples\n        if isinstance(self.ptotal_dist_range, range):\n            r1 = self.ptotal_dist_range.start if self.ptotal_dist_range.start is not None else 0\n            r2 = self.ptotal_dist_range.stop\n        else:\n            r1, r2 = self.ptotal_dist_range[0], self.ptotal_dist_range[1]\n\n        distances = np.linspace(r1, r2, num_points)\n        probabilities = self.probability(distances)\n\n        if self.dist_type == \"cylindrical\":\n            # For cylindrical: weight by 2\u03c0r (density at radius r in 2D)\n            weights = 2 * np.pi * distances\n            # Normalize by total cylindrical area in the range\n            total_area = np.pi * (r2**2 - r1**2)\n            integrand = probabilities * weights / total_area\n        else:  # spherical\n            # For spherical: weight by 4\u03c0r\u00b2 (density at radius r in 3D)\n            weights = 4 * np.pi * distances ** 2\n            # Normalize by total spherical volume in the range\n            total_volume = (4/3) * np.pi * (r2**3 - r1**3)\n            integrand = probabilities * weights / total_volume\n\n        ptotal = trapz(integrand, distances)\n        return ptotal\n\n    def set_probability_func(self):\n        \"\"\"Set up function for calculating probability\"\"\"\n        keys = [\"mean\", \"stdev\", \"pmax\"]\n        kwargs = {key: getattr(self, key) for key in keys}\n        probability = partial(gaussian, **kwargs)\n\n        # Verify maximum probability\n        # (is not self.pmax if self.mean outside distance range)\n        bounds = (self.min_dist, min(self.max_dist, 1e9))\n        pmax = (\n            self.pmax\n            if self.mean &gt;= bounds[0] and self.mean &lt;= bounds[1]\n            else probability(np.asarray(bounds)).max()\n        )\n        if pmax &gt; 1:\n            d = minimize_scalar(\n                lambda x: (probability(x) - 1) ** 2, method=\"bounded\", bounds=bounds\n            ).x\n            warn = (\n                \"\\nWarning: Maximum probability=%.3f is greater than 1. \"\n                \"Probability crosses 1 at distance %.3g.\\n\"\n            ) % (pmax, d)\n            if self.ptotal is not None:\n                warn += \" ptotal may not be reached.\"\n            print(warn, flush=True)\n            self.probability = lambda dist: np.fmin(probability(dist), 1.0)\n        else:\n            self.probability = probability\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GaussianDropoff.calc_pmax_from_ptotal","title":"<code>calc_pmax_from_ptotal()</code>","text":"<p>Calculate the pmax value such that the expected overall connection probability to all possible targets within the distance range [r1, r2]= <code>ptotal_dist_range</code> equals ptotal, assuming homogeneous cell density. That is, integral_r1^r2 {g(r)p(r)dr} = ptotal, where g is the Gaussian function with pmax, p(r) is the cell density per unit distance at r normalized by total cell number within the distance range. For cylindrical distance, p(r) = 2 * r / (r2^2 - r1^2) For spherical distance, p(r) = 3 * r^2 / (r2^3 - r1^3) The solution has a closed form except that te error function erf is in the expression, but only when resulting pmax &lt;= 1.</p> <p>Caveat: When the calculated pmax &gt; 1, the actual overall probability will be lower than expected and all cells within certain distance will be always connected. This usually happens when the distance range is set too wide. Because a large population will be included for evaluating ptotal, and there will be a significant drop in the Gaussian function as distance gets further. So, a large pmax will be required to achieve the desired ptotal.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def calc_pmax_from_ptotal(self):\n    \"\"\"\n    Calculate the pmax value such that the expected overall connection\n    probability to all possible targets within the distance range [r1, r2]=\n    `ptotal_dist_range` equals ptotal, assuming homogeneous cell density.\n    That is, integral_r1^r2 {g(r)p(r)dr} = ptotal, where g is the Gaussian\n    function with pmax, p(r) is the cell density per unit distance at r\n    normalized by total cell number within the distance range.\n    For cylindrical distance, p(r) = 2 * r / (r2^2 - r1^2)\n    For spherical distance, p(r) = 3 * r^2 / (r2^3 - r1^3)\n    The solution has a closed form except that te error function erf is in\n    the expression, but only when resulting pmax &lt;= 1.\n\n    Caveat: When the calculated pmax &gt; 1, the actual overall probability\n    will be lower than expected and all cells within certain distance will\n    be always connected. This usually happens when the distance range is\n    set too wide. Because a large population will be included for\n    evaluating ptotal, and there will be a significant drop in the Gaussian\n    function as distance gets further. So, a large pmax will be required to\n    achieve the desired ptotal.\n    \"\"\"\n    mu, sig = self.mean, self.stdev\n    r1, r2 = self.ptotal_dist_range[:2]\n    x1, x2 = (r1 - mu) / sig, (r2 - mu) / sig  # normalized distance\n    if self.dist_type == \"cylindrical\":\n        dr = r2**2 - r1**2\n\n        def F(x):\n            f1 = sig * mu / NORM_COEF * erf(x / 2**0.5)\n            f2 = -2 * sig * sig * gaussian(x, pmax=1.0)\n            return f1 + f2\n    else:\n        dr = r2**3 - r1**3\n\n        def F(x):\n            f1 = 1.5 * sig * (sig**2 + mu**2) / NORM_COEF * erf(x / 2**0.5)\n            f2 = -3 * sig * sig * (2 * mu + sig * x) * gaussian(x, pmax=1.0)\n            return f1 + f2\n\n    return self.ptotal * dr / (F(x2) - F(x1))\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GaussianDropoff.compute_ptotal_integral","title":"<code>compute_ptotal_integral(num_points=1000)</code>","text":"<p>Compute the total integrated probability by numerically integrating the Gaussian probability function over the ptotal_dist_range.</p> <p>This is useful when ptotal was not explicitly set and needs to be calculated from the distribution parameters (pmax, stdev, etc.).</p> Parameters: <p>num_points : int, optional     Number of points to use for numerical integration (default: 1000).</p> Returns: <p>float     The integrated total probability over the ptotal_dist_range.</p> Notes: <p>For cylindrical distance: integrates p(r) * 2\u03c0r over the range, normalized by the total cylindrical area. For spherical distance: integrates p(r) * 4\u03c0r\u00b2 over the range, normalized by the total spherical volume.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def compute_ptotal_integral(self, num_points=1000):\n    \"\"\"\n    Compute the total integrated probability by numerically integrating\n    the Gaussian probability function over the ptotal_dist_range.\n\n    This is useful when ptotal was not explicitly set and needs to be\n    calculated from the distribution parameters (pmax, stdev, etc.).\n\n    Parameters:\n    -----------\n    num_points : int, optional\n        Number of points to use for numerical integration (default: 1000).\n\n    Returns:\n    --------\n    float\n        The integrated total probability over the ptotal_dist_range.\n\n    Notes:\n    ------\n    For cylindrical distance: integrates p(r) * 2\u03c0r over the range,\n    normalized by the total cylindrical area.\n    For spherical distance: integrates p(r) * 4\u03c0r\u00b2 over the range,\n    normalized by the total spherical volume.\n    \"\"\"\n    from scipy.integrate import trapz\n\n    # Handle both range objects and tuples\n    if isinstance(self.ptotal_dist_range, range):\n        r1 = self.ptotal_dist_range.start if self.ptotal_dist_range.start is not None else 0\n        r2 = self.ptotal_dist_range.stop\n    else:\n        r1, r2 = self.ptotal_dist_range[0], self.ptotal_dist_range[1]\n\n    distances = np.linspace(r1, r2, num_points)\n    probabilities = self.probability(distances)\n\n    if self.dist_type == \"cylindrical\":\n        # For cylindrical: weight by 2\u03c0r (density at radius r in 2D)\n        weights = 2 * np.pi * distances\n        # Normalize by total cylindrical area in the range\n        total_area = np.pi * (r2**2 - r1**2)\n        integrand = probabilities * weights / total_area\n    else:  # spherical\n        # For spherical: weight by 4\u03c0r\u00b2 (density at radius r in 3D)\n        weights = 4 * np.pi * distances ** 2\n        # Normalize by total spherical volume in the range\n        total_volume = (4/3) * np.pi * (r2**3 - r1**3)\n        integrand = probabilities * weights / total_volume\n\n    ptotal = trapz(integrand, distances)\n    return ptotal\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GaussianDropoff.set_probability_func","title":"<code>set_probability_func()</code>","text":"<p>Set up function for calculating probability</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def set_probability_func(self):\n    \"\"\"Set up function for calculating probability\"\"\"\n    keys = [\"mean\", \"stdev\", \"pmax\"]\n    kwargs = {key: getattr(self, key) for key in keys}\n    probability = partial(gaussian, **kwargs)\n\n    # Verify maximum probability\n    # (is not self.pmax if self.mean outside distance range)\n    bounds = (self.min_dist, min(self.max_dist, 1e9))\n    pmax = (\n        self.pmax\n        if self.mean &gt;= bounds[0] and self.mean &lt;= bounds[1]\n        else probability(np.asarray(bounds)).max()\n    )\n    if pmax &gt; 1:\n        d = minimize_scalar(\n            lambda x: (probability(x) - 1) ** 2, method=\"bounded\", bounds=bounds\n        ).x\n        warn = (\n            \"\\nWarning: Maximum probability=%.3f is greater than 1. \"\n            \"Probability crosses 1 at distance %.3g.\\n\"\n        ) % (pmax, d)\n        if self.ptotal is not None:\n            warn += \" ptotal may not be reached.\"\n        print(warn, flush=True)\n        self.probability = lambda dist: np.fmin(probability(dist), 1.0)\n    else:\n        self.probability = probability\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.NormalizedReciprocalRate","title":"<code>bmtool.connectors.NormalizedReciprocalRate</code>","text":"<p>               Bases: <code>ProbabilityFunction</code></p> <p>Reciprocal connection probability given normalized reciprocal rate. Normalized reciprocal rate is defined as the ratio between the reciprocal connection probability and the connection probability for a randomly connected network where the two unidirectional connections between any pair of neurons are independent. NRR = pr / (p0 * p1)</p> <p>Parameters:     NRR: a constant or distance dependent function for normalized reciprocal         rate. When being a function, it should be accept vectorized input. Returns:     A callable object that returns the probability value.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class NormalizedReciprocalRate(ProbabilityFunction):\n    \"\"\"Reciprocal connection probability given normalized reciprocal rate.\n    Normalized reciprocal rate is defined as the ratio between the reciprocal\n    connection probability and the connection probability for a randomly\n    connected network where the two unidirectional connections between any pair\n    of neurons are independent. NRR = pr / (p0 * p1)\n\n    Parameters:\n        NRR: a constant or distance dependent function for normalized reciprocal\n            rate. When being a function, it should be accept vectorized input.\n    Returns:\n        A callable object that returns the probability value.\n    \"\"\"\n\n    def __init__(self, NRR=1.0):\n        self.NRR = NRR if callable(NRR) else lambda *x: NRR\n\n    def probability(self, dist, p0, p1):\n        \"\"\"Allow numpy array input and return probability in numpy array\"\"\"\n        return p0 * p1 * self.NRR(dist)\n\n    def __call__(self, dist, p0, p1, *arg, **kwargs):\n        \"\"\"Return probability for single distance input\"\"\"\n        return self.probability(dist, p0, p1)\n\n    def decisions(self, dist, p0, p1, cond=None):\n        \"\"\"Return bool array of decisions\n        dist: distance (scalar or array). Will be ignored if NRR is constant.\n        p0, p1: forward and backward probability (scalar or array)\n        cond: A tuple (direction, array of outcomes) representing the condition.\n            Conditional probability will be returned if specified. The condition\n            event is determined by connection direction (0 for forward, or 1 for\n            backward) and outcomes (bool array of whether connection exists).\n        \"\"\"\n        dist, p0, p1 = map(np.asarray, (dist, p0, p1))\n        pr = np.empty(dist.shape)\n        pr[:] = self.probability(dist, p0, p1)\n        pr = np.clip(pr, a_min=np.fmax(p0 + p1 - 1.0, 0.0), a_max=np.fmin(p0, p1))\n        if cond is not None:\n            mask = np.asarray(cond[1])\n            pr[mask] /= p1 if cond[0] else p0\n            pr[~mask] = 0.0\n        return decisions(pr)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.NormalizedReciprocalRate.__call__","title":"<code>__call__(dist, p0, p1, *arg, **kwargs)</code>","text":"<p>Return probability for single distance input</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def __call__(self, dist, p0, p1, *arg, **kwargs):\n    \"\"\"Return probability for single distance input\"\"\"\n    return self.probability(dist, p0, p1)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.NormalizedReciprocalRate.decisions","title":"<code>decisions(dist, p0, p1, cond=None)</code>","text":"<p>Return bool array of decisions dist: distance (scalar or array). Will be ignored if NRR is constant. p0, p1: forward and backward probability (scalar or array) cond: A tuple (direction, array of outcomes) representing the condition.     Conditional probability will be returned if specified. The condition     event is determined by connection direction (0 for forward, or 1 for     backward) and outcomes (bool array of whether connection exists).</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def decisions(self, dist, p0, p1, cond=None):\n    \"\"\"Return bool array of decisions\n    dist: distance (scalar or array). Will be ignored if NRR is constant.\n    p0, p1: forward and backward probability (scalar or array)\n    cond: A tuple (direction, array of outcomes) representing the condition.\n        Conditional probability will be returned if specified. The condition\n        event is determined by connection direction (0 for forward, or 1 for\n        backward) and outcomes (bool array of whether connection exists).\n    \"\"\"\n    dist, p0, p1 = map(np.asarray, (dist, p0, p1))\n    pr = np.empty(dist.shape)\n    pr[:] = self.probability(dist, p0, p1)\n    pr = np.clip(pr, a_min=np.fmax(p0 + p1 - 1.0, 0.0), a_max=np.fmin(p0, p1))\n    if cond is not None:\n        mask = np.asarray(cond[1])\n        pr[mask] /= p1 if cond[0] else p0\n        pr[~mask] = 0.0\n    return decisions(pr)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.NormalizedReciprocalRate.probability","title":"<code>probability(dist, p0, p1)</code>","text":"<p>Allow numpy array input and return probability in numpy array</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def probability(self, dist, p0, p1):\n    \"\"\"Allow numpy array input and return probability in numpy array\"\"\"\n    return p0 * p1 * self.NRR(dist)\n</code></pre>"},{"location":"api/connectors/#connector-base-classes","title":"Connector Base Classes","text":""},{"location":"api/connectors/#bmtool.connectors.AbstractConnector","title":"<code>bmtool.connectors.AbstractConnector</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for connectors</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class AbstractConnector(ABC):\n    \"\"\"Abstract base class for connectors\"\"\"\n\n    @abstractmethod\n    def setup_nodes(self, source=None, target=None):\n        \"\"\"After network nodes are added to the BMTK network. Pass in the\n        Nodepool objects of source and target nodes using this method.\n        Must run this before building connections.\"\"\"\n        return NotImplemented\n\n    @abstractmethod\n    def edge_params(self, **kwargs):\n        \"\"\"Create the arguments for BMTK add_edges() method including the\n        `connection_rule` method.\"\"\"\n        return NotImplemented\n\n    @staticmethod\n    def constant_function(val):\n        \"\"\"Convert a constant to a constant function\"\"\"\n\n        def constant(*arg):\n            return val\n\n        return constant\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.AbstractConnector.constant_function","title":"<code>constant_function(val)</code>  <code>staticmethod</code>","text":"<p>Convert a constant to a constant function</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@staticmethod\ndef constant_function(val):\n    \"\"\"Convert a constant to a constant function\"\"\"\n\n    def constant(*arg):\n        return val\n\n    return constant\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.AbstractConnector.edge_params","title":"<code>edge_params(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create the arguments for BMTK add_edges() method including the <code>connection_rule</code> method.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@abstractmethod\ndef edge_params(self, **kwargs):\n    \"\"\"Create the arguments for BMTK add_edges() method including the\n    `connection_rule` method.\"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.AbstractConnector.setup_nodes","title":"<code>setup_nodes(source=None, target=None)</code>  <code>abstractmethod</code>","text":"<p>After network nodes are added to the BMTK network. Pass in the Nodepool objects of source and target nodes using this method. Must run this before building connections.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>@abstractmethod\ndef setup_nodes(self, source=None, target=None):\n    \"\"\"After network nodes are added to the BMTK network. Pass in the\n    Nodepool objects of source and target nodes using this method.\n    Must run this before building connections.\"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.is_same_pop","title":"<code>bmtool.connectors.is_same_pop(source, target, quick=False)</code>","text":"<p>Check whether two NodePool objects direct to the same population</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def is_same_pop(source, target, quick=False):\n    \"\"\"Check whether two NodePool objects direct to the same population\"\"\"\n    if quick:\n        # Quick check (compare filter conditions)\n        same = (\n            source.network_name == target.network_name\n            and source._NodePool__properties == target._NodePool__properties\n        )\n    else:\n        # Strict check (compare all nodes)\n        same = (\n            source.network_name == target.network_name\n            and len(source) == len(target)\n            and all([s.node_id == t.node_id for s, t in zip(source, target)])\n        )\n    return same\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.Timer","title":"<code>bmtool.connectors.Timer</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>bmtool/connectors.py</code> <pre><code>class Timer(object):\n    def __init__(self, unit=\"sec\"):\n        if unit == \"ms\":\n            self.scale = 1e3\n        elif unit == \"us\":\n            self.scale = 1e6\n        elif unit == \"min\":\n            self.scale = 1 / 60\n        else:\n            self.scale = 1\n            unit = \"sec\"\n        self.unit = unit\n        self.start()\n\n    def start(self):\n        self._start = time.perf_counter()\n\n    def end(self):\n        return (time.perf_counter() - self._start) * self.scale\n\n    def report(self, msg=\"Run time\"):\n        print((msg + \": %.3f \" + self.unit) % self.end(), flush=True)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.pr_2_rho","title":"<code>bmtool.connectors.pr_2_rho(p0, p1, pr)</code>","text":"<p>Calculate correlation coefficient rho given reciprocal probability pr</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def pr_2_rho(p0, p1, pr):\n    \"\"\"Calculate correlation coefficient rho given reciprocal probability pr\"\"\"\n    for p in (p0, p1):\n        assert p &gt; 0 and p &lt; 1\n    assert pr &gt;= 0 and pr &lt;= p0 and pr &lt;= p1 and pr &gt;= p0 + p1 - 1\n    return (pr - p0 * p1) / (p0 * (1 - p0) * p1 * (1 - p1)) ** 0.5\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.rho_2_pr","title":"<code>bmtool.connectors.rho_2_pr(p0, p1, rho)</code>","text":"<p>Calculate reciprocal probability pr given correlation coefficient rho</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def rho_2_pr(p0, p1, rho):\n    \"\"\"Calculate reciprocal probability pr given correlation coefficient rho\"\"\"\n    for p in (p0, p1):\n        assert p &gt; 0 and p &lt; 1\n    pr = p0 * p1 + rho * (p0 * (1 - p0) * p1 * (1 - p1)) ** 0.5\n    if not (pr &gt;= 0 and pr &lt;= p0 and pr &lt;= p1 and pr &gt;= p0 + p1 - 1):\n        pr0, pr = pr, np.max((0.0, p0 + p1 - 1, np.min((p0, p1, pr))))\n        rho0, rho = rho, (pr - p0 * p1) / (p0 * (1 - p0) * p1 * (1 - p1)) ** 0.5\n        print(\n            \"rho changed from %.3f to %.3f; pr changed from %.3f to %.3f\" % (rho0, rho, pr0, pr),\n            flush=True,\n        )\n    return pr\n</code></pre>"},{"location":"api/connectors/#connector-implementations","title":"Connector Implementations","text":""},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector","title":"<code>bmtool.connectors.ReciprocalConnector</code>","text":"<p>               Bases: <code>AbstractConnector</code></p> <p>Object for buiilding connections in bmtk network model with reciprocal probability within a single population (or between two populations).</p> <p>Algorithm:     Create random connection for every pair of cells independently,     following a bivariate Bernoulli distribution. Each variable is 0 or 1,     whether a connection exists in a forward or backward direction. There     are four possible outcomes for each pair, no connection, unidirectional     connection in two ways, and reciprocal connection. The probability of     each outcome forms a contingency table.         b a c k w a r d     f   ---------------     o  |   |  0  |  1  |  The total forward connection probability is     r  |---|-----|-----|  p0 = p10 + p11     w  | 0 | p00 | p01 |  The total backward connection probability is     a  |---|-----|-----|  p1 = p01 + p11     r  | 1 | p10 | p11 |  The reciprocal connection probability is     d   ---------------   pr = p11     The distribution can be characterized by three parameters, p0, p1, pr.     pr = p0 * p1 when two directions are independent. The correlation     coefficient rho between the two has a relation with pr as follow.     rho = (pr-p0p1) / (p0(1-p0)p1(1-p1))^(1/2)     Generating random outcome consists of two steps. First draw random     outcome for forward connection with probability p0. Then draw backward     outcome following a conditional probability given the forward outcome,     represented by p0, p1, and either pr or rho.</p> <p>Use with BMTK:     1. Create this object with parameters.</p> <pre><code>    connector = ReciprocalConnector(**parameters)\n\n2. After network nodes are added to the BMTK network. Pass in the\nNodepool objects of source and target nodes using setup_nodes() method.\n\n    source = net.nodes(**source_filter)\n    target = net.nodes(**target_filter)\n    connector.setup_nodes(source, target)\n\n3. Use edge_params() method to get the arguments for BMTK add_edges()\nmethod including the `connection_rule` method.\n\n    net.add_edges(**connector.edge_params(),\n                  **other_source_to_target_edge_properties)\n\nIf the source and target are two different populations, do this again\nfor the backward connections (from target to source population).\n\n    net.add_edges(**connector.edge_params(),\n                  **other_target_to_source_edge_properties)\n\n4. When executing net.build(), BMTK uses built-in `one_to_all` iterator\nthat calls the make_forward_connection() method to build connections\nfrom source to target. If the two are different populations,\n`all_to_one` iterator that calls the make_backward_connection() method\nis then used to build connections from target to source.\nDuring the initial iteration when make_forward_connection() is called,\nthe algorithm is run to generate a connection matrix for both forward\nand backward connections. In the iterations afterward, it's only\nassigning the generated connections in BMTK.\n</code></pre> <p>Parameters:     p0, p1: Probability of forward and backward connection. It can be a         constant or a deterministic function whose value must be within         range [0, 1], otherwise incorrect value may occur in the algorithm.         When p0, p1 are constant, the connection is homogenous.     symmetric_p1: Whether p0 and p1 are identical. When the probabilities         are equal for forward and backward connections, set this to True,         Argument p1 will be ignored. This is forced to be True when the         population is recurrent, i.e., the source and target are the same.         This is forced to be False if symmetric_p1_arg is False.     p0_arg, p1_arg: Input argument(s) for p0 and p1 function, e.g.,         p0(p0_arg). It can be a constant or a deterministic function whose         input arguments are two node objects in BMTK, e.g.,         p0_arg(src_node,trg_node), p1_arg(trg_node,src_node). The latter         has reversed order since it's for backward connection. They are         usually distance between two nodes which is used for distance         dependent connection probability, where the order does not matter.         When p0 and p1 does not need inputs arguments, set p0_arg and         p1_arg to None as so by default. Functions p0 and p1 need to accept         one unused positional argument as placeholder, e.g., p0(args), so         it does not raise an error when p0(None) is called.     symmetric_p1_arg: Whether p0_arg and p1_arg are identical. If this is         set to True, argument p1_arg will be ignored. This is forced to be         True when the population is recurrent.     pr, pr_arg: Probability of reciprocal connection and its first input         argument when it is a function, similar to p0, p0_arg, p1, p1_arg.         It can be a function when it has an explicit relation with some node         properties such as distance. A function pr requires two additional         positional arguments p0 and p1 even if they are not used, i.e.,         pr(pr_arg, p0, p1), just in case pr is dependent on p0 and p1, e.g.,         when normalized reciprocal rate NRR = pr/(p0p1) is given.         When pr_arg is a string, the same value as p1_arg will be used for         pr_arg if the string contains '1', e.g., '1', 'p1'. Otherwise, e.g.,         '', '0', 'p0', p0_arg will be used for pr_arg. Specifying this can         avoid recomputing pr_arg when it's given by p0_arg or p1_arg.     estimate_rho: Whether estimate rho that result in an overall pr. This         is forced to be False if pr is a function or if rho is specified.         To estimate rho, all the pairs with possible connections, meaning         p0 and p1 are both non-zero for these pairs, are used to estimate         a value of rho that will result in an expected number of reciprocal         connections with the given pr. Note that pr is not over all pairs         of source and target cells but only those has a chance to connect,         e.g., for only pair of cells within some distance range. The         estimation is done before generating random connections. The values         of p0, p0_arg, p1, p1_arg can be cached during estimation of rho         and retrieved when generating random connections for performance.     dist_range_forward: If specified, when estimating rho, consider only         cell pairs whose distance (p0_arg) is within the specified range.     dist_range_backward: Similar to dist_range_forward but consider         backward distance range (p1_arg) instead. If both are specified,         consider only cell pairs whose both distances are within range. If         neither is specified, infer valid pairs by non-zero connection         probability.     rho: The correlation coefficient rho. When specified, do not estimate         it but instead use the given value throughout, pr will not be used.         In cases where both p0 and p1 are simple functions, i.e., are         constant on their support, e.g., function UniformInRange(), the         estimation of rho will be equal to pr_2_rho(p0, p1, pr) where p0,         p1 are non-zero. Estimation is not necessary. Directly set rho.     n_syn0, n_syn1: Number of synapses in the forward and backward         connection if connected. It can be a constant or a (deterministic         or random) function whose input arguments are two node objects in         BMTK like p0_arg, p1_arg. n_syn1 is force to be the same as n_syn0         when the population is recurrent. Warning: The number must not be         greater than 255 since it will be converted to uint8 when written         into the connection matrix to reduce memory consumption.     autapses: Whether to allow connecting a cell to itself. Default: False.         This is ignored when the population is not recurrent.     quick_pop_check: Whether to use quick method to check if source and         target populations are the same. Default: False.         Quick method checks only whether filter conditions match.         Strict method checks whether all node id's match considering order.     cache_data: Whether to cache the values of p0, p0_arg, p1, p1_arg         during estimation of rho. This improves performance when         estimate_rho is True while not creating a significant overhead in         the opposite case. However, it requires large memory allocation         as the population size grows. Set it to False if there is a memory         issue.     verbose: Whether show verbose information in console.</p> <p>Returns:     An object that works with BMTK to build edges in a network.</p> <p>Important attributes:     vars: Dictionary that stores part of the original input parameters.     source, target: NodePool objects for the source and target populations.     recurrent: Whether the source and target populations are the same.     callable_set: Set of arguments that are functions but not constants.     cache: ConnectorCache object for caching data.     conn_mat: Connection matrix     stage: Indicator of stage. 0 for forward and 1 for backward connection.     conn_prop: List of two dictionaries that stores properties of connected         pairs, for forward and backward connections respectively. In each         dictionary, each key is the source node id and the value is a         dictionary, where each key is the target node id that the source         node connects to, and the value is the value of p0_arg or p1_arg.         Example: [{sid0: {tid0: p0_arg0, tid1: p0_arg1, ...},                    sid1: {...}, sid2: {...}, ... },                   {tid2: {sid3: p1_arg0, sid4: p1_arg1, ...},                    tid3: {...}, tid4: {...}, ... }]         This is useful when properties of edges such as distance is used to         determine other edge properties such as delay. So the distance does         not need to be calculated repeatedly. The connector can be passed         as an argument for the functions that generates additional edge         properties, so that they can access the information here.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class ReciprocalConnector(AbstractConnector):\n    \"\"\"\n    Object for buiilding connections in bmtk network model with reciprocal\n    probability within a single population (or between two populations).\n\n    Algorithm:\n        Create random connection for every pair of cells independently,\n        following a bivariate Bernoulli distribution. Each variable is 0 or 1,\n        whether a connection exists in a forward or backward direction. There\n        are four possible outcomes for each pair, no connection, unidirectional\n        connection in two ways, and reciprocal connection. The probability of\n        each outcome forms a contingency table.\n            b a c k w a r d\n        f   ---------------\n        o  |   |  0  |  1  |  The total forward connection probability is\n        r  |---|-----|-----|  p0 = p10 + p11\n        w  | 0 | p00 | p01 |  The total backward connection probability is\n        a  |---|-----|-----|  p1 = p01 + p11\n        r  | 1 | p10 | p11 |  The reciprocal connection probability is\n        d   ---------------   pr = p11\n        The distribution can be characterized by three parameters, p0, p1, pr.\n        pr = p0 * p1 when two directions are independent. The correlation\n        coefficient rho between the two has a relation with pr as follow.\n        rho = (pr-p0*p1) / (p0*(1-p0)*p1*(1-p1))^(1/2)\n        Generating random outcome consists of two steps. First draw random\n        outcome for forward connection with probability p0. Then draw backward\n        outcome following a conditional probability given the forward outcome,\n        represented by p0, p1, and either pr or rho.\n\n    Use with BMTK:\n        1. Create this object with parameters.\n\n            connector = ReciprocalConnector(**parameters)\n\n        2. After network nodes are added to the BMTK network. Pass in the\n        Nodepool objects of source and target nodes using setup_nodes() method.\n\n            source = net.nodes(**source_filter)\n            target = net.nodes(**target_filter)\n            connector.setup_nodes(source, target)\n\n        3. Use edge_params() method to get the arguments for BMTK add_edges()\n        method including the `connection_rule` method.\n\n            net.add_edges(**connector.edge_params(),\n                          **other_source_to_target_edge_properties)\n\n        If the source and target are two different populations, do this again\n        for the backward connections (from target to source population).\n\n            net.add_edges(**connector.edge_params(),\n                          **other_target_to_source_edge_properties)\n\n        4. When executing net.build(), BMTK uses built-in `one_to_all` iterator\n        that calls the make_forward_connection() method to build connections\n        from source to target. If the two are different populations,\n        `all_to_one` iterator that calls the make_backward_connection() method\n        is then used to build connections from target to source.\n        During the initial iteration when make_forward_connection() is called,\n        the algorithm is run to generate a connection matrix for both forward\n        and backward connections. In the iterations afterward, it's only\n        assigning the generated connections in BMTK.\n\n    Parameters:\n        p0, p1: Probability of forward and backward connection. It can be a\n            constant or a deterministic function whose value must be within\n            range [0, 1], otherwise incorrect value may occur in the algorithm.\n            When p0, p1 are constant, the connection is homogenous.\n        symmetric_p1: Whether p0 and p1 are identical. When the probabilities\n            are equal for forward and backward connections, set this to True,\n            Argument p1 will be ignored. This is forced to be True when the\n            population is recurrent, i.e., the source and target are the same.\n            This is forced to be False if symmetric_p1_arg is False.\n        p0_arg, p1_arg: Input argument(s) for p0 and p1 function, e.g.,\n            p0(p0_arg). It can be a constant or a deterministic function whose\n            input arguments are two node objects in BMTK, e.g.,\n            p0_arg(src_node,trg_node), p1_arg(trg_node,src_node). The latter\n            has reversed order since it's for backward connection. They are\n            usually distance between two nodes which is used for distance\n            dependent connection probability, where the order does not matter.\n            When p0 and p1 does not need inputs arguments, set p0_arg and\n            p1_arg to None as so by default. Functions p0 and p1 need to accept\n            one unused positional argument as placeholder, e.g., p0(*args), so\n            it does not raise an error when p0(None) is called.\n        symmetric_p1_arg: Whether p0_arg and p1_arg are identical. If this is\n            set to True, argument p1_arg will be ignored. This is forced to be\n            True when the population is recurrent.\n        pr, pr_arg: Probability of reciprocal connection and its first input\n            argument when it is a function, similar to p0, p0_arg, p1, p1_arg.\n            It can be a function when it has an explicit relation with some node\n            properties such as distance. A function pr requires two additional\n            positional arguments p0 and p1 even if they are not used, i.e.,\n            pr(pr_arg, p0, p1), just in case pr is dependent on p0 and p1, e.g.,\n            when normalized reciprocal rate NRR = pr/(p0*p1) is given.\n            When pr_arg is a string, the same value as p1_arg will be used for\n            pr_arg if the string contains '1', e.g., '1', 'p1'. Otherwise, e.g.,\n            '', '0', 'p0', p0_arg will be used for pr_arg. Specifying this can\n            avoid recomputing pr_arg when it's given by p0_arg or p1_arg.\n        estimate_rho: Whether estimate rho that result in an overall pr. This\n            is forced to be False if pr is a function or if rho is specified.\n            To estimate rho, all the pairs with possible connections, meaning\n            p0 and p1 are both non-zero for these pairs, are used to estimate\n            a value of rho that will result in an expected number of reciprocal\n            connections with the given pr. Note that pr is not over all pairs\n            of source and target cells but only those has a chance to connect,\n            e.g., for only pair of cells within some distance range. The\n            estimation is done before generating random connections. The values\n            of p0, p0_arg, p1, p1_arg can be cached during estimation of rho\n            and retrieved when generating random connections for performance.\n        dist_range_forward: If specified, when estimating rho, consider only\n            cell pairs whose distance (p0_arg) is within the specified range.\n        dist_range_backward: Similar to dist_range_forward but consider\n            backward distance range (p1_arg) instead. If both are specified,\n            consider only cell pairs whose both distances are within range. If\n            neither is specified, infer valid pairs by non-zero connection\n            probability.\n        rho: The correlation coefficient rho. When specified, do not estimate\n            it but instead use the given value throughout, pr will not be used.\n            In cases where both p0 and p1 are simple functions, i.e., are\n            constant on their support, e.g., function UniformInRange(), the\n            estimation of rho will be equal to pr_2_rho(p0, p1, pr) where p0,\n            p1 are non-zero. Estimation is not necessary. Directly set rho.\n        n_syn0, n_syn1: Number of synapses in the forward and backward\n            connection if connected. It can be a constant or a (deterministic\n            or random) function whose input arguments are two node objects in\n            BMTK like p0_arg, p1_arg. n_syn1 is force to be the same as n_syn0\n            when the population is recurrent. Warning: The number must not be\n            greater than 255 since it will be converted to uint8 when written\n            into the connection matrix to reduce memory consumption.\n        autapses: Whether to allow connecting a cell to itself. Default: False.\n            This is ignored when the population is not recurrent.\n        quick_pop_check: Whether to use quick method to check if source and\n            target populations are the same. Default: False.\n            Quick method checks only whether filter conditions match.\n            Strict method checks whether all node id's match considering order.\n        cache_data: Whether to cache the values of p0, p0_arg, p1, p1_arg\n            during estimation of rho. This improves performance when\n            estimate_rho is True while not creating a significant overhead in\n            the opposite case. However, it requires large memory allocation\n            as the population size grows. Set it to False if there is a memory\n            issue.\n        verbose: Whether show verbose information in console.\n\n    Returns:\n        An object that works with BMTK to build edges in a network.\n\n    Important attributes:\n        vars: Dictionary that stores part of the original input parameters.\n        source, target: NodePool objects for the source and target populations.\n        recurrent: Whether the source and target populations are the same.\n        callable_set: Set of arguments that are functions but not constants.\n        cache: ConnectorCache object for caching data.\n        conn_mat: Connection matrix\n        stage: Indicator of stage. 0 for forward and 1 for backward connection.\n        conn_prop: List of two dictionaries that stores properties of connected\n            pairs, for forward and backward connections respectively. In each\n            dictionary, each key is the source node id and the value is a\n            dictionary, where each key is the target node id that the source\n            node connects to, and the value is the value of p0_arg or p1_arg.\n            Example: [{sid0: {tid0: p0_arg0, tid1: p0_arg1, ...},\n                       sid1: {...}, sid2: {...}, ... },\n                      {tid2: {sid3: p1_arg0, sid4: p1_arg1, ...},\n                       tid3: {...}, tid4: {...}, ... }]\n            This is useful when properties of edges such as distance is used to\n            determine other edge properties such as delay. So the distance does\n            not need to be calculated repeatedly. The connector can be passed\n            as an argument for the functions that generates additional edge\n            properties, so that they can access the information here.\n    \"\"\"\n\n    def __init__(\n        self,\n        p0=1.0,\n        p1=1.0,\n        symmetric_p1=False,\n        p0_arg=None,\n        p1_arg=None,\n        symmetric_p1_arg=False,\n        pr=0.0,\n        pr_arg=None,\n        estimate_rho=True,\n        rho=None,\n        dist_range_forward=None,\n        dist_range_backward=None,\n        n_syn0=1,\n        n_syn1=1,\n        autapses=False,\n        quick_pop_check=False,\n        cache_data=True,\n        verbose=True,\n        save_report=True,\n        report_name=None,\n    ):\n        args = locals()\n        var_set = (\"p0\", \"p0_arg\", \"p1\", \"p1_arg\", \"pr\", \"pr_arg\", \"n_syn0\", \"n_syn1\")\n        self.vars = {key: args[key] for key in var_set}\n\n        self.symmetric_p1 = symmetric_p1 and symmetric_p1_arg\n        self.symmetric_p1_arg = symmetric_p1_arg\n\n        self.estimate_rho = estimate_rho and not callable(pr) and rho is None\n        self.dist_range_forward = dist_range_forward\n        self.dist_range_backward = dist_range_backward\n        self.rho = rho\n\n        self.autapses = autapses\n        self.quick = quick_pop_check\n        self.cache = self.ConnectorCache(cache_data and self.estimate_rho)\n        self.verbose = verbose\n        self.save_report = save_report\n\n        if report_name is None:\n            report_name = globals().get(\"report_name\", \"default_report.csv\")\n        self.report_name = report_name\n\n        self.conn_prop = [{}, {}]\n        self.stage = 0\n        self.iter_count = 0\n\n    # *** Two methods executed during bmtk edge creation net.add_edges() ***\n    def setup_nodes(self, source=None, target=None):\n        \"\"\"Must run this before building connections\"\"\"\n        if self.stage:\n            # check whether the correct populations\n            if (\n                source is None\n                or target is None\n                or not is_same_pop(source, self.target, quick=self.quick)\n                or not is_same_pop(target, self.source, quick=self.quick)\n            ):\n                raise ValueError(\"Source or target population not consistent.\")\n            # Skip adding nodes for the backward stage.\n            return\n\n        # Update node pools\n        self.source = source\n        self.target = target\n        if self.source is None or len(self.source) == 0:\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(f\"{src_str} nodes do not exists\")\n        if self.target is None or len(self.target) == 0:\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(f\"{trg_str} nodes do not exists\")\n\n        # Setup nodes\n        self.recurrent = is_same_pop(self.source, self.target, quick=self.quick)\n        self.source_ids = [s.node_id for s in self.source]\n        self.n_source = len(self.source_ids)\n        self.source_list = list(self.source)\n        if self.recurrent:\n            self.target_ids = self.source_ids\n            self.n_target = self.n_source\n            self.target_list = self.source_list\n        else:\n            self.target_ids = [t.node_id for t in self.target]\n            self.n_target = len(self.target_ids)\n            self.target_list = list(self.target)\n\n        # Setup for recurrent connection\n        if self.recurrent:\n            self.symmetric_p1_arg = True\n            self.symmetric_p1 = True\n            self.vars[\"n_syn1\"] = self.vars[\"n_syn0\"]\n        if self.symmetric_p1_arg:\n            self.vars[\"p1_arg\"] = self.vars[\"p0_arg\"]\n        if self.symmetric_p1:\n            self.vars[\"p1\"] = self.vars[\"p0\"]\n\n    def edge_params(self):\n        \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n        if self.stage == 0:\n            params = {\n                \"source\": self.source,\n                \"target\": self.target,\n                \"iterator\": \"one_to_all\",\n                \"connection_rule\": self.make_forward_connection,\n            }\n        else:\n            params = {\n                \"source\": self.target,\n                \"target\": self.source,\n                \"iterator\": \"all_to_one\",\n                \"connection_rule\": self.make_backward_connection,\n            }\n        self.stage += 1\n        return params\n\n    # *** Methods executed during bmtk network.build() ***\n    # *** Helper functions ***\n    class ConnectorCache(object):\n        def __init__(self, enable=True):\n            self.enable = enable\n            self._output = {}\n            self.cache_dict = {}\n            self.set_next_it()\n            self.write_mode()\n\n        def cache_output(self, func, func_name, cache=True):\n            if self.enable and cache:\n                self.cache_dict[func_name] = func\n                self._output[func_name] = []\n                output = self._output[func_name]\n\n                def writer(*args):\n                    val = func(*args)\n                    output.append(val)\n                    return val\n\n                setattr(self, func_name, writer)\n            else:\n                setattr(self, func_name, func)\n\n        def write_mode(self):\n            for val in self._output.values():\n                val.clear()\n            self.mode = \"write\"\n            self.iter_count = 0\n\n        def fetch_output(self, func_name, fetch=True):\n            output = self._output[func_name]\n\n            if fetch:\n\n                def reader(*args):\n                    return output[self.iter_count]\n\n                setattr(self, func_name, reader)\n            else:\n                setattr(self, func_name, self.cache_dict[func_name])\n\n        def read_mode(self):\n            if self.enable and len(self.cache_dict):\n                # check whether outputs were written correctly\n                output_len = [len(val) for val in self._output.values()]\n                # whether any stored and have the same length\n                valid = [n for n in output_len if n]\n                flag = len(valid) &gt; 0 and all(n == valid[0] for n in valid[1:])\n                if flag:\n                    for func_name, out_len in zip(self._output, output_len):\n                        fetch = out_len &gt; 0\n                        if not fetch:\n                            print(\n                                \"\\nWarning: Cache did not work properly for \" + func_name + \"\\n\",\n                                flush=True,\n                            )\n                        self.fetch_output(func_name, fetch)\n                    self.iter_count = 0\n                else:\n                    # if output not correct, disable and use original function\n                    print(\"\\nWarning: Cache did not work properly.\\n\", flush=True)\n                    for func_name in self.cache_dict:\n                        self.fetch_output(func_name, False)\n                    self.enable = False\n            self.mode = \"read\"\n\n        def set_next_it(self):\n            if self.enable:\n\n                def next_it():\n                    self.iter_count += 1\n            else:\n\n                def next_it():\n                    pass\n\n            self.next_it = next_it\n\n    def node_2_idx_input(self, var_func, reverse=False):\n        \"\"\"Convert a function that accept nodes as input\n        to accept indices as input\"\"\"\n        if reverse:\n\n            def idx_2_var(j, i):\n                return var_func(self.target_list[j], self.source_list[i])\n        else:\n\n            def idx_2_var(i, j):\n                return var_func(self.source_list[i], self.target_list[j])\n\n        return idx_2_var\n\n    def iterate_pairs(self):\n        \"\"\"Generate indices of source and target for each case\"\"\"\n        if self.recurrent:\n            if self.autapses:\n                for i in range(self.n_source):\n                    for j in range(i, self.n_target):\n                        yield i, j\n            else:\n                for i in range(self.n_source - 1):\n                    for j in range(i + 1, self.n_target):\n                        yield i, j\n        else:\n            for i in range(self.n_source):\n                for j in range(self.n_target):\n                    yield i, j\n\n    def calc_pair(self, i, j):\n        \"\"\"Calculate intermediate data that can be cached\"\"\"\n        cache = self.cache\n        # cache = self  # test performance for not using cache\n        p0_arg = cache.p0_arg(i, j)\n        p1_arg = p0_arg if self.symmetric_p1_arg else cache.p1_arg(j, i)\n        p0 = cache.p0(p0_arg)\n        p1 = p0 if self.symmetric_p1 else cache.p1(p1_arg)\n        return p0_arg, p1_arg, p0, p1\n\n    def setup_conditional_backward_probability(self):\n        \"\"\"Create a function that calculates the conditional probability of\n        backward connection given the forward connection outcome 'cond'\"\"\"\n        # For all cases, assume p0, p1, pr are all within [0, 1] already.\n        self.wrong_pr = False\n        if self.rho is None:\n            # Determine by pr for each pair\n            if self.verbose:\n\n                def cond_backward(cond, p0, p1, pr):\n                    if p0 &gt; 0:\n                        pr_bound = (p0 + p1 - 1, min(p0, p1))\n                        # check whether pr within bounds\n                        if pr &lt; pr_bound[0] or pr &gt; pr_bound[1]:\n                            self.wrong_pr = True\n                            pr = min(max(pr, pr_bound[0]), pr_bound[1])\n                        return pr / p0 if cond else (p1 - pr) / (1 - p0)\n                    else:\n                        return p1\n            else:\n\n                def cond_backward(cond, p0, p1, pr):\n                    if p0 &gt; 0:\n                        pr_bound = (p0 + p1 - 1, min(p0, p1))\n                        pr = min(max(pr, pr_bound[0]), pr_bound[1])\n                        return pr / p0 if cond else (p1 - pr) / (1 - p0)\n                    else:\n                        return p1\n        elif self.rho == 0:\n            # Independent case\n            def cond_backward(cond, p0, p1, pr):\n                return p1\n        else:\n            # Dependent with fixed correlation coefficient rho\n            def cond_backward(cond, p0, p1, pr):\n                # Standard deviation of r.v. for p1\n                sd = ((1 - p1) * p1) ** 0.5\n                # Z-score of random variable for p0\n                zs = ((1 - p0) / p0) ** 0.5 if cond else -((p0 / (1 - p0)) ** 0.5)\n                return p1 + self.rho * sd * zs\n\n        self.cond_backward = cond_backward\n\n    def add_conn_prop(self, src, trg, prop, stage=0):\n        \"\"\"Store p0_arg and p1_arg for a connected pair\"\"\"\n        sid = self.source_ids[src]\n        tid = self.target_ids[trg]\n        conn_dict = self.conn_prop[stage]\n        if stage:\n            sid, tid = tid, sid  # during backward, from target to source\n        trg_dict = conn_dict.setdefault(sid, {})\n        trg_dict[tid] = prop\n\n    def get_conn_prop(self, sid, tid):\n        \"\"\"Get stored value given node ids in a connection\"\"\"\n        return self.conn_prop[self.stage][sid][tid]\n\n    # *** A sequence of major methods executed during build ***\n    def setup_variables(self):\n        # If pr_arg is string, use the same value as p0_arg or p1_arg\n        if isinstance(self.vars[\"pr_arg\"], str):\n            pr_arg_func = \"p1_arg\" if \"1\" in self.vars[\"pr_arg\"] else \"p0_arg\"\n            self.vars[\"pr_arg\"] = self.vars[pr_arg_func]\n        else:\n            pr_arg_func = None\n\n        callable_set = set()\n        # Make constant variables constant functions\n        for name, var in self.vars.items():\n            if callable(var):\n                callable_set.add(name)  # record callable variables\n                setattr(self, name, var)\n            else:\n                setattr(self, name, self.constant_function(var))\n        self.callable_set = callable_set\n\n        # Make callable variables except a few, accept index input instead\n        for name in callable_set - {\"p0\", \"p1\", \"pr\"}:\n            var = self.vars[name]\n            setattr(self, name, self.node_2_idx_input(var, \"1\" in name))\n\n        # Set up function for pr_arg if use value from p0_arg or p1_arg\n        if pr_arg_func is None:\n            self._pr_arg = self.pr_arg  # use specified pr_arg\n        else:\n            self._pr_arg_val = 0.0  # storing current value from p_arg\n            p_arg = getattr(self, pr_arg_func)\n\n            def p_arg_4_pr(*args, **kwargs):\n                val = p_arg(*args, **kwargs)\n                self._pr_arg_val = val\n                return val\n\n            setattr(self, pr_arg_func, p_arg_4_pr)\n\n            def pr_arg(self, *arg):\n                return self._pr_arg_val\n\n            self._pr_arg = types.MethodType(pr_arg, self)\n\n    def cache_variables(self):\n        # Select cacheable attrilbutes\n        cache_set = {\"p0\", \"p0_arg\", \"p1\", \"p1_arg\"}\n        if self.symmetric_p1:\n            cache_set.remove(\"p1\")\n        if self.symmetric_p1_arg:\n            cache_set.remove(\"p1_arg\")\n        # Output of callable variables will be cached\n        # Constant functions will be called from cache but output not cached\n        for name in cache_set:\n            var = getattr(self, name)\n            self.cache.cache_output(var, name, name in self.callable_set)\n        if self.verbose and len(self.cache.cache_dict):\n            print(\"Output of %s will be cached.\" % \", \".join(self.cache.cache_dict), flush=True)\n\n    def setup_dist_range_checker(self):\n        # Checker that determines whether to consider a pair for rho estimation\n        if self.dist_range_forward is None and self.dist_range_backward is None:\n\n            def checker(var):\n                p0, p1 = var[2:]\n                return p0 &gt; 0 and p1 &gt; 0\n        else:\n\n            def in_range(p_arg, dist_range):\n                return p_arg &gt;= dist_range[0] and p_arg &lt;= dist_range[1]\n\n            r0, r1 = self.dist_range_forward, self.dist_range_backward\n            if r1 is None:\n\n                def checker(var):\n                    return in_range(var[0], r0)\n            elif r0 is None:\n\n                def checker(var):\n                    return in_range(var[1], r1)\n            else:\n\n                def checker(var):\n                    return in_range(var[0], r0) and in_range(var[1], r1)\n\n        return checker\n\n    def initialize(self):\n        self.setup_variables()\n        self.cache_variables()\n        # Intialize connection matrix and get nubmer of pairs\n        self.end_stage = 0 if self.recurrent else 1\n        shape = (self.end_stage + 1, self.n_source, self.n_target)\n        self.conn_mat = np.zeros(shape, dtype=np.uint8)  # 1 byte per entry\n\n    def initial_all_to_all(self):\n        \"\"\"The major part of the algorithm run at beginning of BMTK iterator\"\"\"\n        if self.verbose:\n            src_str, trg_str = self.get_nodes_info()\n            print(\n                \"\\nStart building connection between: \\n  \" + src_str + \"\\n  \" + trg_str, flush=True\n            )\n        self.initialize()\n        cache = self.cache  # write mode\n\n        # Estimate pr\n        if self.verbose:\n            self.timer = Timer()\n        if self.estimate_rho:\n            dist_range_checker = self.setup_dist_range_checker()\n            p0p1_sum = 0.0\n            norm_fac_sum = 0.0\n            n = 0\n            # Make sure each cacheable function runs excatly once per iteration\n            for i, j in self.iterate_pairs():\n                var = self.calc_pair(i, j)\n                valid = dist_range_checker(var)\n                if valid:\n                    n += 1\n                    p0, p1 = var[2:]\n                    p0p1_sum += p0 * p1\n                    norm_fac_sum += (p0 * (1 - p0) * p1 * (1 - p1)) ** 0.5\n            if norm_fac_sum &gt; 0:\n                rho = (self.pr() * n - p0p1_sum) / norm_fac_sum\n                if abs(rho) &gt; 1:\n                    print(\n                        \"\\nWarning: Estimated value of rho=%.3f \"\n                        \"outside the range [-1, 1].\" % rho,\n                        flush=True,\n                    )\n                    rho = np.clip(rho, -1, 1).item()\n                    print(\"Force rho to be %.0f.\\n\" % rho, flush=True)\n                elif self.verbose:\n                    print(\"Estimated value of rho=%.3f\" % rho, flush=True)\n                self.rho = rho\n            else:\n                self.rho = 0\n\n            if self.verbose:\n                self.timer.report(\"Time for estimating rho\")\n\n        # Setup function for calculating conditional backward probability\n        self.setup_conditional_backward_probability()\n\n        # Make random connections\n        cache.read_mode()\n        possible_count = 0 if self.recurrent else np.zeros(3)\n        for i, j in self.iterate_pairs():\n            p0_arg, p1_arg, p0, p1 = self.calc_pair(i, j)\n            # Check whether at all possible and count\n            forward = p0 &gt; 0\n            backward = p1 &gt; 0\n            if self.recurrent:\n                possible_count += forward\n            else:\n                possible_count += [forward, backward, forward and backward]\n\n            # Make random decision\n            if forward:\n                forward = decision(p0)\n            if backward:\n                pr = self.pr(self._pr_arg(i, j), p0, p1)\n                backward = decision(self.cond_backward(forward, p0, p1, pr))\n\n            # Make connection\n            if forward:\n                n_forward = self.n_syn0(i, j)\n                self.add_conn_prop(i, j, p0_arg, 0)\n                self.conn_mat[0, i, j] = n_forward\n            if backward:\n                n_backward = self.n_syn1(j, i)\n                if self.recurrent:\n                    if i != j:\n                        self.conn_mat[0, j, i] = n_backward\n                        self.add_conn_prop(j, i, p1_arg, 0)\n                else:\n                    self.conn_mat[1, i, j] = n_backward\n                    self.add_conn_prop(i, j, p1_arg, 1)\n            self.cache.next_it()\n        self.cache.write_mode()  # clear memory\n        self.possible_count = possible_count\n\n        if self.verbose:\n            self.timer.report(\"Total time for creating connection matrix\")\n            if self.wrong_pr:\n                print(\"Warning: Value of 'pr' outside the bounds occurred.\\n\", flush=True)\n            self.connection_number_info()\n        if self.save_report:\n            self.save_connection_report()\n\n    def make_connection(self):\n        \"\"\"Assign number of synapses per iteration.\n        Use iterator one_to_all for forward and all_to_one for backward.\n        \"\"\"\n        nsyns = self.conn_mat[self.stage, self.iter_count, :]\n        self.iter_count += 1\n\n        # Detect end of iteration\n        if self.iter_count == self.n_source:\n            self.iter_count = 0\n            if self.stage == self.end_stage:\n                if self.verbose:\n                    self.timer.report(\"Done! \\nTime for building connections\")\n                self.free_memory()\n        return nsyns\n\n    def make_forward_connection(self, source, targets, *args, **kwargs):\n        \"\"\"Function to be called by BMTK iterator for forward connection\"\"\"\n        # Initialize in the first iteration\n        if self.iter_count == 0:\n            self.stage = 0\n            self.initial_all_to_all()\n            if self.verbose:\n                print(\"Assigning forward connections.\", flush=True)\n                self.timer.start()\n        return self.make_connection()\n\n    def make_backward_connection(self, targets, source, *args, **kwargs):\n        \"\"\"Function to be called by BMTK iterator for backward connection\"\"\"\n        if self.iter_count == 0:\n            self.stage = 1\n            if self.verbose:\n                print(\"Assigning backward connections.\", flush=True)\n        return self.make_connection()\n\n    def free_memory(self):\n        \"\"\"Free up memory after connections are built\"\"\"\n        # Do not clear self.conn_prop if it will be used by conn.add_properties\n        variables = (\"conn_mat\", \"source_list\", \"target_list\", \"source_ids\", \"target_ids\")\n        for var in variables:\n            setattr(self, var, None)\n\n    # *** Helper functions for verbose ***\n    def get_nodes_info(self):\n        \"\"\"Get strings with source and target population information\"\"\"\n        source_str = self.source.network_name + \": \" + self.source.filter_str\n        target_str = self.target.network_name + \": \" + self.target.filter_str\n        return source_str, target_str\n\n    def connection_number(self):\n        \"\"\"\n        Return the number of the following:\n        n_conn: connected pairs [forward, (backward,) reciprocal]\n        n_poss: possible connections (prob&gt;0) [forward, (backward, reciprocal)]\n        n_pair: pairs of cells\n        proportion: of connections in possible and total pairs\n        \"\"\"\n        conn_mat = self.conn_mat.astype(bool)\n        n_conn = np.count_nonzero(conn_mat, axis=(1, 2))\n        n_poss = np.array(self.possible_count)\n        n_pair = conn_mat.size / 2\n        if self.recurrent:\n            n_recp = np.count_nonzero(conn_mat[0] &amp; conn_mat[0].T)\n            if self.autapses:\n                n_recp -= np.count_nonzero(np.diag(conn_mat[0]))\n            n_recp //= 2\n            n_conn -= n_recp\n            n_poss = n_poss[None]\n            n_pair += (1 if self.autapses else -1) * self.n_source / 2\n        else:\n            n_recp = np.count_nonzero(conn_mat[0] &amp; conn_mat[1])\n        n_conn = np.append(n_conn, n_recp)\n        n_pair = int(n_pair)\n        fraction = np.array([n_conn / n_poss, n_conn / n_pair])\n        fraction[np.isnan(fraction)] = 0.0\n        return n_conn, n_poss, n_pair, fraction\n\n    def connection_number_info(self):\n        \"\"\"Print connection numbers after connections built\"\"\"\n\n        def arr2str(a, f):\n            return \", \".join([f] * a.size) % tuple(a.tolist())\n\n        n_conn, n_poss, n_pair, fraction = self.connection_number()\n        conn_type = \"(all, reciprocal)\" if self.recurrent else \"(forward, backward, reciprocal)\"\n        print(\"Numbers of \" + conn_type + \" connections:\", flush=True)\n        print(\"Number of connected pairs: (%s)\" % arr2str(n_conn, \"%d\"), flush=True)\n        print(\"Number of possible connections: (%s)\" % arr2str(n_poss, \"%d\"), flush=True)\n        print(\n            \"Fraction of connected pairs in possible ones: (%s)\"\n            % arr2str(100 * fraction[0], \"%.2f%%\"),\n            flush=True,\n        )\n        print(\"Number of total pairs: %d\" % n_pair, flush=True)\n        print(\n            \"Fraction of connected pairs in all pairs: (%s)\\n\"\n            % arr2str(100 * fraction[1], \"%.2f%%\"),\n            flush=True,\n        )\n\n    def save_connection_report(self):\n        \"\"\"Save connections into a CSV file to be read from later\"\"\"\n        src_str, trg_str = self.get_nodes_info()\n        n_conn, n_poss, n_pair, fraction = self.connection_number()\n\n        # Extract the population name from source_str and target_str\n        data = {\n            \"Source\": [src_str],\n            \"Target\": [trg_str],\n            \"Percent connectionivity within possible connections\": [fraction[0] * 100],\n            \"Percent connectionivity within all connections\": [fraction[1] * 100],\n        }\n        df = pd.DataFrame(data)\n\n        # Append the data to the CSV file\n        try:\n            # Check if the file exists by trying to read it\n            existing_df = pd.read_csv(self.report_name)\n            # If no exception is raised, append without header\n            df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n        except FileNotFoundError:\n            # If the file does not exist, write with header\n            df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.add_conn_prop","title":"<code>add_conn_prop(src, trg, prop, stage=0)</code>","text":"<p>Store p0_arg and p1_arg for a connected pair</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def add_conn_prop(self, src, trg, prop, stage=0):\n    \"\"\"Store p0_arg and p1_arg for a connected pair\"\"\"\n    sid = self.source_ids[src]\n    tid = self.target_ids[trg]\n    conn_dict = self.conn_prop[stage]\n    if stage:\n        sid, tid = tid, sid  # during backward, from target to source\n    trg_dict = conn_dict.setdefault(sid, {})\n    trg_dict[tid] = prop\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.calc_pair","title":"<code>calc_pair(i, j)</code>","text":"<p>Calculate intermediate data that can be cached</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def calc_pair(self, i, j):\n    \"\"\"Calculate intermediate data that can be cached\"\"\"\n    cache = self.cache\n    # cache = self  # test performance for not using cache\n    p0_arg = cache.p0_arg(i, j)\n    p1_arg = p0_arg if self.symmetric_p1_arg else cache.p1_arg(j, i)\n    p0 = cache.p0(p0_arg)\n    p1 = p0 if self.symmetric_p1 else cache.p1(p1_arg)\n    return p0_arg, p1_arg, p0, p1\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.connection_number","title":"<code>connection_number()</code>","text":"<p>Return the number of the following: n_conn: connected pairs [forward, (backward,) reciprocal] n_poss: possible connections (prob&gt;0) [forward, (backward, reciprocal)] n_pair: pairs of cells proportion: of connections in possible and total pairs</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def connection_number(self):\n    \"\"\"\n    Return the number of the following:\n    n_conn: connected pairs [forward, (backward,) reciprocal]\n    n_poss: possible connections (prob&gt;0) [forward, (backward, reciprocal)]\n    n_pair: pairs of cells\n    proportion: of connections in possible and total pairs\n    \"\"\"\n    conn_mat = self.conn_mat.astype(bool)\n    n_conn = np.count_nonzero(conn_mat, axis=(1, 2))\n    n_poss = np.array(self.possible_count)\n    n_pair = conn_mat.size / 2\n    if self.recurrent:\n        n_recp = np.count_nonzero(conn_mat[0] &amp; conn_mat[0].T)\n        if self.autapses:\n            n_recp -= np.count_nonzero(np.diag(conn_mat[0]))\n        n_recp //= 2\n        n_conn -= n_recp\n        n_poss = n_poss[None]\n        n_pair += (1 if self.autapses else -1) * self.n_source / 2\n    else:\n        n_recp = np.count_nonzero(conn_mat[0] &amp; conn_mat[1])\n    n_conn = np.append(n_conn, n_recp)\n    n_pair = int(n_pair)\n    fraction = np.array([n_conn / n_poss, n_conn / n_pair])\n    fraction[np.isnan(fraction)] = 0.0\n    return n_conn, n_poss, n_pair, fraction\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.connection_number_info","title":"<code>connection_number_info()</code>","text":"<p>Print connection numbers after connections built</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def connection_number_info(self):\n    \"\"\"Print connection numbers after connections built\"\"\"\n\n    def arr2str(a, f):\n        return \", \".join([f] * a.size) % tuple(a.tolist())\n\n    n_conn, n_poss, n_pair, fraction = self.connection_number()\n    conn_type = \"(all, reciprocal)\" if self.recurrent else \"(forward, backward, reciprocal)\"\n    print(\"Numbers of \" + conn_type + \" connections:\", flush=True)\n    print(\"Number of connected pairs: (%s)\" % arr2str(n_conn, \"%d\"), flush=True)\n    print(\"Number of possible connections: (%s)\" % arr2str(n_poss, \"%d\"), flush=True)\n    print(\n        \"Fraction of connected pairs in possible ones: (%s)\"\n        % arr2str(100 * fraction[0], \"%.2f%%\"),\n        flush=True,\n    )\n    print(\"Number of total pairs: %d\" % n_pair, flush=True)\n    print(\n        \"Fraction of connected pairs in all pairs: (%s)\\n\"\n        % arr2str(100 * fraction[1], \"%.2f%%\"),\n        flush=True,\n    )\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.edge_params","title":"<code>edge_params()</code>","text":"<p>Create the arguments for BMTK add_edges() method</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def edge_params(self):\n    \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n    if self.stage == 0:\n        params = {\n            \"source\": self.source,\n            \"target\": self.target,\n            \"iterator\": \"one_to_all\",\n            \"connection_rule\": self.make_forward_connection,\n        }\n    else:\n        params = {\n            \"source\": self.target,\n            \"target\": self.source,\n            \"iterator\": \"all_to_one\",\n            \"connection_rule\": self.make_backward_connection,\n        }\n    self.stage += 1\n    return params\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.free_memory","title":"<code>free_memory()</code>","text":"<p>Free up memory after connections are built</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def free_memory(self):\n    \"\"\"Free up memory after connections are built\"\"\"\n    # Do not clear self.conn_prop if it will be used by conn.add_properties\n    variables = (\"conn_mat\", \"source_list\", \"target_list\", \"source_ids\", \"target_ids\")\n    for var in variables:\n        setattr(self, var, None)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.get_conn_prop","title":"<code>get_conn_prop(sid, tid)</code>","text":"<p>Get stored value given node ids in a connection</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def get_conn_prop(self, sid, tid):\n    \"\"\"Get stored value given node ids in a connection\"\"\"\n    return self.conn_prop[self.stage][sid][tid]\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.get_nodes_info","title":"<code>get_nodes_info()</code>","text":"<p>Get strings with source and target population information</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def get_nodes_info(self):\n    \"\"\"Get strings with source and target population information\"\"\"\n    source_str = self.source.network_name + \": \" + self.source.filter_str\n    target_str = self.target.network_name + \": \" + self.target.filter_str\n    return source_str, target_str\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.initial_all_to_all","title":"<code>initial_all_to_all()</code>","text":"<p>The major part of the algorithm run at beginning of BMTK iterator</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def initial_all_to_all(self):\n    \"\"\"The major part of the algorithm run at beginning of BMTK iterator\"\"\"\n    if self.verbose:\n        src_str, trg_str = self.get_nodes_info()\n        print(\n            \"\\nStart building connection between: \\n  \" + src_str + \"\\n  \" + trg_str, flush=True\n        )\n    self.initialize()\n    cache = self.cache  # write mode\n\n    # Estimate pr\n    if self.verbose:\n        self.timer = Timer()\n    if self.estimate_rho:\n        dist_range_checker = self.setup_dist_range_checker()\n        p0p1_sum = 0.0\n        norm_fac_sum = 0.0\n        n = 0\n        # Make sure each cacheable function runs excatly once per iteration\n        for i, j in self.iterate_pairs():\n            var = self.calc_pair(i, j)\n            valid = dist_range_checker(var)\n            if valid:\n                n += 1\n                p0, p1 = var[2:]\n                p0p1_sum += p0 * p1\n                norm_fac_sum += (p0 * (1 - p0) * p1 * (1 - p1)) ** 0.5\n        if norm_fac_sum &gt; 0:\n            rho = (self.pr() * n - p0p1_sum) / norm_fac_sum\n            if abs(rho) &gt; 1:\n                print(\n                    \"\\nWarning: Estimated value of rho=%.3f \"\n                    \"outside the range [-1, 1].\" % rho,\n                    flush=True,\n                )\n                rho = np.clip(rho, -1, 1).item()\n                print(\"Force rho to be %.0f.\\n\" % rho, flush=True)\n            elif self.verbose:\n                print(\"Estimated value of rho=%.3f\" % rho, flush=True)\n            self.rho = rho\n        else:\n            self.rho = 0\n\n        if self.verbose:\n            self.timer.report(\"Time for estimating rho\")\n\n    # Setup function for calculating conditional backward probability\n    self.setup_conditional_backward_probability()\n\n    # Make random connections\n    cache.read_mode()\n    possible_count = 0 if self.recurrent else np.zeros(3)\n    for i, j in self.iterate_pairs():\n        p0_arg, p1_arg, p0, p1 = self.calc_pair(i, j)\n        # Check whether at all possible and count\n        forward = p0 &gt; 0\n        backward = p1 &gt; 0\n        if self.recurrent:\n            possible_count += forward\n        else:\n            possible_count += [forward, backward, forward and backward]\n\n        # Make random decision\n        if forward:\n            forward = decision(p0)\n        if backward:\n            pr = self.pr(self._pr_arg(i, j), p0, p1)\n            backward = decision(self.cond_backward(forward, p0, p1, pr))\n\n        # Make connection\n        if forward:\n            n_forward = self.n_syn0(i, j)\n            self.add_conn_prop(i, j, p0_arg, 0)\n            self.conn_mat[0, i, j] = n_forward\n        if backward:\n            n_backward = self.n_syn1(j, i)\n            if self.recurrent:\n                if i != j:\n                    self.conn_mat[0, j, i] = n_backward\n                    self.add_conn_prop(j, i, p1_arg, 0)\n            else:\n                self.conn_mat[1, i, j] = n_backward\n                self.add_conn_prop(i, j, p1_arg, 1)\n        self.cache.next_it()\n    self.cache.write_mode()  # clear memory\n    self.possible_count = possible_count\n\n    if self.verbose:\n        self.timer.report(\"Total time for creating connection matrix\")\n        if self.wrong_pr:\n            print(\"Warning: Value of 'pr' outside the bounds occurred.\\n\", flush=True)\n        self.connection_number_info()\n    if self.save_report:\n        self.save_connection_report()\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.iterate_pairs","title":"<code>iterate_pairs()</code>","text":"<p>Generate indices of source and target for each case</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def iterate_pairs(self):\n    \"\"\"Generate indices of source and target for each case\"\"\"\n    if self.recurrent:\n        if self.autapses:\n            for i in range(self.n_source):\n                for j in range(i, self.n_target):\n                    yield i, j\n        else:\n            for i in range(self.n_source - 1):\n                for j in range(i + 1, self.n_target):\n                    yield i, j\n    else:\n        for i in range(self.n_source):\n            for j in range(self.n_target):\n                yield i, j\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.make_backward_connection","title":"<code>make_backward_connection(targets, source, *args, **kwargs)</code>","text":"<p>Function to be called by BMTK iterator for backward connection</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_backward_connection(self, targets, source, *args, **kwargs):\n    \"\"\"Function to be called by BMTK iterator for backward connection\"\"\"\n    if self.iter_count == 0:\n        self.stage = 1\n        if self.verbose:\n            print(\"Assigning backward connections.\", flush=True)\n    return self.make_connection()\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.make_connection","title":"<code>make_connection()</code>","text":"<p>Assign number of synapses per iteration. Use iterator one_to_all for forward and all_to_one for backward.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_connection(self):\n    \"\"\"Assign number of synapses per iteration.\n    Use iterator one_to_all for forward and all_to_one for backward.\n    \"\"\"\n    nsyns = self.conn_mat[self.stage, self.iter_count, :]\n    self.iter_count += 1\n\n    # Detect end of iteration\n    if self.iter_count == self.n_source:\n        self.iter_count = 0\n        if self.stage == self.end_stage:\n            if self.verbose:\n                self.timer.report(\"Done! \\nTime for building connections\")\n            self.free_memory()\n    return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.make_forward_connection","title":"<code>make_forward_connection(source, targets, *args, **kwargs)</code>","text":"<p>Function to be called by BMTK iterator for forward connection</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_forward_connection(self, source, targets, *args, **kwargs):\n    \"\"\"Function to be called by BMTK iterator for forward connection\"\"\"\n    # Initialize in the first iteration\n    if self.iter_count == 0:\n        self.stage = 0\n        self.initial_all_to_all()\n        if self.verbose:\n            print(\"Assigning forward connections.\", flush=True)\n            self.timer.start()\n    return self.make_connection()\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.node_2_idx_input","title":"<code>node_2_idx_input(var_func, reverse=False)</code>","text":"<p>Convert a function that accept nodes as input to accept indices as input</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def node_2_idx_input(self, var_func, reverse=False):\n    \"\"\"Convert a function that accept nodes as input\n    to accept indices as input\"\"\"\n    if reverse:\n\n        def idx_2_var(j, i):\n            return var_func(self.target_list[j], self.source_list[i])\n    else:\n\n        def idx_2_var(i, j):\n            return var_func(self.source_list[i], self.target_list[j])\n\n    return idx_2_var\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.save_connection_report","title":"<code>save_connection_report()</code>","text":"<p>Save connections into a CSV file to be read from later</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def save_connection_report(self):\n    \"\"\"Save connections into a CSV file to be read from later\"\"\"\n    src_str, trg_str = self.get_nodes_info()\n    n_conn, n_poss, n_pair, fraction = self.connection_number()\n\n    # Extract the population name from source_str and target_str\n    data = {\n        \"Source\": [src_str],\n        \"Target\": [trg_str],\n        \"Percent connectionivity within possible connections\": [fraction[0] * 100],\n        \"Percent connectionivity within all connections\": [fraction[1] * 100],\n    }\n    df = pd.DataFrame(data)\n\n    # Append the data to the CSV file\n    try:\n        # Check if the file exists by trying to read it\n        existing_df = pd.read_csv(self.report_name)\n        # If no exception is raised, append without header\n        df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n    except FileNotFoundError:\n        # If the file does not exist, write with header\n        df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.setup_conditional_backward_probability","title":"<code>setup_conditional_backward_probability()</code>","text":"<p>Create a function that calculates the conditional probability of backward connection given the forward connection outcome 'cond'</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def setup_conditional_backward_probability(self):\n    \"\"\"Create a function that calculates the conditional probability of\n    backward connection given the forward connection outcome 'cond'\"\"\"\n    # For all cases, assume p0, p1, pr are all within [0, 1] already.\n    self.wrong_pr = False\n    if self.rho is None:\n        # Determine by pr for each pair\n        if self.verbose:\n\n            def cond_backward(cond, p0, p1, pr):\n                if p0 &gt; 0:\n                    pr_bound = (p0 + p1 - 1, min(p0, p1))\n                    # check whether pr within bounds\n                    if pr &lt; pr_bound[0] or pr &gt; pr_bound[1]:\n                        self.wrong_pr = True\n                        pr = min(max(pr, pr_bound[0]), pr_bound[1])\n                    return pr / p0 if cond else (p1 - pr) / (1 - p0)\n                else:\n                    return p1\n        else:\n\n            def cond_backward(cond, p0, p1, pr):\n                if p0 &gt; 0:\n                    pr_bound = (p0 + p1 - 1, min(p0, p1))\n                    pr = min(max(pr, pr_bound[0]), pr_bound[1])\n                    return pr / p0 if cond else (p1 - pr) / (1 - p0)\n                else:\n                    return p1\n    elif self.rho == 0:\n        # Independent case\n        def cond_backward(cond, p0, p1, pr):\n            return p1\n    else:\n        # Dependent with fixed correlation coefficient rho\n        def cond_backward(cond, p0, p1, pr):\n            # Standard deviation of r.v. for p1\n            sd = ((1 - p1) * p1) ** 0.5\n            # Z-score of random variable for p0\n            zs = ((1 - p0) / p0) ** 0.5 if cond else -((p0 / (1 - p0)) ** 0.5)\n            return p1 + self.rho * sd * zs\n\n    self.cond_backward = cond_backward\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.ReciprocalConnector.setup_nodes","title":"<code>setup_nodes(source=None, target=None)</code>","text":"<p>Must run this before building connections</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def setup_nodes(self, source=None, target=None):\n    \"\"\"Must run this before building connections\"\"\"\n    if self.stage:\n        # check whether the correct populations\n        if (\n            source is None\n            or target is None\n            or not is_same_pop(source, self.target, quick=self.quick)\n            or not is_same_pop(target, self.source, quick=self.quick)\n        ):\n            raise ValueError(\"Source or target population not consistent.\")\n        # Skip adding nodes for the backward stage.\n        return\n\n    # Update node pools\n    self.source = source\n    self.target = target\n    if self.source is None or len(self.source) == 0:\n        src_str, trg_str = self.get_nodes_info()\n        raise ValueError(f\"{src_str} nodes do not exists\")\n    if self.target is None or len(self.target) == 0:\n        src_str, trg_str = self.get_nodes_info()\n        raise ValueError(f\"{trg_str} nodes do not exists\")\n\n    # Setup nodes\n    self.recurrent = is_same_pop(self.source, self.target, quick=self.quick)\n    self.source_ids = [s.node_id for s in self.source]\n    self.n_source = len(self.source_ids)\n    self.source_list = list(self.source)\n    if self.recurrent:\n        self.target_ids = self.source_ids\n        self.n_target = self.n_source\n        self.target_list = self.source_list\n    else:\n        self.target_ids = [t.node_id for t in self.target]\n        self.n_target = len(self.target_ids)\n        self.target_list = list(self.target)\n\n    # Setup for recurrent connection\n    if self.recurrent:\n        self.symmetric_p1_arg = True\n        self.symmetric_p1 = True\n        self.vars[\"n_syn1\"] = self.vars[\"n_syn0\"]\n    if self.symmetric_p1_arg:\n        self.vars[\"p1_arg\"] = self.vars[\"p0_arg\"]\n    if self.symmetric_p1:\n        self.vars[\"p1\"] = self.vars[\"p0\"]\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector","title":"<code>bmtool.connectors.UnidirectionConnector</code>","text":"<p>               Bases: <code>AbstractConnector</code></p> <p>Object for building unidirectional connections in bmtk network model with given probability within a single population (or between two populations).</p> <p>Parameters:     p, p_arg: Probability of forward connection and its input argument when         it is a function, similar to p0, p0_arg in ReciprocalConnector. It         can be a constant or a deterministic function whose value must be         within range [0, 1]. When p is constant, the connection is         homogenous.     n_syn: Number of synapses in the forward connection if connected. It         can be a constant or a (deterministic or random) function whose         input arguments are two node objects in BMTK like p_arg.     verbose: Whether show verbose information in console.</p> <p>Returns:     An object that works with BMTK to build edges in a network.</p> <p>Important attributes:     vars: Dictionary that stores part of the original input parameters.     source, target: NodePool objects for the source and target populations.     conn_prop: A dictionaries that stores properties of connected pairs.     Each key is the source node id and the value is a dictionary, where     each key is the target node id that the source node connects to, and     the value is the value of p_arg.         Example: {sid0: {tid0: p_arg0, tid1: p_arg1, ...},                   sid1: {...}, sid2: {...}, ... }         This is useful in similar manner as in ReciprocalConnector.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class UnidirectionConnector(AbstractConnector):\n    \"\"\"\n    Object for building unidirectional connections in bmtk network model with\n    given probability within a single population (or between two populations).\n\n    Parameters:\n        p, p_arg: Probability of forward connection and its input argument when\n            it is a function, similar to p0, p0_arg in ReciprocalConnector. It\n            can be a constant or a deterministic function whose value must be\n            within range [0, 1]. When p is constant, the connection is\n            homogenous.\n        n_syn: Number of synapses in the forward connection if connected. It\n            can be a constant or a (deterministic or random) function whose\n            input arguments are two node objects in BMTK like p_arg.\n        verbose: Whether show verbose information in console.\n\n    Returns:\n        An object that works with BMTK to build edges in a network.\n\n    Important attributes:\n        vars: Dictionary that stores part of the original input parameters.\n        source, target: NodePool objects for the source and target populations.\n        conn_prop: A dictionaries that stores properties of connected pairs.\n        Each key is the source node id and the value is a dictionary, where\n        each key is the target node id that the source node connects to, and\n        the value is the value of p_arg.\n            Example: {sid0: {tid0: p_arg0, tid1: p_arg1, ...},\n                      sid1: {...}, sid2: {...}, ... }\n            This is useful in similar manner as in ReciprocalConnector.\n    \"\"\"\n\n    def __init__(\n        self, p=1.0, p_arg=None, n_syn=1, verbose=True, save_report=True, report_name=None\n    ):\n        args = locals()\n        var_set = (\"p\", \"p_arg\", \"n_syn\")\n        self.vars = {key: args[key] for key in var_set}\n\n        self.verbose = verbose\n        self.save_report = save_report\n        if report_name is None:\n            report_name = globals().get(\"report_name\", \"default_report.csv\")\n        self.report_name = report_name\n\n        self.conn_prop = {}\n        self.iter_count = 0\n\n    # *** Two methods executed during bmtk edge creation net.add_edges() ***\n    def setup_nodes(self, source=None, target=None):\n        \"\"\"Must run this before building connections\"\"\"\n        # Update node pools\n        self.source = source\n        self.target = target\n        if self.source is None or len(self.source) == 0:\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(f\"{src_str} nodes do not exists\")\n        if self.target is None or len(self.target) == 0:\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(f\"{trg_str} nodes do not exists\")\n        self.n_pair = len(self.source) * len(self.target)\n\n    def edge_params(self):\n        \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n        params = {\n            \"source\": self.source,\n            \"target\": self.target,\n            \"iterator\": \"one_to_one\",\n            \"connection_rule\": self.make_connection,\n        }\n        return params\n\n    # *** Methods executed during bmtk network.build() ***\n    # *** Helper functions ***\n    def add_conn_prop(self, sid, tid, prop):\n        \"\"\"Store p0_arg and p1_arg for a connected pair\"\"\"\n        trg_dict = self.conn_prop.setdefault(sid, {})\n        trg_dict[tid] = prop\n\n    def get_conn_prop(self, sid, tid):\n        \"\"\"Get stored value given node ids in a connection\"\"\"\n        return self.conn_prop[sid][tid]\n\n    def setup_variables(self):\n        \"\"\"Make constant variables constant functions\"\"\"\n        for name, var in self.vars.items():\n            if callable(var):\n                setattr(self, name, var)\n            else:\n                setattr(self, name, self.constant_function(var))\n\n    def initialize(self):\n        self.setup_variables()\n        self.n_conn = 0\n        self.n_poss = 0\n        if self.verbose:\n            self.timer = Timer()\n\n    def make_connection(self, source, target, *args, **kwargs):\n        \"\"\"Assign number of synapses per iteration using one_to_one iterator\"\"\"\n        # Initialize in the first iteration\n        if self.iter_count == 0:\n            self.initialize()\n            if self.verbose:\n                src_str, trg_str = self.get_nodes_info()\n                print(\n                    \"\\nStart building connection \\n  from \" + src_str + \"\\n  to \" + trg_str,\n                    flush=True,\n                )\n\n        # Make random connections\n\n        p_arg = self.p_arg(source, target)\n        p = self.p(p_arg)\n        possible = p &gt; 0\n        self.n_poss += possible\n        if possible and decision(p):\n            nsyns = self.n_syn(source, target)\n            self.add_conn_prop(source.node_id, target.node_id, p_arg)\n            self.n_conn += 1\n        else:\n            nsyns = 0\n\n        self.iter_count += 1\n\n        # Detect end of iteration\n        if self.iter_count == self.n_pair:\n            if self.verbose:\n                self.connection_number_info()\n                self.timer.report(\"Done! \\nTime for building connections\")\n            if self.save_report:\n                self.save_connection_report()\n\n        return nsyns\n\n    # *** Helper functions for verbose ***\n    def get_nodes_info(self):\n        \"\"\"Get strings with source and target population information\"\"\"\n        source_str = self.source.network_name + \": \" + self.source.filter_str\n        target_str = self.target.network_name + \": \" + self.target.filter_str\n        return source_str, target_str\n\n    def connection_number_info(self):\n        \"\"\"Print connection numbers after connections built\"\"\"\n        print(\"Number of connected pairs: %d\" % self.n_conn, flush=True)\n        print(\"Number of possible connections: %d\" % self.n_poss, flush=True)\n        print(\n            \"Fraction of connected pairs in possible ones: %.2f%%\"\n            % (100.0 * self.n_conn / self.n_poss)\n            if self.n_poss\n            else 0.0\n        )\n        print(\"Number of total pairs: %d\" % self.n_pair, flush=True)\n        print(\n            \"Fraction of connected pairs in all pairs: %.2f%%\\n\"\n            % (100.0 * self.n_conn / self.n_pair),\n            flush=True,\n        )\n\n    def save_connection_report(self):\n        \"\"\"Save connections into a CSV file to be read from later\"\"\"\n        src_str, trg_str = self.get_nodes_info()\n\n        possible_fraction = 100.0 * self.n_conn / self.n_poss\n        all_fraction = 100.0 * self.n_conn / self.n_pair\n\n        # Extract the population name from source_str and target_str\n        data = {\n            \"Source\": [src_str],\n            \"Target\": [trg_str],\n            \"Percent connectionivity within possible connections\": [possible_fraction],\n            \"Percent connectionivity within all connections\": [all_fraction],\n        }\n        df = pd.DataFrame(data)\n\n        # Append the data to the CSV file\n        try:\n            # Check if the file exists by trying to read it\n            existing_df = pd.read_csv(self.report_name)\n            # If no exception is raised, append without header\n            df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n        except FileNotFoundError:\n            # If the file does not exist, write with header\n            df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.add_conn_prop","title":"<code>add_conn_prop(sid, tid, prop)</code>","text":"<p>Store p0_arg and p1_arg for a connected pair</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def add_conn_prop(self, sid, tid, prop):\n    \"\"\"Store p0_arg and p1_arg for a connected pair\"\"\"\n    trg_dict = self.conn_prop.setdefault(sid, {})\n    trg_dict[tid] = prop\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.connection_number_info","title":"<code>connection_number_info()</code>","text":"<p>Print connection numbers after connections built</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def connection_number_info(self):\n    \"\"\"Print connection numbers after connections built\"\"\"\n    print(\"Number of connected pairs: %d\" % self.n_conn, flush=True)\n    print(\"Number of possible connections: %d\" % self.n_poss, flush=True)\n    print(\n        \"Fraction of connected pairs in possible ones: %.2f%%\"\n        % (100.0 * self.n_conn / self.n_poss)\n        if self.n_poss\n        else 0.0\n    )\n    print(\"Number of total pairs: %d\" % self.n_pair, flush=True)\n    print(\n        \"Fraction of connected pairs in all pairs: %.2f%%\\n\"\n        % (100.0 * self.n_conn / self.n_pair),\n        flush=True,\n    )\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.edge_params","title":"<code>edge_params()</code>","text":"<p>Create the arguments for BMTK add_edges() method</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def edge_params(self):\n    \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n    params = {\n        \"source\": self.source,\n        \"target\": self.target,\n        \"iterator\": \"one_to_one\",\n        \"connection_rule\": self.make_connection,\n    }\n    return params\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.get_conn_prop","title":"<code>get_conn_prop(sid, tid)</code>","text":"<p>Get stored value given node ids in a connection</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def get_conn_prop(self, sid, tid):\n    \"\"\"Get stored value given node ids in a connection\"\"\"\n    return self.conn_prop[sid][tid]\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.get_nodes_info","title":"<code>get_nodes_info()</code>","text":"<p>Get strings with source and target population information</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def get_nodes_info(self):\n    \"\"\"Get strings with source and target population information\"\"\"\n    source_str = self.source.network_name + \": \" + self.source.filter_str\n    target_str = self.target.network_name + \": \" + self.target.filter_str\n    return source_str, target_str\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.make_connection","title":"<code>make_connection(source, target, *args, **kwargs)</code>","text":"<p>Assign number of synapses per iteration using one_to_one iterator</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_connection(self, source, target, *args, **kwargs):\n    \"\"\"Assign number of synapses per iteration using one_to_one iterator\"\"\"\n    # Initialize in the first iteration\n    if self.iter_count == 0:\n        self.initialize()\n        if self.verbose:\n            src_str, trg_str = self.get_nodes_info()\n            print(\n                \"\\nStart building connection \\n  from \" + src_str + \"\\n  to \" + trg_str,\n                flush=True,\n            )\n\n    # Make random connections\n\n    p_arg = self.p_arg(source, target)\n    p = self.p(p_arg)\n    possible = p &gt; 0\n    self.n_poss += possible\n    if possible and decision(p):\n        nsyns = self.n_syn(source, target)\n        self.add_conn_prop(source.node_id, target.node_id, p_arg)\n        self.n_conn += 1\n    else:\n        nsyns = 0\n\n    self.iter_count += 1\n\n    # Detect end of iteration\n    if self.iter_count == self.n_pair:\n        if self.verbose:\n            self.connection_number_info()\n            self.timer.report(\"Done! \\nTime for building connections\")\n        if self.save_report:\n            self.save_connection_report()\n\n    return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.save_connection_report","title":"<code>save_connection_report()</code>","text":"<p>Save connections into a CSV file to be read from later</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def save_connection_report(self):\n    \"\"\"Save connections into a CSV file to be read from later\"\"\"\n    src_str, trg_str = self.get_nodes_info()\n\n    possible_fraction = 100.0 * self.n_conn / self.n_poss\n    all_fraction = 100.0 * self.n_conn / self.n_pair\n\n    # Extract the population name from source_str and target_str\n    data = {\n        \"Source\": [src_str],\n        \"Target\": [trg_str],\n        \"Percent connectionivity within possible connections\": [possible_fraction],\n        \"Percent connectionivity within all connections\": [all_fraction],\n    }\n    df = pd.DataFrame(data)\n\n    # Append the data to the CSV file\n    try:\n        # Check if the file exists by trying to read it\n        existing_df = pd.read_csv(self.report_name)\n        # If no exception is raised, append without header\n        df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n    except FileNotFoundError:\n        # If the file does not exist, write with header\n        df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.setup_nodes","title":"<code>setup_nodes(source=None, target=None)</code>","text":"<p>Must run this before building connections</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def setup_nodes(self, source=None, target=None):\n    \"\"\"Must run this before building connections\"\"\"\n    # Update node pools\n    self.source = source\n    self.target = target\n    if self.source is None or len(self.source) == 0:\n        src_str, trg_str = self.get_nodes_info()\n        raise ValueError(f\"{src_str} nodes do not exists\")\n    if self.target is None or len(self.target) == 0:\n        src_str, trg_str = self.get_nodes_info()\n        raise ValueError(f\"{trg_str} nodes do not exists\")\n    self.n_pair = len(self.source) * len(self.target)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.UnidirectionConnector.setup_variables","title":"<code>setup_variables()</code>","text":"<p>Make constant variables constant functions</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def setup_variables(self):\n    \"\"\"Make constant variables constant functions\"\"\"\n    for name, var in self.vars.items():\n        if callable(var):\n            setattr(self, name, var)\n        else:\n            setattr(self, name, self.constant_function(var))\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GapJunction","title":"<code>bmtool.connectors.GapJunction</code>","text":"<p>               Bases: <code>UnidirectionConnector</code></p> <p>Object for buiilding gap junction connections in bmtk network model with given probabilities within a single population which is uncorrelated with the recurrent chemical synapses in this population.</p> <p>Parameters:     p, p_arg: Probability of forward connection and its input argument when         it is a function, similar to p0, p0_arg in ReciprocalConnector. It         can be a constant or a deterministic function whose value must be         within range [0, 1]. When p is constant, the connection is         homogenous.     verbose: Whether show verbose information in console.</p> <p>Returns:     An object that works with BMTK to build edges in a network.</p> <p>Important attributes:     Similar to <code>UnidirectionConnector</code>.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class GapJunction(UnidirectionConnector):\n    \"\"\"\n    Object for buiilding gap junction connections in bmtk network model with\n    given probabilities within a single population which is uncorrelated with\n    the recurrent chemical synapses in this population.\n\n    Parameters:\n        p, p_arg: Probability of forward connection and its input argument when\n            it is a function, similar to p0, p0_arg in ReciprocalConnector. It\n            can be a constant or a deterministic function whose value must be\n            within range [0, 1]. When p is constant, the connection is\n            homogenous.\n        verbose: Whether show verbose information in console.\n\n    Returns:\n        An object that works with BMTK to build edges in a network.\n\n    Important attributes:\n        Similar to `UnidirectionConnector`.\n    \"\"\"\n\n    def __init__(self, p=1.0, p_arg=None, verbose=True, save_report=True, report_name=None):\n        super().__init__(\n            p=p, p_arg=p_arg, verbose=verbose, save_report=save_report, report_name=None\n        )\n\n    def setup_nodes(self, source=None, target=None):\n        super().setup_nodes(source=source, target=target)\n        if len(self.source) != len(self.target):\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(\n                f\"Source and target must be the same for \"\n                f\"gap junction. Nodes are {src_str} and {trg_str}\"\n            )\n        self.n_source = len(self.source)\n\n    def make_connection(self, source, target, *args, **kwargs):\n        \"\"\"Assign gap junction per iteration using one_to_one iterator\"\"\"\n        # Initialize in the first iteration\n        if self.iter_count == 0:\n            self.initialize()\n            if self.verbose:\n                src_str, _ = self.get_nodes_info()\n                print(\"\\nStart building gap junction \\n  in \" + src_str, flush=True)\n\n        # Consider each pair only once\n        nsyns = 0\n        i, j = divmod(self.iter_count, self.n_source)\n        if i &lt; j:\n            p_arg = self.p_arg(source, target)\n            p = self.p(p_arg)\n            possible = p &gt; 0\n            self.n_poss += possible\n            if possible and decision(p):\n                nsyns = 1\n                sid, tid = source.node_id, target.node_id\n                self.add_conn_prop(sid, tid, p_arg)\n                self.add_conn_prop(tid, sid, p_arg)\n                self.n_conn += 1\n\n        self.iter_count += 1\n\n        # Detect end of iteration\n        if self.iter_count == self.n_pair:\n            if self.verbose:\n                self.connection_number_info()\n                self.timer.report(\"Done! \\nTime for building connections\")\n            if self.save_report:\n                self.save_connection_report()\n        return nsyns\n\n    def connection_number_info(self):\n        n_pair = self.n_pair\n        self.n_pair = (n_pair - len(self.source)) // 2\n        super().connection_number_info()\n        self.n_pair = n_pair\n\n    def save_connection_report(self):\n        \"\"\"Save connections into a CSV file to be read from later\"\"\"\n        src_str, trg_str = self.get_nodes_info()\n        n_pair = self.n_pair\n        fraction_0 = self.n_conn / self.n_poss if self.n_poss else 0.0\n        fraction_1 = self.n_conn / self.n_pair\n\n        # Convert fraction to percentage and prepare data for the DataFrame\n        data = {\n            \"Source\": [src_str + \"Gap\"],\n            \"Target\": [trg_str + \"Gap\"],\n            \"Percent connectionivity within possible connections\": [fraction_0 * 100],\n            \"Percent connectionivity within all connections\": [fraction_1 * 100],\n        }\n        df = pd.DataFrame(data)\n\n        # Append the data to the CSV file\n        try:\n            # Check if the file exists by trying to read it\n            existing_df = pd.read_csv(self.report_name)\n            # If no exception is raised, append without header\n            df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n        except FileNotFoundError:\n            # If the file does not exist, write with header\n            df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GapJunction.make_connection","title":"<code>make_connection(source, target, *args, **kwargs)</code>","text":"<p>Assign gap junction per iteration using one_to_one iterator</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_connection(self, source, target, *args, **kwargs):\n    \"\"\"Assign gap junction per iteration using one_to_one iterator\"\"\"\n    # Initialize in the first iteration\n    if self.iter_count == 0:\n        self.initialize()\n        if self.verbose:\n            src_str, _ = self.get_nodes_info()\n            print(\"\\nStart building gap junction \\n  in \" + src_str, flush=True)\n\n    # Consider each pair only once\n    nsyns = 0\n    i, j = divmod(self.iter_count, self.n_source)\n    if i &lt; j:\n        p_arg = self.p_arg(source, target)\n        p = self.p(p_arg)\n        possible = p &gt; 0\n        self.n_poss += possible\n        if possible and decision(p):\n            nsyns = 1\n            sid, tid = source.node_id, target.node_id\n            self.add_conn_prop(sid, tid, p_arg)\n            self.add_conn_prop(tid, sid, p_arg)\n            self.n_conn += 1\n\n    self.iter_count += 1\n\n    # Detect end of iteration\n    if self.iter_count == self.n_pair:\n        if self.verbose:\n            self.connection_number_info()\n            self.timer.report(\"Done! \\nTime for building connections\")\n        if self.save_report:\n            self.save_connection_report()\n    return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.GapJunction.save_connection_report","title":"<code>save_connection_report()</code>","text":"<p>Save connections into a CSV file to be read from later</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def save_connection_report(self):\n    \"\"\"Save connections into a CSV file to be read from later\"\"\"\n    src_str, trg_str = self.get_nodes_info()\n    n_pair = self.n_pair\n    fraction_0 = self.n_conn / self.n_poss if self.n_poss else 0.0\n    fraction_1 = self.n_conn / self.n_pair\n\n    # Convert fraction to percentage and prepare data for the DataFrame\n    data = {\n        \"Source\": [src_str + \"Gap\"],\n        \"Target\": [trg_str + \"Gap\"],\n        \"Percent connectionivity within possible connections\": [fraction_0 * 100],\n        \"Percent connectionivity within all connections\": [fraction_1 * 100],\n    }\n    df = pd.DataFrame(data)\n\n    # Append the data to the CSV file\n    try:\n        # Check if the file exists by trying to read it\n        existing_df = pd.read_csv(self.report_name)\n        # If no exception is raised, append without header\n        df.to_csv(self.report_name, mode=\"a\", header=False, index=False)\n    except FileNotFoundError:\n        # If the file does not exist, write with header\n        df.to_csv(self.report_name, mode=\"w\", header=True, index=False)\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.CorrelatedGapJunction","title":"<code>bmtool.connectors.CorrelatedGapJunction</code>","text":"<p>               Bases: <code>GapJunction</code></p> <p>Object for buiilding gap junction connections in bmtk network model with given probabilities within a single population which could be correlated with the recurrent chemical synapses in this population.</p> <p>Parameters:     p_non, p_uni, p_rec: Probabilities of gap junction connection for each         pair of cells given the following three conditions of chemical         synaptic connections between them, no connection, unidirectional,         and reciprocal, respectively. It can be a constant or a         deterministic function whose value must be within range [0, 1].     p_arg: Input argument for p_non, p_uni, or p_rec, when any of them is a         function, similar to p0_arg, p1_arg in ReciprocalConnector.     connector: Connector object used to generate the chemical synapses of         within this population, which contains the connection information         in its attribute <code>conn_prop</code>. So this connector should have         generated the chemical synapses before generating the gap junction.     verbose: Whether show verbose information in console.</p> <p>Returns:     An object that works with BMTK to build edges in a network.</p> <p>Important attributes:     Similar to <code>UnidirectionConnector</code>.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class CorrelatedGapJunction(GapJunction):\n    \"\"\"\n    Object for buiilding gap junction connections in bmtk network model with\n    given probabilities within a single population which could be correlated\n    with the recurrent chemical synapses in this population.\n\n    Parameters:\n        p_non, p_uni, p_rec: Probabilities of gap junction connection for each\n            pair of cells given the following three conditions of chemical\n            synaptic connections between them, no connection, unidirectional,\n            and reciprocal, respectively. It can be a constant or a\n            deterministic function whose value must be within range [0, 1].\n        p_arg: Input argument for p_non, p_uni, or p_rec, when any of them is a\n            function, similar to p0_arg, p1_arg in ReciprocalConnector.\n        connector: Connector object used to generate the chemical synapses of\n            within this population, which contains the connection information\n            in its attribute `conn_prop`. So this connector should have\n            generated the chemical synapses before generating the gap junction.\n        verbose: Whether show verbose information in console.\n\n    Returns:\n        An object that works with BMTK to build edges in a network.\n\n    Important attributes:\n        Similar to `UnidirectionConnector`.\n    \"\"\"\n\n    def __init__(\n        self,\n        p_non=1.0,\n        p_uni=1.0,\n        p_rec=1.0,\n        p_arg=None,\n        connector=None,\n        verbose=True,\n        save_report=True,\n        report_name=None,\n    ):\n        super().__init__(\n            p=p_non, p_arg=p_arg, verbose=verbose, save_report=save_report, report_name=None\n        )\n        self.vars[\"p_non\"] = self.vars.pop(\"p\")\n        self.vars[\"p_uni\"] = p_uni\n        self.vars[\"p_rec\"] = p_rec\n        self.connector = connector\n        conn_prop = connector.conn_prop\n        if isinstance(conn_prop, list):\n            conn_prop = conn_prop[0]\n        self.ref_conn_prop = conn_prop\n\n    def conn_exist(self, sid, tid):\n        trg_dict = self.ref_conn_prop.get(sid)\n        if trg_dict is not None and tid in trg_dict:\n            return True, trg_dict[tid]\n        else:\n            return False, None\n\n    def connection_type(self, sid, tid):\n        conn0, prop0 = self.conn_exist(sid, tid)\n        conn1, prop1 = self.conn_exist(tid, sid)\n        return conn0 + conn1, prop0 if conn0 else prop1\n\n    def initialize(self):\n        self.has_p_arg = self.vars[\"p_arg\"] is not None\n        if not self.has_p_arg:\n            var = self.connector.vars\n            self.vars[\"p_arg\"] = var.get(\"p_arg\", var.get(\"p0_arg\", None))\n        super().initialize()\n        self.ps = [self.p_non, self.p_uni, self.p_rec]\n\n    def make_connection(self, source, target, *args, **kwargs):\n        \"\"\"Assign gap junction per iteration using one_to_one iterator\"\"\"\n        # Initialize in the first iteration\n        if self.iter_count == 0:\n            self.initialize()\n            if self.verbose:\n                src_str, _ = self.get_nodes_info()\n                print(\"\\nStart building gap junction \\n  in \" + src_str, flush=True)\n\n        # Consider each pair only once\n        nsyns = 0\n        i, j = divmod(self.iter_count, self.n_source)\n        if i &lt; j:\n            sid, tid = source.node_id, target.node_id\n            conn_type, p_arg = self.connection_type(sid, tid)\n            if self.has_p_arg or not conn_type:\n                p_arg = self.p_arg(source, target)\n            p = self.ps[conn_type](p_arg)\n            possible = p &gt; 0\n            self.n_poss += possible\n            if possible and decision(p):\n                nsyns = 1\n                self.add_conn_prop(sid, tid, p_arg)\n                self.add_conn_prop(tid, sid, p_arg)\n                self.n_conn += 1\n\n        self.iter_count += 1\n\n        # Detect end of iteration\n        if self.iter_count == self.n_pair:\n            if self.verbose:\n                self.connection_number_info()\n                self.timer.report(\"Done! \\nTime for building connections\")\n            if self.save_report:\n                self.save_connection_report()\n        return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.CorrelatedGapJunction.make_connection","title":"<code>make_connection(source, target, *args, **kwargs)</code>","text":"<p>Assign gap junction per iteration using one_to_one iterator</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_connection(self, source, target, *args, **kwargs):\n    \"\"\"Assign gap junction per iteration using one_to_one iterator\"\"\"\n    # Initialize in the first iteration\n    if self.iter_count == 0:\n        self.initialize()\n        if self.verbose:\n            src_str, _ = self.get_nodes_info()\n            print(\"\\nStart building gap junction \\n  in \" + src_str, flush=True)\n\n    # Consider each pair only once\n    nsyns = 0\n    i, j = divmod(self.iter_count, self.n_source)\n    if i &lt; j:\n        sid, tid = source.node_id, target.node_id\n        conn_type, p_arg = self.connection_type(sid, tid)\n        if self.has_p_arg or not conn_type:\n            p_arg = self.p_arg(source, target)\n        p = self.ps[conn_type](p_arg)\n        possible = p &gt; 0\n        self.n_poss += possible\n        if possible and decision(p):\n            nsyns = 1\n            self.add_conn_prop(sid, tid, p_arg)\n            self.add_conn_prop(tid, sid, p_arg)\n            self.n_conn += 1\n\n    self.iter_count += 1\n\n    # Detect end of iteration\n    if self.iter_count == self.n_pair:\n        if self.verbose:\n            self.connection_number_info()\n            self.timer.report(\"Done! \\nTime for building connections\")\n        if self.save_report:\n            self.save_connection_report()\n    return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.OneToOneSequentialConnector","title":"<code>bmtool.connectors.OneToOneSequentialConnector</code>","text":"<p>               Bases: <code>AbstractConnector</code></p> <p>Object for buiilding one to one correspondence connections in bmtk network model with between two populations. One of the population can consist of multiple sub-populations. These sub-populations need to be added sequentially using setup_nodes() and edge_params() methods followed by BMTK add_edges() method. For example, to connect 30 nodes in population A to 30 nodes in populations B1, B2, B3, each with 10 nodes, set up as follows.     connector = OneToOneSequentialConnector(parameters)     connector.setup_nodes(source=A, target=B1)     net.add_edges(connector.edge_params())     connector.setup_nodes(target=B2)     net.add_edges(connector.edge_params())     connector.setup_nodes(target=B3)     net.add_edges(connector.edge_params()) After BMTK executes net.build(), the first 10 nodes in A will connect one- to-one to the 10 nodes in B1, then the 11 to 20-th nodes to those in B2, finally the 21 to 30-th nodes to those in B3. This connector is useful for creating input drives to a population. Each node in it receives an independent drive from a unique source node.</p> <p>Parameters:     n_syn: Number of synapses in each connection. It accepts only constant         for now.     partition_source: Whether the source population consists of multiple         sub-populations. By default, the source has one population, and the         target can have multiple sub-populations. If set to true, the         source can have multiple sub-populations and the target has only         one population.     verbose: Whether show verbose information in console.</p> <p>Returns:     An object that works with BMTK to build edges in a network.</p> <p>Important attributes:     source: NodePool object for the single population.     targets: List of NodePool objects for the multiple sub-populations.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>class OneToOneSequentialConnector(AbstractConnector):\n    \"\"\"Object for buiilding one to one correspondence connections in bmtk\n    network model with between two populations. One of the population can\n    consist of multiple sub-populations. These sub-populations need to be added\n    sequentially using setup_nodes() and edge_params() methods followed by BMTK\n    add_edges() method. For example, to connect 30 nodes in population A to 30\n    nodes in populations B1, B2, B3, each with 10 nodes, set up as follows.\n        connector = OneToOneSequentialConnector(**parameters)\n        connector.setup_nodes(source=A, target=B1)\n        net.add_edges(**connector.edge_params())\n        connector.setup_nodes(target=B2)\n        net.add_edges(**connector.edge_params())\n        connector.setup_nodes(target=B3)\n        net.add_edges(**connector.edge_params())\n    After BMTK executes net.build(), the first 10 nodes in A will connect one-\n    to-one to the 10 nodes in B1, then the 11 to 20-th nodes to those in B2,\n    finally the 21 to 30-th nodes to those in B3.\n    This connector is useful for creating input drives to a population. Each\n    node in it receives an independent drive from a unique source node.\n\n    Parameters:\n        n_syn: Number of synapses in each connection. It accepts only constant\n            for now.\n        partition_source: Whether the source population consists of multiple\n            sub-populations. By default, the source has one population, and the\n            target can have multiple sub-populations. If set to true, the\n            source can have multiple sub-populations and the target has only\n            one population.\n        verbose: Whether show verbose information in console.\n\n    Returns:\n        An object that works with BMTK to build edges in a network.\n\n    Important attributes:\n        source: NodePool object for the single population.\n        targets: List of NodePool objects for the multiple sub-populations.\n    \"\"\"\n\n    def __init__(self, n_syn=1, partition_source=False, verbose=True):\n        self.n_syn = int(n_syn)\n        self.partition_source = partition_source\n        self.verbose = verbose\n\n        self.targets = []\n        self.n_source = 0\n        self.idx_range = [0]\n        self.target_count = 0\n        self.iter_count = 0\n\n    # *** Two methods executed during bmtk edge creation net.add_edges() ***\n    def setup_nodes(self, source=None, target=None):\n        \"\"\"Must run this before building connections\"\"\"\n        # Update node pools\n        if self.partition_source:\n            source, target = target, source\n        if self.target_count == 0:\n            if source is None or len(source) == 0:\n                src_str, trg_str = self.get_nodes_info()\n                raise ValueError(\n                    (f\"{trg_str}\" if self.partition_source else f\"{src_str}\")\n                    + \" nodes do not exists\"\n                )\n            self.source = source\n            self.n_source = len(source)\n        if target is None or len(target) == 0:\n            raise ValueError(\n                (\"Source\" if self.partition_source else \"Target\") + \" nodes do not exists\"\n            )\n\n        self.targets.append(target)\n        self.idx_range.append(self.idx_range[-1] + len(target))\n        self.target_count += 1\n\n        if self.idx_range[-1] &gt; self.n_source:\n            if self.partition_source:\n                raise ValueError(\n                    \"Total target populations exceed the source population.\"\n                    if self.partition_source\n                    else \"Total source populations exceed the target population.\"\n                )\n\n        if self.verbose and self.idx_range[-1] == self.n_source:\n            print(\n                \"All \"\n                + (\"source\" if self.partition_source else \"target\")\n                + \" population partitions are filled.\",\n                flush=True,\n            )\n\n    def edge_params(self, target_pop_idx=-1):\n        \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n        if self.partition_source:\n            params = {\n                \"source\": self.targets[target_pop_idx],\n                \"target\": self.source,\n                \"iterator\": \"one_to_all\",\n            }\n        else:\n            params = {\n                \"source\": self.source,\n                \"target\": self.targets[target_pop_idx],\n                \"iterator\": \"all_to_one\",\n            }\n        params[\"connection_rule\"] = self.make_connection\n        return params\n\n    # *** Methods executed during bmtk network.build() ***\n    def make_connection(self, source, targets, *args, **kwargs):\n        \"\"\"Assign one connection per iteration using all_to_one iterator\"\"\"\n        # Initialize in the first iteration\n        if self.verbose:\n            if self.iter_count == 0:\n                # Very beginning\n                self.target_count = 0\n                src_str, trg_str = self.get_nodes_info()\n                print(\n                    \"\\nStart building connection \"\n                    + (\"to \" if self.partition_source else \"from \")\n                    + src_str,\n                    flush=True,\n                )\n                self.timer = Timer()\n\n            if self.iter_count == self.idx_range[self.target_count]:\n                # Beginning of each target population\n                src_str, trg_str = self.get_nodes_info(self.target_count)\n                print(\n                    (\"  %d. \" % self.target_count)\n                    + (\"from \" if self.partition_source else \"to \")\n                    + trg_str,\n                    flush=True,\n                )\n                self.target_count += 1\n                self.timer_part = Timer()\n\n        # Make connection\n        nsyns = np.zeros(self.n_source, dtype=int)\n        nsyns[self.iter_count] = self.n_syn\n        self.iter_count += 1\n\n        # Detect end of iteration\n        if self.verbose:\n            if self.iter_count == self.idx_range[self.target_count]:\n                # End of each target population\n                self.timer_part.report(\"    Time for this partition\")\n            if self.iter_count == self.n_source:\n                # Very end\n                self.timer.report(\"Done! \\nTime for building connections\")\n        return nsyns\n\n    # *** Helper functions for verbose ***\n    def get_nodes_info(self, target_pop_idx=-1):\n        \"\"\"Get strings with source and target population information\"\"\"\n        target = self.targets[target_pop_idx]\n        source_str = self.source.network_name + \": \" + self.source.filter_str\n        target_str = target.network_name + \": \" + target.filter_str\n        return source_str, target_str\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.OneToOneSequentialConnector.edge_params","title":"<code>edge_params(target_pop_idx=-1)</code>","text":"<p>Create the arguments for BMTK add_edges() method</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def edge_params(self, target_pop_idx=-1):\n    \"\"\"Create the arguments for BMTK add_edges() method\"\"\"\n    if self.partition_source:\n        params = {\n            \"source\": self.targets[target_pop_idx],\n            \"target\": self.source,\n            \"iterator\": \"one_to_all\",\n        }\n    else:\n        params = {\n            \"source\": self.source,\n            \"target\": self.targets[target_pop_idx],\n            \"iterator\": \"all_to_one\",\n        }\n    params[\"connection_rule\"] = self.make_connection\n    return params\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.OneToOneSequentialConnector.get_nodes_info","title":"<code>get_nodes_info(target_pop_idx=-1)</code>","text":"<p>Get strings with source and target population information</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def get_nodes_info(self, target_pop_idx=-1):\n    \"\"\"Get strings with source and target population information\"\"\"\n    target = self.targets[target_pop_idx]\n    source_str = self.source.network_name + \": \" + self.source.filter_str\n    target_str = target.network_name + \": \" + target.filter_str\n    return source_str, target_str\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.OneToOneSequentialConnector.make_connection","title":"<code>make_connection(source, targets, *args, **kwargs)</code>","text":"<p>Assign one connection per iteration using all_to_one iterator</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def make_connection(self, source, targets, *args, **kwargs):\n    \"\"\"Assign one connection per iteration using all_to_one iterator\"\"\"\n    # Initialize in the first iteration\n    if self.verbose:\n        if self.iter_count == 0:\n            # Very beginning\n            self.target_count = 0\n            src_str, trg_str = self.get_nodes_info()\n            print(\n                \"\\nStart building connection \"\n                + (\"to \" if self.partition_source else \"from \")\n                + src_str,\n                flush=True,\n            )\n            self.timer = Timer()\n\n        if self.iter_count == self.idx_range[self.target_count]:\n            # Beginning of each target population\n            src_str, trg_str = self.get_nodes_info(self.target_count)\n            print(\n                (\"  %d. \" % self.target_count)\n                + (\"from \" if self.partition_source else \"to \")\n                + trg_str,\n                flush=True,\n            )\n            self.target_count += 1\n            self.timer_part = Timer()\n\n    # Make connection\n    nsyns = np.zeros(self.n_source, dtype=int)\n    nsyns[self.iter_count] = self.n_syn\n    self.iter_count += 1\n\n    # Detect end of iteration\n    if self.verbose:\n        if self.iter_count == self.idx_range[self.target_count]:\n            # End of each target population\n            self.timer_part.report(\"    Time for this partition\")\n        if self.iter_count == self.n_source:\n            # Very end\n            self.timer.report(\"Done! \\nTime for building connections\")\n    return nsyns\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.OneToOneSequentialConnector.setup_nodes","title":"<code>setup_nodes(source=None, target=None)</code>","text":"<p>Must run this before building connections</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def setup_nodes(self, source=None, target=None):\n    \"\"\"Must run this before building connections\"\"\"\n    # Update node pools\n    if self.partition_source:\n        source, target = target, source\n    if self.target_count == 0:\n        if source is None or len(source) == 0:\n            src_str, trg_str = self.get_nodes_info()\n            raise ValueError(\n                (f\"{trg_str}\" if self.partition_source else f\"{src_str}\")\n                + \" nodes do not exists\"\n            )\n        self.source = source\n        self.n_source = len(source)\n    if target is None or len(target) == 0:\n        raise ValueError(\n            (\"Source\" if self.partition_source else \"Target\") + \" nodes do not exists\"\n        )\n\n    self.targets.append(target)\n    self.idx_range.append(self.idx_range[-1] + len(target))\n    self.target_count += 1\n\n    if self.idx_range[-1] &gt; self.n_source:\n        if self.partition_source:\n            raise ValueError(\n                \"Total target populations exceed the source population.\"\n                if self.partition_source\n                else \"Total source populations exceed the target population.\"\n            )\n\n    if self.verbose and self.idx_range[-1] == self.n_source:\n        print(\n            \"All \"\n            + (\"source\" if self.partition_source else \"target\")\n            + \" population partitions are filled.\",\n            flush=True,\n        )\n</code></pre>"},{"location":"api/connectors/#synapse-helper-functions","title":"Synapse Helper Functions","text":""},{"location":"api/connectors/#bmtool.connectors.syn_const_delay","title":"<code>bmtool.connectors.syn_const_delay(source=None, target=None, dist=100, min_delay=SYN_MIN_DELAY, velocity=SYN_VELOCITY, fluc_stdev=FLUC_STDEV, delay_bound=(DELAY_LOWBOUND, DELAY_UPBOUND), connector=None)</code>","text":"<p>Synapse delay constant with some random fluctuation.</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_const_delay(\n    source=None,\n    target=None,\n    dist=100,\n    min_delay=SYN_MIN_DELAY,\n    velocity=SYN_VELOCITY,\n    fluc_stdev=FLUC_STDEV,\n    delay_bound=(DELAY_LOWBOUND, DELAY_UPBOUND),\n    connector=None,\n):\n    \"\"\"Synapse delay constant with some random fluctuation.\"\"\"\n    del_fluc = fluc_stdev * rng.normal()\n    delay = dist / SYN_VELOCITY + SYN_MIN_DELAY + del_fluc\n    delay = min(max(delay, DELAY_LOWBOUND), DELAY_UPBOUND)\n    return delay\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.syn_dist_delay_feng","title":"<code>bmtool.connectors.syn_dist_delay_feng(source, target, min_delay=SYN_MIN_DELAY, velocity=SYN_VELOCITY, fluc_stdev=FLUC_STDEV, delay_bound=(DELAY_LOWBOUND, DELAY_UPBOUND), connector=None)</code>","text":"<p>Synpase delay linearly dependent on distance. min_delay: minimum delay (ms) velocity: synapse conduction velocity (micron/ms) fluc_stdev: standard deviation of random Gaussian fluctuation (ms) delay_bound: (lower, upper) bounds of delay (ms) connector: connector object from which to read distance</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_dist_delay_feng(\n    source,\n    target,\n    min_delay=SYN_MIN_DELAY,\n    velocity=SYN_VELOCITY,\n    fluc_stdev=FLUC_STDEV,\n    delay_bound=(DELAY_LOWBOUND, DELAY_UPBOUND),\n    connector=None,\n):\n    \"\"\"Synpase delay linearly dependent on distance.\n    min_delay: minimum delay (ms)\n    velocity: synapse conduction velocity (micron/ms)\n    fluc_stdev: standard deviation of random Gaussian fluctuation (ms)\n    delay_bound: (lower, upper) bounds of delay (ms)\n    connector: connector object from which to read distance\n    \"\"\"\n    if connector is None:\n        dist = euclid_dist(target[\"positions\"], source[\"positions\"])\n    else:\n        dist = connector.get_conn_prop(source.node_id, target.node_id)\n    del_fluc = fluc_stdev * rng.normal()\n    delay = dist / velocity + min_delay + del_fluc\n    delay = min(max(delay, delay_bound[0]), delay_bound[1])\n    return delay\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.syn_section_PN","title":"<code>bmtool.connectors.syn_section_PN(source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs)</code>","text":"<p>Synapse location follows a Bernoulli distribution, with probability p to obtain the former in sec_id and sec_x</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_section_PN(source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs):\n    \"\"\"Synapse location follows a Bernoulli distribution, with probability p\n    to obtain the former in sec_id and sec_x\"\"\"\n    syn_loc = int(not decision(p))\n    return sec_id[syn_loc], sec_x[syn_loc]\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.syn_const_delay_feng_section_PN","title":"<code>bmtool.connectors.syn_const_delay_feng_section_PN(source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs)</code>","text":"<p>Assign both synapse delay and location with constant distance assumed</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_const_delay_feng_section_PN(\n    source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs\n):\n    \"\"\"Assign both synapse delay and location with constant distance assumed\"\"\"\n    delay = syn_const_delay(source, target, **kwargs)\n    s_id, s_x = syn_section_PN(source, target, p=p, sec_id=sec_id, sec_x=sec_x)\n    return delay, s_id, s_x\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.syn_dist_delay_feng_section_PN","title":"<code>bmtool.connectors.syn_dist_delay_feng_section_PN(source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs)</code>","text":"<p>Assign both synapse delay and location</p> Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_dist_delay_feng_section_PN(\n    source, target, p=0.9, sec_id=(1, 2), sec_x=(0.4, 0.6), **kwargs\n):\n    \"\"\"Assign both synapse delay and location\"\"\"\n    delay = syn_dist_delay_feng(source, target, **kwargs)\n    s_id, s_x = syn_section_PN(source, target, p=p, sec_id=sec_id, sec_x=sec_x)\n    return delay, s_id, s_x\n</code></pre>"},{"location":"api/connectors/#bmtool.connectors.syn_uniform_delay_section","title":"<code>bmtool.connectors.syn_uniform_delay_section(source, target, low=DELAY_LOWBOUND, high=DELAY_UPBOUND, **kwargs)</code>","text":"Source code in <code>bmtool/connectors.py</code> <pre><code>def syn_uniform_delay_section(source, target, low=DELAY_LOWBOUND, high=DELAY_UPBOUND, **kwargs):\n    return rng.uniform(low, high)\n</code></pre>"},{"location":"api/graphs/","title":"Graphs API Reference","text":"<p>This page provides API reference documentation for the Graphs module, which contains functions for network graph analysis and visualization.</p>"},{"location":"api/graphs/#graph-generation-functions","title":"Graph Generation Functions","text":""},{"location":"api/graphs/#bmtool.graphs.generate_graph","title":"<code>bmtool.graphs.generate_graph(config, source, target)</code>","text":"<p>Generate a NetworkX graph from BMTK network configuration.</p> Parameters: <p>config : str     Path to a BMTK simulation config file. source : str     Network name for source nodes. target : str     Network name for target nodes.</p> Returns: <p>nx.DiGraph     A directed graph representing the network with nodes containing     position and population information.</p> Source code in <code>bmtool/graphs.py</code> <pre><code>def generate_graph(config, source, target):\n    \"\"\"\n    Generate a NetworkX graph from BMTK network configuration.\n\n    Parameters:\n    -----------\n    config : str\n        Path to a BMTK simulation config file.\n    source : str\n        Network name for source nodes.\n    target : str\n        Network name for target nodes.\n\n    Returns:\n    --------\n    nx.DiGraph\n        A directed graph representing the network with nodes containing\n        position and population information.\n    \"\"\"\n    nodes, edges = u.load_nodes_edges_from_config(config)\n    nodes_source = nodes[source]\n    nodes_target = nodes[target]\n    if source != target:\n        # Concatenate the DataFrames if source and target are different nodes\n        nodes = pd.concat([nodes_source, nodes_target])\n    else:\n        nodes = nodes[source]\n    edge_to_grap = source + \"_to_\" + target\n    edges = edges[edge_to_grap]\n\n    # Create an empty graph\n    G = nx.DiGraph()\n\n    # Add nodes to the graph with their positions and labels\n    for index, node_data in nodes.iterrows():\n        G.add_node(\n            index,\n            pos=(node_data[\"pos_x\"], node_data[\"pos_y\"], node_data[\"pos_z\"]),\n            label=node_data[\"pop_name\"],\n        )\n\n    # Add edges to the graph\n    for _, row in edges.iterrows():\n        G.add_edge(row[\"source_node_id\"], row[\"target_node_id\"])\n\n    return G\n</code></pre>"},{"location":"api/graphs/#graph-export-functions","title":"Graph Export Functions","text":""},{"location":"api/graphs/#bmtool.graphs.export_node_connections_to_csv","title":"<code>bmtool.graphs.export_node_connections_to_csv(Graph, filename)</code>","text":"<p>Generate a CSV file with node type and all incoming connections that node has.</p> Parameters: <p>Graph : nx.DiGraph     A directed graph object from NetworkX. filename : str     Path and filename for the output CSV file (must end in .csv).</p> Returns: <p>None     The function saves the results to the specified CSV file.</p> Notes: <p>The resulting CSV file will have the node label as the first column, followed by columns for each type of incoming connection.</p> Source code in <code>bmtool/graphs.py</code> <pre><code>def export_node_connections_to_csv(Graph, filename):\n    \"\"\"\n    Generate a CSV file with node type and all incoming connections that node has.\n\n    Parameters:\n    -----------\n    Graph : nx.DiGraph\n        A directed graph object from NetworkX.\n    filename : str\n        Path and filename for the output CSV file (must end in .csv).\n\n    Returns:\n    --------\n    None\n        The function saves the results to the specified CSV file.\n\n    Notes:\n    ------\n    The resulting CSV file will have the node label as the first column,\n    followed by columns for each type of incoming connection.\n    \"\"\"\n    # Create an empty dictionary to store the connections for each node\n    node_connections = {}\n\n    # Iterate over each node in the graph\n    for node in Graph.nodes():\n        # Initialize a dictionary to store the outgoing connections for the current node\n        connections = {}\n        node_label = Graph.nodes[node][\"label\"]\n\n        # Iterate over each presuccessor (ingoing neighbor) of the current node\n        for successor in Graph.predecessors(node):\n            # Get the label of the successor node\n            successor_label = Graph.nodes[successor][\"label\"]\n\n            # Increment the connection count for the current node and successor label\n            connections[f\"{successor_label} incoming Connections\"] = (\n                connections.get(f\"{successor_label} incoming Connections\", 0) + 1\n            )\n\n        # Add the connections information for the current node to the dictionary\n        connections[\"Node Label\"] = node_label\n        node_connections[node] = connections\n\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(node_connections).fillna(0).T\n\n    # Reorder columns so that 'Node Label' is the leftmost column\n    cols = df.columns.tolist()\n    cols = [\"Node Label\"] + [col for col in cols if col != \"Node Label\"]\n    df = df[cols]\n\n    # Write the DataFrame to a CSV file\n    df.to_csv(filename)\n</code></pre>"},{"location":"api/singlecell/","title":"Single Cell API Reference","text":"<p>This page provides API reference documentation for the Single Cell module, which contains functions and classes for working with individual neuron models.</p>"},{"location":"api/singlecell/#utility-functions","title":"Utility Functions","text":""},{"location":"api/singlecell/#bmtool.singlecell.load_biophys1","title":"<code>bmtool.singlecell.load_biophys1()</code>","text":"<p>Load the Biophys1 template from BMTK if it hasn't been loaded yet.</p> <p>This function checks if the Biophys1 object exists in NEURON's h namespace. If not, it loads the necessary HOC files for Allen Cell Types Database models.</p> Notes: <p>This is primarily used for working with cell models from the Allen Cell Types Database.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def load_biophys1():\n    \"\"\"\n    Load the Biophys1 template from BMTK if it hasn't been loaded yet.\n\n    This function checks if the Biophys1 object exists in NEURON's h namespace.\n    If not, it loads the necessary HOC files for Allen Cell Types Database models.\n\n    Notes:\n    ------\n    This is primarily used for working with cell models from the Allen Cell Types Database.\n    \"\"\"\n    if not hasattr(h, \"Biophys1\"):\n        from bmtk import utils\n\n        module_dir = os.path.dirname(os.path.abspath(utils.__file__))\n        hoc_file = os.path.join(module_dir, \"scripts\", \"bionet\", \"templates\", \"Biophys1.hoc\")\n        h.load_file(\"import3d.hoc\")\n        h.load_file(hoc_file)\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.load_allen_database_cells","title":"<code>bmtool.singlecell.load_allen_database_cells(morphology, dynamic_params, model_processing='aibs_perisomatic')</code>","text":"<p>Create a cell model from the Allen Cell Types Database.</p> Parameters: <p>morphology : str     Path to the morphology file (SWC or ASC format). dynamic_params : str     Path to the JSON file containing biophysical parameters. model_processing : str, optional     Model processing type from the AllenCellType database.     Default is 'aibs_perisomatic'.</p> Returns: <p>callable     A function that, when called, creates and returns a NEURON cell object     with the specified morphology and biophysical properties.</p> Notes: <p>This function creates a closure that loads and returns a cell when called. The cell is created using the Allen Institute's modeling framework.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def load_allen_database_cells(morphology, dynamic_params, model_processing=\"aibs_perisomatic\"):\n    \"\"\"\n    Create a cell model from the Allen Cell Types Database.\n\n    Parameters:\n    -----------\n    morphology : str\n        Path to the morphology file (SWC or ASC format).\n    dynamic_params : str\n        Path to the JSON file containing biophysical parameters.\n    model_processing : str, optional\n        Model processing type from the AllenCellType database.\n        Default is 'aibs_perisomatic'.\n\n    Returns:\n    --------\n    callable\n        A function that, when called, creates and returns a NEURON cell object\n        with the specified morphology and biophysical properties.\n\n    Notes:\n    ------\n    This function creates a closure that loads and returns a cell when called.\n    The cell is created using the Allen Institute's modeling framework.\n    \"\"\"\n    from bmtk.simulator.bionet.default_setters import cell_models\n\n    load_biophys1()\n    model_processing = getattr(cell_models, model_processing)\n    with open(dynamic_params) as f:\n        dynamics_params = json.load(f)\n\n    def create_cell():\n        hobj = h.Biophys1(morphology)\n        hobj = model_processing(hobj, cell=None, dynamics_params=dynamics_params)\n        return hobj\n\n    return create_cell\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.get_target_site","title":"<code>bmtool.singlecell.get_target_site(cell, sec=('soma', 0), loc=0.5, site='')</code>","text":"<p>Get a segment and its section from a cell model using flexible section specification.</p> Parameters: <p>cell : NEURON cell object     The cell object to access sections from. sec : str, int, or tuple, optional     Section specification, which can be:     - str: Section name (defaults to index 0 if multiple sections)     - int: Index into the 'all' section list     - tuple: (section_name, index) for accessing indexed sections     Default is ('soma', 0). loc : float, optional     Location along the section (0-1), default is 0.5 (middle of section). site : str, optional     Name of the site for error messages (e.g., 'injection', 'recording').</p> Returns: <p>tuple     (segment, section) at the specified location</p> Raises: <p>ValueError     If the section cannot be found or accessed.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def get_target_site(cell, sec=(\"soma\", 0), loc=0.5, site=\"\"):\n    \"\"\"\n    Get a segment and its section from a cell model using flexible section specification.\n\n    Parameters:\n    -----------\n    cell : NEURON cell object\n        The cell object to access sections from.\n    sec : str, int, or tuple, optional\n        Section specification, which can be:\n        - str: Section name (defaults to index 0 if multiple sections)\n        - int: Index into the 'all' section list\n        - tuple: (section_name, index) for accessing indexed sections\n        Default is ('soma', 0).\n    loc : float, optional\n        Location along the section (0-1), default is 0.5 (middle of section).\n    site : str, optional\n        Name of the site for error messages (e.g., 'injection', 'recording').\n\n    Returns:\n    --------\n    tuple\n        (segment, section) at the specified location\n\n    Raises:\n    -------\n    ValueError\n        If the section cannot be found or accessed.\n    \"\"\"\n    if isinstance(sec, str):\n        sec = (sec, 0)\n    elif isinstance(sec, int):\n        if not hasattr(cell, \"all\"):\n            raise ValueError(\"Section list named 'all' does not exist in the template.\")\n        sec = (\"all\", sec)\n    loc = float(loc)\n    try:\n        section = next(s for i, s in enumerate(getattr(cell, sec[0])) if i == sec[1])\n        seg = section(loc)\n    except Exception as e0:\n        try:\n            section = eval(\"cell.\" + sec[0])\n            seg = section(loc)\n        except Exception as e:\n            print(e0)\n            print(e)\n            raise ValueError(\"Hint: Are you selecting the correct \" + site + \" location?\")\n    return seg, section\n</code></pre>"},{"location":"api/singlecell/#current-clamp","title":"Current Clamp","text":""},{"location":"api/singlecell/#bmtool.singlecell.CurrentClamp","title":"<code>bmtool.singlecell.CurrentClamp</code>","text":"<p>               Bases: <code>SimulationBase</code></p> Source code in <code>bmtool/singlecell.py</code> <pre><code>class CurrentClamp(SimulationBase):\n    def __init__(\n        self,\n        cell_or_template,\n        post_init_function=None,\n        record_sec=\"soma\",\n        record_loc=0.5,\n        threshold=None,\n        inj_sec=\"soma\",\n        inj_loc=0.5,\n        inj_amp=100.0,\n        inj_delay=100.0,\n        inj_dur=1000.0,\n        tstop=1000.0,\n    ):\n        \"\"\"\n        Initialize a current clamp simulation environment.\n\n        Parameters:\n        -----------\n        cell_or_template : NEURON cell object or str/callable\n            Either a pre-initialized NEURON cell object (NEW API),\n            or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API).\n            The new API (passing a cell object) is recommended for better control and reusability.\n        post_init_function : str, optional\n            Function of the cell to be called after initialization. Only used with legacy API.\n        record_sec : str, int, or tuple, optional\n            Section to record from. Can be:\n            - str: Section name (defaults to index 0 if multiple sections)\n            - int: Index into the 'all' section list\n            - tuple: (section_name, index) for accessing indexed sections\n            Default is 'soma'.\n        record_loc : float, optional\n            Location (0-1) within section to record from. Default is 0.5.\n        threshold : float, optional\n            Spike threshold (mV). If specified, spikes are detected and counted.\n        inj_sec : str, int, or tuple, optional\n            Section for current injection. Same format as record_sec. Default is 'soma'.\n        inj_loc : float, optional\n            Location (0-1) within section for current injection. Default is 0.5.\n        inj_amp : float, optional\n            Current injection amplitude (pA). Default is 100.0.\n        inj_delay : float, optional\n            Start time for current injection (ms). Default is 100.0.\n        inj_dur : float, optional\n            Duration of current injection (ms). Default is 1000.0.\n        tstop : float, optional\n            Total simulation time (ms). Default is 1000.0.\n            Will be extended if necessary to include the full current injection.\n\n        Notes:\n        ------\n        Two approaches are supported:\n\n        1. Pass a pre-built cell object:\n            cell = SimpleSoma()  # or any other cell\n            cc = CurrentClamp(cell, inj_amp=100)\n            t, v = cc.run()\n\n        2. Pass a template name (string) loaded into NEURON:\n            cc = CurrentClamp('CP_Cell', inj_amp=100)\n            t, v = cc.run()\n        \"\"\"\n        # Detect whether we have a cell object or a template name\n        if isinstance(cell_or_template, str) or (\n            callable(cell_or_template) and not hasattr(cell_or_template, \"soma\")\n        ):\n            # Legacy API: create cell from template\n            create_cell = (\n                getattr(h, cell_or_template)\n                if isinstance(cell_or_template, str)\n                else cell_or_template\n            )\n            if callable(cell_or_template):\n                cell = cell_or_template()\n            else:\n                cell = create_cell()\n            if post_init_function:\n                eval(f\"cell.{post_init_function}\")\n        else:\n            # New API: cell object already provided\n            cell = cell_or_template\n\n        # Initialize base class\n        super().__init__(\n            cell=cell,\n            record_sec=record_sec,\n            record_loc=record_loc,\n            inj_sec=inj_sec,\n            inj_loc=inj_loc,\n            threshold=threshold,\n        )\n\n        self.tstop = max(tstop, inj_delay + inj_dur)\n        self.inj_delay = inj_delay\n        self.inj_dur = inj_dur\n        self.inj_amp = inj_amp * 1e-3  # pA to nA\n        self.cell_src = None\n\n        self._setup_experiment()\n\n    def _setup_experiment(self):\n        \"\"\"\n        Set up the simulation environment for current clamp experiments.\n\n        This method:\n        1. Creates the current clamp stimulus at the specified injection site\n        2. Sets up voltage recording at the specified recording site\n        3. Creates vectors to store time and voltage data\n\n        Notes:\n        ------\n        Sets self.cell_src as the current clamp object that can be accessed later.\n        \"\"\"\n        self.inj_seg, _ = get_target_site(self.cell, self.inj_sec, self.inj_loc, \"injection\")\n        self.cell_src = h.IClamp(self.inj_seg)\n        self.cell_src.delay = self.inj_delay\n        self.cell_src.dur = self.inj_dur\n        self.cell_src.amp = self.inj_amp\n\n        self.rec_seg, self.rec_sec = get_target_site(\n            self.cell, self.record_sec, self.record_loc, \"recording\"\n        )\n        self._create_recording_vectors()\n        self._setup_spike_detection()\n\n        print(f\"Injection location: {self.inj_seg}\")\n        print(f\"Recording: {self.rec_seg}._ref_v\")\n\n    def run(self) -&gt; Tuple[list, list]:\n        \"\"\"\n        Run the current clamp simulation and return recorded data.\n\n        This method:\n        1. Initializes the simulation duration\n        2. Runs the NEURON simulation\n        3. Converts recorded vectors to Python lists\n        4. Detects spikes if threshold was specified\n\n        Returns:\n        --------\n        tuple\n            (time_vector, voltage_vector) where:\n            - time_vector: List of time points (ms)\n            - voltage_vector: List of membrane potentials (mV) at those time points\n        \"\"\"\n        print(\"Current clamp simulation running...\")\n        h.tstop = self.tstop\n        h.finitialize(h.v_init)\n        h.continuerun(self.tstop)\n\n        if self.threshold is not None:\n            self.nspks = len(self.tspk_vec)\n            print()\n            print(f\"Number of spikes: {self.nspks:d}\")\n            print()\n\n        return self._convert_vectors_to_python()\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.CurrentClamp.__init__","title":"<code>__init__(cell_or_template, post_init_function=None, record_sec='soma', record_loc=0.5, threshold=None, inj_sec='soma', inj_loc=0.5, inj_amp=100.0, inj_delay=100.0, inj_dur=1000.0, tstop=1000.0)</code>","text":"<p>Initialize a current clamp simulation environment.</p> Parameters: <p>cell_or_template : NEURON cell object or str/callable     Either a pre-initialized NEURON cell object (NEW API),     or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API).     The new API (passing a cell object) is recommended for better control and reusability. post_init_function : str, optional     Function of the cell to be called after initialization. Only used with legacy API. record_sec : str, int, or tuple, optional     Section to record from. Can be:     - str: Section name (defaults to index 0 if multiple sections)     - int: Index into the 'all' section list     - tuple: (section_name, index) for accessing indexed sections     Default is 'soma'. record_loc : float, optional     Location (0-1) within section to record from. Default is 0.5. threshold : float, optional     Spike threshold (mV). If specified, spikes are detected and counted. inj_sec : str, int, or tuple, optional     Section for current injection. Same format as record_sec. Default is 'soma'. inj_loc : float, optional     Location (0-1) within section for current injection. Default is 0.5. inj_amp : float, optional     Current injection amplitude (pA). Default is 100.0. inj_delay : float, optional     Start time for current injection (ms). Default is 100.0. inj_dur : float, optional     Duration of current injection (ms). Default is 1000.0. tstop : float, optional     Total simulation time (ms). Default is 1000.0.     Will be extended if necessary to include the full current injection.</p> Notes: <p>Two approaches are supported:</p> <ol> <li> <p>Pass a pre-built cell object:     cell = SimpleSoma()  # or any other cell     cc = CurrentClamp(cell, inj_amp=100)     t, v = cc.run()</p> </li> <li> <p>Pass a template name (string) loaded into NEURON:     cc = CurrentClamp('CP_Cell', inj_amp=100)     t, v = cc.run()</p> </li> </ol> Source code in <code>bmtool/singlecell.py</code> <pre><code>def __init__(\n    self,\n    cell_or_template,\n    post_init_function=None,\n    record_sec=\"soma\",\n    record_loc=0.5,\n    threshold=None,\n    inj_sec=\"soma\",\n    inj_loc=0.5,\n    inj_amp=100.0,\n    inj_delay=100.0,\n    inj_dur=1000.0,\n    tstop=1000.0,\n):\n    \"\"\"\n    Initialize a current clamp simulation environment.\n\n    Parameters:\n    -----------\n    cell_or_template : NEURON cell object or str/callable\n        Either a pre-initialized NEURON cell object (NEW API),\n        or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API).\n        The new API (passing a cell object) is recommended for better control and reusability.\n    post_init_function : str, optional\n        Function of the cell to be called after initialization. Only used with legacy API.\n    record_sec : str, int, or tuple, optional\n        Section to record from. Can be:\n        - str: Section name (defaults to index 0 if multiple sections)\n        - int: Index into the 'all' section list\n        - tuple: (section_name, index) for accessing indexed sections\n        Default is 'soma'.\n    record_loc : float, optional\n        Location (0-1) within section to record from. Default is 0.5.\n    threshold : float, optional\n        Spike threshold (mV). If specified, spikes are detected and counted.\n    inj_sec : str, int, or tuple, optional\n        Section for current injection. Same format as record_sec. Default is 'soma'.\n    inj_loc : float, optional\n        Location (0-1) within section for current injection. Default is 0.5.\n    inj_amp : float, optional\n        Current injection amplitude (pA). Default is 100.0.\n    inj_delay : float, optional\n        Start time for current injection (ms). Default is 100.0.\n    inj_dur : float, optional\n        Duration of current injection (ms). Default is 1000.0.\n    tstop : float, optional\n        Total simulation time (ms). Default is 1000.0.\n        Will be extended if necessary to include the full current injection.\n\n    Notes:\n    ------\n    Two approaches are supported:\n\n    1. Pass a pre-built cell object:\n        cell = SimpleSoma()  # or any other cell\n        cc = CurrentClamp(cell, inj_amp=100)\n        t, v = cc.run()\n\n    2. Pass a template name (string) loaded into NEURON:\n        cc = CurrentClamp('CP_Cell', inj_amp=100)\n        t, v = cc.run()\n    \"\"\"\n    # Detect whether we have a cell object or a template name\n    if isinstance(cell_or_template, str) or (\n        callable(cell_or_template) and not hasattr(cell_or_template, \"soma\")\n    ):\n        # Legacy API: create cell from template\n        create_cell = (\n            getattr(h, cell_or_template)\n            if isinstance(cell_or_template, str)\n            else cell_or_template\n        )\n        if callable(cell_or_template):\n            cell = cell_or_template()\n        else:\n            cell = create_cell()\n        if post_init_function:\n            eval(f\"cell.{post_init_function}\")\n    else:\n        # New API: cell object already provided\n        cell = cell_or_template\n\n    # Initialize base class\n    super().__init__(\n        cell=cell,\n        record_sec=record_sec,\n        record_loc=record_loc,\n        inj_sec=inj_sec,\n        inj_loc=inj_loc,\n        threshold=threshold,\n    )\n\n    self.tstop = max(tstop, inj_delay + inj_dur)\n    self.inj_delay = inj_delay\n    self.inj_dur = inj_dur\n    self.inj_amp = inj_amp * 1e-3  # pA to nA\n    self.cell_src = None\n\n    self._setup_experiment()\n</code></pre>"},{"location":"api/singlecell/#passive-properties","title":"Passive Properties","text":""},{"location":"api/singlecell/#bmtool.singlecell.Passive","title":"<code>bmtool.singlecell.Passive</code>","text":"<p>               Bases: <code>CurrentClamp</code></p> Source code in <code>bmtool/singlecell.py</code> <pre><code>class Passive(CurrentClamp):\n    def __init__(\n        self,\n        cell_or_template,\n        inj_amp=-100.0,\n        inj_delay=200.0,\n        inj_dur=1000.0,\n        tstop=1200.0,\n        method=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize a passive membrane property simulation environment.\n\n        Parameters:\n        -----------\n        cell_or_template : NEURON cell object or str/callable\n            Either a pre-initialized NEURON cell object (NEW API),\n            or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API).\n        inj_amp : float, optional\n            Current injection amplitude (pA). Default is -100.0 (negative to measure passive properties).\n        inj_delay : float, optional\n            Start time for current injection (ms). Default is 200.0.\n        inj_dur : float, optional\n            Duration of current injection (ms). Default is 1000.0.\n        tstop : float, optional\n            Total simulation time (ms). Default is 1200.0.\n        method : str, optional\n            Method to estimate membrane time constant:\n            - 'simple': Find the time to reach 0.632 of voltage change\n            - 'exp': Fit a single exponential curve\n            - 'exp2': Fit a double exponential curve\n            Default is None, which uses 'simple' when calculations are performed.\n        **kwargs :\n            Additional keyword arguments to pass to the parent CurrentClamp constructor.\n\n        Notes:\n        ------\n        This class is designed for measuring passive membrane properties including\n        input resistance and membrane time constant.\n\n        Raises:\n        -------\n        AssertionError\n            If inj_amp is zero (must be non-zero to measure passive properties).\n        \"\"\"\n        assert inj_amp != 0\n        super().__init__(\n            cell_or_template=cell_or_template,\n            tstop=tstop,\n            inj_amp=inj_amp,\n            inj_delay=inj_delay,\n            inj_dur=inj_dur,\n            **kwargs,\n        )\n        self.inj_stop = inj_delay + inj_dur\n        self.method = method\n        self.tau_methods = {\n            \"simple\": self.tau_simple,\n            \"exp2\": self.tau_double_exponential,\n            \"exp\": self.tau_single_exponential,\n        }\n\n    def tau_simple(self):\n        \"\"\"\n        Calculate membrane time constant using the simple 0.632 criterion method.\n\n        This method calculates the membrane time constant by finding the time it takes\n        for the membrane potential to reach 63.2% (1-1/e) of its final value after\n        a step current injection.\n\n        Returns:\n        --------\n        callable\n            A function that prints the calculation details when called.\n\n        Notes:\n        ------\n        Sets the following attributes:\n        - tau: The calculated membrane time constant in ms\n        \"\"\"\n        v_t_const = self.cell_v_final - self.v_diff / np.e\n        index_v_tau = next(x for x, val in enumerate(self.v_vec_inj) if val &lt;= v_t_const)\n        self.tau = self.t_vec[self.index_v_rest + index_v_tau] - self.v_rest_time  # ms\n\n        def print_calc():\n            print()\n            print(\"Tau Calculation: time until 63.2% of dV\")\n            print(\"v_rest + 0.632*(v_final-v_rest)\")\n            print(\n                f\"{self.v_rest:.2f} + 0.632*({self.cell_v_final:.2f}-({self.v_rest:.2f})) = {v_t_const:.2f} (mV)\"\n            )\n            print(f\"Time where V = {v_t_const:.2f} (mV) is {self.v_rest_time + self.tau:.2f} (ms)\")\n            print(f\"{self.v_rest_time + self.tau:.2f} - {self.v_rest_time:g} = {self.tau:.2f} (ms)\")\n            print()\n\n        return print_calc\n\n    @staticmethod\n    def single_exponential(t, a0, a, tau):\n        \"\"\"\n        Single exponential function for fitting membrane potential response.\n\n        Parameters:\n        -----------\n        t : array-like\n            Time values\n        a0 : float\n            Offset (steady-state) value\n        a : float\n            Amplitude of the exponential component\n        tau : float\n            Time constant of the exponential decay\n\n        Returns:\n        --------\n        array-like\n            Function values at the given time points\n        \"\"\"\n        return a0 + a * np.exp(-t / tau)\n\n    def tau_single_exponential(self):\n        \"\"\"\n        Calculate membrane time constant by fitting a single exponential curve.\n\n        This method:\n        1. Identifies the peak response (for sag characterization)\n        2. Falls back to simple method for initial estimate\n        3. Fits a single exponential function to the membrane potential response\n        4. Sets tau to the exponential time constant\n\n        Returns:\n        --------\n        callable\n            A function that prints the calculation details when called.\n\n        Notes:\n        ------\n        Sets the following attributes:\n        - tau: The calculated membrane time constant in ms\n        - t_peak, v_peak: Time and voltage of peak response\n        - v_sag: Sag potential (difference between peak and steady-state)\n        - v_max_diff: Maximum potential difference from rest\n        - sag_norm: Normalized sag ratio\n        - popt: Optimized parameters from curve fitting\n        - pcov: Covariance matrix of the optimization\n        \"\"\"\n        index_v_peak = (np.sign(self.inj_amp) * self.v_vec_inj).argmax()\n        self.t_peak = self.t_vec_inj[index_v_peak]\n        self.v_peak = self.v_vec_inj[index_v_peak]\n        self.v_sag = self.v_peak - self.cell_v_final\n        self.v_max_diff = self.v_diff + self.v_sag\n        self.sag_norm = self.v_sag / self.v_max_diff\n\n        self.tau_simple()\n\n        p0 = (self.v_diff, self.tau)  # initial estimate\n        v0 = self.v_rest\n\n        def fit_func(t, a, tau):\n            return self.single_exponential(t, a0=v0 - a, a=a, tau=tau)\n\n        bounds = ((-np.inf, 1e-3), np.inf)\n        popt, self.pcov = curve_fit(\n            fit_func, self.t_vec_inj, self.v_vec_inj, p0=p0, bounds=bounds, maxfev=10000\n        )\n        self.popt = np.insert(popt, 0, v0 - popt[0])\n        self.tau = self.popt[2]\n\n        def print_calc():\n            print()\n            print(\n                \"Tau Calculation: Fit a single exponential curve to the membrane potential response\"\n            )\n            print(\"f(t) = a0 + a*exp(-t/tau)\")\n            print(\n                f\"Fit parameters: (a0, a, tau) = ({self.popt[0]:.2f}, {self.popt[1]:.2f}, {self.popt[2]:.2f})\"\n            )\n            print(\n                f\"Membrane time constant is determined from the exponential term: {self.tau:.2f} (ms)\"\n            )\n            print()\n            print(\"Sag potential: v_sag = v_peak - v_final = %.2f (mV)\" % self.v_sag)\n            print(\"Normalized sag potential: v_sag / (v_peak - v_rest) = %.3f\" % self.sag_norm)\n            print()\n\n        return print_calc\n\n    @staticmethod\n    def double_exponential(t, a0, a1, a2, tau1, tau2):\n        \"\"\"\n        Double exponential function for fitting membrane potential response.\n\n        This function is particularly useful for modeling cells with sag responses,\n        where the membrane potential shows two distinct time constants.\n\n        Parameters:\n        -----------\n        t : array-like\n            Time values\n        a0 : float\n            Offset (steady-state) value\n        a1 : float\n            Amplitude of the first exponential component\n        a2 : float\n            Amplitude of the second exponential component\n        tau1 : float\n            Time constant of the first exponential component\n        tau2 : float\n            Time constant of the second exponential component\n\n        Returns:\n        --------\n        array-like\n            Function values at the given time points\n        \"\"\"\n        return a0 + a1 * np.exp(-t / tau1) + a2 * np.exp(-t / tau2)\n\n    def tau_double_exponential(self):\n        \"\"\"\n        Calculate membrane time constant by fitting a double exponential curve.\n\n        This method is useful for cells with sag responses that cannot be\n        fitted well with a single exponential.\n\n        Returns:\n        --------\n        callable\n            A function that prints the calculation details when called.\n\n        Notes:\n        ------\n        Sets the following attributes:\n        - tau: The calculated membrane time constant (the slower of the two time constants)\n        - t_peak, v_peak: Time and voltage of peak response\n        - v_sag: Sag potential (difference between peak and steady-state)\n        - v_max_diff: Maximum potential difference from rest\n        - sag_norm: Normalized sag ratio\n        - popt: Optimized parameters from curve fitting\n        - pcov: Covariance matrix of the optimization\n        \"\"\"\n        index_v_peak = (np.sign(self.inj_amp) * self.v_vec_inj).argmax()\n        self.t_peak = self.t_vec_inj[index_v_peak]\n        self.v_peak = self.v_vec_inj[index_v_peak]\n        self.v_sag = self.v_peak - self.cell_v_final\n        self.v_max_diff = self.v_diff + self.v_sag\n        self.sag_norm = self.v_sag / self.v_max_diff\n\n        self.tau_simple()\n        p0 = (self.v_sag, -self.v_max_diff, self.t_peak, self.tau)  # initial estimate\n        v0 = self.v_rest\n\n        def fit_func(t, a1, a2, tau1, tau2):\n            return self.double_exponential(t, v0 - a1 - a2, a1, a2, tau1, tau2)\n\n        bounds = ((-np.inf, -np.inf, 1e-3, 1e-3), np.inf)\n        popt, self.pcov = curve_fit(\n            fit_func, self.t_vec_inj, self.v_vec_inj, p0=p0, bounds=bounds, maxfev=10000\n        )\n        self.popt = np.insert(popt, 0, v0 - sum(popt[:2]))\n        self.tau = max(self.popt[-2:])\n\n        def print_calc():\n            print()\n            print(\n                \"Tau Calculation: Fit a double exponential curve to the membrane potential response\"\n            )\n            print(\"f(t) = a0 + a1*exp(-t/tau1) + a2*exp(-t/tau2)\")\n            print(\"Constrained by initial value: f(0) = a0 + a1 + a2 = v_rest\")\n            print(\n                \"Fit parameters: (a0, a1, a2, tau1, tau2) = (\"\n                + \", \".join(f\"{x:.2f}\" for x in self.popt)\n                + \")\"\n            )\n            print(\n                f\"Membrane time constant is determined from the slowest exponential term: {self.tau:.2f} (ms)\"\n            )\n            print()\n            print(\"Sag potential: v_sag = v_peak - v_final = %.2f (mV)\" % self.v_sag)\n            print(\"Normalized sag potential: v_sag / (v_peak - v_rest) = %.3f\" % self.sag_norm)\n            print()\n\n        return print_calc\n\n    def double_exponential_fit(self):\n        \"\"\"\n        Get the double exponential fit values for plotting.\n\n        Returns:\n        --------\n        tuple\n            (time_vector, fitted_values) where:\n            - time_vector: Time points starting from rest time\n            - fitted_values: Membrane potential values predicted by the double exponential function\n        \"\"\"\n        t_vec = self.v_rest_time + self.t_vec_inj\n        v_fit = self.double_exponential(self.t_vec_inj, *self.popt)\n        return t_vec, v_fit\n\n    def single_exponential_fit(self):\n        \"\"\"\n        Get the single exponential fit values for plotting.\n\n        Returns:\n        --------\n        tuple\n            (time_vector, fitted_values) where:\n            - time_vector: Time points starting from rest time\n            - fitted_values: Membrane potential values predicted by the single exponential function\n        \"\"\"\n        t_vec = self.v_rest_time + self.t_vec_inj\n        v_fit = self.single_exponential(self.t_vec_inj, *self.popt)\n        return t_vec, v_fit\n\n    def run(self):\n        \"\"\"\n        Run the simulation and calculate passive membrane properties.\n\n        This method:\n        1. Runs the NEURON simulation\n        2. Extracts membrane potential at rest and steady-state\n        3. Calculates input resistance from the step response\n        4. Calculates membrane time constant using the specified method\n        5. Prints detailed calculations for educational purposes\n\n        Returns:\n        --------\n        tuple\n            (time_vector, voltage_vector) from the simulation\n\n        Notes:\n        ------\n        Sets several attributes including:\n        - v_rest: Resting membrane potential\n        - r_in: Input resistance in MOhms\n        - tau: Membrane time constant in ms\n        \"\"\"\n        print(\"Running simulation for passive properties...\")\n        h.tstop = self.tstop\n        h.finitialize(h.v_init)\n        h.continuerun(self.tstop)\n\n        self.index_v_rest = int(self.inj_delay / h.dt)\n        self.index_v_final = int(self.inj_stop / h.dt)\n        self.v_rest = self.v_vec[self.index_v_rest]\n        self.v_rest_time = self.t_vec[self.index_v_rest]\n        self.cell_v_final = self.v_vec[self.index_v_final]\n        self.v_final_time = self.t_vec[self.index_v_final]\n\n        t_idx = slice(self.index_v_rest, self.index_v_final + 1)\n        self.v_vec_inj = np.array(self.v_vec)[t_idx]\n        self.t_vec_inj = np.array(self.t_vec)[t_idx] - self.v_rest_time\n\n        self.v_diff = self.cell_v_final - self.v_rest\n        self.r_in = self.v_diff / self.inj_amp  # MegaOhms\n\n        print_calc = self.tau_methods.get(self.method, self.tau_simple)()\n\n        print()\n        print(f\"V Rest: {self.v_rest:.2f} (mV)\")\n        print(f\"Resistance: {self.r_in:.2f} (MOhms)\")\n        print(f\"Membrane time constant: {self.tau:.2f} (ms)\")\n        print()\n        print(f\"V_rest Calculation: Voltage taken at time {self.v_rest_time:.1f} (ms) is\")\n        print(f\"{self.v_rest:.2f} (mV)\")\n        print()\n        print(\"R_in Calculation: dV/dI = (v_final-v_rest)/(i_final-i_start)\")\n        print(f\"({self.cell_v_final:.2f} - ({self.v_rest:.2f})) / ({self.inj_amp:g} - 0)\")\n        print(\n            f\"{np.sign(self.inj_amp) * self.v_diff:.2f} (mV) / {np.abs(self.inj_amp)} (nA) = {self.r_in:.2f} (MOhms)\"\n        )\n        print_calc()\n\n        return self._convert_vectors_to_python()\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.__init__","title":"<code>__init__(cell_or_template, inj_amp=-100.0, inj_delay=200.0, inj_dur=1000.0, tstop=1200.0, method=None, **kwargs)</code>","text":"<p>Initialize a passive membrane property simulation environment.</p> Parameters: <p>cell_or_template : NEURON cell object or str/callable     Either a pre-initialized NEURON cell object (NEW API),     or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API). inj_amp : float, optional     Current injection amplitude (pA). Default is -100.0 (negative to measure passive properties). inj_delay : float, optional     Start time for current injection (ms). Default is 200.0. inj_dur : float, optional     Duration of current injection (ms). Default is 1000.0. tstop : float, optional     Total simulation time (ms). Default is 1200.0. method : str, optional     Method to estimate membrane time constant:     - 'simple': Find the time to reach 0.632 of voltage change     - 'exp': Fit a single exponential curve     - 'exp2': Fit a double exponential curve     Default is None, which uses 'simple' when calculations are performed. **kwargs :     Additional keyword arguments to pass to the parent CurrentClamp constructor.</p> Notes: <p>This class is designed for measuring passive membrane properties including input resistance and membrane time constant.</p> Raises: <p>AssertionError     If inj_amp is zero (must be non-zero to measure passive properties).</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def __init__(\n    self,\n    cell_or_template,\n    inj_amp=-100.0,\n    inj_delay=200.0,\n    inj_dur=1000.0,\n    tstop=1200.0,\n    method=None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize a passive membrane property simulation environment.\n\n    Parameters:\n    -----------\n    cell_or_template : NEURON cell object or str/callable\n        Either a pre-initialized NEURON cell object (NEW API),\n        or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API).\n    inj_amp : float, optional\n        Current injection amplitude (pA). Default is -100.0 (negative to measure passive properties).\n    inj_delay : float, optional\n        Start time for current injection (ms). Default is 200.0.\n    inj_dur : float, optional\n        Duration of current injection (ms). Default is 1000.0.\n    tstop : float, optional\n        Total simulation time (ms). Default is 1200.0.\n    method : str, optional\n        Method to estimate membrane time constant:\n        - 'simple': Find the time to reach 0.632 of voltage change\n        - 'exp': Fit a single exponential curve\n        - 'exp2': Fit a double exponential curve\n        Default is None, which uses 'simple' when calculations are performed.\n    **kwargs :\n        Additional keyword arguments to pass to the parent CurrentClamp constructor.\n\n    Notes:\n    ------\n    This class is designed for measuring passive membrane properties including\n    input resistance and membrane time constant.\n\n    Raises:\n    -------\n    AssertionError\n        If inj_amp is zero (must be non-zero to measure passive properties).\n    \"\"\"\n    assert inj_amp != 0\n    super().__init__(\n        cell_or_template=cell_or_template,\n        tstop=tstop,\n        inj_amp=inj_amp,\n        inj_delay=inj_delay,\n        inj_dur=inj_dur,\n        **kwargs,\n    )\n    self.inj_stop = inj_delay + inj_dur\n    self.method = method\n    self.tau_methods = {\n        \"simple\": self.tau_simple,\n        \"exp2\": self.tau_double_exponential,\n        \"exp\": self.tau_single_exponential,\n    }\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.tau_simple","title":"<code>tau_simple()</code>","text":"<p>Calculate membrane time constant using the simple 0.632 criterion method.</p> <p>This method calculates the membrane time constant by finding the time it takes for the membrane potential to reach 63.2% (1-1/e) of its final value after a step current injection.</p> Returns: <p>callable     A function that prints the calculation details when called.</p> Notes: <p>Sets the following attributes: - tau: The calculated membrane time constant in ms</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def tau_simple(self):\n    \"\"\"\n    Calculate membrane time constant using the simple 0.632 criterion method.\n\n    This method calculates the membrane time constant by finding the time it takes\n    for the membrane potential to reach 63.2% (1-1/e) of its final value after\n    a step current injection.\n\n    Returns:\n    --------\n    callable\n        A function that prints the calculation details when called.\n\n    Notes:\n    ------\n    Sets the following attributes:\n    - tau: The calculated membrane time constant in ms\n    \"\"\"\n    v_t_const = self.cell_v_final - self.v_diff / np.e\n    index_v_tau = next(x for x, val in enumerate(self.v_vec_inj) if val &lt;= v_t_const)\n    self.tau = self.t_vec[self.index_v_rest + index_v_tau] - self.v_rest_time  # ms\n\n    def print_calc():\n        print()\n        print(\"Tau Calculation: time until 63.2% of dV\")\n        print(\"v_rest + 0.632*(v_final-v_rest)\")\n        print(\n            f\"{self.v_rest:.2f} + 0.632*({self.cell_v_final:.2f}-({self.v_rest:.2f})) = {v_t_const:.2f} (mV)\"\n        )\n        print(f\"Time where V = {v_t_const:.2f} (mV) is {self.v_rest_time + self.tau:.2f} (ms)\")\n        print(f\"{self.v_rest_time + self.tau:.2f} - {self.v_rest_time:g} = {self.tau:.2f} (ms)\")\n        print()\n\n    return print_calc\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.tau_single_exponential","title":"<code>tau_single_exponential()</code>","text":"<p>Calculate membrane time constant by fitting a single exponential curve.</p> <p>This method: 1. Identifies the peak response (for sag characterization) 2. Falls back to simple method for initial estimate 3. Fits a single exponential function to the membrane potential response 4. Sets tau to the exponential time constant</p> Returns: <p>callable     A function that prints the calculation details when called.</p> Notes: <p>Sets the following attributes: - tau: The calculated membrane time constant in ms - t_peak, v_peak: Time and voltage of peak response - v_sag: Sag potential (difference between peak and steady-state) - v_max_diff: Maximum potential difference from rest - sag_norm: Normalized sag ratio - popt: Optimized parameters from curve fitting - pcov: Covariance matrix of the optimization</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def tau_single_exponential(self):\n    \"\"\"\n    Calculate membrane time constant by fitting a single exponential curve.\n\n    This method:\n    1. Identifies the peak response (for sag characterization)\n    2. Falls back to simple method for initial estimate\n    3. Fits a single exponential function to the membrane potential response\n    4. Sets tau to the exponential time constant\n\n    Returns:\n    --------\n    callable\n        A function that prints the calculation details when called.\n\n    Notes:\n    ------\n    Sets the following attributes:\n    - tau: The calculated membrane time constant in ms\n    - t_peak, v_peak: Time and voltage of peak response\n    - v_sag: Sag potential (difference between peak and steady-state)\n    - v_max_diff: Maximum potential difference from rest\n    - sag_norm: Normalized sag ratio\n    - popt: Optimized parameters from curve fitting\n    - pcov: Covariance matrix of the optimization\n    \"\"\"\n    index_v_peak = (np.sign(self.inj_amp) * self.v_vec_inj).argmax()\n    self.t_peak = self.t_vec_inj[index_v_peak]\n    self.v_peak = self.v_vec_inj[index_v_peak]\n    self.v_sag = self.v_peak - self.cell_v_final\n    self.v_max_diff = self.v_diff + self.v_sag\n    self.sag_norm = self.v_sag / self.v_max_diff\n\n    self.tau_simple()\n\n    p0 = (self.v_diff, self.tau)  # initial estimate\n    v0 = self.v_rest\n\n    def fit_func(t, a, tau):\n        return self.single_exponential(t, a0=v0 - a, a=a, tau=tau)\n\n    bounds = ((-np.inf, 1e-3), np.inf)\n    popt, self.pcov = curve_fit(\n        fit_func, self.t_vec_inj, self.v_vec_inj, p0=p0, bounds=bounds, maxfev=10000\n    )\n    self.popt = np.insert(popt, 0, v0 - popt[0])\n    self.tau = self.popt[2]\n\n    def print_calc():\n        print()\n        print(\n            \"Tau Calculation: Fit a single exponential curve to the membrane potential response\"\n        )\n        print(\"f(t) = a0 + a*exp(-t/tau)\")\n        print(\n            f\"Fit parameters: (a0, a, tau) = ({self.popt[0]:.2f}, {self.popt[1]:.2f}, {self.popt[2]:.2f})\"\n        )\n        print(\n            f\"Membrane time constant is determined from the exponential term: {self.tau:.2f} (ms)\"\n        )\n        print()\n        print(\"Sag potential: v_sag = v_peak - v_final = %.2f (mV)\" % self.v_sag)\n        print(\"Normalized sag potential: v_sag / (v_peak - v_rest) = %.3f\" % self.sag_norm)\n        print()\n\n    return print_calc\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.tau_double_exponential","title":"<code>tau_double_exponential()</code>","text":"<p>Calculate membrane time constant by fitting a double exponential curve.</p> <p>This method is useful for cells with sag responses that cannot be fitted well with a single exponential.</p> Returns: <p>callable     A function that prints the calculation details when called.</p> Notes: <p>Sets the following attributes: - tau: The calculated membrane time constant (the slower of the two time constants) - t_peak, v_peak: Time and voltage of peak response - v_sag: Sag potential (difference between peak and steady-state) - v_max_diff: Maximum potential difference from rest - sag_norm: Normalized sag ratio - popt: Optimized parameters from curve fitting - pcov: Covariance matrix of the optimization</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def tau_double_exponential(self):\n    \"\"\"\n    Calculate membrane time constant by fitting a double exponential curve.\n\n    This method is useful for cells with sag responses that cannot be\n    fitted well with a single exponential.\n\n    Returns:\n    --------\n    callable\n        A function that prints the calculation details when called.\n\n    Notes:\n    ------\n    Sets the following attributes:\n    - tau: The calculated membrane time constant (the slower of the two time constants)\n    - t_peak, v_peak: Time and voltage of peak response\n    - v_sag: Sag potential (difference between peak and steady-state)\n    - v_max_diff: Maximum potential difference from rest\n    - sag_norm: Normalized sag ratio\n    - popt: Optimized parameters from curve fitting\n    - pcov: Covariance matrix of the optimization\n    \"\"\"\n    index_v_peak = (np.sign(self.inj_amp) * self.v_vec_inj).argmax()\n    self.t_peak = self.t_vec_inj[index_v_peak]\n    self.v_peak = self.v_vec_inj[index_v_peak]\n    self.v_sag = self.v_peak - self.cell_v_final\n    self.v_max_diff = self.v_diff + self.v_sag\n    self.sag_norm = self.v_sag / self.v_max_diff\n\n    self.tau_simple()\n    p0 = (self.v_sag, -self.v_max_diff, self.t_peak, self.tau)  # initial estimate\n    v0 = self.v_rest\n\n    def fit_func(t, a1, a2, tau1, tau2):\n        return self.double_exponential(t, v0 - a1 - a2, a1, a2, tau1, tau2)\n\n    bounds = ((-np.inf, -np.inf, 1e-3, 1e-3), np.inf)\n    popt, self.pcov = curve_fit(\n        fit_func, self.t_vec_inj, self.v_vec_inj, p0=p0, bounds=bounds, maxfev=10000\n    )\n    self.popt = np.insert(popt, 0, v0 - sum(popt[:2]))\n    self.tau = max(self.popt[-2:])\n\n    def print_calc():\n        print()\n        print(\n            \"Tau Calculation: Fit a double exponential curve to the membrane potential response\"\n        )\n        print(\"f(t) = a0 + a1*exp(-t/tau1) + a2*exp(-t/tau2)\")\n        print(\"Constrained by initial value: f(0) = a0 + a1 + a2 = v_rest\")\n        print(\n            \"Fit parameters: (a0, a1, a2, tau1, tau2) = (\"\n            + \", \".join(f\"{x:.2f}\" for x in self.popt)\n            + \")\"\n        )\n        print(\n            f\"Membrane time constant is determined from the slowest exponential term: {self.tau:.2f} (ms)\"\n        )\n        print()\n        print(\"Sag potential: v_sag = v_peak - v_final = %.2f (mV)\" % self.v_sag)\n        print(\"Normalized sag potential: v_sag / (v_peak - v_rest) = %.3f\" % self.sag_norm)\n        print()\n\n    return print_calc\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.double_exponential_fit","title":"<code>double_exponential_fit()</code>","text":"<p>Get the double exponential fit values for plotting.</p> Returns: <p>tuple     (time_vector, fitted_values) where:     - time_vector: Time points starting from rest time     - fitted_values: Membrane potential values predicted by the double exponential function</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def double_exponential_fit(self):\n    \"\"\"\n    Get the double exponential fit values for plotting.\n\n    Returns:\n    --------\n    tuple\n        (time_vector, fitted_values) where:\n        - time_vector: Time points starting from rest time\n        - fitted_values: Membrane potential values predicted by the double exponential function\n    \"\"\"\n    t_vec = self.v_rest_time + self.t_vec_inj\n    v_fit = self.double_exponential(self.t_vec_inj, *self.popt)\n    return t_vec, v_fit\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Passive.single_exponential_fit","title":"<code>single_exponential_fit()</code>","text":"<p>Get the single exponential fit values for plotting.</p> Returns: <p>tuple     (time_vector, fitted_values) where:     - time_vector: Time points starting from rest time     - fitted_values: Membrane potential values predicted by the single exponential function</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def single_exponential_fit(self):\n    \"\"\"\n    Get the single exponential fit values for plotting.\n\n    Returns:\n    --------\n    tuple\n        (time_vector, fitted_values) where:\n        - time_vector: Time points starting from rest time\n        - fitted_values: Membrane potential values predicted by the single exponential function\n    \"\"\"\n    t_vec = self.v_rest_time + self.t_vec_inj\n    v_fit = self.single_exponential(self.t_vec_inj, *self.popt)\n    return t_vec, v_fit\n</code></pre>"},{"location":"api/singlecell/#frequency-current-f-i-analysis","title":"Frequency-Current (F-I) Analysis","text":""},{"location":"api/singlecell/#bmtool.singlecell.FI","title":"<code>bmtool.singlecell.FI</code>","text":"<p>               Bases: <code>SimulationBase</code></p> Source code in <code>bmtool/singlecell.py</code> <pre><code>class FI(SimulationBase):\n    def __init__(\n        self,\n        cell_or_template,\n        post_init_function=None,\n        i_start=0.0,\n        i_stop=1050.0,\n        i_increment=100.0,\n        tstart=50.0,\n        tdur=1000.0,\n        threshold=0.0,\n        record_sec=\"soma\",\n        record_loc=0.5,\n        inj_sec=\"soma\",\n        inj_loc=0.5,\n    ):\n        \"\"\"\n        Initialize a frequency-current (F-I) curve simulation environment.\n\n        Parameters:\n        -----------\n        cell_or_template : NEURON cell object or str/callable\n            Either a pre-initialized NEURON cell object (NEW API),\n            or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API).\n        post_init_function : str, optional\n            Function of the cell to be called after initialization. Only used with legacy API.\n        i_start : float, optional\n            Initial current injection amplitude (pA). Default is 0.0.\n        i_stop : float, optional\n            Maximum current injection amplitude (pA). Default is 1050.0.\n        i_increment : float, optional\n            Amplitude increment between trials (pA). Default is 100.0.\n        tstart : float, optional\n            Current injection start time (ms). Default is 50.0.\n        tdur : float, optional\n            Current injection duration (ms). Default is 1000.0.\n        threshold : float, optional\n            Spike threshold (mV). Default is 0.0.\n        record_sec : str, int, or tuple, optional\n            Section to record from. Same format as in CurrentClamp. Default is 'soma'.\n        record_loc : float, optional\n            Location (0-1) within section to record from. Default is 0.5.\n        inj_sec : str, int, or tuple, optional\n            Section for current injection. Same format as record_sec. Default is 'soma'.\n        inj_loc : float, optional\n            Location (0-1) within section for current injection. Default is 0.5.\n\n        Notes:\n        ------\n        Two approaches are supported:\n\n        1. Pass a pre-built cell object:\n            cell = SimpleSoma()\n            fi = FI(cell, i_start=0, i_stop=500, i_increment=100)\n            amps, spikes = fi.run()\n\n        2. Pass a template name (string) loaded into NEURON:\n            fi = FI('CP_Cell', i_start=0, i_stop=500, i_increment=100)\n            amps, spikes = fi.run()\n        \"\"\"\n        # Detect whether we have a cell object or a template name\n        if isinstance(cell_or_template, str) or (\n            callable(cell_or_template) and not hasattr(cell_or_template, \"soma\")\n        ):\n            # Legacy API: create cell from template\n            create_cell = (\n                getattr(h, cell_or_template)\n                if isinstance(cell_or_template, str)\n                else cell_or_template\n            )\n            if callable(cell_or_template):\n                cell = cell_or_template()\n            else:\n                cell = create_cell()\n            if post_init_function:\n                eval(f\"cell.{post_init_function}\")\n        else:\n            # New API: cell object already provided\n            cell = cell_or_template\n\n        # Initialize base class\n        super().__init__(\n            cell=cell,\n            record_sec=record_sec,\n            record_loc=record_loc,\n            inj_sec=inj_sec,\n            inj_loc=inj_loc,\n            threshold=threshold,\n        )\n\n        self.i_start = i_start * 1e-3  # pA to nA\n        self.i_stop = i_stop * 1e-3\n        self.i_increment = i_increment * 1e-3\n        self.tstart = tstart\n        self.tdur = tdur\n        self.tstop = tstart + tdur\n\n        self.ntrials = int((self.i_stop - self.i_start) // self.i_increment + 1)\n        self.amps = (self.i_start + np.arange(self.ntrials) * self.i_increment).tolist()\n        self.nspks = []\n\n        self._setup_experiment()\n\n    def _setup_experiment(self):\n        \"\"\"\n        Set up recording vectors and injection site for F-I curve.\n        Uses single-cell approach with state reset between trials.\n        \"\"\"\n        self.inj_seg, _ = get_target_site(self.cell, self.inj_sec, self.inj_loc, \"injection\")\n        self.rec_seg, self.rec_sec = get_target_site(\n            self.cell, self.record_sec, self.record_loc, \"recording\"\n        )\n\n        # Create IClamp (will modify amplitude for each trial)\n        self.iclamp = h.IClamp(self.inj_seg)\n        self.iclamp.delay = self.tstart\n        self.iclamp.dur = self.tdur\n\n        self._create_recording_vectors()\n        self._setup_spike_detection()\n\n        print(f\"Injection location: {self.inj_seg}\")\n        print(f\"Recording: {self.rec_seg}._ref_v\")\n\n    def run(self):\n        \"\"\"\n        Run the F-I curve simulation with current amplitude sweep.\n\n        Uses state reset between each trial to allow parameter modifications\n        if needed, while maintaining single-cell efficiency.\n\n        Returns:\n        --------\n        tuple\n            (current_amplitudes, spike_counts) where:\n            - current_amplitudes: List of current injection amplitudes (pA)\n            - spike_counts: List of spike counts corresponding to each amplitude\n        \"\"\"\n        print(\"Running simulations for FI curve...\")\n        self.nspks = []\n\n        for amp in self.amps:\n            # Set current amplitude for this trial\n            self.iclamp.amp = amp\n\n            # Reset NEURON state and clear vectors\n            self.reset_state()\n\n            # Run simulation for this amplitude\n            h.finitialize(h.v_init)\n            h.continuerun(self.tstop)\n\n            # Count spikes in this trial\n            if self.threshold is not None and self.tspk_vec is not None:\n                nsp = len(self.tspk_vec)\n                self.nspks.append(nsp)\n            else:\n                self.nspks.append(0)\n\n        print()\n        print(\"Results\")\n        # Create a nice dataframe output\n        data = {\n            \"Injection (pA):\": [amp * 1000 for amp in self.amps],\n            \"number of spikes\": self.nspks,\n        }\n        df = pd.DataFrame(data)\n        print(df)\n        print()\n\n        return [amp * 1000 for amp in self.amps], self.nspks\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.FI.__init__","title":"<code>__init__(cell_or_template, post_init_function=None, i_start=0.0, i_stop=1050.0, i_increment=100.0, tstart=50.0, tdur=1000.0, threshold=0.0, record_sec='soma', record_loc=0.5, inj_sec='soma', inj_loc=0.5)</code>","text":"<p>Initialize a frequency-current (F-I) curve simulation environment.</p> Parameters: <p>cell_or_template : NEURON cell object or str/callable     Either a pre-initialized NEURON cell object (NEW API),     or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API). post_init_function : str, optional     Function of the cell to be called after initialization. Only used with legacy API. i_start : float, optional     Initial current injection amplitude (pA). Default is 0.0. i_stop : float, optional     Maximum current injection amplitude (pA). Default is 1050.0. i_increment : float, optional     Amplitude increment between trials (pA). Default is 100.0. tstart : float, optional     Current injection start time (ms). Default is 50.0. tdur : float, optional     Current injection duration (ms). Default is 1000.0. threshold : float, optional     Spike threshold (mV). Default is 0.0. record_sec : str, int, or tuple, optional     Section to record from. Same format as in CurrentClamp. Default is 'soma'. record_loc : float, optional     Location (0-1) within section to record from. Default is 0.5. inj_sec : str, int, or tuple, optional     Section for current injection. Same format as record_sec. Default is 'soma'. inj_loc : float, optional     Location (0-1) within section for current injection. Default is 0.5.</p> Notes: <p>Two approaches are supported:</p> <ol> <li> <p>Pass a pre-built cell object:     cell = SimpleSoma()     fi = FI(cell, i_start=0, i_stop=500, i_increment=100)     amps, spikes = fi.run()</p> </li> <li> <p>Pass a template name (string) loaded into NEURON:     fi = FI('CP_Cell', i_start=0, i_stop=500, i_increment=100)     amps, spikes = fi.run()</p> </li> </ol> Source code in <code>bmtool/singlecell.py</code> <pre><code>def __init__(\n    self,\n    cell_or_template,\n    post_init_function=None,\n    i_start=0.0,\n    i_stop=1050.0,\n    i_increment=100.0,\n    tstart=50.0,\n    tdur=1000.0,\n    threshold=0.0,\n    record_sec=\"soma\",\n    record_loc=0.5,\n    inj_sec=\"soma\",\n    inj_loc=0.5,\n):\n    \"\"\"\n    Initialize a frequency-current (F-I) curve simulation environment.\n\n    Parameters:\n    -----------\n    cell_or_template : NEURON cell object or str/callable\n        Either a pre-initialized NEURON cell object (NEW API),\n        or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API).\n    post_init_function : str, optional\n        Function of the cell to be called after initialization. Only used with legacy API.\n    i_start : float, optional\n        Initial current injection amplitude (pA). Default is 0.0.\n    i_stop : float, optional\n        Maximum current injection amplitude (pA). Default is 1050.0.\n    i_increment : float, optional\n        Amplitude increment between trials (pA). Default is 100.0.\n    tstart : float, optional\n        Current injection start time (ms). Default is 50.0.\n    tdur : float, optional\n        Current injection duration (ms). Default is 1000.0.\n    threshold : float, optional\n        Spike threshold (mV). Default is 0.0.\n    record_sec : str, int, or tuple, optional\n        Section to record from. Same format as in CurrentClamp. Default is 'soma'.\n    record_loc : float, optional\n        Location (0-1) within section to record from. Default is 0.5.\n    inj_sec : str, int, or tuple, optional\n        Section for current injection. Same format as record_sec. Default is 'soma'.\n    inj_loc : float, optional\n        Location (0-1) within section for current injection. Default is 0.5.\n\n    Notes:\n    ------\n    Two approaches are supported:\n\n    1. Pass a pre-built cell object:\n        cell = SimpleSoma()\n        fi = FI(cell, i_start=0, i_stop=500, i_increment=100)\n        amps, spikes = fi.run()\n\n    2. Pass a template name (string) loaded into NEURON:\n        fi = FI('CP_Cell', i_start=0, i_stop=500, i_increment=100)\n        amps, spikes = fi.run()\n    \"\"\"\n    # Detect whether we have a cell object or a template name\n    if isinstance(cell_or_template, str) or (\n        callable(cell_or_template) and not hasattr(cell_or_template, \"soma\")\n    ):\n        # Legacy API: create cell from template\n        create_cell = (\n            getattr(h, cell_or_template)\n            if isinstance(cell_or_template, str)\n            else cell_or_template\n        )\n        if callable(cell_or_template):\n            cell = cell_or_template()\n        else:\n            cell = create_cell()\n        if post_init_function:\n            eval(f\"cell.{post_init_function}\")\n    else:\n        # New API: cell object already provided\n        cell = cell_or_template\n\n    # Initialize base class\n    super().__init__(\n        cell=cell,\n        record_sec=record_sec,\n        record_loc=record_loc,\n        inj_sec=inj_sec,\n        inj_loc=inj_loc,\n        threshold=threshold,\n    )\n\n    self.i_start = i_start * 1e-3  # pA to nA\n    self.i_stop = i_stop * 1e-3\n    self.i_increment = i_increment * 1e-3\n    self.tstart = tstart\n    self.tdur = tdur\n    self.tstop = tstart + tdur\n\n    self.ntrials = int((self.i_stop - self.i_start) // self.i_increment + 1)\n    self.amps = (self.i_start + np.arange(self.ntrials) * self.i_increment).tolist()\n    self.nspks = []\n\n    self._setup_experiment()\n</code></pre>"},{"location":"api/singlecell/#impedance-analysis","title":"Impedance Analysis","text":""},{"location":"api/singlecell/#bmtool.singlecell.ZAP","title":"<code>bmtool.singlecell.ZAP</code>","text":"<p>               Bases: <code>CurrentClamp</code></p> Source code in <code>bmtool/singlecell.py</code> <pre><code>class ZAP(CurrentClamp):\n    def __init__(\n        self,\n        cell_or_template,\n        inj_amp=100.0,\n        inj_delay=200.0,\n        inj_dur=15000.0,\n        tstop=15500.0,\n        fstart=0.0,\n        fend=15.0,\n        chirp_type=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize a ZAP (impedance amplitude profile) simulation environment.\n\n        Parameters:\n        -----------\n        cell_or_template : NEURON cell object or str/callable\n            Either a pre-initialized NEURON cell object (NEW API),\n            or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API).\n        inj_amp : float, optional\n            Current injection amplitude (pA). Default is 100.0.\n        inj_delay : float, optional\n            Start time for current injection (ms). Default is 200.0.\n        inj_dur : float, optional\n            Duration of current injection (ms). Default is 15000.0.\n        tstop : float, optional\n            Total simulation time (ms). Default is 15500.0.\n        fstart : float, optional\n            Starting frequency of the chirp current (Hz). Default is 0.0.\n        fend : float, optional\n            Ending frequency of the chirp current (Hz). Default is 15.0.\n        chirp_type : str, optional\n            Type of chirp current determining how frequency increases over time:\n            - 'linear': Linear increase in frequency (default if None)\n            - 'exponential': Exponential increase in frequency\n        **kwargs :\n            Additional keyword arguments to pass to the parent CurrentClamp constructor.\n\n        Notes:\n        ------\n        This class is designed for measuring the frequency-dependent impedance profile\n        of a neuron using a chirp current that sweeps through frequencies.\n\n        Raises:\n        -------\n        AssertionError\n            - If inj_amp is zero\n            - If chirp_type is 'exponential' and either fstart or fend is &lt;= 0\n        \"\"\"\n        assert inj_amp != 0\n        super().__init__(\n            cell_or_template=cell_or_template,\n            tstop=tstop,\n            inj_amp=inj_amp,\n            inj_delay=inj_delay,\n            inj_dur=inj_dur,\n            **kwargs,\n        )\n        self.inj_stop = inj_delay + inj_dur\n        self.fstart = fstart\n        self.fend = fend\n        self.chirp_type = chirp_type\n        self.chirp_func = {\"linear\": self.linear_chirp, \"exponential\": self.exponential_chirp}\n        if chirp_type == \"exponential\":\n            assert fstart &gt; 0 and fend &gt; 0\n\n    def linear_chirp(self, t, f0, f1):\n        \"\"\"\n        Generate a chirp current with linearly increasing frequency.\n\n        Parameters:\n        -----------\n        t : ndarray\n            Time vector (ms)\n        f0 : float\n            Start frequency (kHz)\n        f1 : float\n            End frequency (kHz)\n\n        Returns:\n        --------\n        ndarray\n            Current values with amplitude self.inj_amp and frequency\n            increasing linearly from f0 to f1 Hz over time t\n        \"\"\"\n        return self.inj_amp * np.sin(np.pi * (2 * f0 + (f1 - f0) / t[-1] * t) * t)\n\n    def exponential_chirp(self, t, f0, f1):\n        \"\"\"\n        Generate a chirp current with exponentially increasing frequency.\n\n        Parameters:\n        -----------\n        t : ndarray\n            Time vector (ms)\n        f0 : float\n            Start frequency (kHz), must be &gt; 0\n        f1 : float\n            End frequency (kHz), must be &gt; 0\n\n        Returns:\n        --------\n        ndarray\n            Current values with amplitude self.inj_amp and frequency\n            increasing exponentially from f0 to f1 Hz over time t\n\n        Notes:\n        ------\n        For exponential chirp, both f0 and f1 must be positive.\n        \"\"\"\n        L = np.log(f1 / f0) / t[-1]\n        return self.inj_amp * np.sin(np.pi * 2 * f0 / L * (np.exp(L * t) - 1))\n\n    def zap_current(self):\n        \"\"\"\n        Create a frequency-modulated (chirp) current for probing impedance.\n\n        This method:\n        1. Sets up time vectors for the simulation and current injection\n        2. Creates a chirp current based on the specified parameters (linear or exponential)\n        3. Prepares the current vector for NEURON playback\n\n        Notes:\n        ------\n        The chirp current increases in frequency from fstart to fend Hz over the duration\n        of the injection. This allows frequency-dependent impedance to be measured in\n        a single simulation.\n        \"\"\"\n        self.dt = dt = h.dt\n        self.index_v_rest = int(self.inj_delay / dt)\n        self.index_v_final = int(self.inj_stop / dt)\n\n        t = np.arange(int(self.tstop / dt) + 1) * dt\n        t_inj = t[: self.index_v_final - self.index_v_rest + 1]\n        f0 = self.fstart * 1e-3  # Hz to 1/ms\n        f1 = self.fend * 1e-3\n        chirp_func = self.chirp_func.get(self.chirp_type, self.linear_chirp)\n        self.zap_vec_inj = chirp_func(t_inj, f0, f1)\n        i_inj = np.zeros_like(t)\n        i_inj[self.index_v_rest : self.index_v_final + 1] = self.zap_vec_inj\n\n        self.zap_vec = h.Vector()\n        self.zap_vec.from_python(i_inj)\n        self.zap_vec.play(self.cell_src._ref_amp, dt)\n\n    def get_impedance(self, smooth=1):\n        \"\"\"\n        Calculate and extract the frequency-dependent impedance profile.\n\n        This method:\n        1. Filters the impedance to the frequency range of interest\n        2. Optionally applies smoothing to reduce noise\n        3. Identifies the resonant frequency (peak impedance)\n\n        Parameters:\n        -----------\n        smooth : int, optional\n            Window size for smoothing the impedance. Default is 1 (no smoothing).\n\n        Returns:\n        --------\n        tuple\n            (frequencies, impedance_values) in the range of interest\n\n        Notes:\n        ------\n        Sets self.peak_freq to the resonant frequency (frequency of maximum impedance).\n        \"\"\"\n        f_idx = (self.freq &gt; min(self.fstart, self.fend)) &amp; (\n            self.freq &lt; max(self.fstart, self.fend)\n        )\n        impedance = self.impedance\n        if smooth &gt; 1:\n            impedance = np.convolve(impedance, np.ones(smooth) / smooth, mode=\"same\")\n        freq, impedance = self.freq[f_idx], impedance[f_idx]\n        self.peak_freq = freq[np.argmax(impedance)]\n        print(f\"Resonant Peak Frequency: {self.peak_freq:.3g} (Hz)\")\n        return freq, impedance\n\n    def run(self) -&gt; Tuple[list, list]:\n        \"\"\"\n        Run the ZAP simulation and calculate the impedance profile.\n\n        This method:\n        1. Sets up the chirp current\n        2. Runs the NEURON simulation\n        3. Calculates the impedance using FFT\n        4. Prints a summary of the frequency range and analysis method\n\n        Returns:\n        --------\n        tuple\n            (time_vector, voltage_vector) from the simulation\n\n        Notes:\n        ------\n        Sets several attributes including:\n        - Z: Complex impedance values (from FFT)\n        - freq: Frequency values for the impedance profile\n        - impedance: Absolute impedance values\n        \"\"\"\n        print(\"ZAP current simulation running...\")\n        self.zap_current()\n        h.tstop = self.tstop\n        h.finitialize(h.v_init)\n        h.continuerun(self.tstop)\n\n        self.zap_vec.resize(self.t_vec.size())\n        self.v_rest = self.v_vec[self.index_v_rest]\n        self.v_rest_time = self.t_vec[self.index_v_rest]\n\n        t_idx = slice(self.index_v_rest, self.index_v_final + 1)\n        self.v_vec_inj = np.array(self.v_vec)[t_idx] - self.v_rest\n        self.t_vec_inj = np.array(self.t_vec)[t_idx] - self.v_rest_time\n\n        self.cell_v_amp_max = np.abs(self.v_vec_inj).max()\n        self.Z = np.fft.rfft(self.v_vec_inj) / np.fft.rfft(self.zap_vec_inj)  # MOhms\n        self.freq = np.fft.rfftfreq(self.zap_vec_inj.size, d=self.dt * 1e-3)  # ms to sec\n        self.impedance = np.abs(self.Z)\n\n        print()\n        print(\n            \"Chirp current injection with frequency changing from \"\n            f\"{self.fstart:g} to {self.fend:g} Hz over {self.inj_dur * 1e-3:g} seconds\"\n        )\n        print(\n            \"Impedance is calculated as the ratio of FFT amplitude \"\n            \"of membrane voltage to FFT amplitude of chirp current\"\n        )\n        print()\n        return self._convert_vectors_to_python()\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.ZAP.__init__","title":"<code>__init__(cell_or_template, inj_amp=100.0, inj_delay=200.0, inj_dur=15000.0, tstop=15500.0, fstart=0.0, fend=15.0, chirp_type=None, **kwargs)</code>","text":"<p>Initialize a ZAP (impedance amplitude profile) simulation environment.</p> Parameters: <p>cell_or_template : NEURON cell object or str/callable     Either a pre-initialized NEURON cell object (NEW API),     or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API). inj_amp : float, optional     Current injection amplitude (pA). Default is 100.0. inj_delay : float, optional     Start time for current injection (ms). Default is 200.0. inj_dur : float, optional     Duration of current injection (ms). Default is 15000.0. tstop : float, optional     Total simulation time (ms). Default is 15500.0. fstart : float, optional     Starting frequency of the chirp current (Hz). Default is 0.0. fend : float, optional     Ending frequency of the chirp current (Hz). Default is 15.0. chirp_type : str, optional     Type of chirp current determining how frequency increases over time:     - 'linear': Linear increase in frequency (default if None)     - 'exponential': Exponential increase in frequency **kwargs :     Additional keyword arguments to pass to the parent CurrentClamp constructor.</p> Notes: <p>This class is designed for measuring the frequency-dependent impedance profile of a neuron using a chirp current that sweeps through frequencies.</p> Raises: <p>AssertionError     - If inj_amp is zero     - If chirp_type is 'exponential' and either fstart or fend is &lt;= 0</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def __init__(\n    self,\n    cell_or_template,\n    inj_amp=100.0,\n    inj_delay=200.0,\n    inj_dur=15000.0,\n    tstop=15500.0,\n    fstart=0.0,\n    fend=15.0,\n    chirp_type=None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize a ZAP (impedance amplitude profile) simulation environment.\n\n    Parameters:\n    -----------\n    cell_or_template : NEURON cell object or str/callable\n        Either a pre-initialized NEURON cell object (NEW API),\n        or the name of a cell template located in HOC / a callable that creates a cell (LEGACY API).\n    inj_amp : float, optional\n        Current injection amplitude (pA). Default is 100.0.\n    inj_delay : float, optional\n        Start time for current injection (ms). Default is 200.0.\n    inj_dur : float, optional\n        Duration of current injection (ms). Default is 15000.0.\n    tstop : float, optional\n        Total simulation time (ms). Default is 15500.0.\n    fstart : float, optional\n        Starting frequency of the chirp current (Hz). Default is 0.0.\n    fend : float, optional\n        Ending frequency of the chirp current (Hz). Default is 15.0.\n    chirp_type : str, optional\n        Type of chirp current determining how frequency increases over time:\n        - 'linear': Linear increase in frequency (default if None)\n        - 'exponential': Exponential increase in frequency\n    **kwargs :\n        Additional keyword arguments to pass to the parent CurrentClamp constructor.\n\n    Notes:\n    ------\n    This class is designed for measuring the frequency-dependent impedance profile\n    of a neuron using a chirp current that sweeps through frequencies.\n\n    Raises:\n    -------\n    AssertionError\n        - If inj_amp is zero\n        - If chirp_type is 'exponential' and either fstart or fend is &lt;= 0\n    \"\"\"\n    assert inj_amp != 0\n    super().__init__(\n        cell_or_template=cell_or_template,\n        tstop=tstop,\n        inj_amp=inj_amp,\n        inj_delay=inj_delay,\n        inj_dur=inj_dur,\n        **kwargs,\n    )\n    self.inj_stop = inj_delay + inj_dur\n    self.fstart = fstart\n    self.fend = fend\n    self.chirp_type = chirp_type\n    self.chirp_func = {\"linear\": self.linear_chirp, \"exponential\": self.exponential_chirp}\n    if chirp_type == \"exponential\":\n        assert fstart &gt; 0 and fend &gt; 0\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.ZAP.linear_chirp","title":"<code>linear_chirp(t, f0, f1)</code>","text":"<p>Generate a chirp current with linearly increasing frequency.</p> Parameters: <p>t : ndarray     Time vector (ms) f0 : float     Start frequency (kHz) f1 : float     End frequency (kHz)</p> Returns: <p>ndarray     Current values with amplitude self.inj_amp and frequency     increasing linearly from f0 to f1 Hz over time t</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def linear_chirp(self, t, f0, f1):\n    \"\"\"\n    Generate a chirp current with linearly increasing frequency.\n\n    Parameters:\n    -----------\n    t : ndarray\n        Time vector (ms)\n    f0 : float\n        Start frequency (kHz)\n    f1 : float\n        End frequency (kHz)\n\n    Returns:\n    --------\n    ndarray\n        Current values with amplitude self.inj_amp and frequency\n        increasing linearly from f0 to f1 Hz over time t\n    \"\"\"\n    return self.inj_amp * np.sin(np.pi * (2 * f0 + (f1 - f0) / t[-1] * t) * t)\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.ZAP.exponential_chirp","title":"<code>exponential_chirp(t, f0, f1)</code>","text":"<p>Generate a chirp current with exponentially increasing frequency.</p> Parameters: <p>t : ndarray     Time vector (ms) f0 : float     Start frequency (kHz), must be &gt; 0 f1 : float     End frequency (kHz), must be &gt; 0</p> Returns: <p>ndarray     Current values with amplitude self.inj_amp and frequency     increasing exponentially from f0 to f1 Hz over time t</p> Notes: <p>For exponential chirp, both f0 and f1 must be positive.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def exponential_chirp(self, t, f0, f1):\n    \"\"\"\n    Generate a chirp current with exponentially increasing frequency.\n\n    Parameters:\n    -----------\n    t : ndarray\n        Time vector (ms)\n    f0 : float\n        Start frequency (kHz), must be &gt; 0\n    f1 : float\n        End frequency (kHz), must be &gt; 0\n\n    Returns:\n    --------\n    ndarray\n        Current values with amplitude self.inj_amp and frequency\n        increasing exponentially from f0 to f1 Hz over time t\n\n    Notes:\n    ------\n    For exponential chirp, both f0 and f1 must be positive.\n    \"\"\"\n    L = np.log(f1 / f0) / t[-1]\n    return self.inj_amp * np.sin(np.pi * 2 * f0 / L * (np.exp(L * t) - 1))\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.ZAP.zap_current","title":"<code>zap_current()</code>","text":"<p>Create a frequency-modulated (chirp) current for probing impedance.</p> <p>This method: 1. Sets up time vectors for the simulation and current injection 2. Creates a chirp current based on the specified parameters (linear or exponential) 3. Prepares the current vector for NEURON playback</p> Notes: <p>The chirp current increases in frequency from fstart to fend Hz over the duration of the injection. This allows frequency-dependent impedance to be measured in a single simulation.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def zap_current(self):\n    \"\"\"\n    Create a frequency-modulated (chirp) current for probing impedance.\n\n    This method:\n    1. Sets up time vectors for the simulation and current injection\n    2. Creates a chirp current based on the specified parameters (linear or exponential)\n    3. Prepares the current vector for NEURON playback\n\n    Notes:\n    ------\n    The chirp current increases in frequency from fstart to fend Hz over the duration\n    of the injection. This allows frequency-dependent impedance to be measured in\n    a single simulation.\n    \"\"\"\n    self.dt = dt = h.dt\n    self.index_v_rest = int(self.inj_delay / dt)\n    self.index_v_final = int(self.inj_stop / dt)\n\n    t = np.arange(int(self.tstop / dt) + 1) * dt\n    t_inj = t[: self.index_v_final - self.index_v_rest + 1]\n    f0 = self.fstart * 1e-3  # Hz to 1/ms\n    f1 = self.fend * 1e-3\n    chirp_func = self.chirp_func.get(self.chirp_type, self.linear_chirp)\n    self.zap_vec_inj = chirp_func(t_inj, f0, f1)\n    i_inj = np.zeros_like(t)\n    i_inj[self.index_v_rest : self.index_v_final + 1] = self.zap_vec_inj\n\n    self.zap_vec = h.Vector()\n    self.zap_vec.from_python(i_inj)\n    self.zap_vec.play(self.cell_src._ref_amp, dt)\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.ZAP.get_impedance","title":"<code>get_impedance(smooth=1)</code>","text":"<p>Calculate and extract the frequency-dependent impedance profile.</p> <p>This method: 1. Filters the impedance to the frequency range of interest 2. Optionally applies smoothing to reduce noise 3. Identifies the resonant frequency (peak impedance)</p> Parameters: <p>smooth : int, optional     Window size for smoothing the impedance. Default is 1 (no smoothing).</p> Returns: <p>tuple     (frequencies, impedance_values) in the range of interest</p> Notes: <p>Sets self.peak_freq to the resonant frequency (frequency of maximum impedance).</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def get_impedance(self, smooth=1):\n    \"\"\"\n    Calculate and extract the frequency-dependent impedance profile.\n\n    This method:\n    1. Filters the impedance to the frequency range of interest\n    2. Optionally applies smoothing to reduce noise\n    3. Identifies the resonant frequency (peak impedance)\n\n    Parameters:\n    -----------\n    smooth : int, optional\n        Window size for smoothing the impedance. Default is 1 (no smoothing).\n\n    Returns:\n    --------\n    tuple\n        (frequencies, impedance_values) in the range of interest\n\n    Notes:\n    ------\n    Sets self.peak_freq to the resonant frequency (frequency of maximum impedance).\n    \"\"\"\n    f_idx = (self.freq &gt; min(self.fstart, self.fend)) &amp; (\n        self.freq &lt; max(self.fstart, self.fend)\n    )\n    impedance = self.impedance\n    if smooth &gt; 1:\n        impedance = np.convolve(impedance, np.ones(smooth) / smooth, mode=\"same\")\n    freq, impedance = self.freq[f_idx], impedance[f_idx]\n    self.peak_freq = freq[np.argmax(impedance)]\n    print(f\"Resonant Peak Frequency: {self.peak_freq:.3g} (Hz)\")\n    return freq, impedance\n</code></pre>"},{"location":"api/singlecell/#cell-profiler","title":"Cell Profiler","text":""},{"location":"api/singlecell/#bmtool.singlecell.Profiler","title":"<code>bmtool.singlecell.Profiler</code>","text":"<p>All in one single cell profiler</p> <p>This Profiler now supports being initialized with either explicit <code>template_dir</code> and <code>mechanism_dir</code> paths or with a BMTK <code>config</code> file (which should contain <code>components.templates_dir</code> and <code>components.mechanisms_dir</code>). When <code>config</code> is provided it will be used to load mechanisms and templates via the utility helpers.</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>class Profiler:\n    \"\"\"All in one single cell profiler\n\n    This Profiler now supports being initialized with either explicit\n    `template_dir` and `mechanism_dir` paths or with a BMTK `config` file\n    (which should contain `components.templates_dir` and\n    `components.mechanisms_dir`). When `config` is provided it will be used\n    to load mechanisms and templates via the utility helpers.\n    \"\"\"\n\n    def __init__(\n        self, template_dir: str = None, mechanism_dir: str = None, dt=None, config: str = None\n    ):\n        # initialize to None and then prefer config-derived paths if provided\n        self.template_dir = None\n        self.mechanism_dir = None\n        self.templates = None  # Initialize templates attribute\n        self.config = config  # Store config path\n        self.last_figure = None  # Store reference to last generated figure\n\n        # If a BMTK config is provided, load mechanisms/templates from it\n        if config is not None:\n            try:\n                # load and apply the config values for directories\n                conf = load_config(config)\n                # conf behaves like a dict returned by bmtk Config.from_json\n                try:\n                    comps = conf[\"components\"]\n                except Exception:\n                    comps = getattr(conf, \"components\", None)\n\n                if comps is not None:\n                    # support dict-like and object-like components\n                    try:\n                        self.template_dir = comps.get(\"templates_dir\")\n                    except Exception:\n                        self.template_dir = getattr(comps, \"templates_dir\", None)\n                    try:\n                        self.mechanism_dir = comps.get(\"mechanisms_dir\")\n                    except Exception:\n                        self.mechanism_dir = getattr(comps, \"mechanisms_dir\", None)\n\n                # actually load mechanisms and templates using the helper\n                load_templates_from_config(config)\n            except Exception:\n                # fall back to explicit dirs if config parsing/loading fails\n                print(\"failed\")\n\n        else:\n            # fall back to explicit args if not set by config\n            if not self.template_dir:\n                self.template_dir = template_dir\n            if not self.mechanism_dir:\n                self.mechanism_dir = mechanism_dir\n\n            # template_dir is required for loading templates later\n            if self.template_dir is None:\n                raise ValueError(\n                    \"Profiler requires either 'template_dir' or a 'config' containing components.templates_dir\"\n                )\n\n            self.templates = None\n\n            self.load_templates()\n\n        h.load_file(\"stdrun.hoc\")\n        if dt is not None:\n            h.dt = dt\n            h.steps_per_ms = 1 / h.dt\n\n    def load_templates(self, hoc_template_file=None):\n        if self.templates is None:  # Can really only do this once\n            # Check if we have a config file - if so, extract templates from node configs\n            if hasattr(self, \"config\") and self.config is not None:\n                try:\n                    from bmtool.util.util import load_nodes_from_config\n\n                    nodes_networks = load_nodes_from_config(config=self.config)\n                    template_names = set()\n                    for nodes in nodes_networks:\n                        try:\n                            cell_template_names = nodes_networks[nodes][\"model_template\"].unique()\n                            # Clean up template names (remove 'hoc:' prefix if present)\n                            for template in cell_template_names:\n                                if isinstance(template, str):\n                                    # Remove 'hoc:' prefix if present\n                                    clean_name = (\n                                        template.replace(\"hoc:\", \"\")\n                                        if template.startswith(\"hoc:\")\n                                        else template\n                                    )\n                                    template_names.add(clean_name)\n                        except:\n                            # If fails, means no model_templates in that network\n                            pass\n\n                    self.templates = sorted(list(template_names))\n                    self.hoc_templates = []  # Templates loaded via config, not hoc files\n\n                except Exception as e:\n                    print(f\"Failed to load templates from config: {e}\")\n            else:\n                # Traditional loading with template_dir and mechanism_dir\n                if (\n                    self.mechanism_dir != \"./\"\n                    and self.mechanism_dir != \".\"\n                    and self.mechanism_dir != \"././\"\n                ):\n                    neuron.load_mechanisms(self.mechanism_dir)\n                h_base = set(dir(h))\n\n                cwd = os.getcwd()\n                os.chdir(self.template_dir)\n                if not hoc_template_file:\n                    self.hoc_templates = glob.glob(\"*.hoc\")\n                    for hoc_template in self.hoc_templates:\n                        h.load_file(str(hoc_template))\n                else:\n                    self.hoc_templates = [hoc_template_file]\n                    h.load_file(hoc_template_file)\n\n                os.chdir(cwd)\n\n                h_loaded = dir(h)\n                self.templates = [x for x in h_loaded if x not in h_base]\n\n        return self.templates\n\n    def passive_properties(\n        self,\n        template_name: str,\n        post_init_function: str = None,\n        record_sec: str = \"soma\",\n        inj_sec: str = \"soma\",\n        plot: bool = True,\n        method=None,\n        **kwargs,\n    ) -&gt; Tuple[list, list]:\n        \"\"\"\n        Calculates passive properties for the specified cell template_name\n\n        Parameters\n        ==========\n        template_name: str or callable\n            name of the cell template located in hoc\n            or a function that creates and returns a cell object\n        post_init_function: str\n            function of the cell to be called after the cell has been initialized (like insert_mechs(123))\n        record_sec: str\n            section of the cell you want to record spikes from (default: soma)\n        inj_sec: str\n            section of the cell you want to inject current to (default: soma)\n        plot: bool\n            automatically plot the cell profile\n        method: str\n            method to estimate membrane time constant (see Passive)\n        **kwargs:\n            extra key word arguments for Passive()\n\n        Returns time (ms), membrane voltage (mV)\n        \"\"\"\n        passive = Passive(\n            template_name,\n            post_init_function=post_init_function,\n            record_sec=record_sec,\n            inj_sec=inj_sec,\n            method=method,\n            **kwargs,\n        )\n        time, amp = passive.run()\n\n        if plot:\n            plt.figure()\n            t_array = np.array(time)\n            amp_array = np.array(amp)\n            t_idx = (t_array &gt;= passive.inj_delay) &amp; (\n                t_array &lt;= passive.inj_delay + passive.inj_dur\n            )\n            plt.plot(t_array[t_idx], amp_array[t_idx])\n            if passive.method == \"exp2\":\n                plt.plot(*passive.double_exponential_fit(), \"r:\", label=\"double exponential fit\")\n                plt.legend()\n            elif passive.method == \"exp\":\n                plt.plot(*passive.single_exponential_fit(), \"r:\", label=\"single exponential fit\")\n                plt.legend()\n            plt.title(\"Passive Cell Current Injection\")\n            plt.xlabel(\"Time (ms)\")\n            plt.ylabel(\"Membrane Potential (mV)\")\n            self.last_figure = plt.gcf()\n            plt.show()\n\n        return time, amp\n\n    def current_injection(\n        self,\n        template_name: str,\n        post_init_function: str = None,\n        record_sec: str = \"soma\",\n        inj_sec: str = \"soma\",\n        plot: bool = True,\n        **kwargs,\n    ) -&gt; Tuple[list, list]:\n        ccl = CurrentClamp(\n            template_name,\n            post_init_function=post_init_function,\n            record_sec=record_sec,\n            inj_sec=inj_sec,\n            **kwargs,\n        )\n        time, amp = ccl.run()\n\n        if plot:\n            plt.figure()\n            plt.plot(time, amp)\n            plt.title(\"Current Injection\")\n            plt.xlabel(\"Time (ms)\")\n            plt.ylabel(\"Membrane Potential (mV)\")\n            plt.xlim(ccl.inj_delay - 10, ccl.inj_delay + ccl.inj_dur + 10)\n            self.last_figure = plt.gcf()\n            plt.show()\n\n        return time, amp\n\n    def fi_curve(\n        self,\n        template_name: str,\n        post_init_function: str = None,\n        record_sec: str = \"soma\",\n        inj_sec: str = \"soma\",\n        plot: bool = True,\n        **kwargs,\n    ) -&gt; Tuple[list, list]:\n        \"\"\"\n        Calculates an FI curve for the specified cell template_name\n\n        Parameters\n        ==========\n        template_name: str or callable\n            name of the cell template located in hoc\n            or a function that creates and returns a cell object\n        post_init_function: str\n            function of the cell to be called after the cell has been initialized (like insert_mechs(123))\n        record_sec: str\n            section of the cell you want to record spikes from (default: soma)\n        inj_sec: str\n            section of the cell you want to inject current to (default: soma)\n        plot: bool\n            automatically plot an fi curve\n\n        Returns the injection amplitudes (pA) used, number of spikes per amplitude supplied\n            list(amps), list(# of spikes)\n        \"\"\"\n        fi = FI(\n            template_name,\n            post_init_function=post_init_function,\n            record_sec=record_sec,\n            inj_sec=inj_sec,\n            **kwargs,\n        )\n        amp, nspk = fi.run()\n\n        if plot:\n            plt.figure()\n            plt.plot(amp, nspk)\n            plt.title(\"FI Curve\")\n            plt.xlabel(\"Injection (pA)\")\n            plt.ylabel(\"# Spikes\")\n            self.last_figure = plt.gcf()\n            plt.show()\n\n        return amp, nspk\n\n    def impedance_amplitude_profile(\n        self,\n        template_name: str,\n        post_init_function: str = None,\n        record_sec: str = \"soma\",\n        inj_sec: str = \"soma\",\n        plot: bool = True,\n        chirp_type=None,\n        smooth: int = 9,\n        **kwargs,\n    ) -&gt; Tuple[list, list]:\n        \"\"\"\n        chirp_type: str\n            Type of chirp current (see ZAP)\n        smooth: int\n            Window size for smoothing the impedance in frequency domain\n        **kwargs:\n            extra key word arguments for ZAP()\n        \"\"\"\n        zap = ZAP(\n            template_name,\n            post_init_function=post_init_function,\n            record_sec=record_sec,\n            inj_sec=inj_sec,\n            chirp_type=chirp_type,\n            **kwargs,\n        )\n        time, amp = zap.run()\n\n        if plot:\n            plt.figure()\n            plt.plot(time, amp)\n            plt.title(\"ZAP Response\")\n            plt.xlabel(\"Time (ms)\")\n            plt.ylabel(\"Membrane Potential (mV)\")\n            self.last_figure = plt.gcf()\n\n            plt.figure()\n            plt.plot(time, zap.zap_vec)\n            plt.title(\"ZAP Current\")\n            plt.xlabel(\"Time (ms)\")\n            plt.ylabel(\"Current Injection (nA)\")\n            # Note: This will overwrite last_figure with the current plot\n            self.last_figure = plt.gcf()\n\n            plt.figure()\n            plt.plot(*zap.get_impedance(smooth=smooth))\n            plt.title(\"Impedance Amplitude Profile\")\n            plt.xlabel(\"Frequency (Hz)\")\n            plt.ylabel(\"Impedance (MOhms)\")\n            self.last_figure = plt.gcf()\n            plt.show()\n\n        return time, amp\n\n    def interactive_runner(self):\n        \"\"\"Interactive runner for single cell profiling with GUI widgets.\n\n        This method creates an interactive interface using ipywidgets that allows\n        users to select templates and analysis methods, adjust parameters, and run\n        simulations with real-time plotting.\n        \"\"\"\n        try:\n            import ipywidgets as widgets\n            from IPython.display import display\n        except ImportError:\n            raise ImportError(\n                \"ipywidgets and matplotlib are required for interactive mode. Install with: pip install ipywidgets matplotlib\"\n            )\n\n        # Get available templates\n        available_templates = self.load_templates()\n\n        # Check what NEURON objects are available\n        import neuron\n\n        h = neuron.h\n\n        # Create widgets\n        template_dropdown = widgets.Dropdown(\n            options=available_templates,\n            value=available_templates[0] if available_templates else None,\n            description=\"Template:\",\n            style={\"description_width\": \"80px\"},\n            layout=widgets.Layout(width=\"300px\"),\n        )\n\n        method_dropdown = widgets.Dropdown(\n            options=[\n                \"passive_properties\",\n                \"current_injection\",\n                \"fi_curve\",\n                \"impedance_amplitude_profile\",\n            ],\n            value=\"passive_properties\",\n            description=\"Method:\",\n            style={\"description_width\": \"80px\"},\n            layout=widgets.Layout(width=\"300px\"),\n        )\n\n        # Default values based on method - from basic_settings in single_cell_tuning.ipynb\n        method_defaults = {\n            \"passive_properties\": {\n                \"inj_amp\": -20.0,\n                \"inj_delay\": 1500.0,\n                \"inj_dur\": 1000.0,\n                \"tstop\": 2500.0,\n                \"tau_method\": \"exp2\",\n            },\n            \"current_injection\": {\n                \"inj_amp\": 50.0,\n                \"inj_delay\": 1500.0,\n                \"inj_dur\": 1000.0,\n                \"tstop\": 3000.0,\n            },\n            \"fi_curve\": {\n                \"i_start\": -100.0,\n                \"i_stop\": 800.0,\n                \"i_increment\": 20.0,\n                \"inj_delay\": 1500.0,\n                \"inj_dur\": 1000.0,\n            },\n            \"impedance_amplitude_profile\": {\n                \"inj_amp\": 100.0,\n                \"inj_delay\": 1000.0,\n                \"inj_dur\": 15000.0,\n                \"tstop\": 15500.0,\n                \"fstart\": 0.0,\n                \"fend\": 15.0,\n                \"chirp_type\": \"linear\",\n            },\n        }\n\n        # Common parameters - always plot results, no need for toggle\n\n        # Method-specific parameters - styled like synapses.py sliders\n        slider_style = {\"description_width\": \"initial\"}\n        slider_layout = None  # Use default width for longer sliders\n        text_style = {\"description_width\": \"initial\"}\n        text_layout = widgets.Layout(width=\"200px\")\n\n        # Initialize sliders with default values for passive_properties (initial method)\n        defaults = method_defaults[\"passive_properties\"]\n\n        inj_amp_slider = widgets.FloatSlider(\n            value=defaults[\"inj_amp\"],\n            min=-500.0,\n            max=1000.0,\n            step=10.0,\n            description=\"Injection Amp (pA):\",\n            style=slider_style,\n        )\n        inj_delay_slider = widgets.FloatSlider(\n            value=defaults[\"inj_delay\"],\n            min=0.0,\n            max=3000.0,\n            step=10.0,\n            description=\"Injection Delay (ms):\",\n            style=slider_style,\n        )\n        inj_dur_slider = widgets.FloatSlider(\n            value=defaults[\"inj_dur\"],\n            min=100.0,\n            max=20000.0,\n            step=100.0,\n            description=\"Injection Duration (ms):\",\n            style=slider_style,\n        )\n        tstop_slider = widgets.FloatSlider(\n            value=defaults[\"tstop\"],\n            min=500.0,\n            max=25000.0,\n            step=100.0,\n            description=\"Total Time (ms):\",\n            style=slider_style,\n        )\n\n        # FI curve specific\n        fi_defaults = method_defaults[\"fi_curve\"]\n        i_start_slider = widgets.FloatSlider(\n            value=fi_defaults[\"i_start\"],\n            min=-500.0,\n            max=500.0,\n            step=10.0,\n            description=\"I Start (pA):\",\n            style=slider_style,\n        )\n        i_stop_slider = widgets.FloatSlider(\n            value=fi_defaults[\"i_stop\"],\n            min=0.0,\n            max=2000.0,\n            step=50.0,\n            description=\"I Stop (pA):\",\n            style=slider_style,\n        )\n        i_increment_slider = widgets.FloatSlider(\n            value=fi_defaults[\"i_increment\"],\n            min=10.0,\n            max=500.0,\n            step=10.0,\n            description=\"I Increment (pA):\",\n            style=slider_style,\n        )\n\n        # ZAP specific\n        zap_defaults = method_defaults[\"impedance_amplitude_profile\"]\n        fstart_slider = widgets.FloatSlider(\n            value=zap_defaults[\"fstart\"],\n            min=0.0,\n            max=50.0,\n            step=1.0,\n            description=\"Start Freq (Hz):\",\n            style=slider_style,\n        )\n        fend_slider = widgets.FloatSlider(\n            value=zap_defaults[\"fend\"],\n            min=1.0,\n            max=100.0,\n            step=1.0,\n            description=\"End Freq (Hz):\",\n            style=slider_style,\n        )\n        chirp_dropdown = widgets.Dropdown(\n            options=[\"linear\", \"exponential\"],\n            value=zap_defaults[\"chirp_type\"],\n            description=\"Chirp Type:\",\n            style=slider_style,\n        )\n\n        # Passive properties specific\n        tau_method_dropdown = widgets.Dropdown(\n            options=[\"simple\", \"exp\", \"exp2\"],\n            value=defaults[\"tau_method\"],\n            description=\"Tau Method:\",\n            style=slider_style,\n        )\n\n        # Sections\n        record_sec_text = widgets.Text(\n            value=\"soma\", description=\"Record Section:\", style=text_style, layout=text_layout\n        )\n        inj_sec_text = widgets.Text(\n            value=\"soma\", description=\"Injection Section:\", style=text_style, layout=text_layout\n        )\n\n        # Post init function\n        post_init_text = widgets.Text(\n            value=\"\",\n            description=\"Post Init Function:\",\n            placeholder=\"e.g., insert_mechs(123)\",\n            style={\"description_width\": \"initial\"},\n            layout=widgets.Layout(width=\"300px\"),\n        )\n\n        run_button = widgets.Button(\n            description=\"Run Analysis\",\n            button_style=\"primary\",\n            icon=\"play\",\n            layout=widgets.Layout(width=\"140px\"),\n        )\n\n        reset_button = widgets.Button(\n            description=\"Reset to Defaults\",\n            button_style=\"warning\",\n            icon=\"refresh\",\n            layout=widgets.Layout(width=\"150px\"),\n        )\n\n        save_path_text = widgets.Text(\n            value=\"\",\n            description=\"Save Path:\",\n            placeholder=\"e.g., plot.png\",\n            style={\"description_width\": \"initial\"},\n            layout=widgets.Layout(width=\"300px\"),\n        )\n\n        save_button = widgets.Button(\n            description=\"Save Plot\",\n            button_style=\"success\",\n            icon=\"save\",\n            layout=widgets.Layout(width=\"120px\"),\n        )\n\n        output_area = widgets.Output(\n            layout=widgets.Layout(border=\"1px solid #ccc\", padding=\"10px\", margin=\"10px 0 0 0\")\n        )\n\n        # Layout containers - organized like synapse tuner\n        # Top row - template and method selection\n        selection_row = widgets.HBox(\n            [template_dropdown, method_dropdown], layout=widgets.Layout(margin=\"0 0 10px 0\")\n        )\n\n        # Button row - main controls\n        button_row = widgets.HBox(\n            [run_button, reset_button, save_button], layout=widgets.Layout(margin=\"0 0 10px 0\")\n        )\n\n        # Section row - recording and injection sections\n        section_row = widgets.HBox(\n            [record_sec_text, inj_sec_text, post_init_text],\n            layout=widgets.Layout(margin=\"0 0 10px 0\"),\n        )\n\n        # Save row\n        save_row = widgets.HBox([save_path_text], layout=widgets.Layout(margin=\"0 0 10px 0\"))\n\n        # Parameter columns - organized in columns like synapse tuner\n        injection_params_col1 = widgets.VBox(\n            [inj_amp_slider, inj_delay_slider], layout=widgets.Layout(margin=\"0 10px 0 0\")\n        )\n\n        injection_params_col2 = widgets.VBox(\n            [inj_dur_slider, tstop_slider], layout=widgets.Layout(margin=\"0 0 0 10px\")\n        )\n\n        # Passive properties specific columns\n        passive_params_col1 = widgets.VBox(\n            [inj_amp_slider, inj_delay_slider], layout=widgets.Layout(margin=\"0 10px 0 0\")\n        )\n\n        passive_params_col2 = widgets.VBox(\n            [inj_dur_slider, tstop_slider, tau_method_dropdown],\n            layout=widgets.Layout(margin=\"0 0 0 10px\"),\n        )\n\n        fi_params_col1 = widgets.VBox(\n            [i_start_slider, i_stop_slider], layout=widgets.Layout(margin=\"0 10px 0 0\")\n        )\n\n        fi_params_col2 = widgets.VBox(\n            [\n                i_increment_slider,\n                inj_dur_slider,  # Use duration for FI curve too\n            ],\n            layout=widgets.Layout(margin=\"0 0 0 10px\"),\n        )\n\n        zap_params_col1 = widgets.VBox(\n            [inj_amp_slider, inj_delay_slider, inj_dur_slider],\n            layout=widgets.Layout(margin=\"0 10px 0 0\"),\n        )\n\n        zap_params_col2 = widgets.VBox(\n            [tstop_slider, fstart_slider, fend_slider, chirp_dropdown],\n            layout=widgets.Layout(margin=\"0 0 0 10px\"),\n        )\n\n        # Function to update slider values based on method defaults\n        def update_slider_values(method):\n            \"\"\"Update slider values to match the defaults for the selected method\"\"\"\n            if method in method_defaults:\n                defaults = method_defaults[method]\n\n                # Update common sliders if they exist in defaults\n                if \"inj_amp\" in defaults:\n                    inj_amp_slider.value = defaults[\"inj_amp\"]\n                if \"inj_delay\" in defaults:\n                    inj_delay_slider.value = defaults[\"inj_delay\"]\n                if \"inj_dur\" in defaults:\n                    inj_dur_slider.value = defaults[\"inj_dur\"]\n                if \"tstop\" in defaults:\n                    tstop_slider.value = defaults[\"tstop\"]\n\n                # Update method-specific sliders\n                if method == \"fi_curve\":\n                    if \"i_start\" in defaults:\n                        i_start_slider.value = defaults[\"i_start\"]\n                    if \"i_stop\" in defaults:\n                        i_stop_slider.value = defaults[\"i_stop\"]\n                    if \"i_increment\" in defaults:\n                        i_increment_slider.value = defaults[\"i_increment\"]\n\n                elif method == \"impedance_amplitude_profile\":\n                    if \"fstart\" in defaults:\n                        fstart_slider.value = defaults[\"fstart\"]\n                    if \"fend\" in defaults:\n                        fend_slider.value = defaults[\"fend\"]\n                    if \"chirp_type\" in defaults:\n                        chirp_dropdown.value = defaults[\"chirp_type\"]\n\n                elif method == \"passive_properties\":\n                    if \"tau_method\" in defaults:\n                        tau_method_dropdown.value = defaults[\"tau_method\"]\n\n        # Function to update parameter visibility based on selected method\n        def update_params(*args):\n            method = method_dropdown.value\n\n            # Update slider values to defaults for the selected method\n            update_slider_values(method)\n\n            # Update parameter column visibility\n            if method == \"passive_properties\":\n                param_columns.children = [widgets.HBox([passive_params_col1, passive_params_col2])]\n            elif method == \"current_injection\":\n                param_columns.children = [\n                    widgets.HBox([injection_params_col1, injection_params_col2])\n                ]\n            elif method == \"fi_curve\":\n                param_columns.children = [widgets.HBox([fi_params_col1, fi_params_col2])]\n            elif method == \"impedance_amplitude_profile\":\n                param_columns.children = [widgets.HBox([zap_params_col1, zap_params_col2])]\n\n        method_dropdown.observe(update_params, \"value\")\n\n        # Initialize parameter columns container\n        param_columns = widgets.VBox([widgets.HBox([passive_params_col1, passive_params_col2])])\n\n        # Run function\n        def run_analysis(b):\n            output_area.clear_output()  # Clear immediately on click\n            with output_area:\n                template = template_dropdown.value\n                method = method_dropdown.value\n                record_sec = record_sec_text.value\n                inj_sec = inj_sec_text.value\n                post_init = post_init_text.value if post_init_text.value else None\n\n                kwargs = {\n                    \"record_sec\": record_sec,\n                    \"inj_sec\": inj_sec,\n                    \"plot\": True,  # Always plot results\n                }\n\n                if post_init:\n                    kwargs[\"post_init_function\"] = post_init\n\n                # Add method-specific parameters\n                if method == \"passive_properties\":\n                    kwargs.update(\n                        {\n                            \"inj_amp\": inj_amp_slider.value,\n                            \"inj_delay\": inj_delay_slider.value,\n                            \"inj_dur\": inj_dur_slider.value,\n                            \"tstop\": tstop_slider.value,\n                            \"method\": tau_method_dropdown.value,\n                        }\n                    )\n                elif method == \"current_injection\":\n                    kwargs.update(\n                        {\n                            \"inj_amp\": inj_amp_slider.value,\n                            \"inj_delay\": inj_delay_slider.value,\n                            \"inj_dur\": inj_dur_slider.value,\n                            \"tstop\": tstop_slider.value,\n                        }\n                    )\n                elif method == \"fi_curve\":\n                    kwargs.update(\n                        {\n                            \"i_start\": i_start_slider.value,\n                            \"i_stop\": i_stop_slider.value,\n                            \"i_increment\": i_increment_slider.value,\n                            \"tstart\": inj_delay_slider.value,\n                            \"tdur\": inj_dur_slider.value,\n                        }\n                    )\n                elif method == \"impedance_amplitude_profile\":\n                    kwargs.update(\n                        {\n                            \"inj_amp\": inj_amp_slider.value,\n                            \"inj_delay\": inj_delay_slider.value,\n                            \"inj_dur\": inj_dur_slider.value,\n                            \"tstop\": tstop_slider.value,\n                            \"fstart\": fstart_slider.value,\n                            \"fend\": fend_slider.value,\n                            \"chirp_type\": chirp_dropdown.value,\n                        }\n                    )\n\n                print(\"=\" * 60)\n                print(f\"Running {method} for template: {template}\")\n                print(\"=\" * 60)\n                print(\"Parameters:\")\n                for key, value in kwargs.items():\n                    print(f\"  {key}: {value}\")\n                print(\"-\" * 60)\n\n                try:\n                    if method == \"passive_properties\":\n                        result = self.passive_properties(template, **kwargs)\n\n                    elif method == \"current_injection\":\n                        result = self.current_injection(template, **kwargs)\n                    elif method == \"fi_curve\":\n                        result = self.fi_curve(template, **kwargs)\n                    elif method == \"impedance_amplitude_profile\":\n                        result = self.impedance_amplitude_profile(template, **kwargs)\n\n                except Exception as e:\n                    print(\"=\" * 60)\n                    print(f\"\u2717 Error running analysis: {e}\")\n                    print(\"=\" * 60)\n                    import traceback\n\n                    traceback.print_exc()\n\n        # Reset function\n        def reset_to_defaults(b):\n            output_area.clear_output()  # Clear immediately on click\n            with output_area:\n                method = method_dropdown.value\n                update_slider_values(method)\n                print(f\"Reset all parameters to defaults for {method}\")\n\n        # Save function\n        def save_plot(b):\n            path = save_path_text.value\n            if not path:\n                with output_area:\n                    print(\"Please enter a save path\")\n                return\n            if self.last_figure is None:\n                with output_area:\n                    print(\"No plot to save. Run an analysis first.\")\n                return\n            try:\n                self.last_figure.savefig(path)\n                with output_area:\n                    print(f\"Plot saved to {path}\")\n            except Exception as e:\n                with output_area:\n                    print(f\"Error saving plot: {e}\")\n\n        run_button.on_click(run_analysis)\n        reset_button.on_click(reset_to_defaults)\n        save_button.on_click(save_plot)\n\n        # Create main UI layout - matching synapse tuner structure\n        ui = widgets.VBox(\n            [selection_row, button_row, section_row, param_columns, save_row],\n            layout=widgets.Layout(padding=\"10px\"),\n        )\n\n        # Display the interface - UI on top, output below (like synapse tuner)\n        display(ui)\n        display(output_area)\n\n        # Initial update\n        update_params()\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.__init__","title":"<code>__init__(template_dir=None, mechanism_dir=None, dt=None, config=None)</code>","text":"Source code in <code>bmtool/singlecell.py</code> <pre><code>def __init__(\n    self, template_dir: str = None, mechanism_dir: str = None, dt=None, config: str = None\n):\n    # initialize to None and then prefer config-derived paths if provided\n    self.template_dir = None\n    self.mechanism_dir = None\n    self.templates = None  # Initialize templates attribute\n    self.config = config  # Store config path\n    self.last_figure = None  # Store reference to last generated figure\n\n    # If a BMTK config is provided, load mechanisms/templates from it\n    if config is not None:\n        try:\n            # load and apply the config values for directories\n            conf = load_config(config)\n            # conf behaves like a dict returned by bmtk Config.from_json\n            try:\n                comps = conf[\"components\"]\n            except Exception:\n                comps = getattr(conf, \"components\", None)\n\n            if comps is not None:\n                # support dict-like and object-like components\n                try:\n                    self.template_dir = comps.get(\"templates_dir\")\n                except Exception:\n                    self.template_dir = getattr(comps, \"templates_dir\", None)\n                try:\n                    self.mechanism_dir = comps.get(\"mechanisms_dir\")\n                except Exception:\n                    self.mechanism_dir = getattr(comps, \"mechanisms_dir\", None)\n\n            # actually load mechanisms and templates using the helper\n            load_templates_from_config(config)\n        except Exception:\n            # fall back to explicit dirs if config parsing/loading fails\n            print(\"failed\")\n\n    else:\n        # fall back to explicit args if not set by config\n        if not self.template_dir:\n            self.template_dir = template_dir\n        if not self.mechanism_dir:\n            self.mechanism_dir = mechanism_dir\n\n        # template_dir is required for loading templates later\n        if self.template_dir is None:\n            raise ValueError(\n                \"Profiler requires either 'template_dir' or a 'config' containing components.templates_dir\"\n            )\n\n        self.templates = None\n\n        self.load_templates()\n\n    h.load_file(\"stdrun.hoc\")\n    if dt is not None:\n        h.dt = dt\n        h.steps_per_ms = 1 / h.dt\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.load_templates","title":"<code>load_templates(hoc_template_file=None)</code>","text":"Source code in <code>bmtool/singlecell.py</code> <pre><code>def load_templates(self, hoc_template_file=None):\n    if self.templates is None:  # Can really only do this once\n        # Check if we have a config file - if so, extract templates from node configs\n        if hasattr(self, \"config\") and self.config is not None:\n            try:\n                from bmtool.util.util import load_nodes_from_config\n\n                nodes_networks = load_nodes_from_config(config=self.config)\n                template_names = set()\n                for nodes in nodes_networks:\n                    try:\n                        cell_template_names = nodes_networks[nodes][\"model_template\"].unique()\n                        # Clean up template names (remove 'hoc:' prefix if present)\n                        for template in cell_template_names:\n                            if isinstance(template, str):\n                                # Remove 'hoc:' prefix if present\n                                clean_name = (\n                                    template.replace(\"hoc:\", \"\")\n                                    if template.startswith(\"hoc:\")\n                                    else template\n                                )\n                                template_names.add(clean_name)\n                    except:\n                        # If fails, means no model_templates in that network\n                        pass\n\n                self.templates = sorted(list(template_names))\n                self.hoc_templates = []  # Templates loaded via config, not hoc files\n\n            except Exception as e:\n                print(f\"Failed to load templates from config: {e}\")\n        else:\n            # Traditional loading with template_dir and mechanism_dir\n            if (\n                self.mechanism_dir != \"./\"\n                and self.mechanism_dir != \".\"\n                and self.mechanism_dir != \"././\"\n            ):\n                neuron.load_mechanisms(self.mechanism_dir)\n            h_base = set(dir(h))\n\n            cwd = os.getcwd()\n            os.chdir(self.template_dir)\n            if not hoc_template_file:\n                self.hoc_templates = glob.glob(\"*.hoc\")\n                for hoc_template in self.hoc_templates:\n                    h.load_file(str(hoc_template))\n            else:\n                self.hoc_templates = [hoc_template_file]\n                h.load_file(hoc_template_file)\n\n            os.chdir(cwd)\n\n            h_loaded = dir(h)\n            self.templates = [x for x in h_loaded if x not in h_base]\n\n    return self.templates\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.passive_properties","title":"<code>passive_properties(template_name, post_init_function=None, record_sec='soma', inj_sec='soma', plot=True, method=None, **kwargs)</code>","text":"<p>Calculates passive properties for the specified cell template_name</p>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.passive_properties--parameters","title":"Parameters","text":"<p>template_name: str or callable     name of the cell template located in hoc     or a function that creates and returns a cell object post_init_function: str     function of the cell to be called after the cell has been initialized (like insert_mechs(123)) record_sec: str     section of the cell you want to record spikes from (default: soma) inj_sec: str     section of the cell you want to inject current to (default: soma) plot: bool     automatically plot the cell profile method: str     method to estimate membrane time constant (see Passive) **kwargs:     extra key word arguments for Passive()</p> <p>Returns time (ms), membrane voltage (mV)</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def passive_properties(\n    self,\n    template_name: str,\n    post_init_function: str = None,\n    record_sec: str = \"soma\",\n    inj_sec: str = \"soma\",\n    plot: bool = True,\n    method=None,\n    **kwargs,\n) -&gt; Tuple[list, list]:\n    \"\"\"\n    Calculates passive properties for the specified cell template_name\n\n    Parameters\n    ==========\n    template_name: str or callable\n        name of the cell template located in hoc\n        or a function that creates and returns a cell object\n    post_init_function: str\n        function of the cell to be called after the cell has been initialized (like insert_mechs(123))\n    record_sec: str\n        section of the cell you want to record spikes from (default: soma)\n    inj_sec: str\n        section of the cell you want to inject current to (default: soma)\n    plot: bool\n        automatically plot the cell profile\n    method: str\n        method to estimate membrane time constant (see Passive)\n    **kwargs:\n        extra key word arguments for Passive()\n\n    Returns time (ms), membrane voltage (mV)\n    \"\"\"\n    passive = Passive(\n        template_name,\n        post_init_function=post_init_function,\n        record_sec=record_sec,\n        inj_sec=inj_sec,\n        method=method,\n        **kwargs,\n    )\n    time, amp = passive.run()\n\n    if plot:\n        plt.figure()\n        t_array = np.array(time)\n        amp_array = np.array(amp)\n        t_idx = (t_array &gt;= passive.inj_delay) &amp; (\n            t_array &lt;= passive.inj_delay + passive.inj_dur\n        )\n        plt.plot(t_array[t_idx], amp_array[t_idx])\n        if passive.method == \"exp2\":\n            plt.plot(*passive.double_exponential_fit(), \"r:\", label=\"double exponential fit\")\n            plt.legend()\n        elif passive.method == \"exp\":\n            plt.plot(*passive.single_exponential_fit(), \"r:\", label=\"single exponential fit\")\n            plt.legend()\n        plt.title(\"Passive Cell Current Injection\")\n        plt.xlabel(\"Time (ms)\")\n        plt.ylabel(\"Membrane Potential (mV)\")\n        self.last_figure = plt.gcf()\n        plt.show()\n\n    return time, amp\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.current_injection","title":"<code>current_injection(template_name, post_init_function=None, record_sec='soma', inj_sec='soma', plot=True, **kwargs)</code>","text":"Source code in <code>bmtool/singlecell.py</code> <pre><code>def current_injection(\n    self,\n    template_name: str,\n    post_init_function: str = None,\n    record_sec: str = \"soma\",\n    inj_sec: str = \"soma\",\n    plot: bool = True,\n    **kwargs,\n) -&gt; Tuple[list, list]:\n    ccl = CurrentClamp(\n        template_name,\n        post_init_function=post_init_function,\n        record_sec=record_sec,\n        inj_sec=inj_sec,\n        **kwargs,\n    )\n    time, amp = ccl.run()\n\n    if plot:\n        plt.figure()\n        plt.plot(time, amp)\n        plt.title(\"Current Injection\")\n        plt.xlabel(\"Time (ms)\")\n        plt.ylabel(\"Membrane Potential (mV)\")\n        plt.xlim(ccl.inj_delay - 10, ccl.inj_delay + ccl.inj_dur + 10)\n        self.last_figure = plt.gcf()\n        plt.show()\n\n    return time, amp\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.fi_curve","title":"<code>fi_curve(template_name, post_init_function=None, record_sec='soma', inj_sec='soma', plot=True, **kwargs)</code>","text":"<p>Calculates an FI curve for the specified cell template_name</p>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.fi_curve--parameters","title":"Parameters","text":"<p>template_name: str or callable     name of the cell template located in hoc     or a function that creates and returns a cell object post_init_function: str     function of the cell to be called after the cell has been initialized (like insert_mechs(123)) record_sec: str     section of the cell you want to record spikes from (default: soma) inj_sec: str     section of the cell you want to inject current to (default: soma) plot: bool     automatically plot an fi curve</p> <p>Returns the injection amplitudes (pA) used, number of spikes per amplitude supplied     list(amps), list(# of spikes)</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def fi_curve(\n    self,\n    template_name: str,\n    post_init_function: str = None,\n    record_sec: str = \"soma\",\n    inj_sec: str = \"soma\",\n    plot: bool = True,\n    **kwargs,\n) -&gt; Tuple[list, list]:\n    \"\"\"\n    Calculates an FI curve for the specified cell template_name\n\n    Parameters\n    ==========\n    template_name: str or callable\n        name of the cell template located in hoc\n        or a function that creates and returns a cell object\n    post_init_function: str\n        function of the cell to be called after the cell has been initialized (like insert_mechs(123))\n    record_sec: str\n        section of the cell you want to record spikes from (default: soma)\n    inj_sec: str\n        section of the cell you want to inject current to (default: soma)\n    plot: bool\n        automatically plot an fi curve\n\n    Returns the injection amplitudes (pA) used, number of spikes per amplitude supplied\n        list(amps), list(# of spikes)\n    \"\"\"\n    fi = FI(\n        template_name,\n        post_init_function=post_init_function,\n        record_sec=record_sec,\n        inj_sec=inj_sec,\n        **kwargs,\n    )\n    amp, nspk = fi.run()\n\n    if plot:\n        plt.figure()\n        plt.plot(amp, nspk)\n        plt.title(\"FI Curve\")\n        plt.xlabel(\"Injection (pA)\")\n        plt.ylabel(\"# Spikes\")\n        self.last_figure = plt.gcf()\n        plt.show()\n\n    return amp, nspk\n</code></pre>"},{"location":"api/singlecell/#bmtool.singlecell.Profiler.impedance_amplitude_profile","title":"<code>impedance_amplitude_profile(template_name, post_init_function=None, record_sec='soma', inj_sec='soma', plot=True, chirp_type=None, smooth=9, **kwargs)</code>","text":"<p>chirp_type: str     Type of chirp current (see ZAP) smooth: int     Window size for smoothing the impedance in frequency domain **kwargs:     extra key word arguments for ZAP()</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def impedance_amplitude_profile(\n    self,\n    template_name: str,\n    post_init_function: str = None,\n    record_sec: str = \"soma\",\n    inj_sec: str = \"soma\",\n    plot: bool = True,\n    chirp_type=None,\n    smooth: int = 9,\n    **kwargs,\n) -&gt; Tuple[list, list]:\n    \"\"\"\n    chirp_type: str\n        Type of chirp current (see ZAP)\n    smooth: int\n        Window size for smoothing the impedance in frequency domain\n    **kwargs:\n        extra key word arguments for ZAP()\n    \"\"\"\n    zap = ZAP(\n        template_name,\n        post_init_function=post_init_function,\n        record_sec=record_sec,\n        inj_sec=inj_sec,\n        chirp_type=chirp_type,\n        **kwargs,\n    )\n    time, amp = zap.run()\n\n    if plot:\n        plt.figure()\n        plt.plot(time, amp)\n        plt.title(\"ZAP Response\")\n        plt.xlabel(\"Time (ms)\")\n        plt.ylabel(\"Membrane Potential (mV)\")\n        self.last_figure = plt.gcf()\n\n        plt.figure()\n        plt.plot(time, zap.zap_vec)\n        plt.title(\"ZAP Current\")\n        plt.xlabel(\"Time (ms)\")\n        plt.ylabel(\"Current Injection (nA)\")\n        # Note: This will overwrite last_figure with the current plot\n        self.last_figure = plt.gcf()\n\n        plt.figure()\n        plt.plot(*zap.get_impedance(smooth=smooth))\n        plt.title(\"Impedance Amplitude Profile\")\n        plt.xlabel(\"Frequency (Hz)\")\n        plt.ylabel(\"Impedance (MOhms)\")\n        self.last_figure = plt.gcf()\n        plt.show()\n\n    return time, amp\n</code></pre>"},{"location":"api/singlecell/#helper-functions","title":"Helper Functions","text":""},{"location":"api/singlecell/#bmtool.singlecell.run_and_plot","title":"<code>bmtool.singlecell.run_and_plot(sim, title=None, xlabel='Time (ms)', ylabel='Membrane Potential (mV)', plot=True, plot_injection_only=False)</code>","text":"<p>Helper function for running simulation and plot sim: instance of the simulation class in this module title, xlabel, ylabel: plot labels plot: whether or not to plot plot_injection_only: plot only the injection duration Return: outputs by sim.run()</p> Source code in <code>bmtool/singlecell.py</code> <pre><code>def run_and_plot(\n    sim,\n    title=None,\n    xlabel=\"Time (ms)\",\n    ylabel=\"Membrane Potential (mV)\",\n    plot=True,\n    plot_injection_only=False,\n):\n    \"\"\"Helper function for running simulation and plot\n    sim: instance of the simulation class in this module\n    title, xlabel, ylabel: plot labels\n    plot: whether or not to plot\n    plot_injection_only: plot only the injection duration\n    Return: outputs by sim.run()\n    \"\"\"\n    X, Y = sim.run()\n    X = np.array(X)\n    Y = np.array(Y)\n    if plot:\n        plt.figure()\n        if plot_injection_only:\n            t_idx = (X &gt;= sim.inj_delay) &amp; (X &lt;= sim.inj_delay + sim.inj_dur)\n            plt.plot(X[t_idx], Y[t_idx])\n        else:\n            plt.plot(X, Y)\n        if title is None:\n            title = type(sim).__name__\n        plt.title(title)\n        plt.xlabel(xlabel)\n        plt.ylabel(ylabel)\n    return X, Y\n</code></pre>"},{"location":"api/slurm/","title":"SLURM API Reference","text":"<p>This page provides API reference documentation for the SLURM module, which contains functions and classes for managing batch simulations on SLURM-based HPC clusters.</p>"},{"location":"api/slurm/#utility-functions","title":"Utility Functions","text":""},{"location":"api/slurm/#bmtool.SLURM.check_job_status","title":"<code>bmtool.SLURM.check_job_status(job_id)</code>","text":"<p>Checks the status of a SLURM job using scontrol.</p> <p>Args:     job_id (str): The SLURM job ID.</p> <p>Returns:     str: The state of the job.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def check_job_status(job_id):\n    \"\"\"\n    Checks the status of a SLURM job using scontrol.\n\n    Args:\n        job_id (str): The SLURM job ID.\n\n    Returns:\n        str: The state of the job.\n    \"\"\"\n    try:\n        result = subprocess.run([\"scontrol\", \"show\", \"job\", job_id], capture_output=True, text=True)\n        if result.returncode != 0:\n            # this check is not needed if check_interval is less than 5 min (~300 seconds)\n            if \"slurm_load_jobs error: Invalid job id specified\" in result.stderr:\n                return \"COMPLETED\"  # Treat invalid job ID as completed because scontrol expires and removed job info when done.\n            # raise Exception(f\"Error checking job status: {result.stderr}\")\n\n        job_state = None\n        for line in result.stdout.split(\"\\n\"):\n            if \"JobState=\" in line:\n                job_state = line.strip().split(\"JobState=\")[1].split()[0]\n                break\n\n        if job_state is None:\n            raise Exception(f\"Failed to retrieve job status for job ID: {job_id}\")\n\n        return job_state\n    except Exception as e:\n        print(f\"Exception while checking job status: {e}\", flush=True)\n        return \"UNKNOWN\"\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.submit_job","title":"<code>bmtool.SLURM.submit_job(script_path)</code>","text":"<p>Submits a SLURM job script.</p> <p>Args:     script_path (str): The path to the SLURM job script.</p> <p>Returns:     str: The job ID of the submitted job.</p> <p>Raises:     Exception: If there is an error in submitting the job.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def submit_job(script_path):\n    \"\"\"\n    Submits a SLURM job script.\n\n    Args:\n        script_path (str): The path to the SLURM job script.\n\n    Returns:\n        str: The job ID of the submitted job.\n\n    Raises:\n        Exception: If there is an error in submitting the job.\n    \"\"\"\n    result = subprocess.run([\"sbatch\", script_path], capture_output=True, text=True)\n    if result.returncode != 0:\n        raise Exception(f\"Error submitting job: {result.stderr}\")\n    job_id = result.stdout.strip().split()[-1]\n    return job_id\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.send_teams_message","title":"<code>bmtool.SLURM.send_teams_message(webhook, message)</code>","text":"<p>Sends a message to a teams channel or chat</p> <p>Args:     webhook (str): A microsoft teams webhook     message (str): A message to send in the chat/channel</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def send_teams_message(webhook, message):\n    \"\"\"Sends a message to a teams channel or chat\n\n    Args:\n        webhook (str): A microsoft teams webhook\n        message (str): A message to send in the chat/channel\n    \"\"\"\n    message = {\"text\": f\"{message}\"}\n\n    # Send POST request to trigger the flow\n    response = requests.post(\n        webhook,\n        json=message,  # Using 'json' instead of 'data' for automatic serialization\n        headers={\"Content-Type\": \"application/json\"},\n    )\n</code></pre>"},{"location":"api/slurm/#parameter-sweep-classes","title":"Parameter Sweep Classes","text":""},{"location":"api/slurm/#bmtool.SLURM.seedSweep","title":"<code>bmtool.SLURM.seedSweep</code>","text":"Source code in <code>bmtool/SLURM.py</code> <pre><code>class seedSweep:\n    def __init__(self, json_file_path, param_name):\n        \"\"\"\n        Initializes the seedSweep instance.\n\n        Args:\n            json_file_path (str): Path to the JSON file to be updated.\n            param_name (str): The name of the parameter to be modified.\n        \"\"\"\n        self.json_file_path = json_file_path\n        self.param_name = param_name\n\n    def edit_json(self, new_value):\n        \"\"\"\n        Updates the JSON file with a new parameter value.\n\n        Args:\n            new_value: The new value for the parameter.\n        \"\"\"\n        with open(self.json_file_path, \"r\") as f:\n            data = json.load(f)\n\n        data[self.param_name] = new_value\n\n        with open(self.json_file_path, \"w\") as f:\n            json.dump(data, f, indent=4)\n\n        print(\n            f\"JSON file '{self.json_file_path}' modified successfully with {self.param_name}={new_value}.\",\n            flush=True,\n        )\n\n    def change_json_file_path(self, new_json_file_path):\n        self.json_file_path = new_json_file_path\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.seedSweep.__init__","title":"<code>__init__(json_file_path, param_name)</code>","text":"<p>Initializes the seedSweep instance.</p> <p>Args:     json_file_path (str): Path to the JSON file to be updated.     param_name (str): The name of the parameter to be modified.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def __init__(self, json_file_path, param_name):\n    \"\"\"\n    Initializes the seedSweep instance.\n\n    Args:\n        json_file_path (str): Path to the JSON file to be updated.\n        param_name (str): The name of the parameter to be modified.\n    \"\"\"\n    self.json_file_path = json_file_path\n    self.param_name = param_name\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.seedSweep.edit_json","title":"<code>edit_json(new_value)</code>","text":"<p>Updates the JSON file with a new parameter value.</p> <p>Args:     new_value: The new value for the parameter.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def edit_json(self, new_value):\n    \"\"\"\n    Updates the JSON file with a new parameter value.\n\n    Args:\n        new_value: The new value for the parameter.\n    \"\"\"\n    with open(self.json_file_path, \"r\") as f:\n        data = json.load(f)\n\n    data[self.param_name] = new_value\n\n    with open(self.json_file_path, \"w\") as f:\n        json.dump(data, f, indent=4)\n\n    print(\n        f\"JSON file '{self.json_file_path}' modified successfully with {self.param_name}={new_value}.\",\n        flush=True,\n    )\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.seedSweep.change_json_file_path","title":"<code>change_json_file_path(new_json_file_path)</code>","text":"Source code in <code>bmtool/SLURM.py</code> <pre><code>def change_json_file_path(self, new_json_file_path):\n    self.json_file_path = new_json_file_path\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.multiSeedSweep","title":"<code>bmtool.SLURM.multiSeedSweep</code>","text":"<p>               Bases: <code>seedSweep</code></p> <p>MultSeedSweeps are centered around some base JSON cell file. When that base JSON is updated, the other JSONs change according to their ratio with the base JSON.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>class multiSeedSweep(seedSweep):\n    \"\"\"\n    MultSeedSweeps are centered around some base JSON cell file. When that base JSON is updated, the other JSONs\n    change according to their ratio with the base JSON.\n    \"\"\"\n\n    def __init__(self, base_json_file_path, param_name, syn_dict, base_ratio=1):\n        \"\"\"\n        Initializes the multipleSeedSweep instance.\n\n        Args:\n            base_json_file_path (str): File path for the base JSON file.\n            param_name (str): The name of the parameter to be modified.\n            syn_dict_list (list): A list containing dictionaries with the 'json_file_path' and 'ratio' (in comparison to the base_json) for each JSON file.\n            base_ratio (float): The ratio between the other JSONs; usually the current value for the parameter.\n        \"\"\"\n        super().__init__(base_json_file_path, param_name)\n        self.syn_dict_for_multi = syn_dict\n        self.base_ratio = base_ratio\n\n    def edit_all_jsons(self, new_value):\n        \"\"\"\n        Updates the base JSON file with a new parameter value and then updates the other JSON files based on the ratio.\n\n        Args:\n            new_value: The new value for the parameter in the base JSON.\n        \"\"\"\n        self.edit_json(new_value)\n        base_ratio = self.base_ratio\n\n        json_file_path = self.syn_dict_for_multi[\"json_file_path\"]\n        new_ratio = self.syn_dict_for_multi[\"ratio\"] / base_ratio\n\n        with open(json_file_path, \"r\") as f:\n            data = json.load(f)\n        altered_value = new_ratio * new_value\n        data[self.param_name] = altered_value\n\n        with open(json_file_path, \"w\") as f:\n            json.dump(data, f, indent=4)\n\n        print(\n            f\"JSON file '{json_file_path}' modified successfully with {self.param_name}={altered_value}.\",\n            flush=True,\n        )\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.multiSeedSweep.__init__","title":"<code>__init__(base_json_file_path, param_name, syn_dict, base_ratio=1)</code>","text":"<p>Initializes the multipleSeedSweep instance.</p> <p>Args:     base_json_file_path (str): File path for the base JSON file.     param_name (str): The name of the parameter to be modified.     syn_dict_list (list): A list containing dictionaries with the 'json_file_path' and 'ratio' (in comparison to the base_json) for each JSON file.     base_ratio (float): The ratio between the other JSONs; usually the current value for the parameter.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def __init__(self, base_json_file_path, param_name, syn_dict, base_ratio=1):\n    \"\"\"\n    Initializes the multipleSeedSweep instance.\n\n    Args:\n        base_json_file_path (str): File path for the base JSON file.\n        param_name (str): The name of the parameter to be modified.\n        syn_dict_list (list): A list containing dictionaries with the 'json_file_path' and 'ratio' (in comparison to the base_json) for each JSON file.\n        base_ratio (float): The ratio between the other JSONs; usually the current value for the parameter.\n    \"\"\"\n    super().__init__(base_json_file_path, param_name)\n    self.syn_dict_for_multi = syn_dict\n    self.base_ratio = base_ratio\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.multiSeedSweep.edit_all_jsons","title":"<code>edit_all_jsons(new_value)</code>","text":"<p>Updates the base JSON file with a new parameter value and then updates the other JSON files based on the ratio.</p> <p>Args:     new_value: The new value for the parameter in the base JSON.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def edit_all_jsons(self, new_value):\n    \"\"\"\n    Updates the base JSON file with a new parameter value and then updates the other JSON files based on the ratio.\n\n    Args:\n        new_value: The new value for the parameter in the base JSON.\n    \"\"\"\n    self.edit_json(new_value)\n    base_ratio = self.base_ratio\n\n    json_file_path = self.syn_dict_for_multi[\"json_file_path\"]\n    new_ratio = self.syn_dict_for_multi[\"ratio\"] / base_ratio\n\n    with open(json_file_path, \"r\") as f:\n        data = json.load(f)\n    altered_value = new_ratio * new_value\n    data[self.param_name] = altered_value\n\n    with open(json_file_path, \"w\") as f:\n        json.dump(data, f, indent=4)\n\n    print(\n        f\"JSON file '{json_file_path}' modified successfully with {self.param_name}={altered_value}.\",\n        flush=True,\n    )\n</code></pre>"},{"location":"api/slurm/#simulation-block-management","title":"Simulation Block Management","text":""},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock","title":"<code>bmtool.SLURM.SimulationBlock</code>","text":"Source code in <code>bmtool/SLURM.py</code> <pre><code>class SimulationBlock:\n    def __init__(\n        self,\n        block_name,\n        time,\n        partition,\n        nodes,\n        ntasks,\n        mem,\n        simulation_cases,\n        output_base_dir,\n        account=None,\n        additional_commands=None,\n        status_list=[\"COMPLETED\", \"FAILED\", \"CANCELLED\"],\n        component_path=None,\n    ):\n        \"\"\"\n        Initializes the SimulationBlock instance.\n\n        Args:\n            block_name (str): Name of the block.\n            time (str): Time limit for the job.\n            partition (str): Partition to submit the job to.\n            nodes (int): Number of nodes to request.\n            ntasks (int): Number of tasks.\n            mem (int) : Number of gigabytes (per node)\n            simulation_cases (dict): Dictionary of simulation cases with their commands.\n            output_base_dir (str): Base directory for the output files.\n            account (str) : account to charge on HPC\n            additional commands (list): commands to run before bmtk model starts useful for loading modules\n            status_list (list): List of things to check before running next block.\n                Adding RUNNING runs blocks faster but uses MUCH more resources and is only recommended on large HPC\n        \"\"\"\n        self.block_name = block_name\n        self.time = time\n        self.partition = partition\n        self.nodes = nodes\n        self.ntasks = ntasks\n        self.mem = mem\n        self.simulation_cases = simulation_cases\n        self.output_base_dir = output_base_dir\n        self.account = account\n        self.additional_commands = additional_commands if additional_commands is not None else []\n        self.status_list = status_list\n        self.job_ids = []\n        self.component_path = component_path\n\n    def create_batch_script(self, case_name, command):\n        \"\"\"\n        Creates a SLURM batch script for the given simulation case.\n\n        Args:\n            case_name (str): Name of the simulation case.\n            command (str): Command to run the simulation.\n\n        Returns:\n            str: Path to the batch script file.\n        \"\"\"\n        block_output_dir = os.path.join(\n            self.output_base_dir, self.block_name\n        )  # Create block-specific output folder\n        case_output_dir = os.path.join(\n            block_output_dir, case_name\n        )  # Create case-specific output folder\n        os.makedirs(case_output_dir, exist_ok=True)\n\n        batch_script_path = os.path.join(block_output_dir, f\"{case_name}_script.sh\")\n        additional_commands_str = \"\\n\".join(self.additional_commands)\n        # Conditional account linegit\n        account_line = f\"#SBATCH --account={self.account}\\n\" if self.account else \"\"\n        env_var_component_path = (\n            f\"export COMPONENT_PATH={self.component_path}\" if self.component_path else \"\"\n        )\n        mem_per_cpu = int(\n            np.ceil(int(self.mem) / int(self.ntasks))\n        )  # do ceil cause more mem is always better then less\n\n        # Write the batch script to the file\n        with open(batch_script_path, \"w\") as script_file:\n            script_file.write(\n                f\"\"\"#!/bin/bash\n#SBATCH --job-name={self.block_name}_{case_name}\n#SBATCH --output={block_output_dir}/%x_%j.out\n#SBATCH --error={block_output_dir}/%x_%j.err\n#SBATCH --time={self.time}\n#SBATCH --partition={self.partition}\n#SBATCH --nodes={self.nodes}\n#SBATCH --ntasks={self.ntasks}\n#SBATCH --mem-per-cpu={mem_per_cpu}G\n{account_line}\n\n# Additional user-defined commands\n{additional_commands_str}\n\n#enviroment vars\n{env_var_component_path}\n\nexport OUTPUT_DIR={case_output_dir}\n\n{command}\n\"\"\"\n            )\n\n        # print(f\"Batch script created: {batch_script_path}\", flush=True)\n\n        return batch_script_path\n\n    def submit_block(self):\n        \"\"\"\n        Submits all simulation cases in the block as separate SLURM jobs.\n        \"\"\"\n        for case_name, command in self.simulation_cases.items():\n            script_path = self.create_batch_script(case_name, command)\n            result = subprocess.run([\"sbatch\", script_path], capture_output=True, text=True)\n            if result.returncode == 0:\n                job_id = result.stdout.strip().split()[-1]\n                self.job_ids.append(job_id)\n                print(f\"Submitted {case_name} with job ID {job_id}\", flush=True)\n            else:\n                print(f\"Failed to submit {case_name}: {result.stderr}\", flush=True)\n\n    def check_block_status(self):\n        \"\"\"\n        Checks the status of all jobs in the block.\n\n        Returns:\n            bool: True if all jobs in the block are completed, False otherwise.\n        \"\"\"\n        for job_id in self.job_ids:\n            status = check_job_status(job_id)\n            if status not in self.status_list:\n                return False\n        return True\n\n    def check_block_completed(self):\n        \"\"\"checks if all the jobs in the block have been completed by slurm\n\n        Returns:\n            bool: True if all block jobs have been ran, false if job is still running\n        \"\"\"\n        for job_id in self.job_ids:\n            status = check_job_status(job_id)\n            # print(f\"status of job is {status}\")\n            if (\n                status != \"COMPLETED\"\n            ):  # can add PENDING here for debugging NOT FOR ACTUALLY USING IT\n                return False\n        return True\n\n    def check_block_running(self):\n        \"\"\"checks if a job is running\n\n        Returns:\n            bool: True if jobs are RUNNING false if anything else\n        \"\"\"\n        for job_id in self.job_ids:\n            status = check_job_status(job_id)\n            if status != \"RUNNING\":  #\n                return False\n        return True\n\n    def check_block_submited(self):\n        \"\"\"checks if a job is running\n\n        Returns:\n            bool: True if jobs are RUNNING false if anything else\n        \"\"\"\n        for job_id in self.job_ids:\n            status = check_job_status(job_id)\n            if status != \"PENDING\":  #\n                return False\n        return True\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.__init__","title":"<code>__init__(block_name, time, partition, nodes, ntasks, mem, simulation_cases, output_base_dir, account=None, additional_commands=None, status_list=['COMPLETED', 'FAILED', 'CANCELLED'], component_path=None)</code>","text":"<p>Initializes the SimulationBlock instance.</p> <p>Args:     block_name (str): Name of the block.     time (str): Time limit for the job.     partition (str): Partition to submit the job to.     nodes (int): Number of nodes to request.     ntasks (int): Number of tasks.     mem (int) : Number of gigabytes (per node)     simulation_cases (dict): Dictionary of simulation cases with their commands.     output_base_dir (str): Base directory for the output files.     account (str) : account to charge on HPC     additional commands (list): commands to run before bmtk model starts useful for loading modules     status_list (list): List of things to check before running next block.         Adding RUNNING runs blocks faster but uses MUCH more resources and is only recommended on large HPC</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def __init__(\n    self,\n    block_name,\n    time,\n    partition,\n    nodes,\n    ntasks,\n    mem,\n    simulation_cases,\n    output_base_dir,\n    account=None,\n    additional_commands=None,\n    status_list=[\"COMPLETED\", \"FAILED\", \"CANCELLED\"],\n    component_path=None,\n):\n    \"\"\"\n    Initializes the SimulationBlock instance.\n\n    Args:\n        block_name (str): Name of the block.\n        time (str): Time limit for the job.\n        partition (str): Partition to submit the job to.\n        nodes (int): Number of nodes to request.\n        ntasks (int): Number of tasks.\n        mem (int) : Number of gigabytes (per node)\n        simulation_cases (dict): Dictionary of simulation cases with their commands.\n        output_base_dir (str): Base directory for the output files.\n        account (str) : account to charge on HPC\n        additional commands (list): commands to run before bmtk model starts useful for loading modules\n        status_list (list): List of things to check before running next block.\n            Adding RUNNING runs blocks faster but uses MUCH more resources and is only recommended on large HPC\n    \"\"\"\n    self.block_name = block_name\n    self.time = time\n    self.partition = partition\n    self.nodes = nodes\n    self.ntasks = ntasks\n    self.mem = mem\n    self.simulation_cases = simulation_cases\n    self.output_base_dir = output_base_dir\n    self.account = account\n    self.additional_commands = additional_commands if additional_commands is not None else []\n    self.status_list = status_list\n    self.job_ids = []\n    self.component_path = component_path\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.create_batch_script","title":"<code>create_batch_script(case_name, command)</code>","text":"<p>Creates a SLURM batch script for the given simulation case.</p> <p>Args:     case_name (str): Name of the simulation case.     command (str): Command to run the simulation.</p> <p>Returns:     str: Path to the batch script file.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>    def create_batch_script(self, case_name, command):\n        \"\"\"\n        Creates a SLURM batch script for the given simulation case.\n\n        Args:\n            case_name (str): Name of the simulation case.\n            command (str): Command to run the simulation.\n\n        Returns:\n            str: Path to the batch script file.\n        \"\"\"\n        block_output_dir = os.path.join(\n            self.output_base_dir, self.block_name\n        )  # Create block-specific output folder\n        case_output_dir = os.path.join(\n            block_output_dir, case_name\n        )  # Create case-specific output folder\n        os.makedirs(case_output_dir, exist_ok=True)\n\n        batch_script_path = os.path.join(block_output_dir, f\"{case_name}_script.sh\")\n        additional_commands_str = \"\\n\".join(self.additional_commands)\n        # Conditional account linegit\n        account_line = f\"#SBATCH --account={self.account}\\n\" if self.account else \"\"\n        env_var_component_path = (\n            f\"export COMPONENT_PATH={self.component_path}\" if self.component_path else \"\"\n        )\n        mem_per_cpu = int(\n            np.ceil(int(self.mem) / int(self.ntasks))\n        )  # do ceil cause more mem is always better then less\n\n        # Write the batch script to the file\n        with open(batch_script_path, \"w\") as script_file:\n            script_file.write(\n                f\"\"\"#!/bin/bash\n#SBATCH --job-name={self.block_name}_{case_name}\n#SBATCH --output={block_output_dir}/%x_%j.out\n#SBATCH --error={block_output_dir}/%x_%j.err\n#SBATCH --time={self.time}\n#SBATCH --partition={self.partition}\n#SBATCH --nodes={self.nodes}\n#SBATCH --ntasks={self.ntasks}\n#SBATCH --mem-per-cpu={mem_per_cpu}G\n{account_line}\n\n# Additional user-defined commands\n{additional_commands_str}\n\n#enviroment vars\n{env_var_component_path}\n\nexport OUTPUT_DIR={case_output_dir}\n\n{command}\n\"\"\"\n            )\n\n        # print(f\"Batch script created: {batch_script_path}\", flush=True)\n\n        return batch_script_path\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.submit_block","title":"<code>submit_block()</code>","text":"<p>Submits all simulation cases in the block as separate SLURM jobs.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def submit_block(self):\n    \"\"\"\n    Submits all simulation cases in the block as separate SLURM jobs.\n    \"\"\"\n    for case_name, command in self.simulation_cases.items():\n        script_path = self.create_batch_script(case_name, command)\n        result = subprocess.run([\"sbatch\", script_path], capture_output=True, text=True)\n        if result.returncode == 0:\n            job_id = result.stdout.strip().split()[-1]\n            self.job_ids.append(job_id)\n            print(f\"Submitted {case_name} with job ID {job_id}\", flush=True)\n        else:\n            print(f\"Failed to submit {case_name}: {result.stderr}\", flush=True)\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.check_block_status","title":"<code>check_block_status()</code>","text":"<p>Checks the status of all jobs in the block.</p> <p>Returns:     bool: True if all jobs in the block are completed, False otherwise.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def check_block_status(self):\n    \"\"\"\n    Checks the status of all jobs in the block.\n\n    Returns:\n        bool: True if all jobs in the block are completed, False otherwise.\n    \"\"\"\n    for job_id in self.job_ids:\n        status = check_job_status(job_id)\n        if status not in self.status_list:\n            return False\n    return True\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.check_block_completed","title":"<code>check_block_completed()</code>","text":"<p>checks if all the jobs in the block have been completed by slurm</p> <p>Returns:     bool: True if all block jobs have been ran, false if job is still running</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def check_block_completed(self):\n    \"\"\"checks if all the jobs in the block have been completed by slurm\n\n    Returns:\n        bool: True if all block jobs have been ran, false if job is still running\n    \"\"\"\n    for job_id in self.job_ids:\n        status = check_job_status(job_id)\n        # print(f\"status of job is {status}\")\n        if (\n            status != \"COMPLETED\"\n        ):  # can add PENDING here for debugging NOT FOR ACTUALLY USING IT\n            return False\n    return True\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.check_block_running","title":"<code>check_block_running()</code>","text":"<p>checks if a job is running</p> <p>Returns:     bool: True if jobs are RUNNING false if anything else</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def check_block_running(self):\n    \"\"\"checks if a job is running\n\n    Returns:\n        bool: True if jobs are RUNNING false if anything else\n    \"\"\"\n    for job_id in self.job_ids:\n        status = check_job_status(job_id)\n        if status != \"RUNNING\":  #\n            return False\n    return True\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.SimulationBlock.check_block_submited","title":"<code>check_block_submited()</code>","text":"<p>checks if a job is running</p> <p>Returns:     bool: True if jobs are RUNNING false if anything else</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def check_block_submited(self):\n    \"\"\"checks if a job is running\n\n    Returns:\n        bool: True if jobs are RUNNING false if anything else\n    \"\"\"\n    for job_id in self.job_ids:\n        status = check_job_status(job_id)\n        if status != \"PENDING\":  #\n            return False\n    return True\n</code></pre>"},{"location":"api/slurm/#file-transfer","title":"File Transfer","text":""},{"location":"api/slurm/#bmtool.SLURM.get_relative_path","title":"<code>bmtool.SLURM.get_relative_path(endpoint, absolute_path)</code>","text":"<p>Convert absolute path to relative path for Globus transfer.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def get_relative_path(endpoint, absolute_path):\n    \"\"\"Convert absolute path to relative path for Globus transfer.\"\"\"\n    try:\n        # Get the directories at the mount point\n        result = subprocess.run(\n            [\"globus\", \"ls\", f\"{endpoint}:/\"], capture_output=True, text=True, check=True\n        )\n        dirs = set(result.stdout.splitlines())  # Convert to a set for quicker lookup\n\n        # Split the absolute path into parts\n        path_parts = absolute_path.strip(\"/\").split(\"/\")\n\n        # Find the first matching directory in the list\n        for i, part in enumerate(path_parts):\n            if part + \"/\" in dirs:\n                # The mount point is everything up to and including this directory\n                mount_point = \"/\" + \"/\".join(path_parts[:i])\n                relative_path = absolute_path.replace(mount_point, \"\", 1).lstrip(\"/\")\n                return relative_path\n\n        print(\"Error: Could not determine relative path.\")\n        return None\n    except subprocess.CalledProcessError as e:\n        print(f\"Error retrieving directories from Globus: {e}\")\n        return None\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.globus_transfer","title":"<code>bmtool.SLURM.globus_transfer(source_endpoint, dest_endpoint, source_path, dest_path)</code>","text":"<p>Transfers file using custom globus transfer function. For more info see https://github.com/GregGlickert/transfer-files/blob/main/globus_transfer.sh work in progress still... kinda forgot about this</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def globus_transfer(source_endpoint, dest_endpoint, source_path, dest_path):\n    \"\"\"\n    Transfers file using custom globus transfer function.\n    For more info see https://github.com/GregGlickert/transfer-files/blob/main/globus_transfer.sh\n    work in progress still... kinda forgot about this\n    \"\"\"\n    relative_source_path = get_relative_path(source_endpoint, source_path)\n    if relative_source_path is None:\n        print(\"Transfer aborted: Could not determine relative source path.\")\n        return\n\n    command = f\"globus transfer {source_endpoint}:{relative_source_path} {dest_endpoint}:{dest_path} --label 'bmtool slurm transfer'\"\n    os.system(command)\n</code></pre>"},{"location":"api/slurm/#blockrunner","title":"BlockRunner","text":""},{"location":"api/slurm/#bmtool.SLURM.BlockRunner","title":"<code>bmtool.SLURM.BlockRunner</code>","text":"<p>Class to handle submitting multiple blocks sequentially.</p> <p>Attributes:     blocks (list): List of SimulationBlock instances to be run.     json_editor (seedSweep or multiSweep): Instance of seedSweep to edit JSON file.     param_values (list): List of values for the parameter to be modified.     webhook (str): a microsoft webhook for teams. When used will send teams messages to the hook!</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>class BlockRunner:\n    \"\"\"\n    Class to handle submitting multiple blocks sequentially.\n\n    Attributes:\n        blocks (list): List of SimulationBlock instances to be run.\n        json_editor (seedSweep or multiSweep): Instance of seedSweep to edit JSON file.\n        param_values (list): List of values for the parameter to be modified.\n        webhook (str): a microsoft webhook for teams. When used will send teams messages to the hook!\n    \"\"\"\n\n    def __init__(\n        self,\n        blocks,\n        json_editor=None,\n        json_file_path=None,\n        param_name=None,\n        param_values=None,\n        check_interval=60,\n        syn_dict=None,\n        webhook=None,\n    ):\n        self.blocks = blocks\n        self.json_editor = json_editor\n        self.param_values = param_values\n        self.check_interval = check_interval\n        self.webhook = webhook\n        self.param_name = param_name\n        self.json_file_path = json_file_path\n        self.syn_dict = syn_dict\n        # Store original component paths to restore later\n        self.original_component_paths = [block.component_path for block in self.blocks]\n\n    def restore_component_paths(self):\n        \"\"\"\n        Restores all blocks' component_path to their original values.\n        \"\"\"\n        for i, block in enumerate(self.blocks):\n            block.component_path = self.original_component_paths[i]\n        print(\"Component paths restored to original values.\", flush=True)\n\n    def submit_blocks_sequentially(self):\n        \"\"\"\n        Submits all blocks sequentially, ensuring each block starts only after the previous block has completed or is running.\n        Updates the JSON file with new parameters before each block run.\n        \"\"\"\n        for i, block in enumerate(self.blocks):\n            print(block.output_base_dir)\n            # Update JSON file with new parameter value\n            if self.json_file_path is None and self.param_values is None:\n                source_dir = block.component_path\n                destination_dir = f\"{source_dir}{i+1}\"\n                block.component_path = destination_dir\n                shutil.copytree(\n                    source_dir, destination_dir, dirs_exist_ok=True\n                )  # create new components folder\n                print(f\"skipping json editing for block {block.block_name}\", flush=True)\n            else:\n                if len(self.blocks) != len(self.param_values):\n                    raise Exception(\"Number of blocks needs to each number of params given\")\n                new_value = self.param_values[i]\n                # hope this path is correct\n                source_dir = block.component_path\n                destination_dir = f\"{source_dir}{i+1}\"\n                block.component_path = destination_dir\n\n                shutil.copytree(\n                    source_dir, destination_dir, dirs_exist_ok=True\n                )  # create new components folder\n                json_file_path = os.path.join(destination_dir, self.json_file_path)\n\n                if self.syn_dict is None:\n                    json_editor = seedSweep(json_file_path, self.param_name)\n                    json_editor.edit_json(new_value)\n                else:\n                    # need to keep the orignal around\n                    syn_dict_temp = copy.deepcopy(self.syn_dict)\n                    json_to_be_ratioed = syn_dict_temp[\"json_file_path\"]\n                    corrected_ratio_path = os.path.join(destination_dir, json_to_be_ratioed)\n                    syn_dict_temp[\"json_file_path\"] = corrected_ratio_path\n                    json_editor = multiSeedSweep(\n                        json_file_path, self.param_name, syn_dict=syn_dict_temp, base_ratio=1\n                    )\n                    json_editor.edit_all_jsons(new_value)\n\n            # Submit the block\n            print(f\"Submitting block: {block.block_name}\", flush=True)\n            block.submit_block()\n            if self.webhook:\n                message = f\"SIMULATION UPDATE: Block {i} has been submitted! There are {(len(self.blocks)-1)-i} left to be submitted\"\n                send_teams_message(self.webhook, message)\n\n            # Wait for the block to complete\n            if i == len(self.blocks) - 1:\n                while not block.check_block_status():\n                    print(f\"Waiting for the last block {i} to complete...\")\n                    time.sleep(self.check_interval)\n            else:  # Not the last block so if job is running lets start a new one (checks status list)\n                while not block.check_block_status():\n                    print(f\"Waiting for block {i} to complete...\")\n                    time.sleep(self.check_interval)\n\n            print(f\"Block {block.block_name} completed.\", flush=True)\n        print(\"All blocks are done!\", flush=True)\n        # Restore component paths to their original values\n        self.restore_component_paths()\n        if self.webhook:\n            message = \"SIMULATION UPDATE: Simulation are Done!\"\n            send_teams_message(self.webhook, message)\n\n    def submit_blocks_parallel(self):\n        \"\"\"\n        submits all the blocks at once onto the queue. To do this the components dir will be cloned and each block will have its own.\n        Also the json_file_path should be the path after the components dir\n        \"\"\"\n        for i, block in enumerate(self.blocks):\n            if self.param_values is None:\n                source_dir = block.component_path\n                destination_dir = f\"{source_dir}{i+1}\"\n                block.component_path = destination_dir\n                shutil.copytree(\n                    source_dir, destination_dir, dirs_exist_ok=True\n                )  # create new components folder\n                print(f\"skipping json editing for block {block.block_name}\", flush=True)\n            else:\n                if block.component_path is None:\n                    raise Exception(\n                        \"Unable to use parallel submitter without defining the component path\"\n                    )\n                new_value = self.param_values[i]\n\n                source_dir = block.component_path\n                destination_dir = f\"{source_dir}{i+1}\"\n                block.component_path = destination_dir\n\n                shutil.copytree(\n                    source_dir, destination_dir, dirs_exist_ok=True\n                )  # create new components folder\n                json_file_path = os.path.join(destination_dir, self.json_file_path)\n\n                if self.syn_dict is None:\n                    json_editor = seedSweep(json_file_path, self.param_name)\n                    json_editor.edit_json(new_value)\n                else:\n                    # need to keep the orignal around\n                    syn_dict_temp = copy.deepcopy(self.syn_dict)\n                    json_to_be_ratioed = syn_dict_temp[\"json_file_path\"]\n                    corrected_ratio_path = os.path.join(destination_dir, json_to_be_ratioed)\n                    syn_dict_temp[\"json_file_path\"] = corrected_ratio_path\n                    json_editor = multiSeedSweep(\n                        json_file_path, self.param_name, syn_dict_temp, base_ratio=1\n                    )\n                    json_editor.edit_all_jsons(new_value)\n                # submit block with new component path\n            print(f\"Submitting block: {block.block_name}\", flush=True)\n            block.submit_block()\n            if i == len(self.blocks) - 1:\n                print(\n                    \"\\nEverything has been submitted. You can close out of this or keep this script running to get a message when everything is finished\\n\"\n                )\n                while not block.check_block_status():\n                    print(f\"Waiting for the last block {i} to complete...\")\n                    time.sleep(self.check_interval)\n\n        print(\"All blocks are done!\", flush=True)\n        # Restore component paths to their original values\n        self.restore_component_paths()\n        if self.webhook:\n            message = \"SIMULATION UPDATE: Simulations are Done!\"\n            send_teams_message(self.webhook, message)\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.BlockRunner.__init__","title":"<code>__init__(blocks, json_editor=None, json_file_path=None, param_name=None, param_values=None, check_interval=60, syn_dict=None, webhook=None)</code>","text":"Source code in <code>bmtool/SLURM.py</code> <pre><code>def __init__(\n    self,\n    blocks,\n    json_editor=None,\n    json_file_path=None,\n    param_name=None,\n    param_values=None,\n    check_interval=60,\n    syn_dict=None,\n    webhook=None,\n):\n    self.blocks = blocks\n    self.json_editor = json_editor\n    self.param_values = param_values\n    self.check_interval = check_interval\n    self.webhook = webhook\n    self.param_name = param_name\n    self.json_file_path = json_file_path\n    self.syn_dict = syn_dict\n    # Store original component paths to restore later\n    self.original_component_paths = [block.component_path for block in self.blocks]\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.BlockRunner.submit_blocks_sequentially","title":"<code>submit_blocks_sequentially()</code>","text":"<p>Submits all blocks sequentially, ensuring each block starts only after the previous block has completed or is running. Updates the JSON file with new parameters before each block run.</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def submit_blocks_sequentially(self):\n    \"\"\"\n    Submits all blocks sequentially, ensuring each block starts only after the previous block has completed or is running.\n    Updates the JSON file with new parameters before each block run.\n    \"\"\"\n    for i, block in enumerate(self.blocks):\n        print(block.output_base_dir)\n        # Update JSON file with new parameter value\n        if self.json_file_path is None and self.param_values is None:\n            source_dir = block.component_path\n            destination_dir = f\"{source_dir}{i+1}\"\n            block.component_path = destination_dir\n            shutil.copytree(\n                source_dir, destination_dir, dirs_exist_ok=True\n            )  # create new components folder\n            print(f\"skipping json editing for block {block.block_name}\", flush=True)\n        else:\n            if len(self.blocks) != len(self.param_values):\n                raise Exception(\"Number of blocks needs to each number of params given\")\n            new_value = self.param_values[i]\n            # hope this path is correct\n            source_dir = block.component_path\n            destination_dir = f\"{source_dir}{i+1}\"\n            block.component_path = destination_dir\n\n            shutil.copytree(\n                source_dir, destination_dir, dirs_exist_ok=True\n            )  # create new components folder\n            json_file_path = os.path.join(destination_dir, self.json_file_path)\n\n            if self.syn_dict is None:\n                json_editor = seedSweep(json_file_path, self.param_name)\n                json_editor.edit_json(new_value)\n            else:\n                # need to keep the orignal around\n                syn_dict_temp = copy.deepcopy(self.syn_dict)\n                json_to_be_ratioed = syn_dict_temp[\"json_file_path\"]\n                corrected_ratio_path = os.path.join(destination_dir, json_to_be_ratioed)\n                syn_dict_temp[\"json_file_path\"] = corrected_ratio_path\n                json_editor = multiSeedSweep(\n                    json_file_path, self.param_name, syn_dict=syn_dict_temp, base_ratio=1\n                )\n                json_editor.edit_all_jsons(new_value)\n\n        # Submit the block\n        print(f\"Submitting block: {block.block_name}\", flush=True)\n        block.submit_block()\n        if self.webhook:\n            message = f\"SIMULATION UPDATE: Block {i} has been submitted! There are {(len(self.blocks)-1)-i} left to be submitted\"\n            send_teams_message(self.webhook, message)\n\n        # Wait for the block to complete\n        if i == len(self.blocks) - 1:\n            while not block.check_block_status():\n                print(f\"Waiting for the last block {i} to complete...\")\n                time.sleep(self.check_interval)\n        else:  # Not the last block so if job is running lets start a new one (checks status list)\n            while not block.check_block_status():\n                print(f\"Waiting for block {i} to complete...\")\n                time.sleep(self.check_interval)\n\n        print(f\"Block {block.block_name} completed.\", flush=True)\n    print(\"All blocks are done!\", flush=True)\n    # Restore component paths to their original values\n    self.restore_component_paths()\n    if self.webhook:\n        message = \"SIMULATION UPDATE: Simulation are Done!\"\n        send_teams_message(self.webhook, message)\n</code></pre>"},{"location":"api/slurm/#bmtool.SLURM.BlockRunner.submit_blocks_parallel","title":"<code>submit_blocks_parallel()</code>","text":"<p>submits all the blocks at once onto the queue. To do this the components dir will be cloned and each block will have its own. Also the json_file_path should be the path after the components dir</p> Source code in <code>bmtool/SLURM.py</code> <pre><code>def submit_blocks_parallel(self):\n    \"\"\"\n    submits all the blocks at once onto the queue. To do this the components dir will be cloned and each block will have its own.\n    Also the json_file_path should be the path after the components dir\n    \"\"\"\n    for i, block in enumerate(self.blocks):\n        if self.param_values is None:\n            source_dir = block.component_path\n            destination_dir = f\"{source_dir}{i+1}\"\n            block.component_path = destination_dir\n            shutil.copytree(\n                source_dir, destination_dir, dirs_exist_ok=True\n            )  # create new components folder\n            print(f\"skipping json editing for block {block.block_name}\", flush=True)\n        else:\n            if block.component_path is None:\n                raise Exception(\n                    \"Unable to use parallel submitter without defining the component path\"\n                )\n            new_value = self.param_values[i]\n\n            source_dir = block.component_path\n            destination_dir = f\"{source_dir}{i+1}\"\n            block.component_path = destination_dir\n\n            shutil.copytree(\n                source_dir, destination_dir, dirs_exist_ok=True\n            )  # create new components folder\n            json_file_path = os.path.join(destination_dir, self.json_file_path)\n\n            if self.syn_dict is None:\n                json_editor = seedSweep(json_file_path, self.param_name)\n                json_editor.edit_json(new_value)\n            else:\n                # need to keep the orignal around\n                syn_dict_temp = copy.deepcopy(self.syn_dict)\n                json_to_be_ratioed = syn_dict_temp[\"json_file_path\"]\n                corrected_ratio_path = os.path.join(destination_dir, json_to_be_ratioed)\n                syn_dict_temp[\"json_file_path\"] = corrected_ratio_path\n                json_editor = multiSeedSweep(\n                    json_file_path, self.param_name, syn_dict_temp, base_ratio=1\n                )\n                json_editor.edit_all_jsons(new_value)\n            # submit block with new component path\n        print(f\"Submitting block: {block.block_name}\", flush=True)\n        block.submit_block()\n        if i == len(self.blocks) - 1:\n            print(\n                \"\\nEverything has been submitted. You can close out of this or keep this script running to get a message when everything is finished\\n\"\n            )\n            while not block.check_block_status():\n                print(f\"Waiting for the last block {i} to complete...\")\n                time.sleep(self.check_interval)\n\n    print(\"All blocks are done!\", flush=True)\n    # Restore component paths to their original values\n    self.restore_component_paths()\n    if self.webhook:\n        message = \"SIMULATION UPDATE: Simulations are Done!\"\n        send_teams_message(self.webhook, message)\n</code></pre>"},{"location":"api/synapses/","title":"Synapses API Reference","text":"<p>This page provides API reference documentation for the Synapses module, which contains tools for creating, tuning and optimizing synaptic connections in NEURON models.</p>"},{"location":"api/synapses/#synapse-tuning","title":"Synapse Tuning","text":""},{"location":"api/synapses/#bmtool.synapses.SynapseTuner","title":"<code>bmtool.synapses.SynapseTuner</code>","text":"Source code in <code>bmtool/synapses.py</code> <pre><code>class SynapseTuner:\n    def __init__(\n        self,\n        conn_type_settings: Optional[Dict[str, dict]] = None,\n        connection: Optional[str] = None,\n        current_name: str = \"i\",\n        mechanisms_dir: Optional[str] = None,\n        templates_dir: Optional[str] = None,\n        config: Optional[str] = None,\n        general_settings: Optional[dict] = None,\n        json_folder_path: Optional[str] = None,\n        other_vars_to_record: Optional[List[str]] = None,\n        slider_vars: Optional[List[str]] = None,\n        hoc_cell: Optional[object] = None,\n        network: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the SynapseTuner class with connection type settings, mechanisms, and template directories.\n\n        Parameters:\n        -----------\n        mechanisms_dir : Optional[str]\n            Directory path containing the compiled mod files needed for NEURON mechanisms.\n        templates_dir : Optional[str]\n            Directory path containing cell template files (.hoc or .py) loaded into NEURON.\n        conn_type_settings : Optional[dict]\n            A dictionary containing connection-specific settings, such as synaptic properties and details.\n        connection : Optional[str]\n            Name of the connection type to be used from the conn_type_settings dictionary.\n        general_settings : Optional[dict]\n            General settings dictionary including parameters like simulation time step, duration, and temperature.\n        json_folder_path : Optional[str]\n            Path to folder containing JSON files with additional synaptic properties to update settings.\n        current_name : str, optional\n            Name of the synaptic current variable to be recorded (default is 'i').\n        other_vars_to_record : Optional[list]\n            List of additional synaptic variables to record during the simulation (e.g., 'Pr', 'Use').\n        slider_vars : Optional[list]\n            List of synaptic variables you would like sliders set up for the STP sliders method by default will use all parameters in spec_syn_param.\n        hoc_cell : Optional[object]\n            An already loaded NEURON cell object. If provided, template loading and cell setup will be skipped.\n        network : Optional[str]\n            Name of the specific network dataset to access from the loaded edges data (e.g., 'network_to_network').\n            If not provided, will use all available networks. When a config file is provided, this enables\n            the network dropdown feature in InteractiveTuner for switching between different networks.\n\n        Network Dropdown Feature:\n        -------------------------\n        When initialized with a BMTK config file, the tuner automatically:\n        1. Loads all available network datasets from the config\n        2. Creates a network dropdown in InteractiveTuner (if multiple networks exist)\n        3. Allows dynamic switching between networks, which rebuilds connection types\n        4. Updates connection dropdown options when network is changed\n        5. Preserves current connection if it exists in the new network, otherwise selects the first available\n        \"\"\"\n        self.hoc_cell = hoc_cell\n        # Store config and network information for network dropdown functionality\n        self.config = config  # Store config path for network dropdown functionality\n        self.available_networks = []  # Store available networks from config file\n        self.current_network = network  # Store current network selection\n        # Cache for loaded dynamics params JSON by filename to avoid repeated disk reads\n        self._syn_params_cache = {}\n        h.load_file(\"stdrun.hoc\")\n\n        if hoc_cell is None:\n            if config is None and (mechanisms_dir is None or templates_dir is None):\n                raise ValueError(\n                    \"Either a config file, both mechanisms_dir and templates_dir, or a hoc_cell must be provided.\"\n                )\n\n            if config is None:\n                neuron.load_mechanisms(mechanisms_dir)\n                h.load_file(templates_dir)\n            else:\n                # loads both mech and templates\n                load_templates_from_config(config)\n                # Load available networks from config for network dropdown feature\n                self._load_available_networks()\n                # Prebuild connection type settings for each available network to\n                # make network switching in the UI fast. This will make __init__ slower\n                # but dramatically speed up response when changing the network dropdown.\n                self._prebuilt_conn_type_settings = {}\n                try:\n                    for net in self.available_networks:\n                        self._prebuilt_conn_type_settings[net] = (\n                            self._build_conn_type_settings_from_config(config, network=net)\n                        )\n                except Exception as e:\n                    print(f\"Warning: error prebuilding conn_type_settings for networks: {e}\")\n\n        if conn_type_settings is None:\n            if config is not None:\n                print(\"Building conn_type_settings from BMTK config files...\")\n                # If we prebuilt per-network settings, use the one for the requested network\n                if hasattr(self, \"_prebuilt_conn_type_settings\") and network in getattr(\n                    self, \"_prebuilt_conn_type_settings\", {}\n                ):\n                    conn_type_settings = self._prebuilt_conn_type_settings[network]\n                else:\n                    conn_type_settings = self._build_conn_type_settings_from_config(\n                        config, network=network\n                    )\n                print(\n                    f\"Found {len(conn_type_settings)} connection types: {list(conn_type_settings.keys())}\"\n                )\n\n                # If connection is not specified, use the first available connection\n                if connection is None and conn_type_settings:\n                    connection = list(conn_type_settings.keys())[0]\n                    print(f\"No connection specified, using first available: {connection}\")\n            else:\n                raise ValueError(\"conn_type_settings must be provided if config is not specified.\")\n\n        if connection is None:\n            raise ValueError(\"connection must be provided or inferable from conn_type_settings.\")\n        if connection not in conn_type_settings:\n            raise ValueError(f\"connection '{connection}' not found in conn_type_settings.\")\n\n        self.conn_type_settings: dict = conn_type_settings\n        if json_folder_path:\n            print(f\"updating settings from json path {json_folder_path}\")\n            self._update_spec_syn_param(json_folder_path)\n        # Use default general settings if not provided\n        if general_settings is None:\n            self.general_settings: dict = DEFAULT_GENERAL_SETTINGS.copy()\n        else:\n            # Merge defaults with user-provided\n            self.general_settings = {**DEFAULT_GENERAL_SETTINGS, **general_settings}\n\n        # Store the initial connection name and set up connection\n        self.current_connection = connection\n        self.conn = self.conn_type_settings[connection]\n        self._current_cell_type = self.conn[\"spec_settings\"][\"post_cell\"]\n        self.synaptic_props = self.conn[\"spec_syn_param\"]\n        self.vclamp = self.general_settings[\"vclamp\"]\n        self.current_name = current_name\n        self.other_vars_to_record = other_vars_to_record or []\n        self.ispk = None\n        self.input_mode = False  # Add input_mode attribute\n        self.last_figure = None  # Store reference to last generated figure\n\n        # Store original slider_vars for connection switching\n        self.original_slider_vars = slider_vars or list(self.synaptic_props.keys())\n\n        if slider_vars:\n            # Start by filtering based on keys in slider_vars\n            self.slider_vars = {\n                key: value for key, value in self.synaptic_props.items() if key in slider_vars\n            }\n            # Iterate over slider_vars and check for missing keys in self.synaptic_props\n            for key in slider_vars:\n                # If the key is missing from synaptic_props, get the value using getattr\n                if key not in self.synaptic_props:\n                    try:\n                        self._set_up_cell()\n                        self._set_up_synapse()\n                        value = getattr(self.syn, key)\n                        self.slider_vars[key] = value\n                    except AttributeError as e:\n                        print(f\"Error accessing '{key}' in syn {self.syn}: {e}\")\n        else:\n            self.slider_vars = self.synaptic_props\n\n        h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"]\n        h.dt = self.general_settings[\"dt\"]  # Time step (resolution) of the simulation in ms\n        h.steps_per_ms = 1 / h.dt\n        h.celsius = self.general_settings[\"celsius\"]\n\n        # get some stuff set up we need for both SingleEvent and Interactive Tuner\n        # Only set up cell if hoc_cell was not provided\n        if self.hoc_cell is None:\n            self._set_up_cell()\n        else:\n            self.cell = self.hoc_cell\n        self._set_up_synapse()\n\n        self.nstim = h.NetStim()\n        self.nstim2 = h.NetStim()\n\n        self.vcl = h.VClamp(self.cell.soma[0](0.5))\n\n        self.nc = h.NetCon(\n            self.nstim,\n            self.syn,\n            self.general_settings[\"threshold\"],\n            self.general_settings[\"delay\"],\n            self.general_settings[\"weight\"],\n        )\n        self.nc2 = h.NetCon(\n            self.nstim2,\n            self.syn,\n            self.general_settings[\"threshold\"],\n            self.general_settings[\"delay\"],\n            self.general_settings[\"weight\"],\n        )\n\n        self._set_up_recorders()\n\n    def _build_conn_type_settings_from_config(\n        self, config_path: str, node_set: Optional[str] = None, network: Optional[str] = None\n    ) -&gt; Dict[str, dict]:\n        \"\"\"\n        Build conn_type_settings from BMTK simulation and circuit config files using the method used by relation matrix function in util.\n\n        Parameters:\n        -----------\n        config_path : str\n            Path to the simulation config JSON file.\n        node_set : Optional[str]\n            Specific node set to filter connections for. If None, processes all connections.\n        network : Optional[str]\n            Name of the specific network dataset to access (e.g., 'network_to_network').\n            If None, processes all available networks.\n\n        Returns:\n        --------\n        Dict[str, dict]\n            Dictionary with connection names as keys and connection settings as values.\n\n        NOTE: a lot of this code could probs be made a bit more simple or just removed i kinda tried a bunch of things and it works now\n        but is kinda complex and some code is probs note needed\n\n        \"\"\"\n        # Load configuration and get nodes and edges using util.py methods\n        config = load_config(config_path)\n        # Ensure the config dict knows its source path so path substitutions can be resolved\n        try:\n            # load_config may return a dict; store path used so callers can resolve $COMPONENTS_DIR\n            config[\"config_path\"] = config_path\n        except Exception:\n            pass\n        nodes = load_nodes_from_config(config_path)\n        edges = load_edges_from_config(config_path)\n\n        conn_type_settings = {}\n\n        # If a specific network is requested, only process that one\n        if network:\n            if network not in edges:\n                print(\n                    f\"Warning: Network '{network}' not found in edges. Available networks: {list(edges.keys())}\"\n                )\n                return conn_type_settings\n            edge_datasets = {network: edges[network]}\n        else:\n            edge_datasets = edges\n\n        # Process each edge dataset using the util.py approach\n        for edge_dataset_name, edge_df in edge_datasets.items():\n            if edge_df.empty:\n                continue\n\n            # Create merged DataFrames with source and target node information like util.py does\n            source_node_df = None\n            target_node_df = None\n\n            # First, try to deterministically parse the edge_dataset_name for patterns like '&lt;src&gt;_to_&lt;tgt&gt;'\n            # e.g., 'network_to_network', 'extnet_to_network'\n            if \"_to_\" in edge_dataset_name:\n                parts = edge_dataset_name.split(\"_to_\")\n                if len(parts) == 2:\n                    src_name, tgt_name = parts\n                    if src_name in nodes:\n                        source_node_df = nodes[src_name].add_prefix(\"source_\")\n                    if tgt_name in nodes:\n                        target_node_df = nodes[tgt_name].add_prefix(\"target_\")\n\n            # If not found by parsing name, fall back to inspecting a sample edge row which contains\n            # explicit 'source_population' and 'target_population' fields (this avoids reversing source/target)\n            if source_node_df is None or target_node_df is None:\n                sample_edge = edge_df.iloc[0] if len(edge_df) &gt; 0 else None\n                if sample_edge is not None:\n                    # Use explicit population names from the edge entry\n                    source_pop_name = sample_edge.get(\"source_population\", \"\")\n                    target_pop_name = sample_edge.get(\"target_population\", \"\")\n                    if source_pop_name in nodes:\n                        source_node_df = nodes[source_pop_name].add_prefix(\"source_\")\n                    if target_pop_name in nodes:\n                        target_node_df = nodes[target_pop_name].add_prefix(\"target_\")\n\n            # As a last resort, attempt to heuristically match by prefix/suffix of the dataset name\n            if source_node_df is None or target_node_df is None:\n                for pop_name, node_df in nodes.items():\n                    if source_node_df is None and (\n                        edge_dataset_name.startswith(pop_name)\n                        or edge_dataset_name.endswith(pop_name)\n                    ):\n                        source_node_df = node_df.add_prefix(\"source_\")\n                    elif target_node_df is None and (\n                        edge_dataset_name.startswith(pop_name)\n                        or edge_dataset_name.endswith(pop_name)\n                    ):\n                        target_node_df = node_df.add_prefix(\"target_\")\n\n            # If we still don't have the node data, skip this edge dataset\n            if source_node_df is None or target_node_df is None:\n                print(f\"Warning: Could not find node data for edge dataset {edge_dataset_name}\")\n                continue\n\n            # Merge edge data with source node info\n            edges_with_source = pd.merge(\n                edge_df.reset_index(),\n                source_node_df,\n                how=\"left\",\n                left_on=\"source_node_id\",\n                right_index=True,\n            )\n\n            # Merge with target node info\n            edges_with_nodes = pd.merge(\n                edges_with_source,\n                target_node_df,\n                how=\"left\",\n                left_on=\"target_node_id\",\n                right_index=True,\n            )\n\n            # Get unique edge types from the merged dataset\n            if \"edge_type_id\" in edges_with_nodes.columns:\n                edge_types = edges_with_nodes[\"edge_type_id\"].unique()\n            else:\n                edge_types = [0]  # Single edge type\n\n            # Process each edge type\n            for edge_type_id in edge_types:\n                # Filter edges for this type\n                if \"edge_type_id\" in edges_with_nodes.columns:\n                    edge_type_data = edges_with_nodes[\n                        edges_with_nodes[\"edge_type_id\"] == edge_type_id\n                    ]\n                else:\n                    edge_type_data = edges_with_nodes\n\n                if len(edge_type_data) == 0:\n                    continue\n\n                # Get representative edge for this type\n                edge_info = edge_type_data.iloc[0]\n\n                # Skip gap junctions\n                if (\n                    \"is_gap_junction\" in edge_info\n                    and pd.notna(edge_info[\"is_gap_junction\"])\n                    and edge_info[\"is_gap_junction\"]\n                ):\n                    continue\n\n                # Get population names from the merged data (this is the key improvement!)\n                source_pop = edge_info.get(\"source_pop_name\", \"\")\n                target_pop = edge_info.get(\"target_pop_name\", \"\")\n\n                # Get target cell template from the merged data\n                target_model_template = edge_info.get(\"target_model_template\", \"\")\n                if target_model_template.startswith(\"hoc:\"):\n                    target_cell_type = target_model_template.replace(\"hoc:\", \"\")\n                else:\n                    target_cell_type = target_model_template\n\n                # Create connection name using the actual population names\n                if source_pop and target_pop:\n                    conn_name = f\"{source_pop}2{target_pop}\"\n                else:\n                    conn_name = f\"{edge_dataset_name}_type_{edge_type_id}\"\n\n                # Get synaptic model template\n                model_template = edge_info.get(\"model_template\", \"exp2syn\")\n\n                # Build connection settings early so we can attach metadata like dynamics file name\n                conn_settings = {\n                    \"spec_settings\": {\n                        \"post_cell\": target_cell_type,\n                        \"vclamp_amp\": -70.0,  # Default voltage clamp amplitude\n                        \"sec_x\": 0.5,  # Default location on section\n                        \"sec_id\": 0,  # Default to soma\n                        # level_of_detail may be overridden by dynamics params below\n                        \"level_of_detail\": model_template,\n                    },\n                    \"spec_syn_param\": {},\n                }\n\n                # Load synaptic parameters from dynamics_params file if available.\n                # NOTE: the edge DataFrame produced by load_edges_from_config/load_sonata_edges_to_dataframe\n                # already contains the 'dynamics_params' column (from the CSV) or the\n                # flattened H5 dynamics_params attributes (prefixed with 'dynamics_params/').\n                # Prefer the direct 'dynamics_params' column value from the merged DataFrame\n                # rather than performing ad-hoc string parsing here.\n                syn_params = {}\n                dynamics_file_name = None\n                # Prefer a top-level 'dynamics_params' column if present\n                if \"dynamics_params\" in edge_info and pd.notna(edge_info.get(\"dynamics_params\")):\n                    val = edge_info.get(\"dynamics_params\")\n                    # Some CSV loaders can produce bytes or numpy types; coerce to str\n                    try:\n                        dynamics_file_name = str(val).strip()\n                    except Exception:\n                        dynamics_file_name = None\n\n                # If we found a dynamics file name, use it directly (skip token parsing)\n                if dynamics_file_name and dynamics_file_name.upper() != \"NULL\":\n                    try:\n                        conn_settings[\"spec_settings\"][\"dynamics_params_file\"] = dynamics_file_name\n                        # use a cache to avoid re-reading the same JSON multiple times\n                        if dynamics_file_name in self._syn_params_cache:\n                            syn_params = self._syn_params_cache[dynamics_file_name]\n                        else:\n                            syn_params = self._load_synaptic_params_from_config(\n                                config, dynamics_file_name\n                            )\n                            # cache result (even if empty dict) to avoid repeated lookups\n                            self._syn_params_cache[dynamics_file_name] = syn_params\n                    except Exception as e:\n                        print(\n                            f\"Warning: could not load dynamics_params file '{dynamics_file_name}' for edge {edge_dataset_name}: {e}\"\n                        )\n\n                # If a dynamics params JSON filename was provided, prefer using its basename\n                # as the connection name so that the UI matches the JSON definitions.\n                if dynamics_file_name:\n                    try:\n                        json_base = os.path.splitext(os.path.basename(dynamics_file_name))[0]\n                        # Ensure uniqueness in conn_type_settings\n                        if json_base in conn_type_settings:\n                            # Append edge_type_id to disambiguate\n                            json_base = f\"{json_base}_type_{edge_type_id}\"\n                        conn_name = json_base\n                    except Exception:\n                        pass\n\n                # If the dynamics params defined a level_of_detail, override the default\n                if isinstance(syn_params, dict) and \"level_of_detail\" in syn_params:\n                    conn_settings[\"spec_settings\"][\"level_of_detail\"] = syn_params.get(\n                        \"level_of_detail\", model_template\n                    )\n\n                # Add synaptic parameters, excluding level_of_detail\n                for key, value in syn_params.items():\n                    if key != \"level_of_detail\":\n                        conn_settings[\"spec_syn_param\"][key] = value\n                else:\n                    # Fallback: some SONATA/H5 edge files expose dynamics params as flattened\n                    # columns named like 'dynamics_params/&lt;param&gt;'. If no filename was given,\n                    # gather any such columns from edge_info and use them as spec_syn_param.\n                    for col in edge_info.index:\n                        if isinstance(col, str) and col.startswith(\"dynamics_params/\"):\n                            param_key = col.split(\"/\", 1)[1]\n                            try:\n                                val = edge_info[col]\n                                if pd.notna(val):\n                                    conn_settings[\"spec_syn_param\"][param_key] = val\n                            except Exception:\n                                # Ignore malformed entries\n                                pass\n\n                # Add weight from edge info if available\n                if \"syn_weight\" in edge_info and pd.notna(edge_info[\"syn_weight\"]):\n                    conn_settings[\"spec_syn_param\"][\"initW\"] = float(edge_info[\"syn_weight\"])\n\n                # Handle afferent section information\n                if \"afferent_section_id\" in edge_info and pd.notna(\n                    edge_info[\"afferent_section_id\"]\n                ):\n                    conn_settings[\"spec_settings\"][\"sec_id\"] = int(edge_info[\"afferent_section_id\"])\n\n                if \"afferent_section_pos\" in edge_info and pd.notna(\n                    edge_info[\"afferent_section_pos\"]\n                ):\n                    conn_settings[\"spec_settings\"][\"sec_x\"] = float(\n                        edge_info[\"afferent_section_pos\"]\n                    )\n\n                # Store in connection settings\n                conn_type_settings[conn_name] = conn_settings\n\n        return conn_type_settings\n\n    def _load_available_networks(self) -&gt; None:\n        \"\"\"\n        Load available network names from the config file for the network dropdown feature.\n\n        This method is automatically called during initialization when a config file is provided.\n        It populates the available_networks list which enables the network dropdown in\n        InteractiveTuner when multiple networks are available.\n\n        Network Dropdown Behavior:\n        -------------------------\n        - If only one network exists: No network dropdown is shown\n        - If multiple networks exist: Network dropdown appears next to connection dropdown\n        - Networks are loaded from the edges data in the config file\n        - Current network defaults to the first available if not specified during init\n        \"\"\"\n        if self.config is None:\n            self.available_networks = []\n            return\n\n        try:\n            edges = load_edges_from_config(self.config)\n            self.available_networks = list(edges.keys())\n\n            # Set current network to first available if not specified\n            if self.current_network is None and self.available_networks:\n                self.current_network = self.available_networks[0]\n        except Exception as e:\n            print(f\"Warning: Could not load networks from config: {e}\")\n            self.available_networks = []\n\n    def _load_synaptic_params_from_config(self, config: dict, dynamics_params: str) -&gt; dict:\n        \"\"\"\n        Load synaptic parameters from dynamics params file using config information.\n\n        Parameters:\n        -----------\n        config : dict\n            BMTK configuration dictionary\n        dynamics_params : str\n            Dynamics parameters filename\n\n        Returns:\n        --------\n        dict\n            Synaptic parameters dictionary\n        \"\"\"\n        try:\n            # Get the synaptic models directory from config\n            synaptic_models_dir = config.get(\"components\", {}).get(\"synaptic_models_dir\", \"\")\n            if synaptic_models_dir:\n                # Handle path variables\n                if synaptic_models_dir.startswith(\"$\"):\n                    # This is a placeholder, try to resolve it\n                    config_dir = os.path.dirname(config.get(\"config_path\", \"\"))\n                    synaptic_models_dir = synaptic_models_dir.replace(\n                        \"$COMPONENTS_DIR\", os.path.join(config_dir, \"components\")\n                    )\n                    synaptic_models_dir = synaptic_models_dir.replace(\"$BASE_DIR\", config_dir)\n\n                dynamics_file = os.path.join(synaptic_models_dir, dynamics_params)\n\n                if os.path.exists(dynamics_file):\n                    with open(dynamics_file, \"r\") as f:\n                        return json.load(f)\n                else:\n                    print(f\"Warning: Dynamics params file not found: {dynamics_file}\")\n        except Exception as e:\n            print(f\"Warning: Error loading synaptic parameters: {e}\")\n\n        return {}\n\n    @classmethod\n    def list_connections_from_config(\n        cls, config_path: str, network: Optional[str] = None\n    ) -&gt; Dict[str, dict]:\n        \"\"\"\n        Class method to list all available connections from a BMTK config file without creating a tuner.\n\n        Parameters:\n        -----------\n        config_path : str\n            Path to the simulation config JSON file.\n        network : Optional[str]\n            Name of the specific network dataset to access (e.g., 'network_to_network').\n            If None, processes all available networks.\n\n        Returns:\n        --------\n        Dict[str, dict]\n            Dictionary with connection names as keys and connection info as values.\n        \"\"\"\n        # Create a temporary instance just to use the parsing methods\n        temp_tuner = cls.__new__(cls)  # Create without calling __init__\n        conn_type_settings = temp_tuner._build_conn_type_settings_from_config(\n            config_path, network=network\n        )\n\n        # Create a summary of connections with key info\n        connections_summary = {}\n        for conn_name, settings in conn_type_settings.items():\n            connections_summary[conn_name] = {\n                \"post_cell\": settings[\"spec_settings\"][\"post_cell\"],\n                \"synapse_type\": settings[\"spec_settings\"][\"level_of_detail\"],\n                \"parameters\": list(settings[\"spec_syn_param\"].keys()),\n            }\n\n        return connections_summary\n\n    def _switch_connection(self, new_connection: str) -&gt; None:\n        \"\"\"\n        Switch to a different connection type and update all related properties.\n\n        Parameters:\n        -----------\n        new_connection : str\n            Name of the new connection type to switch to.\n        \"\"\"\n        if new_connection not in self.conn_type_settings:\n            raise ValueError(f\"Connection '{new_connection}' not found in conn_type_settings.\")\n\n        # Update current connection\n        self.current_connection = new_connection\n        self.conn = self.conn_type_settings[new_connection]\n        self.synaptic_props = self.conn[\"spec_syn_param\"]\n\n        # Update slider vars for new connection\n        if hasattr(self, \"original_slider_vars\"):\n            # Filter slider vars based on new connection's parameters\n            self.slider_vars = {\n                key: value\n                for key, value in self.synaptic_props.items()\n                if key in self.original_slider_vars\n            }\n\n            # Check for missing keys and try to get them from the synapse\n            for key in self.original_slider_vars:\n                if key not in self.synaptic_props:\n                    try:\n                        # We'll get this after recreating the synapse\n                        pass\n                    except AttributeError as e:\n                        print(\n                            f\"Warning: Could not access '{key}' for connection '{new_connection}': {e}\"\n                        )\n        else:\n            self.slider_vars = self.synaptic_props\n\n        # Need to recreate the cell if it's different\n        if self.hoc_cell is None:\n            # Check if we need a different cell type\n            new_cell_type = self.conn[\"spec_settings\"][\"post_cell\"]\n            if not hasattr(self, \"_current_cell_type\") or self._current_cell_type != new_cell_type:\n                self._current_cell_type = new_cell_type\n                self._set_up_cell()\n\n        # Recreate synapse for new connection\n        self._set_up_synapse()\n\n        # Update any missing slider vars from the new synapse\n        if hasattr(self, \"original_slider_vars\"):\n            for key in self.original_slider_vars:\n                if key not in self.synaptic_props:\n                    try:\n                        value = getattr(self.syn, key)\n                        self.slider_vars[key] = value\n                    except AttributeError as e:\n                        print(\n                            f\"Warning: Could not access '{key}' for connection '{new_connection}': {e}\"\n                        )\n\n        # Recreate NetCon connections with new synapse\n        self.nc = h.NetCon(\n            self.nstim,\n            self.syn,\n            self.general_settings[\"threshold\"],\n            self.general_settings[\"delay\"],\n            self.general_settings[\"weight\"],\n        )\n        self.nc2 = h.NetCon(\n            self.nstim2,\n            self.syn,\n            self.general_settings[\"threshold\"],\n            self.general_settings[\"delay\"],\n            self.general_settings[\"weight\"],\n        )\n\n        # Recreate voltage clamp with potentially new cell\n        self.vcl = h.VClamp(self.cell.soma[0](0.5))\n\n        # Recreate recorders for new synapse\n        self._set_up_recorders()\n\n        # Reset NEURON state\n        h.finitialize()\n\n        print(f\"Successfully switched to connection: {new_connection}\")\n\n    def _switch_network(self, new_network: str) -&gt; None:\n        \"\"\"\n        Switch to a different network and rebuild conn_type_settings for the new network.\n\n        This method is called when the user selects a different network from the network\n        dropdown in InteractiveTuner. It performs a complete rebuild of the connection\n        types available for the new network.\n\n        Parameters:\n        -----------\n        new_network : str\n            Name of the new network to switch to.\n\n        Network Switching Process:\n        -------------------------\n        1. Validates the new network exists in available_networks\n        2. Rebuilds conn_type_settings using the new network's edge data\n        3. Updates the connection dropdown with new network's available connections\n        4. Preserves current connection if it exists in new network\n        5. Falls back to first available connection if current doesn't exist\n        6. Recreates synapses and NEURON objects for the new connection\n        7. Updates UI components to reflect the changes\n        \"\"\"\n        if new_network not in self.available_networks:\n            print(\n                f\"Warning: Network '{new_network}' not found in available networks: {self.available_networks}\"\n            )\n            return\n\n        if new_network == self.current_network:\n            return  # No change needed\n\n        # Update current network\n        self.current_network = new_network\n\n        # Switch conn_type_settings using prebuilt data if available, otherwise build on-demand\n        if self.config:\n            print(f\"Switching connections for network: {new_network}\")\n            if (\n                hasattr(self, \"_prebuilt_conn_type_settings\")\n                and new_network in self._prebuilt_conn_type_settings\n            ):\n                self.conn_type_settings = self._prebuilt_conn_type_settings[new_network]\n            else:\n                # Fallback: build on-demand (slower)\n                self.conn_type_settings = self._build_conn_type_settings_from_config(\n                    self.config, network=new_network\n                )\n\n            # Update available connections and select first one if current doesn't exist\n            available_connections = list(self.conn_type_settings.keys())\n            if self.current_connection not in available_connections and available_connections:\n                self.current_connection = available_connections[0]\n                print(\n                    f\"Connection '{self.current_connection}' not available in new network. Switched to: {available_connections[0]}\"\n                )\n\n            # Switch to the (potentially new) connection\n            if self.current_connection in self.conn_type_settings:\n                self._switch_connection(self.current_connection)\n\n            print(f\"Successfully switched to network: {new_network}\")\n            print(f\"Available connections: {available_connections}\")\n\n    def _update_spec_syn_param(self, json_folder_path: str) -&gt; None:\n        \"\"\"\n        Update specific synaptic parameters using JSON files located in the specified folder.\n\n        Parameters:\n        -----------\n        json_folder_path : str\n            Path to folder containing JSON files, where each JSON file corresponds to a connection type.\n        \"\"\"\n        if not self.conn_type_settings:\n            return\n        for conn_type, settings in self.conn_type_settings.items():\n            json_file_path = os.path.join(json_folder_path, f\"{conn_type}.json\")\n            if os.path.exists(json_file_path):\n                with open(json_file_path, \"r\") as json_file:\n                    json_data = json.load(json_file)\n                    settings[\"spec_syn_param\"].update(json_data)\n            else:\n                print(f\"JSON file for {conn_type} not found.\")\n\n    def _set_up_cell(self) -&gt; None:\n        \"\"\"\n        Set up the neuron cell based on the specified connection settings.\n        This method is only called when hoc_cell is not provided.\n        \"\"\"\n        if self.hoc_cell is None:\n            self.cell = getattr(h, self.conn[\"spec_settings\"][\"post_cell\"])()\n        else:\n            self.cell = self.hoc_cell\n\n    def _set_up_synapse(self) -&gt; None:\n        \"\"\"\n        Set up the synapse on the target cell according to the synaptic parameters in `conn_type_settings`.\n\n        Notes:\n        ------\n        - `_set_up_cell()` should be called before setting up the synapse.\n        - Synapse location, type, and properties are specified within `spec_syn_param` and `spec_settings`.\n        \"\"\"\n        try:\n            self.syn = getattr(h, self.conn[\"spec_settings\"][\"level_of_detail\"])(\n                list(self.cell.all)[self.conn[\"spec_settings\"][\"sec_id\"]](\n                    self.conn[\"spec_settings\"][\"sec_x\"]\n                )\n            )\n        except:\n            raise Exception(\"Make sure the mod file exist you are trying to load check spelling!\")\n        for key, value in self.conn[\"spec_syn_param\"].items():\n            if isinstance(value, (int, float)):\n                if hasattr(self.syn, key):\n                    setattr(self.syn, key, value)\n                else:\n                    print(\n                        f\"Warning: {key} cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\"\n                    )\n\n    def _set_up_recorders(self) -&gt; None:\n        \"\"\"\n        Set up recording vectors to capture simulation data.\n\n        The method sets up recorders for:\n        - Synaptic current specified by `current_name`\n        - Other specified synaptic variables (`other_vars_to_record`)\n        - Time, soma voltage, and voltage clamp current for all simulations.\n        \"\"\"\n        self.rec_vectors = {}\n        for var in self.other_vars_to_record:\n            self.rec_vectors[var] = h.Vector()\n            ref_attr = f\"_ref_{var}\"\n            if hasattr(self.syn, ref_attr):\n                self.rec_vectors[var].record(getattr(self.syn, ref_attr))\n            else:\n                print(\n                    f\"Warning: {ref_attr} not found in the syn object. Use vars() to inspect available attributes.\"\n                )\n\n        # Record synaptic current\n        self.rec_vectors[self.current_name] = h.Vector()\n        ref_attr = f\"_ref_{self.current_name}\"\n        if hasattr(self.syn, ref_attr):\n            self.rec_vectors[self.current_name].record(getattr(self.syn, ref_attr))\n        else:\n            print(\"Warning: Synaptic current recorder not set up correctly.\")\n\n        # Record time, synaptic events, soma voltage, and voltage clamp current\n        self.t = h.Vector()\n        self.tspk = h.Vector()\n        self.soma_v = h.Vector()\n        self.ivcl = h.Vector()\n\n        self.t.record(h._ref_t)\n        self.nc.record(self.tspk)\n        self.nc2.record(self.tspk)\n        self.soma_v.record(self.cell.soma[0](0.5)._ref_v)\n        self.ivcl.record(self.vcl._ref_i)\n\n    def SingleEvent(self, plot_and_print=True):\n        \"\"\"\n        Simulate a single synaptic event by delivering an input stimulus to the synapse.\n\n        The method sets up the neuron cell, synapse, stimulus, and voltage clamp,\n        and then runs the NEURON simulation for a single event. The single synaptic event will occur at general_settings['tstart']\n        Will display graphs and synaptic properies works best with a jupyter notebook\n        \"\"\"\n        self.ispk = None\n\n        # user slider values if the sliders are set up\n        if hasattr(self, \"dynamic_sliders\"):\n            syn_props = {var: slider.value for var, slider in self.dynamic_sliders.items()}\n            self._set_syn_prop(**syn_props)\n\n        # sets values based off optimizer\n        if hasattr(self, \"using_optimizer\"):\n            for name, value in zip(self.param_names, self.params):\n                setattr(self.syn, name, value)\n\n        # Set up the stimulus\n        self.nstim.start = self.general_settings[\"tstart\"]\n        self.nstim.noise = 0\n        self.nstim2.start = h.tstop\n        self.nstim2.noise = 0\n\n        # Set up voltage clamp\n        vcldur = [[0, 0, 0], [self.general_settings[\"tstart\"], h.tstop, 1e9]]\n        for i in range(3):\n            self.vcl.amp[i] = self.conn[\"spec_settings\"][\"vclamp_amp\"]\n            self.vcl.dur[i] = vcldur[1][i]\n\n        # Run simulation\n        h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"]\n        self.nstim.interval = self.general_settings[\"tdur\"]\n        self.nstim.number = 1\n        self.nstim2.start = h.tstop\n        h.run()\n\n        current = np.array(self.rec_vectors[self.current_name])\n        syn_props = self._get_syn_prop(\n            rise_interval=self.general_settings[\"rise_interval\"], dt=h.dt\n        )\n        current = (current - syn_props[\"baseline\"]) * 1000  # Convert to pA\n        current_integral = np.trapz(current, dx=h.dt)  # pA\u00b7ms\n\n        if plot_and_print:\n            self._plot_model(\n                [\n                    self.general_settings[\"tstart\"] - 5,\n                    self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"],\n                ]\n            )\n            for prop in syn_props.items():\n                print(prop)\n            print(f\"Current Integral in pA*ms: {current_integral:.2f}\")\n\n        self.rise_time = syn_props[\"rise_time\"]\n        self.decay_time = syn_props[\"decay_time\"]\n\n    def _find_first(self, x):\n        \"\"\"\n        Find the index of the first non-zero element in a given array.\n\n        Parameters:\n        -----------\n        x : np.array\n            The input array to search.\n\n        Returns:\n        --------\n        int\n            Index of the first non-zero element, or None if none exist.\n        \"\"\"\n        x = np.asarray(x)\n        idx = np.nonzero(x)[0]\n        return idx[0] if idx.size else None\n\n    def _get_syn_prop(self, rise_interval=(0.2, 0.8), dt=h.dt, short=False):\n        \"\"\"\n        Calculate synaptic properties such as peak amplitude, latency, rise time, decay time, and half-width.\n\n        Parameters:\n        -----------\n        rise_interval : tuple of floats, optional\n            Fractional rise time interval to calculate (default is (0.2, 0.8)).\n        dt : float, optional\n            Time step of the simulation (default is NEURON's `h.dt`).\n        short : bool, optional\n            If True, only return baseline and sign without calculating full properties.\n\n        Returns:\n        --------\n        dict\n            A dictionary containing the synaptic properties: baseline, sign, peak amplitude, latency, rise time,\n            decay time, and half-width.\n        \"\"\"\n        if self.vclamp:\n            isyn = self.ivcl\n        else:\n            isyn = self.rec_vectors[self.current_name]\n        isyn = np.asarray(isyn)\n        tspk = np.asarray(self.tspk)\n        if tspk.size:\n            tspk = tspk[0]\n\n        ispk = int(np.floor(tspk / dt))\n        baseline = isyn[ispk]\n        isyn = isyn[ispk:] - baseline\n        # print(np.abs(isyn))\n        # print(np.argmax(np.abs(isyn)))\n        # print(isyn[np.argmax(np.abs(isyn))])\n        # print(np.sign(isyn[np.argmax(np.abs(isyn))]))\n        sign = np.sign(isyn[np.argmax(np.abs(isyn))])\n        if short:\n            return {\"baseline\": baseline, \"sign\": sign}\n        isyn *= sign\n        # print(isyn)\n        # peak amplitude\n        ipk, _ = find_peaks(isyn)\n        ipk = ipk[0]\n        peak = isyn[ipk]\n        # latency\n        istart = self._find_first(np.diff(isyn[: ipk + 1]) &gt; 0)\n        latency = dt * (istart + 1)\n        # rise time\n        rt1 = self._find_first(isyn[istart : ipk + 1] &gt; rise_interval[0] * peak)\n        rt2 = self._find_first(isyn[istart : ipk + 1] &gt; rise_interval[1] * peak)\n        rise_time = (rt2 - rt1) * dt\n        # decay time\n        iend = self._find_first(np.diff(isyn[ipk:]) &gt; 0)\n        iend = isyn.size - 1 if iend is None else iend + ipk\n        decay_len = iend - ipk + 1\n        popt, _ = curve_fit(\n            lambda t, a, tau: a * np.exp(-t / tau),\n            dt * np.arange(decay_len),\n            isyn[ipk : iend + 1],\n            p0=(peak, dt * decay_len / 2),\n        )\n        decay_time = popt[1]\n        # half-width\n        hw1 = self._find_first(isyn[istart : ipk + 1] &gt; 0.5 * peak)\n        hw2 = self._find_first(isyn[ipk:] &lt; 0.5 * peak)\n        hw2 = isyn.size if hw2 is None else hw2 + ipk\n        half_width = dt * (hw2 - hw1)\n        output = {\n            \"baseline\": baseline,\n            \"sign\": sign,\n            \"latency\": latency,\n            \"amp\": peak,\n            \"rise_time\": rise_time,\n            \"decay_time\": decay_time,\n            \"half_width\": half_width,\n        }\n        return output\n\n    def _plot_model(self, xlim):\n        \"\"\"\n        Plots the results of the simulation, including synaptic current, soma voltage,\n        and any additional recorded variables.\n\n        Parameters:\n        -----------\n        xlim : tuple\n            A tuple specifying the limits of the x-axis for the plot (start_time, end_time).\n\n        Notes:\n        ------\n        - The function determines how many plots to generate based on the number of variables recorded.\n        - Synaptic current and either voltage clamp or soma voltage will always be plotted.\n        - If other variables are provided in `other_vars_to_record`, they are also plotted.\n        - The function adjusts the plot layout and removes any extra subplots that are not needed.\n        \"\"\"\n        # Determine the number of plots to generate (at least 2: current and voltage)\n        num_vars_to_plot = 2 + (len(self.other_vars_to_record) if self.other_vars_to_record else 0)\n\n        # Set up figure based on number of plots (2x2 grid max)\n        num_rows = (num_vars_to_plot + 1) // 2  # This ensures we have enough rows\n        fig, axs = plt.subplots(num_rows, 2, figsize=(12, 7))\n        axs = axs.ravel()\n\n        # Plot synaptic current (always included)\n        current = self.rec_vectors[self.current_name]\n        syn_prop = self._get_syn_prop(short=True, dt=h.dt)\n        current = current - syn_prop[\"baseline\"]\n        current = current * 1000\n\n        axs[0].plot(self.t, current)\n        if self.ispk is not None:\n            for num in range(len(self.ispk)):\n                axs[0].text(self.t[self.ispk[num]], current[self.ispk[num]], f\"{str(num + 1)}\")\n\n        axs[0].set_ylabel(\"Synaptic Current (pA)\")\n\n        # Plot voltage clamp or soma voltage (always included)\n        ispk = int(np.round(self.tspk[0] / h.dt))\n        if self.vclamp:\n            baseline = self.ivcl[ispk]\n            ivcl_plt = np.array(self.ivcl) - baseline\n            ivcl_plt[:ispk] = 0\n            axs[1].plot(self.t, 1000 * ivcl_plt)\n            axs[1].set_ylabel(\"VClamp Current (pA)\")\n        else:\n            soma_v_plt = np.array(self.soma_v)\n            soma_v_plt[:ispk] = soma_v_plt[ispk]\n\n            axs[1].plot(self.t, soma_v_plt)\n            axs[1].set_ylabel(\"Soma Voltage (mV)\")\n\n        # Plot any other variables from other_vars_to_record, if provided\n        if self.other_vars_to_record:\n            for i, var in enumerate(self.other_vars_to_record, start=2):\n                if var in self.rec_vectors:\n                    axs[i].plot(self.t, self.rec_vectors[var])\n                    axs[i].set_ylabel(f\"{var.capitalize()}\")\n\n        # Adjust the layout\n        for i, ax in enumerate(axs[:num_vars_to_plot]):\n            ax.set_xlim(*xlim)\n            if i &gt;= num_vars_to_plot - 2:  # Add x-label to the last row\n                ax.set_xlabel(\"Time (ms)\")\n\n        # Remove extra subplots if less than 4 plots are present\n        if num_vars_to_plot &lt; len(axs):\n            for j in range(num_vars_to_plot, len(axs)):\n                fig.delaxes(axs[j])\n\n        # plt.tight_layout()\n        fig.suptitle(f\"Connection: {self.current_connection}\")\n        self.last_figure = plt.gcf()\n        plt.show()\n\n    def _set_drive_train(self, freq=50.0, delay=250.0):\n        \"\"\"\n        Configures trains of 12 action potentials at a specified frequency and delay period\n        between pulses 8 and 9.\n\n        Parameters:\n        -----------\n        freq : float, optional\n            Frequency of the pulse train in Hz. Default is 50 Hz.\n        delay : float, optional\n            Delay period in milliseconds between the 8th and 9th pulses. Default is 250 ms.\n\n        Returns:\n        --------\n        tstop : float\n            The time at which the last pulse stops.\n\n        Notes:\n        ------\n        - This function is based on experiments from the Allen Database.\n        \"\"\"\n        # lets also set the train drive and delay here\n        self.train_freq = freq\n        self.train_delay = delay\n\n        n_init_pulse = 8\n        n_ending_pulse = 4\n        self.nstim.start = self.general_settings[\"tstart\"]\n        self.nstim.interval = 1000 / freq\n        self.nstim2.interval = 1000 / freq\n        self.nstim.number = n_init_pulse\n        self.nstim2.number = n_ending_pulse\n        self.nstim2.start = self.nstim.start + (n_init_pulse - 1) * self.nstim.interval + delay\n        tstop = self.nstim2.start + n_ending_pulse * self.nstim2.interval\n        return tstop\n\n    def _response_amplitude(self):\n        \"\"\"\n        Calculates the amplitude of synaptic responses for each pulse in a train.\n\n        Returns:\n        --------\n        amp : list\n            A list containing the peak amplitudes for each pulse in the recorded synaptic current.\n\n        Notes:\n        ------\n        This method:\n        1. Extracts and normalizes the synaptic current\n        2. Identifies spike times and segments the current accordingly\n        3. Calculates the peak response amplitude for each segment\n        4. Records the indices of peak amplitudes for visualization\n\n        The amplitude values are returned in the original current units (before pA conversion).\n        \"\"\"\n        isyn = np.array(self.rec_vectors[self.current_name].to_python())\n        tspk = np.append(np.asarray(self.tspk), h.tstop)\n        syn_prop = self._get_syn_prop(short=True, dt=h.dt)\n        # print(\"syn_prp[sign] = \" + str(syn_prop['sign']))\n        isyn = isyn - syn_prop[\"baseline\"]\n        isyn *= syn_prop[\"sign\"]\n        ispk = np.floor((tspk + self.general_settings[\"delay\"]) / h.dt).astype(int)\n\n        try:\n            amp = [isyn[ispk[i] : ispk[i + 1]].max() for i in range(ispk.size - 1)]\n            # indexs of where the max of the synaptic current is at. This is then plotted\n            self.ispk = [\n                np.argmax(isyn[ispk[i] : ispk[i + 1]]) + ispk[i] for i in range(ispk.size - 1)\n            ]\n        # Sometimes the sim can cutoff at the peak of synaptic activity. So we just reduce the range by 1 and ingore that point\n        except:\n            amp = [isyn[ispk[i] : ispk[i + 1]].max() for i in range(ispk.size - 2)]\n            self.ispk = [\n                np.argmax(isyn[ispk[i] : ispk[i + 1]]) + ispk[i] for i in range(ispk.size - 2)\n            ]\n\n        return amp\n\n    def _find_max_amp(self, amp):\n        \"\"\"\n        Determines the maximum amplitude from the response data and returns the max in pA\n\n        Parameters:\n        -----------\n        amp : array-like\n            Array containing the amplitudes of synaptic responses.\n\n        Returns:\n        --------\n        max_amp : float\n            The maximum or minimum amplitude based on the sign of the response.\n        \"\"\"\n        max_amp = max(amp)\n        min_amp = min(amp)\n        if abs(min_amp) &gt; max_amp:\n            return min_amp * 1000  # scale unit\n        return max_amp * 1000  # scale unit\n\n    def _calc_ppr_induction_recovery(self, amp, normalize_by_trial=True, print_math=True):\n        \"\"\"\n        Calculates paired-pulse ratio, induction, recovery, and simple PPR metrics from response amplitudes.\n\n        Parameters:\n        -----------\n        amp : array-like\n            Array containing the amplitudes of synaptic responses to a pulse train.\n        normalize_by_trial : bool, optional\n            If True, normalize the amplitudes within each trial. Default is True.\n        print_math : bool, optional\n            If True, print detailed calculation steps and explanations. Default is True.\n\n        Returns:\n        --------\n        tuple\n            A tuple containing:\n            - ppr: Paired-pulse ratio (2nd pulse - 1st pulse) normalized by 90th percentile amplitude\n            - induction: Measure of facilitation/depression during initial pulses\n            - recovery: Measure of recovery after the delay period\n            - simple_ppr: Simple paired-pulse ratio (2nd pulse / 1st pulse)\n\n        Notes:\n        ------\n        - PPR &gt; 0 indicates facilitation, PPR &lt; 0 indicates depression\n        - Simple PPR &gt; 1 indicates facilitation, Simple PPR &lt; 1 indicates depression\n        - Induction &gt; 0 indicates facilitation, Induction &lt; 0 indicates depression\n        - Recovery compares the response after delay to the initial pulses\n        \"\"\"\n        amp = np.array(amp)\n        amp = amp * 1000  # scale up\n        amp = amp.reshape(-1, amp.shape[-1])\n\n        # Calculate 90th percentile amplitude for normalization\n        percentile_90 = np.percentile(amp, 90)\n\n        def format_array(arr):\n            \"\"\"Format an array to 2 significant figures for cleaner output.\"\"\"\n            return np.array2string(arr, precision=2, separator=\", \", suppress_small=True)\n\n        if print_math:\n            print(\"\\n\" + \"=\" * 40)\n            print(\n                f\"Short Term Plasticity Results for {self.train_freq}Hz with {self.train_delay} Delay\"\n            )\n            print(\"=\" * 40)\n            print(\"Simple PPR: Above 1 is facilitating, below 1 is depressing\")\n            print(\"PPR:        Above 0 is facilitating, below 0 is depressing.\")\n            print(\"Induction:  Above 0 is facilitating, below 0 is depressing.\")\n            print(\"Recovery:   A measure of how fast STP decays.\\n\")\n\n            # Simple PPR Calculation: Avg 2nd pulse / Avg 1st pulse\n            simple_ppr = np.mean(amp[:, 1:2]) / np.mean(amp[:, 0:1])\n            print(\"Simple Paired Pulse Ratio (PPR)\")\n            print(\"    Calculation: Avg 2nd pulse / Avg 1st pulse\")\n            print(\n                f\"    Values: {np.mean(amp[:, 1:2]):.3f} / {np.mean(amp[:, 0:1]):.3f} = {simple_ppr:.3f}\\n\"\n            )\n\n            # PPR Calculation: (Avg 2nd pulse - Avg 1st pulse) / 90th percentile amplitude\n            ppr = (np.mean(amp[:, 1:2]) - np.mean(amp[:, 0:1])) / percentile_90\n            print(\"Paired Pulse Response (PPR)\")\n            print(\"    Calculation: (Avg 2nd pulse - Avg 1st pulse) / 90th percentile amplitude\")\n            print(\n                f\"    Values: ({np.mean(amp[:, 1:2]):.3f} - {np.mean(amp[:, 0:1]):.3f}) / {percentile_90:.3f} = {ppr:.3f}\\n\"\n            )\n\n            # Induction Calculation: (Avg (6th, 7th, 8th pulses) - Avg 1st pulse) / 90th percentile amplitude\n            induction = (np.mean(amp[:, 5:8]) - np.mean(amp[:, :1])) / percentile_90\n            print(\"Induction\")\n            print(\n                \"    Calculation: (Avg(6th, 7th, 8th pulses) - Avg 1st pulse) / 90th percentile amplitude\"\n            )\n            print(\n                f\"    Values: {np.mean(amp[:, 5:8]):.3f} - {np.mean(amp[:, :1]):.3f} / {percentile_90:.3f} = {induction:.3f}\\n\"\n            )\n\n            # Recovery Calculation: (Avg (9th, 10th, 11th, 12th pulses) - Avg (1st, 2nd, 3rd, 4th pulses)) / 90th percentile amplitude\n            recovery = (np.mean(amp[:, 8:12]) - np.mean(amp[:, :4])) / percentile_90\n            print(\"Recovery\")\n            print(\n                \"    Calculation: (Avg(9th, 10th, 11th, 12th pulses) - Avg(1st to 4th pulses)) / 90th percentile amplitude\"\n            )\n            print(\n                f\"    Values: {np.mean(amp[:, 8:12]):.3f} - {np.mean(amp[:, :4]):.3f} / {percentile_90:.3f} = {recovery:.3f}\\n\"\n            )\n\n            print(\"=\" * 40 + \"\\n\")\n\n        # Calculate final metrics\n        ppr = (np.mean(amp[:, 1:2]) - np.mean(amp[:, 0:1])) / percentile_90\n        induction = (np.mean(amp[:, 5:8]) - np.mean(amp[:, :1])) / percentile_90\n        recovery = (np.mean(amp[:, 8:12]) - np.mean(amp[:, :4])) / percentile_90\n        simple_ppr = np.mean(amp[:, 1:2]) / np.mean(amp[:, 0:1])\n\n        return ppr, induction, recovery, simple_ppr\n\n    def _set_syn_prop(self, **kwargs):\n        \"\"\"\n        Sets the synaptic parameters based on user inputs from sliders.\n\n        Parameters:\n        -----------\n        **kwargs : dict\n            Synaptic properties (such as weight, Use, tau_f, tau_d) as keyword arguments.\n        \"\"\"\n        for key, value in kwargs.items():\n            setattr(self.syn, key, value)\n\n    def _simulate_model(self, input_frequency, delay, vclamp=None):\n        \"\"\"\n        Runs the simulation with the specified input frequency, delay, and voltage clamp settings.\n\n        Parameters:\n        -----------\n        input_frequency : float\n            Frequency of the input drive train in Hz.\n        delay : float\n            Delay period in milliseconds between the 8th and 9th pulses.\n        vclamp : bool or None, optional\n            Whether to use voltage clamp. If None, the current setting is used. Default is None.\n\n        Notes:\n        ------\n        This method handles two different input modes:\n        - Standard train mode with 8 initial pulses followed by a delay and 4 additional pulses\n        - Continuous input mode where stimulation continues for a specified duration\n        \"\"\"\n        if not self.input_mode:\n            self.tstop = self._set_drive_train(input_frequency, delay)\n            h.tstop = self.tstop\n\n            vcldur = [[0, 0, 0], [self.general_settings[\"tstart\"], self.tstop, 1e9]]\n            for i in range(3):\n                self.vcl.amp[i] = self.conn[\"spec_settings\"][\"vclamp_amp\"]\n                self.vcl.dur[i] = vcldur[1][i]\n            h.finitialize(70 * mV)\n            h.continuerun(self.tstop * ms)\n            # h.run()\n        else:\n            # Continuous input mode: ensure simulation runs long enough for the full stimulation duration\n            self.tstop = (\n                self.general_settings[\"tstart\"] + self.w_duration.value + 300\n            )  # 300ms buffer time\n            self.nstim.interval = 1000 / input_frequency\n            self.nstim.number = np.ceil(self.w_duration.value / 1000 * input_frequency + 1)\n            self.nstim2.number = 0\n\n            h.finitialize(70 * mV)\n            h.continuerun(self.tstop * ms)\n            # h.run()\n\n    def InteractiveTuner(self):\n        \"\"\"\n        Sets up interactive sliders for tuning short-term plasticity (STP) parameters in a Jupyter Notebook.\n\n        This method creates an interactive UI with sliders for:\n        - Network selection dropdown (if multiple networks available and config provided)\n        - Connection type selection dropdown\n        - Input frequency\n        - Delay between pulse trains\n        - Duration of stimulation (for continuous input mode)\n        - Synaptic parameters (e.g., Use, tau_f, tau_d) based on the syn model\n\n        It also provides buttons for:\n        - Running a single event simulation\n        - Running a train input simulation\n        - Toggling voltage clamp mode\n        - Switching between standard and continuous input modes\n\n        Network Dropdown Feature:\n        ------------------------\n        When the SynapseTuner is initialized with a BMTK config file containing multiple networks:\n        - A network dropdown appears next to the connection dropdown\n        - Users can dynamically switch between networks (e.g., 'network_to_network', 'external_to_network')\n        - Switching networks rebuilds available connections and updates the connection dropdown\n        - The current connection is preserved if it exists in the new network\n        - If multiple networks exist but only one is specified during init, that network is used as default\n\n        Notes:\n        ------\n        Ideal for exploratory parameter tuning and interactive visualization of\n        synapse behavior with different parameter values and stimulation protocols.\n        The network dropdown feature enables comprehensive exploration of multi-network\n        BMTK simulations without needing to reinitialize the tuner.\n        \"\"\"\n        # Widgets setup (Sliders)\n        freqs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200]\n        delays = [125, 250, 500, 1000, 2000, 4000]\n        durations = [100, 300, 500, 1000, 2000, 5000, 10000]\n        freq0 = 50\n        delay0 = 250\n        duration0 = 300\n        vlamp_status = self.vclamp\n\n        # Connection dropdown\n        connection_options = sorted(list(self.conn_type_settings.keys()))\n        w_connection = widgets.Dropdown(\n            options=connection_options,\n            value=self.current_connection,\n            description=\"Connection:\",\n            style={\"description_width\": \"initial\"},\n        )\n\n        # Network dropdown - only shown if config was provided and multiple networks are available\n        # This enables users to switch between different network datasets dynamically\n        w_network = None\n        if self.config is not None and len(self.available_networks) &gt; 1:\n            w_network = widgets.Dropdown(\n                options=self.available_networks,\n                value=self.current_network,\n                description=\"Network:\",\n                style={\"description_width\": \"initial\"},\n            )\n\n        w_run = widgets.Button(description=\"Run Train\", icon=\"history\", button_style=\"primary\")\n        w_single = widgets.Button(description=\"Single Event\", icon=\"check\", button_style=\"success\")\n        w_vclamp = widgets.ToggleButton(\n            value=vlamp_status,\n            description=\"Voltage Clamp\",\n            icon=\"fast-backward\",\n            button_style=\"warning\",\n        )\n\n        # Voltage clamp amplitude input\n        default_vclamp_amp = getattr(self.conn[\"spec_settings\"], \"vclamp_amp\", -70.0)\n        w_vclamp_amp = widgets.FloatText(\n            value=default_vclamp_amp,\n            description=\"V_clamp (mV):\",\n            step=5.0,\n            style={\"description_width\": \"initial\"},\n            layout=widgets.Layout(width=\"150px\"),\n        )\n\n        w_input_mode = widgets.ToggleButton(\n            value=False, description=\"Continuous input\", icon=\"eject\", button_style=\"info\"\n        )\n        w_input_freq = widgets.SelectionSlider(options=freqs, value=freq0, description=\"Input Freq\")\n\n        # Sliders for delay and duration\n        self.w_delay = widgets.SelectionSlider(options=delays, value=delay0, description=\"Delay\")\n        self.w_duration = widgets.SelectionSlider(\n            options=durations, value=duration0, description=\"Duration\"\n        )\n\n        # Save functionality widgets\n        save_path_text = widgets.Text(\n            value=\"plot.png\", description=\"Save path:\", layout=widgets.Layout(width=\"300px\")\n        )\n        save_button = widgets.Button(description=\"Save Plot\", icon=\"save\", button_style=\"success\")\n\n        def save_plot(b):\n            if hasattr(self, \"last_figure\") and self.last_figure is not None:\n                try:\n                    # Create a new figure with just the first subplot (synaptic current)\n                    fig, ax = plt.subplots(figsize=(8, 6))\n\n                    # Get the axes from the original figure\n                    original_axes = self.last_figure.get_axes()\n                    if len(original_axes) &gt; 0:\n                        first_ax = original_axes[0]\n\n                        # Copy the data from the first subplot\n                        for line in first_ax.get_lines():\n                            ax.plot(\n                                line.get_xdata(),\n                                line.get_ydata(),\n                                color=line.get_color(),\n                                label=line.get_label(),\n                            )\n\n                        # Copy axis labels and title\n                        ax.set_xlabel(first_ax.get_xlabel())\n                        ax.set_ylabel(first_ax.get_ylabel())\n                        ax.set_title(first_ax.get_title())\n                        ax.set_xlim(first_ax.get_xlim())\n                        ax.legend()\n                        ax.grid(True)\n\n                        # Save the new figure\n                        fig.savefig(save_path_text.value)\n                        plt.close(fig)  # Close the temporary figure\n                        print(f\"Synaptic current plot saved to {save_path_text.value}\")\n                    else:\n                        print(\"No subplots found in the figure\")\n\n                except Exception as e:\n                    print(f\"Error saving plot: {e}\")\n            else:\n                print(\"No plot to save\")\n\n        save_button.on_click(save_plot)\n\n        def create_dynamic_sliders():\n            \"\"\"Create sliders based on current connection's parameters\"\"\"\n            sliders = {}\n            for key, value in self.slider_vars.items():\n                if isinstance(value, (int, float)):  # Only create sliders for numeric values\n                    if hasattr(self.syn, key):\n                        if value == 0:\n                            print(\n                                f\"{key} was set to zero, going to try to set a range of values, try settings the {key} to a nonzero value if you dont like the range!\"\n                            )\n                            slider = widgets.FloatSlider(\n                                value=value, min=0, max=1000, step=1, description=key\n                            )\n                        else:\n                            slider = widgets.FloatSlider(\n                                value=value, min=0, max=value * 20, step=value / 5, description=key\n                            )\n                        sliders[key] = slider\n                    else:\n                        print(f\"skipping slider for {key} due to not being a synaptic variable\")\n            return sliders\n\n        # Generate sliders dynamically based on valid numeric entries in self.slider_vars\n        self.dynamic_sliders = create_dynamic_sliders()\n        print(\n            \"Setting up slider! The sliders ranges are set by their init value so try changing that if you dont like the slider range!\"\n        )\n\n        # Create output widget for displaying results\n        output_widget = widgets.Output()\n\n        def run_single_event(*args):\n            clear_output()\n            display(ui)\n            display(output_widget)\n\n            self.vclamp = w_vclamp.value\n            # Update voltage clamp amplitude if voltage clamp is enabled\n            if self.vclamp:\n                # Update the voltage clamp amplitude settings\n                self.conn[\"spec_settings\"][\"vclamp_amp\"] = w_vclamp_amp.value\n                # Update general settings if they exist\n                if hasattr(self, \"general_settings\"):\n                    self.general_settings[\"vclamp_amp\"] = w_vclamp_amp.value\n            # Update synaptic properties based on slider values\n            self.ispk = None\n\n            # Clear previous results and run simulation\n            output_widget.clear_output()\n            with output_widget:\n                self.SingleEvent()\n\n        def on_connection_change(*args):\n            \"\"\"Handle connection dropdown change\"\"\"\n            try:\n                new_connection = w_connection.value\n                if new_connection != self.current_connection:\n                    # Switch to new connection\n                    self._switch_connection(new_connection)\n\n                    # Recreate dynamic sliders for new connection\n                    self.dynamic_sliders = create_dynamic_sliders()\n\n                    # Update UI\n                    update_ui_layout()\n                    update_ui()\n\n            except Exception as e:\n                print(f\"Error switching connection: {e}\")\n\n        def on_network_change(*args):\n            \"\"\"\n            Handle network dropdown change events.\n\n            This callback is triggered when the user selects a different network from\n            the network dropdown. It coordinates the complete switching process:\n            1. Calls _switch_network() to rebuild connections for the new network\n            2. Updates the connection dropdown options with new network's connections\n            3. Recreates dynamic sliders for new connection parameters\n            4. Refreshes the entire UI to reflect all changes\n            \"\"\"\n            if w_network is None:\n                return\n            try:\n                new_network = w_network.value\n                if new_network != self.current_network:\n                    # Switch to new network\n                    self._switch_network(new_network)\n\n                    # Update connection dropdown options with new network's connections\n                    connection_options = list(self.conn_type_settings.keys())\n                    w_connection.options = connection_options\n                    if connection_options:\n                        w_connection.value = self.current_connection\n\n                    # Recreate dynamic sliders for new connection\n                    self.dynamic_sliders = create_dynamic_sliders()\n\n                    # Update UI\n                    update_ui_layout()\n                    update_ui()\n\n            except Exception as e:\n                print(f\"Error switching network: {e}\")\n\n        def update_ui_layout():\n            \"\"\"\n            Update the UI layout with new sliders and network dropdown.\n\n            This function reconstructs the entire UI layout including:\n            - Network dropdown (if available) and connection dropdown in the top row\n            - Button controls and input mode toggles\n            - Parameter sliders arranged in columns\n            \"\"\"\n            nonlocal ui, slider_columns\n\n            # Add the dynamic sliders to the UI\n            slider_widgets = [slider for slider in self.dynamic_sliders.values()]\n\n            if slider_widgets:\n                half = len(slider_widgets) // 2\n                col1 = VBox(slider_widgets[:half])\n                col2 = VBox(slider_widgets[half:])\n                slider_columns = HBox([col1, col2])\n            else:\n                slider_columns = VBox([])\n\n            # Create button row with voltage clamp controls\n            if w_vclamp.value:  # Show voltage clamp amplitude input when toggle is on\n                button_row = HBox([w_run, w_single, w_vclamp, w_vclamp_amp, w_input_mode])\n            else:  # Hide voltage clamp amplitude input when toggle is off\n                button_row = HBox([w_run, w_single, w_vclamp, w_input_mode])\n\n            # Construct the top row - include network dropdown if available\n            # This creates a horizontal layout with network dropdown (if present) and connection dropdown\n            if w_network is not None:\n                connection_row = HBox([w_network, w_connection])\n            else:\n                connection_row = HBox([w_connection])\n            slider_row = HBox([w_input_freq, self.w_delay, self.w_duration])\n            save_row = HBox([save_path_text, save_button])\n\n            ui = VBox([connection_row, button_row, slider_row, slider_columns, save_row])\n\n        # Function to update UI based on input mode\n        def update_ui(*args):\n            clear_output()\n            display(ui)\n            display(output_widget)\n\n            self.vclamp = w_vclamp.value\n            # Update voltage clamp amplitude if voltage clamp is enabled\n            if self.vclamp:\n                self.conn[\"spec_settings\"][\"vclamp_amp\"] = w_vclamp_amp.value\n                if hasattr(self, \"general_settings\"):\n                    self.general_settings[\"vclamp_amp\"] = w_vclamp_amp.value\n\n            self.input_mode = w_input_mode.value\n            syn_props = {var: slider.value for var, slider in self.dynamic_sliders.items()}\n            self._set_syn_prop(**syn_props)\n\n            # Clear previous results and run simulation\n            output_widget.clear_output()\n            with output_widget:\n                if not self.input_mode:\n                    self._simulate_model(w_input_freq.value, self.w_delay.value, w_vclamp.value)\n                else:\n                    self._simulate_model(w_input_freq.value, self.w_duration.value, w_vclamp.value)\n                amp = self._response_amplitude()\n                self._plot_model(\n                    [self.general_settings[\"tstart\"] - self.nstim.interval / 3, self.tstop]\n                )\n                _ = self._calc_ppr_induction_recovery(amp)\n\n        # Function to switch between delay and duration sliders\n        def switch_slider(*args):\n            if w_input_mode.value:\n                self.w_delay.layout.display = \"none\"  # Hide delay slider\n                self.w_duration.layout.display = \"\"  # Show duration slider\n            else:\n                self.w_delay.layout.display = \"\"  # Show delay slider\n                self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n        # Function to handle voltage clamp toggle\n        def on_vclamp_toggle(*args):\n            \"\"\"Handle voltage clamp toggle changes to show/hide amplitude input\"\"\"\n            update_ui_layout()\n            clear_output()\n            display(ui)\n            display(output_widget)\n\n        # Link widgets to their callback functions\n        w_connection.observe(on_connection_change, names=\"value\")\n        # Link network dropdown callback only if network dropdown was created\n        if w_network is not None:\n            w_network.observe(on_network_change, names=\"value\")\n        w_input_mode.observe(switch_slider, names=\"value\")\n        w_vclamp.observe(on_vclamp_toggle, names=\"value\")\n\n        # Hide the duration slider initially until the user selects it\n        self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n        w_single.on_click(run_single_event)\n        w_run.on_click(update_ui)\n\n        # Initial UI setup\n        slider_columns = VBox([])\n        ui = VBox([])\n        update_ui_layout()\n\n        display(ui)\n        update_ui()\n\n    def stp_frequency_response(\n        self,\n        freqs=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200],\n        delay=250,\n        plot=True,\n        log_plot=True,\n    ):\n        \"\"\"\n        Analyze synaptic response across different stimulation frequencies.\n\n        This method systematically tests how the synapse model responds to different\n        stimulation frequencies, calculating key short-term plasticity (STP) metrics\n        for each frequency.\n\n        Parameters:\n        -----------\n        freqs : list, optional\n            List of frequencies to analyze (in Hz). Default covers a wide range from 1-200 Hz.\n        delay : float, optional\n            Delay between pulse trains in ms. Default is 250 ms.\n        plot : bool, optional\n            Whether to plot the results. Default is True.\n        log_plot : bool, optional\n            Whether to use logarithmic scale for frequency axis. Default is True.\n\n        Returns:\n        --------\n        dict\n            Dictionary containing frequency-dependent metrics with keys:\n            - 'frequencies': List of tested frequencies\n            - 'ppr': Paired-pulse ratios at each frequency\n            - 'simple_ppr': Simple paired-pulse ratios (2nd/1st pulse) at each frequency\n            - 'induction': Induction values at each frequency\n            - 'recovery': Recovery values at each frequency\n\n        Notes:\n        ------\n        This method is particularly useful for characterizing the frequency-dependent\n        behavior of synapses, such as identifying facilitating vs. depressing regimes\n        or the frequency at which a synapse transitions between these behaviors.\n        \"\"\"\n        results = {\n            \"frequencies\": freqs,\n            \"ppr\": [],\n            \"induction\": [],\n            \"recovery\": [],\n            \"simple_ppr\": [],\n        }\n\n        # Store original state\n        original_ispk = self.ispk\n\n        for freq in tqdm(freqs, desc=\"Analyzing frequencies\"):\n            self._simulate_model(freq, delay)\n            amp = self._response_amplitude()\n            ppr, induction, recovery, simple_ppr = self._calc_ppr_induction_recovery(\n                amp, print_math=False\n            )\n\n            results[\"ppr\"].append(float(ppr))\n            results[\"induction\"].append(float(induction))\n            results[\"recovery\"].append(float(recovery))\n            results[\"simple_ppr\"].append(float(simple_ppr))\n\n        # Restore original state\n        self.ispk = original_ispk\n\n        if plot:\n            self._plot_frequency_analysis(results, log_plot=log_plot)\n\n        return results\n\n    def _plot_frequency_analysis(self, results, log_plot):\n        \"\"\"\n        Plot the frequency-dependent synaptic properties.\n\n        Parameters:\n        -----------\n        results : dict\n            Dictionary containing frequency analysis results with keys:\n            - 'frequencies': List of tested frequencies\n            - 'ppr': Paired-pulse ratios at each frequency\n            - 'simple_ppr': Simple paired-pulse ratios at each frequency\n            - 'induction': Induction values at each frequency\n            - 'recovery': Recovery values at each frequency\n        log_plot : bool\n            Whether to use logarithmic scale for frequency axis\n\n        Notes:\n        ------\n        Creates a figure with three subplots showing:\n        1. Paired-pulse ratios (both normalized and simple) vs. frequency\n        2. Induction vs. frequency\n        3. Recovery vs. frequency\n\n        Each plot includes a horizontal reference line at y=0 or y=1 to indicate\n        the boundary between facilitation and depression.\n        \"\"\"\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n\n        # Plot both PPR measures\n        if log_plot:\n            ax1.semilogx(results[\"frequencies\"], results[\"ppr\"], \"o-\", label=\"Normalized PPR\")\n            ax1.semilogx(results[\"frequencies\"], results[\"simple_ppr\"], \"s-\", label=\"Simple PPR\")\n        else:\n            ax1.plot(results[\"frequencies\"], results[\"ppr\"], \"o-\", label=\"Normalized PPR\")\n            ax1.plot(results[\"frequencies\"], results[\"simple_ppr\"], \"s-\", label=\"Simple PPR\")\n        ax1.axhline(y=1, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax1.set_xlabel(\"Frequency (Hz)\")\n        ax1.set_ylabel(\"Paired Pulse Ratio\")\n        ax1.set_title(\"PPR vs Frequency\")\n        ax1.legend()\n        ax1.grid(True)\n\n        # Plot Induction\n        if log_plot:\n            ax2.semilogx(results[\"frequencies\"], results[\"induction\"], \"o-\")\n        else:\n            ax2.plot(results[\"frequencies\"], results[\"induction\"], \"o-\")\n        ax2.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax2.set_xlabel(\"Frequency (Hz)\")\n        ax2.set_ylabel(\"Induction\")\n        ax2.set_title(\"Induction vs Frequency\")\n        ax2.grid(True)\n\n        # Plot Recovery\n        if log_plot:\n            ax3.semilogx(results[\"frequencies\"], results[\"recovery\"], \"o-\")\n        else:\n            ax3.plot(results[\"frequencies\"], results[\"recovery\"], \"o-\")\n        ax3.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax3.set_xlabel(\"Frequency (Hz)\")\n        ax3.set_ylabel(\"Recovery\")\n        ax3.set_title(\"Recovery vs Frequency\")\n        ax3.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n\n    def generate_synaptic_table(self, stp_frequency=50.0, stp_delay=250.0, plot=True):\n        \"\"\"\n        Generate a comprehensive table of synaptic parameters for all connections.\n\n        This method iterates through all available connections, runs simulations to\n        characterize each synapse, and compiles the results into a pandas DataFrame.\n\n        Parameters:\n        -----------\n        stp_frequency : float, optional\n            Frequency in Hz to use for STP (short-term plasticity) analysis. Default is 50.0 Hz.\n        stp_delay : float, optional\n            Delay in ms between pulse trains for STP analysis. Default is 250.0 ms.\n        plot : bool, optional\n            Whether to display the resulting table. Default is True.\n\n        Returns:\n        --------\n        pd.DataFrame\n            DataFrame containing synaptic parameters for each connection with columns:\n            - connection: Connection name\n            - rise_time: 20-80% rise time (ms)\n            - decay_time: Decay time constant (ms)\n            - latency: Response latency (ms)\n            - half_width: Response half-width (ms)\n            - peak_amplitude: Peak synaptic current amplitude (pA)\n            - baseline: Baseline current (pA)\n            - ppr: Paired-pulse ratio (normalized)\n            - simple_ppr: Simple paired-pulse ratio (2nd/1st pulse)\n            - induction: STP induction measure\n            - recovery: STP recovery measure\n\n        Notes:\n        ------\n        This method temporarily switches between connections to characterize each one,\n        then restores the original connection. The STP metrics are calculated at the\n        specified frequency and delay.\n        \"\"\"\n        # Store original connection to restore later\n        original_connection = self.current_connection\n\n        # Initialize results list\n        results = []\n\n        print(f\"Analyzing {len(self.conn_type_settings)} connections...\")\n\n        for conn_name in tqdm(self.conn_type_settings.keys(), desc=\"Analyzing connections\"):\n            try:\n                # Switch to this connection\n                self._switch_connection(conn_name)\n\n                # Run single event analysis\n                self.SingleEvent(plot_and_print=False)\n\n                # Get synaptic properties from the single event\n                syn_props = self._get_syn_prop()\n\n                # Run STP analysis at specified frequency\n                stp_results = self.stp_frequency_response(\n                    freqs=[stp_frequency], delay=stp_delay, plot=False, log_plot=False\n                )\n\n                # Extract STP metrics for this frequency\n                freq_idx = 0  # Only one frequency tested\n                ppr = stp_results[\"ppr\"][freq_idx]\n                induction = stp_results[\"induction\"][freq_idx]\n                recovery = stp_results[\"recovery\"][freq_idx]\n                simple_ppr = stp_results[\"simple_ppr\"][freq_idx]\n\n                # Compile results for this connection\n                conn_results = {\n                    \"connection\": conn_name,\n                    \"rise_time\": float(self.rise_time),\n                    \"decay_time\": float(self.decay_time),\n                    \"latency\": float(syn_props.get(\"latency\", 0)),\n                    \"half_width\": float(syn_props.get(\"half_width\", 0)),\n                    \"peak_amplitude\": float(syn_props.get(\"amp\", 0)),\n                    \"baseline\": float(syn_props.get(\"baseline\", 0)),\n                    \"ppr\": float(ppr),\n                    \"simple_ppr\": float(simple_ppr),\n                    \"induction\": float(induction),\n                    \"recovery\": float(recovery),\n                }\n\n                results.append(conn_results)\n\n            except Exception as e:\n                print(f\"Warning: Failed to analyze connection '{conn_name}': {e}\")\n                # Add partial results if possible\n                results.append(\n                    {\n                        \"connection\": conn_name,\n                        \"rise_time\": float(\"nan\"),\n                        \"decay_time\": float(\"nan\"),\n                        \"latency\": float(\"nan\"),\n                        \"half_width\": float(\"nan\"),\n                        \"peak_amplitude\": float(\"nan\"),\n                        \"baseline\": float(\"nan\"),\n                        \"ppr\": float(\"nan\"),\n                        \"simple_ppr\": float(\"nan\"),\n                        \"induction\": float(\"nan\"),\n                        \"recovery\": float(\"nan\"),\n                    }\n                )\n\n        # Restore original connection\n        if original_connection in self.conn_type_settings:\n            self._switch_connection(original_connection)\n\n        # Create DataFrame\n        df = pd.DataFrame(results)\n\n        # Set connection as index for better display\n        df = df.set_index(\"connection\")\n\n        if plot:\n            # Display the table\n            print(\"\\nSynaptic Parameters Table:\")\n            print(\"=\" * 80)\n            display(df.round(4))\n\n            # Optional: Create a simple bar plot for key metrics\n            try:\n                fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n                fig.suptitle(\n                    f\"Synaptic Parameters Across Connections (STP at {stp_frequency}Hz)\",\n                    fontsize=16,\n                )\n\n                # Plot rise/decay times\n                df[[\"rise_time\", \"decay_time\"]].plot(kind=\"bar\", ax=axes[0, 0])\n                axes[0, 0].set_title(\"Rise and Decay Times\")\n                axes[0, 0].set_ylabel(\"Time (ms)\")\n                axes[0, 0].tick_params(axis=\"x\", rotation=45)\n\n                # Plot PPR metrics\n                df[[\"ppr\", \"simple_ppr\"]].plot(kind=\"bar\", ax=axes[0, 1])\n                axes[0, 1].set_title(\"Paired-Pulse Ratios\")\n                axes[0, 1].axhline(y=1, color=\"gray\", linestyle=\"--\", alpha=0.5)\n                axes[0, 1].tick_params(axis=\"x\", rotation=45)\n\n                # Plot induction\n                df[\"induction\"].plot(kind=\"bar\", ax=axes[1, 0], color=\"green\")\n                axes[1, 0].set_title(\"STP Induction\")\n                axes[1, 0].axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n                axes[1, 0].set_ylabel(\"Induction\")\n                axes[1, 0].tick_params(axis=\"x\", rotation=45)\n\n                # Plot recovery\n                df[\"recovery\"].plot(kind=\"bar\", ax=axes[1, 1], color=\"orange\")\n                axes[1, 1].set_title(\"STP Recovery\")\n                axes[1, 1].axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n                axes[1, 1].set_ylabel(\"Recovery\")\n                axes[1, 1].tick_params(axis=\"x\", rotation=45)\n\n                plt.tight_layout()\n                plt.show()\n\n            except Exception as e:\n                print(f\"Warning: Could not create plots: {e}\")\n\n        return df\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner.__init__","title":"<code>__init__(conn_type_settings=None, connection=None, current_name='i', mechanisms_dir=None, templates_dir=None, config=None, general_settings=None, json_folder_path=None, other_vars_to_record=None, slider_vars=None, hoc_cell=None, network=None)</code>","text":"<p>Initialize the SynapseTuner class with connection type settings, mechanisms, and template directories.</p> Parameters: <p>mechanisms_dir : Optional[str]     Directory path containing the compiled mod files needed for NEURON mechanisms. templates_dir : Optional[str]     Directory path containing cell template files (.hoc or .py) loaded into NEURON. conn_type_settings : Optional[dict]     A dictionary containing connection-specific settings, such as synaptic properties and details. connection : Optional[str]     Name of the connection type to be used from the conn_type_settings dictionary. general_settings : Optional[dict]     General settings dictionary including parameters like simulation time step, duration, and temperature. json_folder_path : Optional[str]     Path to folder containing JSON files with additional synaptic properties to update settings. current_name : str, optional     Name of the synaptic current variable to be recorded (default is 'i'). other_vars_to_record : Optional[list]     List of additional synaptic variables to record during the simulation (e.g., 'Pr', 'Use'). slider_vars : Optional[list]     List of synaptic variables you would like sliders set up for the STP sliders method by default will use all parameters in spec_syn_param. hoc_cell : Optional[object]     An already loaded NEURON cell object. If provided, template loading and cell setup will be skipped. network : Optional[str]     Name of the specific network dataset to access from the loaded edges data (e.g., 'network_to_network').     If not provided, will use all available networks. When a config file is provided, this enables     the network dropdown feature in InteractiveTuner for switching between different networks.</p> Network Dropdown Feature: <p>When initialized with a BMTK config file, the tuner automatically: 1. Loads all available network datasets from the config 2. Creates a network dropdown in InteractiveTuner (if multiple networks exist) 3. Allows dynamic switching between networks, which rebuilds connection types 4. Updates connection dropdown options when network is changed 5. Preserves current connection if it exists in the new network, otherwise selects the first available</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def __init__(\n    self,\n    conn_type_settings: Optional[Dict[str, dict]] = None,\n    connection: Optional[str] = None,\n    current_name: str = \"i\",\n    mechanisms_dir: Optional[str] = None,\n    templates_dir: Optional[str] = None,\n    config: Optional[str] = None,\n    general_settings: Optional[dict] = None,\n    json_folder_path: Optional[str] = None,\n    other_vars_to_record: Optional[List[str]] = None,\n    slider_vars: Optional[List[str]] = None,\n    hoc_cell: Optional[object] = None,\n    network: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the SynapseTuner class with connection type settings, mechanisms, and template directories.\n\n    Parameters:\n    -----------\n    mechanisms_dir : Optional[str]\n        Directory path containing the compiled mod files needed for NEURON mechanisms.\n    templates_dir : Optional[str]\n        Directory path containing cell template files (.hoc or .py) loaded into NEURON.\n    conn_type_settings : Optional[dict]\n        A dictionary containing connection-specific settings, such as synaptic properties and details.\n    connection : Optional[str]\n        Name of the connection type to be used from the conn_type_settings dictionary.\n    general_settings : Optional[dict]\n        General settings dictionary including parameters like simulation time step, duration, and temperature.\n    json_folder_path : Optional[str]\n        Path to folder containing JSON files with additional synaptic properties to update settings.\n    current_name : str, optional\n        Name of the synaptic current variable to be recorded (default is 'i').\n    other_vars_to_record : Optional[list]\n        List of additional synaptic variables to record during the simulation (e.g., 'Pr', 'Use').\n    slider_vars : Optional[list]\n        List of synaptic variables you would like sliders set up for the STP sliders method by default will use all parameters in spec_syn_param.\n    hoc_cell : Optional[object]\n        An already loaded NEURON cell object. If provided, template loading and cell setup will be skipped.\n    network : Optional[str]\n        Name of the specific network dataset to access from the loaded edges data (e.g., 'network_to_network').\n        If not provided, will use all available networks. When a config file is provided, this enables\n        the network dropdown feature in InteractiveTuner for switching between different networks.\n\n    Network Dropdown Feature:\n    -------------------------\n    When initialized with a BMTK config file, the tuner automatically:\n    1. Loads all available network datasets from the config\n    2. Creates a network dropdown in InteractiveTuner (if multiple networks exist)\n    3. Allows dynamic switching between networks, which rebuilds connection types\n    4. Updates connection dropdown options when network is changed\n    5. Preserves current connection if it exists in the new network, otherwise selects the first available\n    \"\"\"\n    self.hoc_cell = hoc_cell\n    # Store config and network information for network dropdown functionality\n    self.config = config  # Store config path for network dropdown functionality\n    self.available_networks = []  # Store available networks from config file\n    self.current_network = network  # Store current network selection\n    # Cache for loaded dynamics params JSON by filename to avoid repeated disk reads\n    self._syn_params_cache = {}\n    h.load_file(\"stdrun.hoc\")\n\n    if hoc_cell is None:\n        if config is None and (mechanisms_dir is None or templates_dir is None):\n            raise ValueError(\n                \"Either a config file, both mechanisms_dir and templates_dir, or a hoc_cell must be provided.\"\n            )\n\n        if config is None:\n            neuron.load_mechanisms(mechanisms_dir)\n            h.load_file(templates_dir)\n        else:\n            # loads both mech and templates\n            load_templates_from_config(config)\n            # Load available networks from config for network dropdown feature\n            self._load_available_networks()\n            # Prebuild connection type settings for each available network to\n            # make network switching in the UI fast. This will make __init__ slower\n            # but dramatically speed up response when changing the network dropdown.\n            self._prebuilt_conn_type_settings = {}\n            try:\n                for net in self.available_networks:\n                    self._prebuilt_conn_type_settings[net] = (\n                        self._build_conn_type_settings_from_config(config, network=net)\n                    )\n            except Exception as e:\n                print(f\"Warning: error prebuilding conn_type_settings for networks: {e}\")\n\n    if conn_type_settings is None:\n        if config is not None:\n            print(\"Building conn_type_settings from BMTK config files...\")\n            # If we prebuilt per-network settings, use the one for the requested network\n            if hasattr(self, \"_prebuilt_conn_type_settings\") and network in getattr(\n                self, \"_prebuilt_conn_type_settings\", {}\n            ):\n                conn_type_settings = self._prebuilt_conn_type_settings[network]\n            else:\n                conn_type_settings = self._build_conn_type_settings_from_config(\n                    config, network=network\n                )\n            print(\n                f\"Found {len(conn_type_settings)} connection types: {list(conn_type_settings.keys())}\"\n            )\n\n            # If connection is not specified, use the first available connection\n            if connection is None and conn_type_settings:\n                connection = list(conn_type_settings.keys())[0]\n                print(f\"No connection specified, using first available: {connection}\")\n        else:\n            raise ValueError(\"conn_type_settings must be provided if config is not specified.\")\n\n    if connection is None:\n        raise ValueError(\"connection must be provided or inferable from conn_type_settings.\")\n    if connection not in conn_type_settings:\n        raise ValueError(f\"connection '{connection}' not found in conn_type_settings.\")\n\n    self.conn_type_settings: dict = conn_type_settings\n    if json_folder_path:\n        print(f\"updating settings from json path {json_folder_path}\")\n        self._update_spec_syn_param(json_folder_path)\n    # Use default general settings if not provided\n    if general_settings is None:\n        self.general_settings: dict = DEFAULT_GENERAL_SETTINGS.copy()\n    else:\n        # Merge defaults with user-provided\n        self.general_settings = {**DEFAULT_GENERAL_SETTINGS, **general_settings}\n\n    # Store the initial connection name and set up connection\n    self.current_connection = connection\n    self.conn = self.conn_type_settings[connection]\n    self._current_cell_type = self.conn[\"spec_settings\"][\"post_cell\"]\n    self.synaptic_props = self.conn[\"spec_syn_param\"]\n    self.vclamp = self.general_settings[\"vclamp\"]\n    self.current_name = current_name\n    self.other_vars_to_record = other_vars_to_record or []\n    self.ispk = None\n    self.input_mode = False  # Add input_mode attribute\n    self.last_figure = None  # Store reference to last generated figure\n\n    # Store original slider_vars for connection switching\n    self.original_slider_vars = slider_vars or list(self.synaptic_props.keys())\n\n    if slider_vars:\n        # Start by filtering based on keys in slider_vars\n        self.slider_vars = {\n            key: value for key, value in self.synaptic_props.items() if key in slider_vars\n        }\n        # Iterate over slider_vars and check for missing keys in self.synaptic_props\n        for key in slider_vars:\n            # If the key is missing from synaptic_props, get the value using getattr\n            if key not in self.synaptic_props:\n                try:\n                    self._set_up_cell()\n                    self._set_up_synapse()\n                    value = getattr(self.syn, key)\n                    self.slider_vars[key] = value\n                except AttributeError as e:\n                    print(f\"Error accessing '{key}' in syn {self.syn}: {e}\")\n    else:\n        self.slider_vars = self.synaptic_props\n\n    h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"]\n    h.dt = self.general_settings[\"dt\"]  # Time step (resolution) of the simulation in ms\n    h.steps_per_ms = 1 / h.dt\n    h.celsius = self.general_settings[\"celsius\"]\n\n    # get some stuff set up we need for both SingleEvent and Interactive Tuner\n    # Only set up cell if hoc_cell was not provided\n    if self.hoc_cell is None:\n        self._set_up_cell()\n    else:\n        self.cell = self.hoc_cell\n    self._set_up_synapse()\n\n    self.nstim = h.NetStim()\n    self.nstim2 = h.NetStim()\n\n    self.vcl = h.VClamp(self.cell.soma[0](0.5))\n\n    self.nc = h.NetCon(\n        self.nstim,\n        self.syn,\n        self.general_settings[\"threshold\"],\n        self.general_settings[\"delay\"],\n        self.general_settings[\"weight\"],\n    )\n    self.nc2 = h.NetCon(\n        self.nstim2,\n        self.syn,\n        self.general_settings[\"threshold\"],\n        self.general_settings[\"delay\"],\n        self.general_settings[\"weight\"],\n    )\n\n    self._set_up_recorders()\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._update_spec_syn_param","title":"<code>_update_spec_syn_param(json_folder_path)</code>","text":"<p>Update specific synaptic parameters using JSON files located in the specified folder.</p> Parameters: <p>json_folder_path : str     Path to folder containing JSON files, where each JSON file corresponds to a connection type.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _update_spec_syn_param(self, json_folder_path: str) -&gt; None:\n    \"\"\"\n    Update specific synaptic parameters using JSON files located in the specified folder.\n\n    Parameters:\n    -----------\n    json_folder_path : str\n        Path to folder containing JSON files, where each JSON file corresponds to a connection type.\n    \"\"\"\n    if not self.conn_type_settings:\n        return\n    for conn_type, settings in self.conn_type_settings.items():\n        json_file_path = os.path.join(json_folder_path, f\"{conn_type}.json\")\n        if os.path.exists(json_file_path):\n            with open(json_file_path, \"r\") as json_file:\n                json_data = json.load(json_file)\n                settings[\"spec_syn_param\"].update(json_data)\n        else:\n            print(f\"JSON file for {conn_type} not found.\")\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._set_up_cell","title":"<code>_set_up_cell()</code>","text":"<p>Set up the neuron cell based on the specified connection settings. This method is only called when hoc_cell is not provided.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _set_up_cell(self) -&gt; None:\n    \"\"\"\n    Set up the neuron cell based on the specified connection settings.\n    This method is only called when hoc_cell is not provided.\n    \"\"\"\n    if self.hoc_cell is None:\n        self.cell = getattr(h, self.conn[\"spec_settings\"][\"post_cell\"])()\n    else:\n        self.cell = self.hoc_cell\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._set_up_synapse","title":"<code>_set_up_synapse()</code>","text":"<p>Set up the synapse on the target cell according to the synaptic parameters in <code>conn_type_settings</code>.</p> Notes: <ul> <li><code>_set_up_cell()</code> should be called before setting up the synapse.</li> <li>Synapse location, type, and properties are specified within <code>spec_syn_param</code> and <code>spec_settings</code>.</li> </ul> Source code in <code>bmtool/synapses.py</code> <pre><code>def _set_up_synapse(self) -&gt; None:\n    \"\"\"\n    Set up the synapse on the target cell according to the synaptic parameters in `conn_type_settings`.\n\n    Notes:\n    ------\n    - `_set_up_cell()` should be called before setting up the synapse.\n    - Synapse location, type, and properties are specified within `spec_syn_param` and `spec_settings`.\n    \"\"\"\n    try:\n        self.syn = getattr(h, self.conn[\"spec_settings\"][\"level_of_detail\"])(\n            list(self.cell.all)[self.conn[\"spec_settings\"][\"sec_id\"]](\n                self.conn[\"spec_settings\"][\"sec_x\"]\n            )\n        )\n    except:\n        raise Exception(\"Make sure the mod file exist you are trying to load check spelling!\")\n    for key, value in self.conn[\"spec_syn_param\"].items():\n        if isinstance(value, (int, float)):\n            if hasattr(self.syn, key):\n                setattr(self.syn, key, value)\n            else:\n                print(\n                    f\"Warning: {key} cannot be assigned as it does not exist in the synapse. Check your mod file or spec_syn_param.\"\n                )\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._set_up_recorders","title":"<code>_set_up_recorders()</code>","text":"<p>Set up recording vectors to capture simulation data.</p> <p>The method sets up recorders for: - Synaptic current specified by <code>current_name</code> - Other specified synaptic variables (<code>other_vars_to_record</code>) - Time, soma voltage, and voltage clamp current for all simulations.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _set_up_recorders(self) -&gt; None:\n    \"\"\"\n    Set up recording vectors to capture simulation data.\n\n    The method sets up recorders for:\n    - Synaptic current specified by `current_name`\n    - Other specified synaptic variables (`other_vars_to_record`)\n    - Time, soma voltage, and voltage clamp current for all simulations.\n    \"\"\"\n    self.rec_vectors = {}\n    for var in self.other_vars_to_record:\n        self.rec_vectors[var] = h.Vector()\n        ref_attr = f\"_ref_{var}\"\n        if hasattr(self.syn, ref_attr):\n            self.rec_vectors[var].record(getattr(self.syn, ref_attr))\n        else:\n            print(\n                f\"Warning: {ref_attr} not found in the syn object. Use vars() to inspect available attributes.\"\n            )\n\n    # Record synaptic current\n    self.rec_vectors[self.current_name] = h.Vector()\n    ref_attr = f\"_ref_{self.current_name}\"\n    if hasattr(self.syn, ref_attr):\n        self.rec_vectors[self.current_name].record(getattr(self.syn, ref_attr))\n    else:\n        print(\"Warning: Synaptic current recorder not set up correctly.\")\n\n    # Record time, synaptic events, soma voltage, and voltage clamp current\n    self.t = h.Vector()\n    self.tspk = h.Vector()\n    self.soma_v = h.Vector()\n    self.ivcl = h.Vector()\n\n    self.t.record(h._ref_t)\n    self.nc.record(self.tspk)\n    self.nc2.record(self.tspk)\n    self.soma_v.record(self.cell.soma[0](0.5)._ref_v)\n    self.ivcl.record(self.vcl._ref_i)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner.SingleEvent","title":"<code>SingleEvent(plot_and_print=True)</code>","text":"<p>Simulate a single synaptic event by delivering an input stimulus to the synapse.</p> <p>The method sets up the neuron cell, synapse, stimulus, and voltage clamp, and then runs the NEURON simulation for a single event. The single synaptic event will occur at general_settings['tstart'] Will display graphs and synaptic properies works best with a jupyter notebook</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def SingleEvent(self, plot_and_print=True):\n    \"\"\"\n    Simulate a single synaptic event by delivering an input stimulus to the synapse.\n\n    The method sets up the neuron cell, synapse, stimulus, and voltage clamp,\n    and then runs the NEURON simulation for a single event. The single synaptic event will occur at general_settings['tstart']\n    Will display graphs and synaptic properies works best with a jupyter notebook\n    \"\"\"\n    self.ispk = None\n\n    # user slider values if the sliders are set up\n    if hasattr(self, \"dynamic_sliders\"):\n        syn_props = {var: slider.value for var, slider in self.dynamic_sliders.items()}\n        self._set_syn_prop(**syn_props)\n\n    # sets values based off optimizer\n    if hasattr(self, \"using_optimizer\"):\n        for name, value in zip(self.param_names, self.params):\n            setattr(self.syn, name, value)\n\n    # Set up the stimulus\n    self.nstim.start = self.general_settings[\"tstart\"]\n    self.nstim.noise = 0\n    self.nstim2.start = h.tstop\n    self.nstim2.noise = 0\n\n    # Set up voltage clamp\n    vcldur = [[0, 0, 0], [self.general_settings[\"tstart\"], h.tstop, 1e9]]\n    for i in range(3):\n        self.vcl.amp[i] = self.conn[\"spec_settings\"][\"vclamp_amp\"]\n        self.vcl.dur[i] = vcldur[1][i]\n\n    # Run simulation\n    h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"]\n    self.nstim.interval = self.general_settings[\"tdur\"]\n    self.nstim.number = 1\n    self.nstim2.start = h.tstop\n    h.run()\n\n    current = np.array(self.rec_vectors[self.current_name])\n    syn_props = self._get_syn_prop(\n        rise_interval=self.general_settings[\"rise_interval\"], dt=h.dt\n    )\n    current = (current - syn_props[\"baseline\"]) * 1000  # Convert to pA\n    current_integral = np.trapz(current, dx=h.dt)  # pA\u00b7ms\n\n    if plot_and_print:\n        self._plot_model(\n            [\n                self.general_settings[\"tstart\"] - 5,\n                self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"],\n            ]\n        )\n        for prop in syn_props.items():\n            print(prop)\n        print(f\"Current Integral in pA*ms: {current_integral:.2f}\")\n\n    self.rise_time = syn_props[\"rise_time\"]\n    self.decay_time = syn_props[\"decay_time\"]\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._get_syn_prop","title":"<code>_get_syn_prop(rise_interval=(0.2, 0.8), dt=h.dt, short=False)</code>","text":"<p>Calculate synaptic properties such as peak amplitude, latency, rise time, decay time, and half-width.</p> Parameters: <p>rise_interval : tuple of floats, optional     Fractional rise time interval to calculate (default is (0.2, 0.8)). dt : float, optional     Time step of the simulation (default is NEURON's <code>h.dt</code>). short : bool, optional     If True, only return baseline and sign without calculating full properties.</p> Returns: <p>dict     A dictionary containing the synaptic properties: baseline, sign, peak amplitude, latency, rise time,     decay time, and half-width.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _get_syn_prop(self, rise_interval=(0.2, 0.8), dt=h.dt, short=False):\n    \"\"\"\n    Calculate synaptic properties such as peak amplitude, latency, rise time, decay time, and half-width.\n\n    Parameters:\n    -----------\n    rise_interval : tuple of floats, optional\n        Fractional rise time interval to calculate (default is (0.2, 0.8)).\n    dt : float, optional\n        Time step of the simulation (default is NEURON's `h.dt`).\n    short : bool, optional\n        If True, only return baseline and sign without calculating full properties.\n\n    Returns:\n    --------\n    dict\n        A dictionary containing the synaptic properties: baseline, sign, peak amplitude, latency, rise time,\n        decay time, and half-width.\n    \"\"\"\n    if self.vclamp:\n        isyn = self.ivcl\n    else:\n        isyn = self.rec_vectors[self.current_name]\n    isyn = np.asarray(isyn)\n    tspk = np.asarray(self.tspk)\n    if tspk.size:\n        tspk = tspk[0]\n\n    ispk = int(np.floor(tspk / dt))\n    baseline = isyn[ispk]\n    isyn = isyn[ispk:] - baseline\n    # print(np.abs(isyn))\n    # print(np.argmax(np.abs(isyn)))\n    # print(isyn[np.argmax(np.abs(isyn))])\n    # print(np.sign(isyn[np.argmax(np.abs(isyn))]))\n    sign = np.sign(isyn[np.argmax(np.abs(isyn))])\n    if short:\n        return {\"baseline\": baseline, \"sign\": sign}\n    isyn *= sign\n    # print(isyn)\n    # peak amplitude\n    ipk, _ = find_peaks(isyn)\n    ipk = ipk[0]\n    peak = isyn[ipk]\n    # latency\n    istart = self._find_first(np.diff(isyn[: ipk + 1]) &gt; 0)\n    latency = dt * (istart + 1)\n    # rise time\n    rt1 = self._find_first(isyn[istart : ipk + 1] &gt; rise_interval[0] * peak)\n    rt2 = self._find_first(isyn[istart : ipk + 1] &gt; rise_interval[1] * peak)\n    rise_time = (rt2 - rt1) * dt\n    # decay time\n    iend = self._find_first(np.diff(isyn[ipk:]) &gt; 0)\n    iend = isyn.size - 1 if iend is None else iend + ipk\n    decay_len = iend - ipk + 1\n    popt, _ = curve_fit(\n        lambda t, a, tau: a * np.exp(-t / tau),\n        dt * np.arange(decay_len),\n        isyn[ipk : iend + 1],\n        p0=(peak, dt * decay_len / 2),\n    )\n    decay_time = popt[1]\n    # half-width\n    hw1 = self._find_first(isyn[istart : ipk + 1] &gt; 0.5 * peak)\n    hw2 = self._find_first(isyn[ipk:] &lt; 0.5 * peak)\n    hw2 = isyn.size if hw2 is None else hw2 + ipk\n    half_width = dt * (hw2 - hw1)\n    output = {\n        \"baseline\": baseline,\n        \"sign\": sign,\n        \"latency\": latency,\n        \"amp\": peak,\n        \"rise_time\": rise_time,\n        \"decay_time\": decay_time,\n        \"half_width\": half_width,\n    }\n    return output\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._set_syn_prop","title":"<code>_set_syn_prop(**kwargs)</code>","text":"<p>Sets the synaptic parameters based on user inputs from sliders.</p> Parameters: <p>**kwargs : dict     Synaptic properties (such as weight, Use, tau_f, tau_d) as keyword arguments.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _set_syn_prop(self, **kwargs):\n    \"\"\"\n    Sets the synaptic parameters based on user inputs from sliders.\n\n    Parameters:\n    -----------\n    **kwargs : dict\n        Synaptic properties (such as weight, Use, tau_f, tau_d) as keyword arguments.\n    \"\"\"\n    for key, value in kwargs.items():\n        setattr(self.syn, key, value)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner._simulate_model","title":"<code>_simulate_model(input_frequency, delay, vclamp=None)</code>","text":"<p>Runs the simulation with the specified input frequency, delay, and voltage clamp settings.</p> Parameters: <p>input_frequency : float     Frequency of the input drive train in Hz. delay : float     Delay period in milliseconds between the 8th and 9th pulses. vclamp : bool or None, optional     Whether to use voltage clamp. If None, the current setting is used. Default is None.</p> Notes: <p>This method handles two different input modes: - Standard train mode with 8 initial pulses followed by a delay and 4 additional pulses - Continuous input mode where stimulation continues for a specified duration</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _simulate_model(self, input_frequency, delay, vclamp=None):\n    \"\"\"\n    Runs the simulation with the specified input frequency, delay, and voltage clamp settings.\n\n    Parameters:\n    -----------\n    input_frequency : float\n        Frequency of the input drive train in Hz.\n    delay : float\n        Delay period in milliseconds between the 8th and 9th pulses.\n    vclamp : bool or None, optional\n        Whether to use voltage clamp. If None, the current setting is used. Default is None.\n\n    Notes:\n    ------\n    This method handles two different input modes:\n    - Standard train mode with 8 initial pulses followed by a delay and 4 additional pulses\n    - Continuous input mode where stimulation continues for a specified duration\n    \"\"\"\n    if not self.input_mode:\n        self.tstop = self._set_drive_train(input_frequency, delay)\n        h.tstop = self.tstop\n\n        vcldur = [[0, 0, 0], [self.general_settings[\"tstart\"], self.tstop, 1e9]]\n        for i in range(3):\n            self.vcl.amp[i] = self.conn[\"spec_settings\"][\"vclamp_amp\"]\n            self.vcl.dur[i] = vcldur[1][i]\n        h.finitialize(70 * mV)\n        h.continuerun(self.tstop * ms)\n        # h.run()\n    else:\n        # Continuous input mode: ensure simulation runs long enough for the full stimulation duration\n        self.tstop = (\n            self.general_settings[\"tstart\"] + self.w_duration.value + 300\n        )  # 300ms buffer time\n        self.nstim.interval = 1000 / input_frequency\n        self.nstim.number = np.ceil(self.w_duration.value / 1000 * input_frequency + 1)\n        self.nstim2.number = 0\n\n        h.finitialize(70 * mV)\n        h.continuerun(self.tstop * ms)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner.InteractiveTuner","title":"<code>InteractiveTuner()</code>","text":"<p>Sets up interactive sliders for tuning short-term plasticity (STP) parameters in a Jupyter Notebook.</p> <p>This method creates an interactive UI with sliders for: - Network selection dropdown (if multiple networks available and config provided) - Connection type selection dropdown - Input frequency - Delay between pulse trains - Duration of stimulation (for continuous input mode) - Synaptic parameters (e.g., Use, tau_f, tau_d) based on the syn model</p> <p>It also provides buttons for: - Running a single event simulation - Running a train input simulation - Toggling voltage clamp mode - Switching between standard and continuous input modes</p> Network Dropdown Feature: <p>When the SynapseTuner is initialized with a BMTK config file containing multiple networks: - A network dropdown appears next to the connection dropdown - Users can dynamically switch between networks (e.g., 'network_to_network', 'external_to_network') - Switching networks rebuilds available connections and updates the connection dropdown - The current connection is preserved if it exists in the new network - If multiple networks exist but only one is specified during init, that network is used as default</p> Notes: <p>Ideal for exploratory parameter tuning and interactive visualization of synapse behavior with different parameter values and stimulation protocols. The network dropdown feature enables comprehensive exploration of multi-network BMTK simulations without needing to reinitialize the tuner.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def InteractiveTuner(self):\n    \"\"\"\n    Sets up interactive sliders for tuning short-term plasticity (STP) parameters in a Jupyter Notebook.\n\n    This method creates an interactive UI with sliders for:\n    - Network selection dropdown (if multiple networks available and config provided)\n    - Connection type selection dropdown\n    - Input frequency\n    - Delay between pulse trains\n    - Duration of stimulation (for continuous input mode)\n    - Synaptic parameters (e.g., Use, tau_f, tau_d) based on the syn model\n\n    It also provides buttons for:\n    - Running a single event simulation\n    - Running a train input simulation\n    - Toggling voltage clamp mode\n    - Switching between standard and continuous input modes\n\n    Network Dropdown Feature:\n    ------------------------\n    When the SynapseTuner is initialized with a BMTK config file containing multiple networks:\n    - A network dropdown appears next to the connection dropdown\n    - Users can dynamically switch between networks (e.g., 'network_to_network', 'external_to_network')\n    - Switching networks rebuilds available connections and updates the connection dropdown\n    - The current connection is preserved if it exists in the new network\n    - If multiple networks exist but only one is specified during init, that network is used as default\n\n    Notes:\n    ------\n    Ideal for exploratory parameter tuning and interactive visualization of\n    synapse behavior with different parameter values and stimulation protocols.\n    The network dropdown feature enables comprehensive exploration of multi-network\n    BMTK simulations without needing to reinitialize the tuner.\n    \"\"\"\n    # Widgets setup (Sliders)\n    freqs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200]\n    delays = [125, 250, 500, 1000, 2000, 4000]\n    durations = [100, 300, 500, 1000, 2000, 5000, 10000]\n    freq0 = 50\n    delay0 = 250\n    duration0 = 300\n    vlamp_status = self.vclamp\n\n    # Connection dropdown\n    connection_options = sorted(list(self.conn_type_settings.keys()))\n    w_connection = widgets.Dropdown(\n        options=connection_options,\n        value=self.current_connection,\n        description=\"Connection:\",\n        style={\"description_width\": \"initial\"},\n    )\n\n    # Network dropdown - only shown if config was provided and multiple networks are available\n    # This enables users to switch between different network datasets dynamically\n    w_network = None\n    if self.config is not None and len(self.available_networks) &gt; 1:\n        w_network = widgets.Dropdown(\n            options=self.available_networks,\n            value=self.current_network,\n            description=\"Network:\",\n            style={\"description_width\": \"initial\"},\n        )\n\n    w_run = widgets.Button(description=\"Run Train\", icon=\"history\", button_style=\"primary\")\n    w_single = widgets.Button(description=\"Single Event\", icon=\"check\", button_style=\"success\")\n    w_vclamp = widgets.ToggleButton(\n        value=vlamp_status,\n        description=\"Voltage Clamp\",\n        icon=\"fast-backward\",\n        button_style=\"warning\",\n    )\n\n    # Voltage clamp amplitude input\n    default_vclamp_amp = getattr(self.conn[\"spec_settings\"], \"vclamp_amp\", -70.0)\n    w_vclamp_amp = widgets.FloatText(\n        value=default_vclamp_amp,\n        description=\"V_clamp (mV):\",\n        step=5.0,\n        style={\"description_width\": \"initial\"},\n        layout=widgets.Layout(width=\"150px\"),\n    )\n\n    w_input_mode = widgets.ToggleButton(\n        value=False, description=\"Continuous input\", icon=\"eject\", button_style=\"info\"\n    )\n    w_input_freq = widgets.SelectionSlider(options=freqs, value=freq0, description=\"Input Freq\")\n\n    # Sliders for delay and duration\n    self.w_delay = widgets.SelectionSlider(options=delays, value=delay0, description=\"Delay\")\n    self.w_duration = widgets.SelectionSlider(\n        options=durations, value=duration0, description=\"Duration\"\n    )\n\n    # Save functionality widgets\n    save_path_text = widgets.Text(\n        value=\"plot.png\", description=\"Save path:\", layout=widgets.Layout(width=\"300px\")\n    )\n    save_button = widgets.Button(description=\"Save Plot\", icon=\"save\", button_style=\"success\")\n\n    def save_plot(b):\n        if hasattr(self, \"last_figure\") and self.last_figure is not None:\n            try:\n                # Create a new figure with just the first subplot (synaptic current)\n                fig, ax = plt.subplots(figsize=(8, 6))\n\n                # Get the axes from the original figure\n                original_axes = self.last_figure.get_axes()\n                if len(original_axes) &gt; 0:\n                    first_ax = original_axes[0]\n\n                    # Copy the data from the first subplot\n                    for line in first_ax.get_lines():\n                        ax.plot(\n                            line.get_xdata(),\n                            line.get_ydata(),\n                            color=line.get_color(),\n                            label=line.get_label(),\n                        )\n\n                    # Copy axis labels and title\n                    ax.set_xlabel(first_ax.get_xlabel())\n                    ax.set_ylabel(first_ax.get_ylabel())\n                    ax.set_title(first_ax.get_title())\n                    ax.set_xlim(first_ax.get_xlim())\n                    ax.legend()\n                    ax.grid(True)\n\n                    # Save the new figure\n                    fig.savefig(save_path_text.value)\n                    plt.close(fig)  # Close the temporary figure\n                    print(f\"Synaptic current plot saved to {save_path_text.value}\")\n                else:\n                    print(\"No subplots found in the figure\")\n\n            except Exception as e:\n                print(f\"Error saving plot: {e}\")\n        else:\n            print(\"No plot to save\")\n\n    save_button.on_click(save_plot)\n\n    def create_dynamic_sliders():\n        \"\"\"Create sliders based on current connection's parameters\"\"\"\n        sliders = {}\n        for key, value in self.slider_vars.items():\n            if isinstance(value, (int, float)):  # Only create sliders for numeric values\n                if hasattr(self.syn, key):\n                    if value == 0:\n                        print(\n                            f\"{key} was set to zero, going to try to set a range of values, try settings the {key} to a nonzero value if you dont like the range!\"\n                        )\n                        slider = widgets.FloatSlider(\n                            value=value, min=0, max=1000, step=1, description=key\n                        )\n                    else:\n                        slider = widgets.FloatSlider(\n                            value=value, min=0, max=value * 20, step=value / 5, description=key\n                        )\n                    sliders[key] = slider\n                else:\n                    print(f\"skipping slider for {key} due to not being a synaptic variable\")\n        return sliders\n\n    # Generate sliders dynamically based on valid numeric entries in self.slider_vars\n    self.dynamic_sliders = create_dynamic_sliders()\n    print(\n        \"Setting up slider! The sliders ranges are set by their init value so try changing that if you dont like the slider range!\"\n    )\n\n    # Create output widget for displaying results\n    output_widget = widgets.Output()\n\n    def run_single_event(*args):\n        clear_output()\n        display(ui)\n        display(output_widget)\n\n        self.vclamp = w_vclamp.value\n        # Update voltage clamp amplitude if voltage clamp is enabled\n        if self.vclamp:\n            # Update the voltage clamp amplitude settings\n            self.conn[\"spec_settings\"][\"vclamp_amp\"] = w_vclamp_amp.value\n            # Update general settings if they exist\n            if hasattr(self, \"general_settings\"):\n                self.general_settings[\"vclamp_amp\"] = w_vclamp_amp.value\n        # Update synaptic properties based on slider values\n        self.ispk = None\n\n        # Clear previous results and run simulation\n        output_widget.clear_output()\n        with output_widget:\n            self.SingleEvent()\n\n    def on_connection_change(*args):\n        \"\"\"Handle connection dropdown change\"\"\"\n        try:\n            new_connection = w_connection.value\n            if new_connection != self.current_connection:\n                # Switch to new connection\n                self._switch_connection(new_connection)\n\n                # Recreate dynamic sliders for new connection\n                self.dynamic_sliders = create_dynamic_sliders()\n\n                # Update UI\n                update_ui_layout()\n                update_ui()\n\n        except Exception as e:\n            print(f\"Error switching connection: {e}\")\n\n    def on_network_change(*args):\n        \"\"\"\n        Handle network dropdown change events.\n\n        This callback is triggered when the user selects a different network from\n        the network dropdown. It coordinates the complete switching process:\n        1. Calls _switch_network() to rebuild connections for the new network\n        2. Updates the connection dropdown options with new network's connections\n        3. Recreates dynamic sliders for new connection parameters\n        4. Refreshes the entire UI to reflect all changes\n        \"\"\"\n        if w_network is None:\n            return\n        try:\n            new_network = w_network.value\n            if new_network != self.current_network:\n                # Switch to new network\n                self._switch_network(new_network)\n\n                # Update connection dropdown options with new network's connections\n                connection_options = list(self.conn_type_settings.keys())\n                w_connection.options = connection_options\n                if connection_options:\n                    w_connection.value = self.current_connection\n\n                # Recreate dynamic sliders for new connection\n                self.dynamic_sliders = create_dynamic_sliders()\n\n                # Update UI\n                update_ui_layout()\n                update_ui()\n\n        except Exception as e:\n            print(f\"Error switching network: {e}\")\n\n    def update_ui_layout():\n        \"\"\"\n        Update the UI layout with new sliders and network dropdown.\n\n        This function reconstructs the entire UI layout including:\n        - Network dropdown (if available) and connection dropdown in the top row\n        - Button controls and input mode toggles\n        - Parameter sliders arranged in columns\n        \"\"\"\n        nonlocal ui, slider_columns\n\n        # Add the dynamic sliders to the UI\n        slider_widgets = [slider for slider in self.dynamic_sliders.values()]\n\n        if slider_widgets:\n            half = len(slider_widgets) // 2\n            col1 = VBox(slider_widgets[:half])\n            col2 = VBox(slider_widgets[half:])\n            slider_columns = HBox([col1, col2])\n        else:\n            slider_columns = VBox([])\n\n        # Create button row with voltage clamp controls\n        if w_vclamp.value:  # Show voltage clamp amplitude input when toggle is on\n            button_row = HBox([w_run, w_single, w_vclamp, w_vclamp_amp, w_input_mode])\n        else:  # Hide voltage clamp amplitude input when toggle is off\n            button_row = HBox([w_run, w_single, w_vclamp, w_input_mode])\n\n        # Construct the top row - include network dropdown if available\n        # This creates a horizontal layout with network dropdown (if present) and connection dropdown\n        if w_network is not None:\n            connection_row = HBox([w_network, w_connection])\n        else:\n            connection_row = HBox([w_connection])\n        slider_row = HBox([w_input_freq, self.w_delay, self.w_duration])\n        save_row = HBox([save_path_text, save_button])\n\n        ui = VBox([connection_row, button_row, slider_row, slider_columns, save_row])\n\n    # Function to update UI based on input mode\n    def update_ui(*args):\n        clear_output()\n        display(ui)\n        display(output_widget)\n\n        self.vclamp = w_vclamp.value\n        # Update voltage clamp amplitude if voltage clamp is enabled\n        if self.vclamp:\n            self.conn[\"spec_settings\"][\"vclamp_amp\"] = w_vclamp_amp.value\n            if hasattr(self, \"general_settings\"):\n                self.general_settings[\"vclamp_amp\"] = w_vclamp_amp.value\n\n        self.input_mode = w_input_mode.value\n        syn_props = {var: slider.value for var, slider in self.dynamic_sliders.items()}\n        self._set_syn_prop(**syn_props)\n\n        # Clear previous results and run simulation\n        output_widget.clear_output()\n        with output_widget:\n            if not self.input_mode:\n                self._simulate_model(w_input_freq.value, self.w_delay.value, w_vclamp.value)\n            else:\n                self._simulate_model(w_input_freq.value, self.w_duration.value, w_vclamp.value)\n            amp = self._response_amplitude()\n            self._plot_model(\n                [self.general_settings[\"tstart\"] - self.nstim.interval / 3, self.tstop]\n            )\n            _ = self._calc_ppr_induction_recovery(amp)\n\n    # Function to switch between delay and duration sliders\n    def switch_slider(*args):\n        if w_input_mode.value:\n            self.w_delay.layout.display = \"none\"  # Hide delay slider\n            self.w_duration.layout.display = \"\"  # Show duration slider\n        else:\n            self.w_delay.layout.display = \"\"  # Show delay slider\n            self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n    # Function to handle voltage clamp toggle\n    def on_vclamp_toggle(*args):\n        \"\"\"Handle voltage clamp toggle changes to show/hide amplitude input\"\"\"\n        update_ui_layout()\n        clear_output()\n        display(ui)\n        display(output_widget)\n\n    # Link widgets to their callback functions\n    w_connection.observe(on_connection_change, names=\"value\")\n    # Link network dropdown callback only if network dropdown was created\n    if w_network is not None:\n        w_network.observe(on_network_change, names=\"value\")\n    w_input_mode.observe(switch_slider, names=\"value\")\n    w_vclamp.observe(on_vclamp_toggle, names=\"value\")\n\n    # Hide the duration slider initially until the user selects it\n    self.w_duration.layout.display = \"none\"  # Hide duration slider\n\n    w_single.on_click(run_single_event)\n    w_run.on_click(update_ui)\n\n    # Initial UI setup\n    slider_columns = VBox([])\n    ui = VBox([])\n    update_ui_layout()\n\n    display(ui)\n    update_ui()\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseTuner.stp_frequency_response","title":"<code>stp_frequency_response(freqs=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200], delay=250, plot=True, log_plot=True)</code>","text":"<p>Analyze synaptic response across different stimulation frequencies.</p> <p>This method systematically tests how the synapse model responds to different stimulation frequencies, calculating key short-term plasticity (STP) metrics for each frequency.</p> Parameters: <p>freqs : list, optional     List of frequencies to analyze (in Hz). Default covers a wide range from 1-200 Hz. delay : float, optional     Delay between pulse trains in ms. Default is 250 ms. plot : bool, optional     Whether to plot the results. Default is True. log_plot : bool, optional     Whether to use logarithmic scale for frequency axis. Default is True.</p> Returns: <p>dict     Dictionary containing frequency-dependent metrics with keys:     - 'frequencies': List of tested frequencies     - 'ppr': Paired-pulse ratios at each frequency     - 'simple_ppr': Simple paired-pulse ratios (2nd/1st pulse) at each frequency     - 'induction': Induction values at each frequency     - 'recovery': Recovery values at each frequency</p> Notes: <p>This method is particularly useful for characterizing the frequency-dependent behavior of synapses, such as identifying facilitating vs. depressing regimes or the frequency at which a synapse transitions between these behaviors.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def stp_frequency_response(\n    self,\n    freqs=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200],\n    delay=250,\n    plot=True,\n    log_plot=True,\n):\n    \"\"\"\n    Analyze synaptic response across different stimulation frequencies.\n\n    This method systematically tests how the synapse model responds to different\n    stimulation frequencies, calculating key short-term plasticity (STP) metrics\n    for each frequency.\n\n    Parameters:\n    -----------\n    freqs : list, optional\n        List of frequencies to analyze (in Hz). Default covers a wide range from 1-200 Hz.\n    delay : float, optional\n        Delay between pulse trains in ms. Default is 250 ms.\n    plot : bool, optional\n        Whether to plot the results. Default is True.\n    log_plot : bool, optional\n        Whether to use logarithmic scale for frequency axis. Default is True.\n\n    Returns:\n    --------\n    dict\n        Dictionary containing frequency-dependent metrics with keys:\n        - 'frequencies': List of tested frequencies\n        - 'ppr': Paired-pulse ratios at each frequency\n        - 'simple_ppr': Simple paired-pulse ratios (2nd/1st pulse) at each frequency\n        - 'induction': Induction values at each frequency\n        - 'recovery': Recovery values at each frequency\n\n    Notes:\n    ------\n    This method is particularly useful for characterizing the frequency-dependent\n    behavior of synapses, such as identifying facilitating vs. depressing regimes\n    or the frequency at which a synapse transitions between these behaviors.\n    \"\"\"\n    results = {\n        \"frequencies\": freqs,\n        \"ppr\": [],\n        \"induction\": [],\n        \"recovery\": [],\n        \"simple_ppr\": [],\n    }\n\n    # Store original state\n    original_ispk = self.ispk\n\n    for freq in tqdm(freqs, desc=\"Analyzing frequencies\"):\n        self._simulate_model(freq, delay)\n        amp = self._response_amplitude()\n        ppr, induction, recovery, simple_ppr = self._calc_ppr_induction_recovery(\n            amp, print_math=False\n        )\n\n        results[\"ppr\"].append(float(ppr))\n        results[\"induction\"].append(float(induction))\n        results[\"recovery\"].append(float(recovery))\n        results[\"simple_ppr\"].append(float(simple_ppr))\n\n    # Restore original state\n    self.ispk = original_ispk\n\n    if plot:\n        self._plot_frequency_analysis(results, log_plot=log_plot)\n\n    return results\n</code></pre>"},{"location":"api/synapses/#gap-junction-tuning","title":"Gap Junction Tuning","text":""},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner","title":"<code>bmtool.synapses.GapJunctionTuner</code>","text":"Source code in <code>bmtool/synapses.py</code> <pre><code>class GapJunctionTuner:\n    def __init__(\n        self,\n        mechanisms_dir: Optional[str] = None,\n        templates_dir: Optional[str] = None,\n        config: Optional[str] = None,\n        general_settings: Optional[dict] = None,\n        conn_type_settings: Optional[dict] = None,\n        hoc_cell: Optional[object] = None,\n    ):\n        \"\"\"\n        Initialize the GapJunctionTuner class.\n\n        Parameters:\n        -----------\n        mechanisms_dir : str\n            Directory path containing the compiled mod files needed for NEURON mechanisms.\n        templates_dir : str\n            Directory path containing cell template files (.hoc or .py) loaded into NEURON.\n        config : str\n            Path to a BMTK config.json file. Can be used to load mechanisms, templates, and other settings.\n        general_settings : dict\n            General settings dictionary including parameters like simulation time step, duration, and temperature.\n        conn_type_settings : dict\n            A dictionary containing connection-specific settings for gap junctions.\n        hoc_cell : object, optional\n            An already loaded NEURON cell object. If provided, template loading and cell creation will be skipped.\n        \"\"\"\n        self.hoc_cell = hoc_cell\n\n        if hoc_cell is None:\n            if config is None and (mechanisms_dir is None or templates_dir is None):\n                raise ValueError(\n                    \"Either a config file, both mechanisms_dir and templates_dir, or a hoc_cell must be provided.\"\n                )\n\n            if config is None:\n                neuron.load_mechanisms(mechanisms_dir)\n                h.load_file(templates_dir)\n            else:\n                # this will load both mechs and templates\n                load_templates_from_config(config)\n\n        # Use default general settings if not provided, merge with user-provided\n        if general_settings is None:\n            self.general_settings: dict = DEFAULT_GAP_JUNCTION_GENERAL_SETTINGS.copy()\n        else:\n            self.general_settings = {**DEFAULT_GAP_JUNCTION_GENERAL_SETTINGS, **general_settings}\n        self.conn_type_settings = conn_type_settings\n        self.vclamp = self.general_settings[\"vclamp\"]\n\n        self._syn_params_cache = {}\n        self.config = config\n        self.available_networks = []\n        self.current_network = None\n        self.last_figure = None\n        if self.conn_type_settings is None and self.config is not None:\n            self.conn_type_settings = self._build_conn_type_settings_from_config(self.config)\n        if self.conn_type_settings is None or len(self.conn_type_settings) == 0:\n            raise ValueError(\n                \"conn_type_settings must be provided or config must be given to load gap junction connections from\"\n            )\n        self.current_connection = list(self.conn_type_settings.keys())[0]\n        self.conn = self.conn_type_settings[self.current_connection]\n\n        h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"] + 100.0\n        h.dt = self.general_settings[\"dt\"]  # Time step (resolution) of the simulation in ms\n        h.steps_per_ms = 1 / h.dt\n        h.celsius = self.general_settings[\"celsius\"]\n\n        # Clean up any existing parallel context before setting up gap junctions\n        try:\n            pc_temp = h.ParallelContext()\n            pc_temp.done()  # Clean up any existing parallel context\n        except:\n            pass  # Ignore errors if no existing context\n\n        # Force cleanup\n        import gc\n\n        gc.collect()\n\n        # set up gap junctions\n        self.pc = h.ParallelContext()\n\n        # Use provided hoc_cell or create new cells\n        if self.hoc_cell is not None:\n            self.cell1 = self.hoc_cell\n            # For gap junctions, we need two cells, so create a second one if using hoc_cell\n            self.cell_name = self.conn[\"cell\"]\n            self.cell2 = getattr(h, self.cell_name)()\n        else:\n            print(self.conn)\n            self.cell_name = self.conn[\"cell\"]\n            self.cell1 = getattr(h, self.cell_name)()\n            self.cell2 = getattr(h, self.cell_name)()\n\n        self.icl = h.IClamp(self.cell1.soma[0](0.5))\n        self.icl.delay = self.general_settings[\"tstart\"]\n        self.icl.dur = self.general_settings[\"tdur\"]\n        self.icl.amp = self.general_settings[\"iclamp_amp\"]  # nA\n\n        sec1 = list(self.cell1.all)[self.conn[\"sec_id\"]]\n        sec2 = list(self.cell2.all)[self.conn[\"sec_id\"]]\n\n        # Use unique IDs to avoid conflicts with existing parallel context setups\n        import time\n\n        unique_id = int(time.time() * 1000) % 10000  # Use timestamp as unique base ID\n\n        self.pc.source_var(sec1(self.conn[\"sec_x\"])._ref_v, unique_id, sec=sec1)\n        self.gap_junc_1 = h.Gap(sec1(0.5))\n        self.pc.target_var(self.gap_junc_1._ref_vgap, unique_id + 1)\n\n        self.pc.source_var(sec2(self.conn[\"sec_x\"])._ref_v, unique_id + 1, sec=sec2)\n        self.gap_junc_2 = h.Gap(sec2(0.5))\n        self.pc.target_var(self.gap_junc_2._ref_vgap, unique_id)\n\n        self.pc.setup_transfer()\n\n        # Now it's safe to initialize NEURON\n        h.finitialize()\n\n    def _load_synaptic_params_from_config(self, config: dict, dynamics_params: str) -&gt; dict:\n        try:\n            # Get the synaptic models directory from config\n            synaptic_models_dir = config.get(\"components\", {}).get(\"synaptic_models_dir\", \"\")\n            if synaptic_models_dir:\n                # Handle path variables\n                if synaptic_models_dir.startswith(\"$\"):\n                    # This is a placeholder, try to resolve it\n                    config_dir = os.path.dirname(config.get(\"config_path\", \"\"))\n                    synaptic_models_dir = synaptic_models_dir.replace(\n                        \"$COMPONENTS_DIR\", os.path.join(config_dir, \"components\")\n                    )\n                    synaptic_models_dir = synaptic_models_dir.replace(\"$BASE_DIR\", config_dir)\n\n                dynamics_file = os.path.join(synaptic_models_dir, dynamics_params)\n\n                if os.path.exists(dynamics_file):\n                    with open(dynamics_file, \"r\") as f:\n                        return json.load(f)\n                else:\n                    print(f\"Warning: Dynamics params file not found: {dynamics_file}\")\n        except Exception as e:\n            print(f\"Warning: Error loading synaptic parameters: {e}\")\n\n        return {}\n\n    def _load_available_networks(self) -&gt; None:\n        \"\"\"\n        Load available network names from the config file for the network dropdown feature.\n\n        This method is automatically called during initialization when a config file is provided.\n        It populates the available_networks list which enables the network dropdown in\n        InteractiveTuner when multiple networks are available.\n\n        Network Dropdown Behavior:\n        -------------------------\n        - If only one network exists: No network dropdown is shown\n        - If multiple networks exist: Network dropdown appears next to connection dropdown\n        - Networks are loaded from the edges data in the config file\n        - Current network defaults to the first available if not specified during init\n        \"\"\"\n        if self.config is None:\n            self.available_networks = []\n            return\n\n        try:\n            edges = load_edges_from_config(self.config)\n            self.available_networks = list(edges.keys())\n\n            # Set current network to first available if not specified\n            if self.current_network is None and self.available_networks:\n                self.current_network = self.available_networks[0]\n        except Exception as e:\n            print(f\"Warning: Could not load networks from config: {e}\")\n            self.available_networks = []\n\n    def _build_conn_type_settings_from_config(self, config_path: str) -&gt; Dict[str, dict]:\n        # Load configuration and get nodes and edges using util.py methods\n        config = load_config(config_path)\n        # Ensure the config dict knows its source path so path substitutions can be resolved\n        try:\n            config[\"config_path\"] = config_path\n        except Exception:\n            pass\n        nodes = load_nodes_from_config(config_path)\n        edges = load_edges_from_config(config_path)\n\n        conn_type_settings = {}\n\n        # Process all edge datasets\n        for edge_dataset_name, edge_df in edges.items():\n            if edge_df.empty:\n                continue\n\n            # Merging with node data to get model templates\n            source_node_df = None\n            target_node_df = None\n\n            # First, try to deterministically parse the edge_dataset_name for patterns like '&lt;src&gt;_to_&lt;tgt&gt;'\n            if \"_to_\" in edge_dataset_name:\n                parts = edge_dataset_name.split(\"_to_\")\n                if len(parts) == 2:\n                    src_name, tgt_name = parts\n                    if src_name in nodes:\n                        source_node_df = nodes[src_name].add_prefix(\"source_\")\n                    if tgt_name in nodes:\n                        target_node_df = nodes[tgt_name].add_prefix(\"target_\")\n\n            # If not found by parsing name, fall back to inspecting a sample edge row\n            if source_node_df is None or target_node_df is None:\n                sample_edge = edge_df.iloc[0] if len(edge_df) &gt; 0 else None\n                if sample_edge is not None:\n                    source_pop_name = sample_edge.get(\"source_population\", \"\")\n                    target_pop_name = sample_edge.get(\"target_population\", \"\")\n                    if source_pop_name in nodes:\n                        source_node_df = nodes[source_pop_name].add_prefix(\"source_\")\n                    if target_pop_name in nodes:\n                        target_node_df = nodes[target_pop_name].add_prefix(\"target_\")\n\n            # As a last resort, attempt to heuristically match\n            if source_node_df is None or target_node_df is None:\n                for pop_name, node_df in nodes.items():\n                    if source_node_df is None and (\n                        edge_dataset_name.startswith(pop_name)\n                        or edge_dataset_name.endswith(pop_name)\n                    ):\n                        source_node_df = node_df.add_prefix(\"source_\")\n                    if target_node_df is None and (\n                        edge_dataset_name.startswith(pop_name)\n                        or edge_dataset_name.endswith(pop_name)\n                    ):\n                        target_node_df = node_df.add_prefix(\"target_\")\n\n            if source_node_df is None or target_node_df is None:\n                print(f\"Warning: Could not find node data for edge dataset {edge_dataset_name}\")\n                continue\n\n            # Merge edge data with source node info\n            edges_with_source = pd.merge(\n                edge_df.reset_index(),\n                source_node_df,\n                how=\"left\",\n                left_on=\"source_node_id\",\n                right_index=True,\n            )\n\n            # Merge with target node info\n            edges_with_nodes = pd.merge(\n                edges_with_source,\n                target_node_df,\n                how=\"left\",\n                left_on=\"target_node_id\",\n                right_index=True,\n            )\n\n            # Skip edge datasets that don't have gap junction information\n            if \"is_gap_junction\" not in edges_with_nodes.columns:\n                continue\n\n            # Filter to only gap junction edges\n            # Handle NaN values in is_gap_junction column\n            gap_junction_mask = edges_with_nodes[\"is_gap_junction\"].fillna(False) == True\n            gap_junction_edges = edges_with_nodes[gap_junction_mask]\n            if gap_junction_edges.empty:\n                continue\n\n            # Get unique edge types from the gap junction edges\n            if \"edge_type_id\" in gap_junction_edges.columns:\n                edge_types = gap_junction_edges[\"edge_type_id\"].unique()\n            else:\n                edge_types = [None]  # Single edge type\n\n            # Process each edge type\n            for edge_type_id in edge_types:\n                # Filter edges for this type\n                if edge_type_id is not None:\n                    edge_type_data = gap_junction_edges[\n                        gap_junction_edges[\"edge_type_id\"] == edge_type_id\n                    ]\n                else:\n                    edge_type_data = gap_junction_edges\n\n                if len(edge_type_data) == 0:\n                    continue\n\n                # Get representative edge for this type\n                edge_info = edge_type_data.iloc[0]\n\n                # Process gap junction\n                source_model_template = edge_info.get(\"source_model_template\", \"\")\n                target_model_template = edge_info.get(\"target_model_template\", \"\")\n\n                source_cell_type = (\n                    source_model_template.replace(\"hoc:\", \"\")\n                    if source_model_template.startswith(\"hoc:\")\n                    else source_model_template\n                )\n                target_cell_type = (\n                    target_model_template.replace(\"hoc:\", \"\")\n                    if target_model_template.startswith(\"hoc:\")\n                    else target_model_template\n                )\n\n                if source_cell_type != target_cell_type:\n                    continue  # Only process gap junctions between same cell types\n\n                source_pop = edge_info.get(\"source_pop_name\", \"\")\n                target_pop = edge_info.get(\"target_pop_name\", \"\")\n\n                conn_name = f\"{source_pop}2{target_pop}_gj\"\n                if edge_type_id is not None:\n                    conn_name += f\"_type_{edge_type_id}\"\n\n                conn_settings = {\n                    \"cell\": source_cell_type,\n                    \"sec_id\": 0,\n                    \"sec_x\": 0.5,\n                    \"iclamp_amp\": -0.01,\n                    \"spec_syn_param\": {},\n                }\n\n                # Load dynamics params\n                dynamics_file_name = edge_info.get(\"dynamics_params\", \"\")\n                if dynamics_file_name and dynamics_file_name.upper() != \"NULL\":\n                    try:\n                        syn_params = self._load_synaptic_params_from_config(\n                            config, dynamics_file_name\n                        )\n                        conn_settings[\"spec_syn_param\"] = syn_params\n                    except Exception as e:\n                        print(\n                            f\"Warning: could not load dynamics_params file '{dynamics_file_name}': {e}\"\n                        )\n\n                conn_type_settings[conn_name] = conn_settings\n\n        return conn_type_settings\n\n    def _switch_connection(self, new_connection: str) -&gt; None:\n        \"\"\"\n        Switch to a different gap junction connection and update all related properties.\n\n        Parameters:\n        -----------\n        new_connection : str\n            Name of the new connection type to switch to.\n        \"\"\"\n        if new_connection not in self.conn_type_settings:\n            raise ValueError(f\"Connection '{new_connection}' not found in conn_type_settings\")\n\n        # Update current connection\n        self.current_connection = new_connection\n        self.conn = self.conn_type_settings[new_connection]\n\n        # Check if cell type changed\n        new_cell_name = self.conn[\"cell\"]\n        if self.cell_name != new_cell_name:\n            self.cell_name = new_cell_name\n\n            # Recreate cells\n            if self.hoc_cell is None:\n                self.cell1 = getattr(h, self.cell_name)()\n                self.cell2 = getattr(h, self.cell_name)()\n            else:\n                # For hoc_cell, recreate the second cell\n                self.cell2 = getattr(h, self.cell_name)()\n\n            # Recreate IClamp\n            self.icl = h.IClamp(self.cell1.soma[0](0.5))\n            self.icl.delay = self.general_settings[\"tstart\"]\n            self.icl.dur = self.general_settings[\"tdur\"]\n            self.icl.amp = self.general_settings[\"iclamp_amp\"]\n        else:\n            # Update IClamp parameters even if same cell type\n            self.icl.amp = self.general_settings[\"iclamp_amp\"]\n\n        # Always recreate gap junctions when switching connections\n        # (even for same cell type, sec_id or sec_x might differ)\n\n        # Clean up previous gap junctions and parallel context\n        if hasattr(self, \"gap_junc_1\"):\n            del self.gap_junc_1\n        if hasattr(self, \"gap_junc_2\"):\n            del self.gap_junc_2\n\n        # Properly clean up the existing parallel context\n        if hasattr(self, \"pc\"):\n            self.pc.done()  # Clean up existing parallel context\n\n        # Force garbage collection and reset NEURON state\n        import gc\n\n        gc.collect()\n        h.finitialize()\n\n        # Create a fresh parallel context after cleanup\n        self.pc = h.ParallelContext()\n\n        try:\n            sec1 = list(self.cell1.all)[self.conn[\"sec_id\"]]\n            sec2 = list(self.cell2.all)[self.conn[\"sec_id\"]]\n\n            # Use unique IDs to avoid conflicts with existing parallel context setups\n            import time\n\n            unique_id = int(time.time() * 1000) % 10000  # Use timestamp as unique base ID\n\n            self.pc.source_var(sec1(self.conn[\"sec_x\"])._ref_v, unique_id, sec=sec1)\n            self.gap_junc_1 = h.Gap(sec1(0.5))\n            self.pc.target_var(self.gap_junc_1._ref_vgap, unique_id + 1)\n\n            self.pc.source_var(sec2(self.conn[\"sec_x\"])._ref_v, unique_id + 1, sec=sec2)\n            self.gap_junc_2 = h.Gap(sec2(0.5))\n            self.pc.target_var(self.gap_junc_2._ref_vgap, unique_id)\n\n            self.pc.setup_transfer()\n        except Exception as e:\n            print(f\"Error setting up gap junctions: {e}\")\n            # Try to continue with basic setup\n            self.gap_junc_1 = h.Gap(list(self.cell1.all)[self.conn[\"sec_id\"]](0.5))\n            self.gap_junc_2 = h.Gap(list(self.cell2.all)[self.conn[\"sec_id\"]](0.5))\n\n        # Reset NEURON state after complete setup\n        h.finitialize()\n\n        print(f\"Successfully switched to connection: {new_connection}\")\n\n    def model(self, resistance):\n        \"\"\"\n        Run a simulation with a specified gap junction resistance.\n\n        Parameters:\n        -----------\n        resistance : float\n            The gap junction resistance value (in MOhm) to use for the simulation.\n\n        Notes:\n        ------\n        This method sets up the gap junction resistance, initializes recording vectors for time\n        and membrane voltages of both cells, and runs the NEURON simulation.\n        \"\"\"\n        self.gap_junc_1.g = resistance\n        self.gap_junc_2.g = resistance\n\n        t_vec = h.Vector()\n        soma_v_1 = h.Vector()\n        soma_v_2 = h.Vector()\n        t_vec.record(h._ref_t)\n        soma_v_1.record(self.cell1.soma[0](0.5)._ref_v)\n        soma_v_2.record(self.cell2.soma[0](0.5)._ref_v)\n\n        self.t_vec = t_vec\n        self.soma_v_1 = soma_v_1\n        self.soma_v_2 = soma_v_2\n\n        h.finitialize(-70 * mV)\n        h.continuerun(h.tstop * ms)\n\n    def plot_model(self):\n        \"\"\"\n        Plot the voltage traces of both cells to visualize gap junction coupling.\n\n        This method creates a plot showing the membrane potential of both cells over time,\n        highlighting the effect of gap junction coupling when a current step is applied to cell 1.\n        \"\"\"\n        t_range = [\n            self.general_settings[\"tstart\"] - 100.0,\n            self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"] + 100.0,\n        ]\n        t = np.array(self.t_vec)\n        v1 = np.array(self.soma_v_1)\n        v2 = np.array(self.soma_v_2)\n        tidx = (t &gt;= t_range[0]) &amp; (t &lt;= t_range[1])\n\n        plt.figure()\n        plt.plot(t[tidx], v1[tidx], \"b\", label=f\"{self.cell_name} 1\")\n        plt.plot(t[tidx], v2[tidx], \"r\", label=f\"{self.cell_name} 2\")\n        plt.title(f\"{self.cell_name} gap junction\")\n        plt.xlabel(\"Time (ms)\")\n        plt.ylabel(\"Membrane Voltage (mV)\")\n        plt.legend()\n        self.last_figure = plt.gcf()\n\n    def coupling_coefficient(self, t, v1, v2, t_start, t_end, dt=h.dt):\n        \"\"\"\n        Calculate the coupling coefficient between two cells connected by a gap junction.\n\n        Parameters:\n        -----------\n        t : array-like\n            Time vector.\n        v1 : array-like\n            Voltage trace of the cell receiving the current injection.\n        v2 : array-like\n            Voltage trace of the coupled cell.\n        t_start : float\n            Start time for calculating the steady-state voltage change.\n        t_end : float\n            End time for calculating the steady-state voltage change.\n        dt : float, optional\n            Time step of the simulation. Default is h.dt.\n\n        Returns:\n        --------\n        float\n            The coupling coefficient, defined as the ratio of voltage change in cell 2\n            to voltage change in cell 1 (\u0394V\u2082/\u0394V\u2081).\n        \"\"\"\n        t = np.asarray(t)\n        v1 = np.asarray(v1)\n        v2 = np.asarray(v2)\n        idx1 = np.nonzero(t &lt; t_start)[0][-1]\n        idx2 = np.nonzero(t &lt; t_end)[0][-1]\n        return (v2[idx2] - v2[idx1]) / (v1[idx2] - v1[idx1])\n\n    def coupling_ratio(self):\n        \"\"\"\n        Calculate the coupling ratio from the last simulation.\n\n        Returns the coupling coefficient (dV2/dV1) using the most recent simulation data\n        stored in self.t_vec, self.soma_v_1, and self.soma_v_2.\n\n        Returns:\n        --------\n        float\n            The coupling coefficient from the last simulation.\n        \"\"\"\n        t_start = self.general_settings[\"tstart\"]\n        t_end = t_start + self.general_settings[\"tdur\"]\n        return self.coupling_coefficient(self.t_vec, self.soma_v_1, self.soma_v_2, t_start, t_end)\n\n    def InteractiveTuner(self):\n        \"\"\"\n        Sets up interactive sliders for tuning gap junction resistance in a Jupyter Notebook.\n\n        This method creates an interactive UI with:\n        - Connection type selection dropdown\n        - Gap junction resistance slider (logarithmic scale)\n        - Voltage clamp toggle and amplitude input\n        - Run button to simulate and plot the coupling\n\n        The UI displays:\n        - Voltage traces of both coupled cells\n        - Coupling coefficient (dV2/dV1)\n\n        Notes:\n        ------\n        Ideal for exploratory parameter tuning and interactive visualization of\n        gap junction coupling between two cells.\n        \"\"\"\n        import ipywidgets as widgets\n        from IPython.display import display, clear_output\n\n        # Connection dropdown\n        connection_options = sorted(list(self.conn_type_settings.keys()))\n        w_connection = widgets.Dropdown(\n            options=connection_options,\n            value=self.current_connection,\n            description=\"Connection:\",\n            style={\"description_width\": \"initial\"},\n        )\n\n        # Resistance slider - typical range for gap junctions\n        w_resistance = widgets.FloatLogSlider(\n            value=1e-3,  # 1 MOhm default\n            base=10,\n            min=-6,  # 1e-6\n            max=0,  # 1\n            step=0.1,\n            description=\"Resistance (MOhm):\",\n            style={\"description_width\": \"initial\"},\n        )\n\n        # Voltage clamp toggle\n        w_vclamp = widgets.ToggleButton(\n            value=self.vclamp,\n            description=\"Voltage Clamp\",\n            button_style=\"\",\n            tooltip=\"Toggle voltage clamp mode\",\n        )\n\n        # Voltage clamp amplitude\n        default_vclamp_amp = self.conn.get(\"vclamp_amp\", -70.0)\n        w_vclamp_amp = widgets.FloatText(\n            value=default_vclamp_amp,\n            description=\"VClamp Amp (mV):\",\n            style={\"description_width\": \"initial\"},\n        )\n\n        # Output widget\n        output_widget = widgets.Output()\n\n        def run_simulation(*args):\n            clear_output(wait=True)\n            display(ui)\n            display(output_widget)\n\n            # Update connection if changed\n            if w_connection.value != self.current_connection:\n                self._switch_connection(w_connection.value)\n\n            # Update vclamp\n            self.vclamp = w_vclamp.value\n\n            with output_widget:\n                clear_output(wait=True)\n                resistance = w_resistance.value\n                print(f\"Running simulation with resistance = {resistance} MOhm\")\n                self.model(resistance)\n                self.plot_model()\n                # Calculate and print coupling coefficient\n                coupling = self.coupling_ratio()\n                print(f\"Coupling coefficient (dV2/dV1): {coupling:.3f}\")\n                plt.show()\n\n        def on_connection_change(*args):\n            if w_connection.value != self.current_connection:\n                self._switch_connection(w_connection.value)\n\n        w_connection.observe(on_connection_change, names=\"value\")\n\n        # Run button\n        run_button = widgets.Button(description=\"Run Simulation\")\n        run_button.on_click(run_simulation)\n\n        # Layout\n        ui = widgets.VBox(\n            [\n                widgets.HBox([w_connection]),\n                widgets.HBox([w_resistance]),\n                widgets.HBox([w_vclamp, w_vclamp_amp]),\n                widgets.HBox([run_button]),\n            ]\n        )\n\n        display(ui)\n        display(output_widget)\n\n    def stp_frequency_response(\n        self,\n        freqs=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 35, 50, 100, 200],\n        delay=250,\n        plot=True,\n        log_plot=True,\n    ):\n        \"\"\"\n        Analyze synaptic response across different stimulation frequencies.\n\n        This method systematically tests how the synapse model responds to different\n        stimulation frequencies, calculating key short-term plasticity (STP) metrics\n        for each frequency.\n\n        Parameters:\n        -----------\n        freqs : list, optional\n            List of frequencies to analyze (in Hz). Default covers a wide range from 1-200 Hz.\n        delay : float, optional\n            Delay between pulse trains in ms. Default is 250 ms.\n        plot : bool, optional\n            Whether to plot the results. Default is True.\n        log_plot : bool, optional\n            Whether to use logarithmic scale for frequency axis. Default is True.\n\n        Returns:\n        --------\n        dict\n            Dictionary containing frequency-dependent metrics with keys:\n            - 'frequencies': List of tested frequencies\n            - 'ppr': Paired-pulse ratios at each frequency\n            - 'simple_ppr': Simple paired-pulse ratios (2nd/1st pulse) at each frequency\n            - 'induction': Induction values at each frequency\n            - 'recovery': Recovery values at each frequency\n\n        Notes:\n        ------\n        This method is particularly useful for characterizing the frequency-dependent\n        behavior of synapses, such as identifying facilitating vs. depressing regimes\n        or the frequency at which a synapse transitions between these behaviors.\n        \"\"\"\n        results = {\n            \"frequencies\": freqs,\n            \"ppr\": [],\n            \"induction\": [],\n            \"recovery\": [],\n            \"simple_ppr\": [],\n        }\n\n        # Store original state\n        original_ispk = self.ispk\n\n        for freq in tqdm(freqs, desc=\"Analyzing frequencies\"):\n            self._simulate_model(freq, delay)\n            amp = self._response_amplitude()\n            ppr, induction, recovery, simple_ppr = self._calc_ppr_induction_recovery(\n                amp, print_math=False\n            )\n\n            results[\"ppr\"].append(float(ppr))\n            results[\"induction\"].append(float(induction))\n            results[\"recovery\"].append(float(recovery))\n            results[\"simple_ppr\"].append(float(simple_ppr))\n\n        # Restore original state\n        self.ispk = original_ispk\n\n        if plot:\n            self._plot_frequency_analysis(results, log_plot=log_plot)\n\n        return results\n\n    def _plot_frequency_analysis(self, results, log_plot):\n        \"\"\"\n        Plot the frequency-dependent synaptic properties.\n\n        Parameters:\n        -----------\n        results : dict\n            Dictionary containing frequency analysis results with keys:\n            - 'frequencies': List of tested frequencies\n            - 'ppr': Paired-pulse ratios at each frequency\n            - 'simple_ppr': Simple paired-pulse ratios at each frequency\n            - 'induction': Induction values at each frequency\n            - 'recovery': Recovery values at each frequency\n        log_plot : bool\n            Whether to use logarithmic scale for frequency axis\n\n        Notes:\n        ------\n        Creates a figure with three subplots showing:\n        1. Paired-pulse ratios (both normalized and simple) vs. frequency\n        2. Induction vs. frequency\n        3. Recovery vs. frequency\n\n        Each plot includes a horizontal reference line at y=0 or y=1 to indicate\n        the boundary between facilitation and depression.\n        \"\"\"\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n\n        # Plot both PPR measures\n        if log_plot:\n            ax1.semilogx(results[\"frequencies\"], results[\"ppr\"], \"o-\", label=\"Normalized PPR\")\n            ax1.semilogx(results[\"frequencies\"], results[\"simple_ppr\"], \"s-\", label=\"Simple PPR\")\n        else:\n            ax1.plot(results[\"frequencies\"], results[\"ppr\"], \"o-\", label=\"Normalized PPR\")\n            ax1.plot(results[\"frequencies\"], results[\"simple_ppr\"], \"s-\", label=\"Simple PPR\")\n        ax1.axhline(y=1, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax1.set_xlabel(\"Frequency (Hz)\")\n        ax1.set_ylabel(\"Paired Pulse Ratio\")\n        ax1.set_title(\"PPR vs Frequency\")\n        ax1.legend()\n        ax1.grid(True)\n\n        # Plot Induction\n        if log_plot:\n            ax2.semilogx(results[\"frequencies\"], results[\"induction\"], \"o-\")\n        else:\n            ax2.plot(results[\"frequencies\"], results[\"induction\"], \"o-\")\n        ax2.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax2.set_xlabel(\"Frequency (Hz)\")\n        ax2.set_ylabel(\"Induction\")\n        ax2.set_title(\"Induction vs Frequency\")\n        ax2.grid(True)\n\n        # Plot Recovery\n        if log_plot:\n            ax3.semilogx(results[\"frequencies\"], results[\"recovery\"], \"o-\")\n        else:\n            ax3.plot(results[\"frequencies\"], results[\"recovery\"], \"o-\")\n        ax3.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n        ax3.set_xlabel(\"Frequency (Hz)\")\n        ax3.set_ylabel(\"Recovery\")\n        ax3.set_title(\"Recovery vs Frequency\")\n        ax3.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n\n    def generate_synaptic_table(self, stp_frequency=50.0, stp_delay=250.0, plot=True):\n        \"\"\"\n        Generate a comprehensive table of synaptic parameters for all connections.\n\n        This method iterates through all available connections, runs simulations to\n        characterize each synapse, and compiles the results into a pandas DataFrame.\n\n        Parameters:\n        -----------\n        stp_frequency : float, optional\n            Frequency in Hz to use for STP (short-term plasticity) analysis. Default is 50.0 Hz.\n        stp_delay : float, optional\n            Delay in ms between pulse trains for STP analysis. Default is 250.0 ms.\n        plot : bool, optional\n            Whether to display the resulting table. Default is True.\n\n        Returns:\n        --------\n        pd.DataFrame\n            DataFrame containing synaptic parameters for each connection with columns:\n            - connection: Connection name\n            - rise_time: 20-80% rise time (ms)\n            - decay_time: Decay time constant (ms)\n            - latency: Response latency (ms)\n            - half_width: Response half-width (ms)\n            - peak_amplitude: Peak synaptic current amplitude (pA)\n            - baseline: Baseline current (pA)\n            - ppr: Paired-pulse ratio (normalized)\n            - simple_ppr: Simple paired-pulse ratio (2nd/1st pulse)\n            - induction: STP induction measure\n            - recovery: STP recovery measure\n\n        Notes:\n        ------\n        This method temporarily switches between connections to characterize each one,\n        then restores the original connection. The STP metrics are calculated at the\n        specified frequency and delay.\n        \"\"\"\n        # Store original connection to restore later\n        original_connection = self.current_connection\n\n        # Initialize results list\n        results = []\n\n        print(f\"Analyzing {len(self.conn_type_settings)} connections...\")\n\n        for conn_name in tqdm(self.conn_type_settings.keys(), desc=\"Analyzing connections\"):\n            try:\n                # Switch to this connection\n                self._switch_connection(conn_name)\n\n                # Run single event analysis\n                self.SingleEvent(plot_and_print=False)\n\n                # Get synaptic properties from the single event\n                syn_props = self._get_syn_prop()\n\n                # Run STP analysis at specified frequency\n                stp_results = self.stp_frequency_response(\n                    freqs=[stp_frequency], delay=stp_delay, plot=False, log_plot=False\n                )\n\n                # Extract STP metrics for this frequency\n                freq_idx = 0  # Only one frequency tested\n                ppr = stp_results[\"ppr\"][freq_idx]\n                induction = stp_results[\"induction\"][freq_idx]\n                recovery = stp_results[\"recovery\"][freq_idx]\n                simple_ppr = stp_results[\"simple_ppr\"][freq_idx]\n\n                # Compile results for this connection\n                conn_results = {\n                    \"connection\": conn_name,\n                    \"rise_time\": float(self.rise_time),\n                    \"decay_time\": float(self.decay_time),\n                    \"latency\": float(syn_props.get(\"latency\", 0)),\n                    \"half_width\": float(syn_props.get(\"half_width\", 0)),\n                    \"peak_amplitude\": float(syn_props.get(\"amp\", 0)),\n                    \"baseline\": float(syn_props.get(\"baseline\", 0)),\n                    \"ppr\": float(ppr),\n                    \"simple_ppr\": float(simple_ppr),\n                    \"induction\": float(induction),\n                    \"recovery\": float(recovery),\n                }\n\n                results.append(conn_results)\n\n            except Exception as e:\n                print(f\"Warning: Failed to analyze connection '{conn_name}': {e}\")\n                # Add partial results if possible\n                results.append(\n                    {\n                        \"connection\": conn_name,\n                        \"rise_time\": float(\"nan\"),\n                        \"decay_time\": float(\"nan\"),\n                        \"latency\": float(\"nan\"),\n                        \"half_width\": float(\"nan\"),\n                        \"peak_amplitude\": float(\"nan\"),\n                        \"baseline\": float(\"nan\"),\n                        \"ppr\": float(\"nan\"),\n                        \"simple_ppr\": float(\"nan\"),\n                        \"induction\": float(\"nan\"),\n                        \"recovery\": float(\"nan\"),\n                    }\n                )\n\n        # Restore original connection\n        if original_connection in self.conn_type_settings:\n            self._switch_connection(original_connection)\n\n        # Create DataFrame\n        df = pd.DataFrame(results)\n\n        # Set connection as index for better display\n        df = df.set_index(\"connection\")\n\n        if plot:\n            # Display the table\n            print(\"\\nSynaptic Parameters Table:\")\n            print(\"=\" * 80)\n            display(df.round(4))\n\n            # Optional: Create a simple bar plot for key metrics\n            try:\n                fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n                fig.suptitle(\n                    f\"Synaptic Parameters Across Connections (STP at {stp_frequency}Hz)\",\n                    fontsize=16,\n                )\n\n                # Plot rise/decay times\n                df[[\"rise_time\", \"decay_time\"]].plot(kind=\"bar\", ax=axes[0, 0])\n                axes[0, 0].set_title(\"Rise and Decay Times\")\n                axes[0, 0].set_ylabel(\"Time (ms)\")\n                axes[0, 0].tick_params(axis=\"x\", rotation=45)\n\n                # Plot PPR metrics\n                df[[\"ppr\", \"simple_ppr\"]].plot(kind=\"bar\", ax=axes[0, 1])\n                axes[0, 1].set_title(\"Paired-Pulse Ratios\")\n                axes[0, 1].axhline(y=1, color=\"gray\", linestyle=\"--\", alpha=0.5)\n                axes[0, 1].tick_params(axis=\"x\", rotation=45)\n\n                # Plot induction\n                df[\"induction\"].plot(kind=\"bar\", ax=axes[1, 0], color=\"green\")\n                axes[1, 0].set_title(\"STP Induction\")\n                axes[1, 0].axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n                axes[1, 0].set_ylabel(\"Induction\")\n                axes[1, 0].tick_params(axis=\"x\", rotation=45)\n\n                # Plot recovery\n                df[\"recovery\"].plot(kind=\"bar\", ax=axes[1, 1], color=\"orange\")\n                axes[1, 1].set_title(\"STP Recovery\")\n                axes[1, 1].axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n                axes[1, 1].set_ylabel(\"Recovery\")\n                axes[1, 1].tick_params(axis=\"x\", rotation=45)\n\n                plt.tight_layout()\n                plt.show()\n\n            except Exception as e:\n                print(f\"Warning: Could not create plots: {e}\")\n\n        return df\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner.__init__","title":"<code>__init__(mechanisms_dir=None, templates_dir=None, config=None, general_settings=None, conn_type_settings=None, hoc_cell=None)</code>","text":"<p>Initialize the GapJunctionTuner class.</p> Parameters: <p>mechanisms_dir : str     Directory path containing the compiled mod files needed for NEURON mechanisms. templates_dir : str     Directory path containing cell template files (.hoc or .py) loaded into NEURON. config : str     Path to a BMTK config.json file. Can be used to load mechanisms, templates, and other settings. general_settings : dict     General settings dictionary including parameters like simulation time step, duration, and temperature. conn_type_settings : dict     A dictionary containing connection-specific settings for gap junctions. hoc_cell : object, optional     An already loaded NEURON cell object. If provided, template loading and cell creation will be skipped.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def __init__(\n    self,\n    mechanisms_dir: Optional[str] = None,\n    templates_dir: Optional[str] = None,\n    config: Optional[str] = None,\n    general_settings: Optional[dict] = None,\n    conn_type_settings: Optional[dict] = None,\n    hoc_cell: Optional[object] = None,\n):\n    \"\"\"\n    Initialize the GapJunctionTuner class.\n\n    Parameters:\n    -----------\n    mechanisms_dir : str\n        Directory path containing the compiled mod files needed for NEURON mechanisms.\n    templates_dir : str\n        Directory path containing cell template files (.hoc or .py) loaded into NEURON.\n    config : str\n        Path to a BMTK config.json file. Can be used to load mechanisms, templates, and other settings.\n    general_settings : dict\n        General settings dictionary including parameters like simulation time step, duration, and temperature.\n    conn_type_settings : dict\n        A dictionary containing connection-specific settings for gap junctions.\n    hoc_cell : object, optional\n        An already loaded NEURON cell object. If provided, template loading and cell creation will be skipped.\n    \"\"\"\n    self.hoc_cell = hoc_cell\n\n    if hoc_cell is None:\n        if config is None and (mechanisms_dir is None or templates_dir is None):\n            raise ValueError(\n                \"Either a config file, both mechanisms_dir and templates_dir, or a hoc_cell must be provided.\"\n            )\n\n        if config is None:\n            neuron.load_mechanisms(mechanisms_dir)\n            h.load_file(templates_dir)\n        else:\n            # this will load both mechs and templates\n            load_templates_from_config(config)\n\n    # Use default general settings if not provided, merge with user-provided\n    if general_settings is None:\n        self.general_settings: dict = DEFAULT_GAP_JUNCTION_GENERAL_SETTINGS.copy()\n    else:\n        self.general_settings = {**DEFAULT_GAP_JUNCTION_GENERAL_SETTINGS, **general_settings}\n    self.conn_type_settings = conn_type_settings\n    self.vclamp = self.general_settings[\"vclamp\"]\n\n    self._syn_params_cache = {}\n    self.config = config\n    self.available_networks = []\n    self.current_network = None\n    self.last_figure = None\n    if self.conn_type_settings is None and self.config is not None:\n        self.conn_type_settings = self._build_conn_type_settings_from_config(self.config)\n    if self.conn_type_settings is None or len(self.conn_type_settings) == 0:\n        raise ValueError(\n            \"conn_type_settings must be provided or config must be given to load gap junction connections from\"\n        )\n    self.current_connection = list(self.conn_type_settings.keys())[0]\n    self.conn = self.conn_type_settings[self.current_connection]\n\n    h.tstop = self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"] + 100.0\n    h.dt = self.general_settings[\"dt\"]  # Time step (resolution) of the simulation in ms\n    h.steps_per_ms = 1 / h.dt\n    h.celsius = self.general_settings[\"celsius\"]\n\n    # Clean up any existing parallel context before setting up gap junctions\n    try:\n        pc_temp = h.ParallelContext()\n        pc_temp.done()  # Clean up any existing parallel context\n    except:\n        pass  # Ignore errors if no existing context\n\n    # Force cleanup\n    import gc\n\n    gc.collect()\n\n    # set up gap junctions\n    self.pc = h.ParallelContext()\n\n    # Use provided hoc_cell or create new cells\n    if self.hoc_cell is not None:\n        self.cell1 = self.hoc_cell\n        # For gap junctions, we need two cells, so create a second one if using hoc_cell\n        self.cell_name = self.conn[\"cell\"]\n        self.cell2 = getattr(h, self.cell_name)()\n    else:\n        print(self.conn)\n        self.cell_name = self.conn[\"cell\"]\n        self.cell1 = getattr(h, self.cell_name)()\n        self.cell2 = getattr(h, self.cell_name)()\n\n    self.icl = h.IClamp(self.cell1.soma[0](0.5))\n    self.icl.delay = self.general_settings[\"tstart\"]\n    self.icl.dur = self.general_settings[\"tdur\"]\n    self.icl.amp = self.general_settings[\"iclamp_amp\"]  # nA\n\n    sec1 = list(self.cell1.all)[self.conn[\"sec_id\"]]\n    sec2 = list(self.cell2.all)[self.conn[\"sec_id\"]]\n\n    # Use unique IDs to avoid conflicts with existing parallel context setups\n    import time\n\n    unique_id = int(time.time() * 1000) % 10000  # Use timestamp as unique base ID\n\n    self.pc.source_var(sec1(self.conn[\"sec_x\"])._ref_v, unique_id, sec=sec1)\n    self.gap_junc_1 = h.Gap(sec1(0.5))\n    self.pc.target_var(self.gap_junc_1._ref_vgap, unique_id + 1)\n\n    self.pc.source_var(sec2(self.conn[\"sec_x\"])._ref_v, unique_id + 1, sec=sec2)\n    self.gap_junc_2 = h.Gap(sec2(0.5))\n    self.pc.target_var(self.gap_junc_2._ref_vgap, unique_id)\n\n    self.pc.setup_transfer()\n\n    # Now it's safe to initialize NEURON\n    h.finitialize()\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner.model","title":"<code>model(resistance)</code>","text":"<p>Run a simulation with a specified gap junction resistance.</p> Parameters: <p>resistance : float     The gap junction resistance value (in MOhm) to use for the simulation.</p> Notes: <p>This method sets up the gap junction resistance, initializes recording vectors for time and membrane voltages of both cells, and runs the NEURON simulation.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def model(self, resistance):\n    \"\"\"\n    Run a simulation with a specified gap junction resistance.\n\n    Parameters:\n    -----------\n    resistance : float\n        The gap junction resistance value (in MOhm) to use for the simulation.\n\n    Notes:\n    ------\n    This method sets up the gap junction resistance, initializes recording vectors for time\n    and membrane voltages of both cells, and runs the NEURON simulation.\n    \"\"\"\n    self.gap_junc_1.g = resistance\n    self.gap_junc_2.g = resistance\n\n    t_vec = h.Vector()\n    soma_v_1 = h.Vector()\n    soma_v_2 = h.Vector()\n    t_vec.record(h._ref_t)\n    soma_v_1.record(self.cell1.soma[0](0.5)._ref_v)\n    soma_v_2.record(self.cell2.soma[0](0.5)._ref_v)\n\n    self.t_vec = t_vec\n    self.soma_v_1 = soma_v_1\n    self.soma_v_2 = soma_v_2\n\n    h.finitialize(-70 * mV)\n    h.continuerun(h.tstop * ms)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner.plot_model","title":"<code>plot_model()</code>","text":"<p>Plot the voltage traces of both cells to visualize gap junction coupling.</p> <p>This method creates a plot showing the membrane potential of both cells over time, highlighting the effect of gap junction coupling when a current step is applied to cell 1.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def plot_model(self):\n    \"\"\"\n    Plot the voltage traces of both cells to visualize gap junction coupling.\n\n    This method creates a plot showing the membrane potential of both cells over time,\n    highlighting the effect of gap junction coupling when a current step is applied to cell 1.\n    \"\"\"\n    t_range = [\n        self.general_settings[\"tstart\"] - 100.0,\n        self.general_settings[\"tstart\"] + self.general_settings[\"tdur\"] + 100.0,\n    ]\n    t = np.array(self.t_vec)\n    v1 = np.array(self.soma_v_1)\n    v2 = np.array(self.soma_v_2)\n    tidx = (t &gt;= t_range[0]) &amp; (t &lt;= t_range[1])\n\n    plt.figure()\n    plt.plot(t[tidx], v1[tidx], \"b\", label=f\"{self.cell_name} 1\")\n    plt.plot(t[tidx], v2[tidx], \"r\", label=f\"{self.cell_name} 2\")\n    plt.title(f\"{self.cell_name} gap junction\")\n    plt.xlabel(\"Time (ms)\")\n    plt.ylabel(\"Membrane Voltage (mV)\")\n    plt.legend()\n    self.last_figure = plt.gcf()\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner.coupling_coefficient","title":"<code>coupling_coefficient(t, v1, v2, t_start, t_end, dt=h.dt)</code>","text":"<p>Calculate the coupling coefficient between two cells connected by a gap junction.</p> Parameters: <p>t : array-like     Time vector. v1 : array-like     Voltage trace of the cell receiving the current injection. v2 : array-like     Voltage trace of the coupled cell. t_start : float     Start time for calculating the steady-state voltage change. t_end : float     End time for calculating the steady-state voltage change. dt : float, optional     Time step of the simulation. Default is h.dt.</p> Returns: <p>float     The coupling coefficient, defined as the ratio of voltage change in cell 2     to voltage change in cell 1 (\u0394V\u2082/\u0394V\u2081).</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def coupling_coefficient(self, t, v1, v2, t_start, t_end, dt=h.dt):\n    \"\"\"\n    Calculate the coupling coefficient between two cells connected by a gap junction.\n\n    Parameters:\n    -----------\n    t : array-like\n        Time vector.\n    v1 : array-like\n        Voltage trace of the cell receiving the current injection.\n    v2 : array-like\n        Voltage trace of the coupled cell.\n    t_start : float\n        Start time for calculating the steady-state voltage change.\n    t_end : float\n        End time for calculating the steady-state voltage change.\n    dt : float, optional\n        Time step of the simulation. Default is h.dt.\n\n    Returns:\n    --------\n    float\n        The coupling coefficient, defined as the ratio of voltage change in cell 2\n        to voltage change in cell 1 (\u0394V\u2082/\u0394V\u2081).\n    \"\"\"\n    t = np.asarray(t)\n    v1 = np.asarray(v1)\n    v2 = np.asarray(v2)\n    idx1 = np.nonzero(t &lt; t_start)[0][-1]\n    idx2 = np.nonzero(t &lt; t_end)[0][-1]\n    return (v2[idx2] - v2[idx1]) / (v1[idx2] - v1[idx1])\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionTuner.InteractiveTuner","title":"<code>InteractiveTuner()</code>","text":"<p>Sets up interactive sliders for tuning gap junction resistance in a Jupyter Notebook.</p> <p>This method creates an interactive UI with: - Connection type selection dropdown - Gap junction resistance slider (logarithmic scale) - Voltage clamp toggle and amplitude input - Run button to simulate and plot the coupling</p> <p>The UI displays: - Voltage traces of both coupled cells - Coupling coefficient (dV2/dV1)</p> Notes: <p>Ideal for exploratory parameter tuning and interactive visualization of gap junction coupling between two cells.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def InteractiveTuner(self):\n    \"\"\"\n    Sets up interactive sliders for tuning gap junction resistance in a Jupyter Notebook.\n\n    This method creates an interactive UI with:\n    - Connection type selection dropdown\n    - Gap junction resistance slider (logarithmic scale)\n    - Voltage clamp toggle and amplitude input\n    - Run button to simulate and plot the coupling\n\n    The UI displays:\n    - Voltage traces of both coupled cells\n    - Coupling coefficient (dV2/dV1)\n\n    Notes:\n    ------\n    Ideal for exploratory parameter tuning and interactive visualization of\n    gap junction coupling between two cells.\n    \"\"\"\n    import ipywidgets as widgets\n    from IPython.display import display, clear_output\n\n    # Connection dropdown\n    connection_options = sorted(list(self.conn_type_settings.keys()))\n    w_connection = widgets.Dropdown(\n        options=connection_options,\n        value=self.current_connection,\n        description=\"Connection:\",\n        style={\"description_width\": \"initial\"},\n    )\n\n    # Resistance slider - typical range for gap junctions\n    w_resistance = widgets.FloatLogSlider(\n        value=1e-3,  # 1 MOhm default\n        base=10,\n        min=-6,  # 1e-6\n        max=0,  # 1\n        step=0.1,\n        description=\"Resistance (MOhm):\",\n        style={\"description_width\": \"initial\"},\n    )\n\n    # Voltage clamp toggle\n    w_vclamp = widgets.ToggleButton(\n        value=self.vclamp,\n        description=\"Voltage Clamp\",\n        button_style=\"\",\n        tooltip=\"Toggle voltage clamp mode\",\n    )\n\n    # Voltage clamp amplitude\n    default_vclamp_amp = self.conn.get(\"vclamp_amp\", -70.0)\n    w_vclamp_amp = widgets.FloatText(\n        value=default_vclamp_amp,\n        description=\"VClamp Amp (mV):\",\n        style={\"description_width\": \"initial\"},\n    )\n\n    # Output widget\n    output_widget = widgets.Output()\n\n    def run_simulation(*args):\n        clear_output(wait=True)\n        display(ui)\n        display(output_widget)\n\n        # Update connection if changed\n        if w_connection.value != self.current_connection:\n            self._switch_connection(w_connection.value)\n\n        # Update vclamp\n        self.vclamp = w_vclamp.value\n\n        with output_widget:\n            clear_output(wait=True)\n            resistance = w_resistance.value\n            print(f\"Running simulation with resistance = {resistance} MOhm\")\n            self.model(resistance)\n            self.plot_model()\n            # Calculate and print coupling coefficient\n            coupling = self.coupling_ratio()\n            print(f\"Coupling coefficient (dV2/dV1): {coupling:.3f}\")\n            plt.show()\n\n    def on_connection_change(*args):\n        if w_connection.value != self.current_connection:\n            self._switch_connection(w_connection.value)\n\n    w_connection.observe(on_connection_change, names=\"value\")\n\n    # Run button\n    run_button = widgets.Button(description=\"Run Simulation\")\n    run_button.on_click(run_simulation)\n\n    # Layout\n    ui = widgets.VBox(\n        [\n            widgets.HBox([w_connection]),\n            widgets.HBox([w_resistance]),\n            widgets.HBox([w_vclamp, w_vclamp_amp]),\n            widgets.HBox([run_button]),\n        ]\n    )\n\n    display(ui)\n    display(output_widget)\n</code></pre>"},{"location":"api/synapses/#optimization-results","title":"Optimization Results","text":""},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizationResult","title":"<code>bmtool.synapses.SynapseOptimizationResult</code>  <code>dataclass</code>","text":"<p>Container for synaptic parameter optimization results</p> Source code in <code>bmtool/synapses.py</code> <pre><code>@dataclass\nclass SynapseOptimizationResult:\n    \"\"\"Container for synaptic parameter optimization results\"\"\"\n\n    optimal_params: Dict[str, float]\n    achieved_metrics: Dict[str, float]\n    target_metrics: Dict[str, float]\n    error: float\n    optimization_path: List[Dict[str, float]]\n</code></pre>"},{"location":"api/synapses/#synapse-optimization","title":"Synapse Optimization","text":""},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer","title":"<code>bmtool.synapses.SynapseOptimizer</code>","text":"Source code in <code>bmtool/synapses.py</code> <pre><code>class SynapseOptimizer:\n    def __init__(self, tuner):\n        \"\"\"\n        Initialize the synapse optimizer with parameter scaling\n\n        Parameters:\n        -----------\n        tuner : SynapseTuner\n            Instance of the SynapseTuner class\n        \"\"\"\n        self.tuner = tuner\n        self.optimization_history = []\n        self.param_scales = {}\n\n    def _normalize_params(self, params: np.ndarray, param_names: List[str]) -&gt; np.ndarray:\n        \"\"\"\n        Normalize parameters to similar scales for better optimization performance.\n\n        Parameters:\n        -----------\n        params : np.ndarray\n            Original parameter values.\n        param_names : List[str]\n            Names of the parameters corresponding to the values.\n\n        Returns:\n        --------\n        np.ndarray\n            Normalized parameter values.\n        \"\"\"\n        return np.array([params[i] / self.param_scales[name] for i, name in enumerate(param_names)])\n\n    def _denormalize_params(\n        self, normalized_params: np.ndarray, param_names: List[str]\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Convert normalized parameters back to original scale.\n\n        Parameters:\n        -----------\n        normalized_params : np.ndarray\n            Normalized parameter values.\n        param_names : List[str]\n            Names of the parameters corresponding to the normalized values.\n\n        Returns:\n        --------\n        np.ndarray\n            Denormalized parameter values in their original scale.\n        \"\"\"\n        return np.array(\n            [normalized_params[i] * self.param_scales[name] for i, name in enumerate(param_names)]\n        )\n\n    def _calculate_metrics(self) -&gt; Dict[str, float]:\n        \"\"\"\n        Calculate standard metrics from the current simulation.\n\n        This method runs either a single event simulation, a train input simulation,\n        or both based on configuration flags, and calculates relevant synaptic metrics.\n\n        Returns:\n        --------\n        Dict[str, float]\n            Dictionary of calculated metrics including:\n            - induction: measure of synaptic facilitation/depression\n            - ppr: paired-pulse ratio\n            - recovery: recovery from facilitation/depression\n            - max_amplitude: maximum synaptic response amplitude\n            - rise_time: time for synaptic response to rise from 20% to 80% of peak\n            - decay_time: time constant of synaptic response decay\n            - latency: synaptic response latency\n            - half_width: synaptic response half-width\n            - baseline: baseline current\n            - amp: peak amplitude from syn_props\n        \"\"\"\n        # Set these to 0 for when we return the dict\n        induction = 0\n        ppr = 0\n        recovery = 0\n        simple_ppr = 0\n        amp = 0\n        rise_time = 0\n        decay_time = 0\n        latency = 0\n        half_width = 0\n        baseline = 0\n        syn_amp = 0\n\n        if self.run_single_event:\n            self.tuner.SingleEvent(plot_and_print=False)\n            # Use the attributes set by SingleEvent method\n            rise_time = getattr(self.tuner, \"rise_time\", 0)\n            decay_time = getattr(self.tuner, \"decay_time\", 0)\n            # Get additional syn_props directly\n            syn_props = self.tuner._get_syn_prop()\n            latency = syn_props.get(\"latency\", 0)\n            half_width = syn_props.get(\"half_width\", 0)\n            baseline = syn_props.get(\"baseline\", 0)\n            syn_amp = syn_props.get(\"amp\", 0)\n\n        if self.run_train_input:\n            self.tuner._simulate_model(self.train_frequency, self.train_delay)\n            amp = self.tuner._response_amplitude()\n            ppr, induction, recovery, simple_ppr = self.tuner._calc_ppr_induction_recovery(\n                amp, print_math=False\n            )\n            amp = self.tuner._find_max_amp(amp)\n\n        return {\n            \"induction\": float(induction),\n            \"ppr\": float(ppr),\n            \"recovery\": float(recovery),\n            \"simple_ppr\": float(simple_ppr),\n            \"max_amplitude\": float(amp),\n            \"rise_time\": float(rise_time),\n            \"decay_time\": float(decay_time),\n            \"latency\": float(latency),\n            \"half_width\": float(half_width),\n            \"baseline\": float(baseline),\n            \"amp\": float(syn_amp),\n        }\n\n    def _default_cost_function(\n        self, metrics: Dict[str, float], target_metrics: Dict[str, float]\n    ) -&gt; float:\n        \"\"\"\n        Default cost function that minimizes the squared difference between achieved and target induction.\n\n        Parameters:\n        -----------\n        metrics : Dict[str, float]\n            Dictionary of calculated metrics from the current simulation.\n        target_metrics : Dict[str, float]\n            Dictionary of target metrics to optimize towards.\n\n        Returns:\n        --------\n        float\n            The squared error between achieved and target induction.\n        \"\"\"\n        return float((metrics[\"induction\"] - target_metrics[\"induction\"]) ** 2)\n\n    def _objective_function(\n        self,\n        normalized_params: np.ndarray,\n        param_names: List[str],\n        cost_function: Callable,\n        target_metrics: Dict[str, float],\n    ) -&gt; float:\n        \"\"\"\n        Calculate error using provided cost function\n        \"\"\"\n        # Denormalize parameters\n        params = self._denormalize_params(normalized_params, param_names)\n\n        # Set parameters\n        for name, value in zip(param_names, params):\n            setattr(self.tuner.syn, name, value)\n\n        # just do this and have the SingleEvent handle it\n        if self.run_single_event:\n            self.tuner.using_optimizer = True\n            self.tuner.param_names = param_names\n            self.tuner.params = params\n\n        # Calculate metrics and error\n        metrics = self._calculate_metrics()\n        error = float(cost_function(metrics, target_metrics))  # Ensure error is scalar\n\n        # Store history with denormalized values\n        history_entry = {\n            \"params\": dict(zip(param_names, params)),\n            \"metrics\": metrics,\n            \"error\": error,\n        }\n        self.optimization_history.append(history_entry)\n\n        return error\n\n    def optimize_parameters(\n        self,\n        target_metrics: Dict[str, float],\n        param_bounds: Dict[str, Tuple[float, float]],\n        run_single_event: bool = False,\n        run_train_input: bool = True,\n        train_frequency: float = 50,\n        train_delay: float = 250,\n        cost_function: Optional[Callable] = None,\n        method: str = \"SLSQP\",\n        init_guess=\"random\",\n    ) -&gt; SynapseOptimizationResult:\n        \"\"\"\n        Optimize synaptic parameters to achieve target metrics.\n\n        Parameters:\n        -----------\n        target_metrics : Dict[str, float]\n            Target values for synaptic metrics (e.g., {'induction': 0.2, 'rise_time': 0.5})\n        param_bounds : Dict[str, Tuple[float, float]]\n            Bounds for each parameter to optimize (e.g., {'tau_d': (5, 50), 'Use': (0.1, 0.9)})\n        run_single_event : bool, optional\n            Whether to run single event simulations during optimization (default: False)\n        run_train_input : bool, optional\n            Whether to run train input simulations during optimization (default: True)\n        train_frequency : float, optional\n            Frequency of the stimulus train in Hz (default: 50)\n        train_delay : float, optional\n            Delay between pulse trains in ms (default: 250)\n        cost_function : Optional[Callable]\n            Custom cost function for optimization. If None, uses default cost function\n            that optimizes induction.\n        method : str, optional\n            Optimization method to use (default: 'SLSQP')\n        init_guess : str, optional\n            Method for initial parameter guess ('random' or 'middle_guess')\n\n        Returns:\n        --------\n        SynapseOptimizationResult\n            Results of the optimization including optimal parameters, achieved metrics,\n            target metrics, final error, and optimization path.\n\n        Notes:\n        ------\n        This function uses scipy.optimize.minimize to find the optimal parameter values\n        that minimize the difference between achieved and target metrics.\n        \"\"\"\n        self.optimization_history = []\n        self.train_frequency = train_frequency\n        self.train_delay = train_delay\n        self.run_single_event = run_single_event\n        self.run_train_input = run_train_input\n\n        param_names = list(param_bounds.keys())\n        bounds = [param_bounds[name] for name in param_names]\n\n        if cost_function is None:\n            cost_function = self._default_cost_function\n\n        # Calculate scaling factors\n        self.param_scales = {\n            name: max(abs(bounds[i][0]), abs(bounds[i][1])) for i, name in enumerate(param_names)\n        }\n\n        # Normalize bounds\n        normalized_bounds = [\n            (b[0] / self.param_scales[name], b[1] / self.param_scales[name])\n            for name, b in zip(param_names, bounds)\n        ]\n\n        # picks with method of init value we want to use\n        if init_guess == \"random\":\n            x0 = np.array([np.random.uniform(b[0], b[1]) for b in bounds])\n        elif init_guess == \"middle_guess\":\n            x0 = [(b[0] + b[1]) / 2 for b in bounds]\n        else:\n            raise Exception(\"Pick a valid init guess method: either 'random' or 'middle_guess'\")\n        normalized_x0 = self._normalize_params(np.array(x0), param_names)\n\n        # Run optimization\n        result = minimize(\n            self._objective_function,\n            normalized_x0,\n            args=(param_names, cost_function, target_metrics),\n            method=method,\n            bounds=normalized_bounds,\n        )\n\n        # Get final parameters and metrics\n        final_params = dict(zip(param_names, self._denormalize_params(result.x, param_names)))\n        for name, value in final_params.items():\n            setattr(self.tuner.syn, name, value)\n        final_metrics = self._calculate_metrics()\n\n        return SynapseOptimizationResult(\n            optimal_params=final_params,\n            achieved_metrics=final_metrics,\n            target_metrics=target_metrics,\n            error=result.fun,\n            optimization_path=self.optimization_history,\n        )\n\n    def plot_optimization_results(self, result: SynapseOptimizationResult):\n        \"\"\"\n        Plot optimization results including convergence and final traces.\n\n        Parameters:\n        -----------\n        result : SynapseOptimizationResult\n            Results from optimization as returned by optimize_parameters()\n\n        Notes:\n        ------\n        This method generates three plots:\n        1. Error convergence plot showing how the error decreased over iterations\n        2. Parameter convergence plots showing how each parameter changed\n        3. Final model response with the optimal parameters\n\n        It also prints a summary of the optimization results including target vs. achieved\n        metrics and the optimal parameter values.\n        \"\"\"\n        # Ensure errors are properly shaped for plotting\n        iterations = range(len(result.optimization_path))\n        errors = np.array([float(h[\"error\"]) for h in result.optimization_path]).flatten()\n\n        # Plot error convergence\n        fig1, ax1 = plt.subplots(figsize=(8, 5))\n        ax1.plot(iterations, errors, label=\"Error\")\n        ax1.set_xlabel(\"Iteration\")\n        ax1.set_ylabel(\"Error\")\n        ax1.set_title(\"Error Convergence\")\n        ax1.set_yscale(\"log\")\n        ax1.legend()\n        plt.tight_layout()\n        plt.show()\n\n        # Plot parameter convergence\n        param_names = list(result.optimal_params.keys())\n        num_params = len(param_names)\n        fig2, axs = plt.subplots(nrows=num_params, ncols=1, figsize=(8, 5 * num_params))\n\n        if num_params == 1:\n            axs = [axs]\n\n        for ax, param in zip(axs, param_names):\n            values = [float(h[\"params\"][param]) for h in result.optimization_path]\n            ax.plot(iterations, values, label=f\"{param}\")\n            ax.set_xlabel(\"Iteration\")\n            ax.set_ylabel(\"Parameter Value\")\n            ax.set_title(f\"Convergence of {param}\")\n            ax.legend()\n\n        plt.tight_layout()\n        plt.show()\n\n        # Print final results\n        print(\"Optimization Results:\")\n        print(f\"Final Error: {float(result.error):.2e}\\n\")\n        print(\"Target Metrics:\")\n        for metric, value in result.target_metrics.items():\n            achieved = result.achieved_metrics.get(metric)\n            if achieved is not None and metric != \"amplitudes\":  # Skip amplitude array\n                print(f\"{metric}: {float(achieved):.3f} (target: {float(value):.3f})\")\n\n        print(\"\\nOptimal Parameters:\")\n        for param, value in result.optimal_params.items():\n            print(f\"{param}: {float(value):.3f}\")\n\n        # Plot final model response\n        if self.run_train_input:\n            self.tuner._plot_model(\n                [\n                    self.tuner.general_settings[\"tstart\"] - self.tuner.nstim.interval / 3,\n                    self.tuner.tstop,\n                ]\n            )\n            amp = self.tuner._response_amplitude()\n            self.tuner._calc_ppr_induction_recovery(amp)\n        if self.run_single_event:\n            self.tuner.ispk = None\n            self.tuner.SingleEvent(plot_and_print=True)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer.__init__","title":"<code>__init__(tuner)</code>","text":"<p>Initialize the synapse optimizer with parameter scaling</p> Parameters: <p>tuner : SynapseTuner     Instance of the SynapseTuner class</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def __init__(self, tuner):\n    \"\"\"\n    Initialize the synapse optimizer with parameter scaling\n\n    Parameters:\n    -----------\n    tuner : SynapseTuner\n        Instance of the SynapseTuner class\n    \"\"\"\n    self.tuner = tuner\n    self.optimization_history = []\n    self.param_scales = {}\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer._normalize_params","title":"<code>_normalize_params(params, param_names)</code>","text":"<p>Normalize parameters to similar scales for better optimization performance.</p> Parameters: <p>params : np.ndarray     Original parameter values. param_names : List[str]     Names of the parameters corresponding to the values.</p> Returns: <p>np.ndarray     Normalized parameter values.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _normalize_params(self, params: np.ndarray, param_names: List[str]) -&gt; np.ndarray:\n    \"\"\"\n    Normalize parameters to similar scales for better optimization performance.\n\n    Parameters:\n    -----------\n    params : np.ndarray\n        Original parameter values.\n    param_names : List[str]\n        Names of the parameters corresponding to the values.\n\n    Returns:\n    --------\n    np.ndarray\n        Normalized parameter values.\n    \"\"\"\n    return np.array([params[i] / self.param_scales[name] for i, name in enumerate(param_names)])\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer._denormalize_params","title":"<code>_denormalize_params(normalized_params, param_names)</code>","text":"<p>Convert normalized parameters back to original scale.</p> Parameters: <p>normalized_params : np.ndarray     Normalized parameter values. param_names : List[str]     Names of the parameters corresponding to the normalized values.</p> Returns: <p>np.ndarray     Denormalized parameter values in their original scale.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _denormalize_params(\n    self, normalized_params: np.ndarray, param_names: List[str]\n) -&gt; np.ndarray:\n    \"\"\"\n    Convert normalized parameters back to original scale.\n\n    Parameters:\n    -----------\n    normalized_params : np.ndarray\n        Normalized parameter values.\n    param_names : List[str]\n        Names of the parameters corresponding to the normalized values.\n\n    Returns:\n    --------\n    np.ndarray\n        Denormalized parameter values in their original scale.\n    \"\"\"\n    return np.array(\n        [normalized_params[i] * self.param_scales[name] for i, name in enumerate(param_names)]\n    )\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer._calculate_metrics","title":"<code>_calculate_metrics()</code>","text":"<p>Calculate standard metrics from the current simulation.</p> <p>This method runs either a single event simulation, a train input simulation, or both based on configuration flags, and calculates relevant synaptic metrics.</p> Returns: <p>Dict[str, float]     Dictionary of calculated metrics including:     - induction: measure of synaptic facilitation/depression     - ppr: paired-pulse ratio     - recovery: recovery from facilitation/depression     - max_amplitude: maximum synaptic response amplitude     - rise_time: time for synaptic response to rise from 20% to 80% of peak     - decay_time: time constant of synaptic response decay     - latency: synaptic response latency     - half_width: synaptic response half-width     - baseline: baseline current     - amp: peak amplitude from syn_props</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _calculate_metrics(self) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate standard metrics from the current simulation.\n\n    This method runs either a single event simulation, a train input simulation,\n    or both based on configuration flags, and calculates relevant synaptic metrics.\n\n    Returns:\n    --------\n    Dict[str, float]\n        Dictionary of calculated metrics including:\n        - induction: measure of synaptic facilitation/depression\n        - ppr: paired-pulse ratio\n        - recovery: recovery from facilitation/depression\n        - max_amplitude: maximum synaptic response amplitude\n        - rise_time: time for synaptic response to rise from 20% to 80% of peak\n        - decay_time: time constant of synaptic response decay\n        - latency: synaptic response latency\n        - half_width: synaptic response half-width\n        - baseline: baseline current\n        - amp: peak amplitude from syn_props\n    \"\"\"\n    # Set these to 0 for when we return the dict\n    induction = 0\n    ppr = 0\n    recovery = 0\n    simple_ppr = 0\n    amp = 0\n    rise_time = 0\n    decay_time = 0\n    latency = 0\n    half_width = 0\n    baseline = 0\n    syn_amp = 0\n\n    if self.run_single_event:\n        self.tuner.SingleEvent(plot_and_print=False)\n        # Use the attributes set by SingleEvent method\n        rise_time = getattr(self.tuner, \"rise_time\", 0)\n        decay_time = getattr(self.tuner, \"decay_time\", 0)\n        # Get additional syn_props directly\n        syn_props = self.tuner._get_syn_prop()\n        latency = syn_props.get(\"latency\", 0)\n        half_width = syn_props.get(\"half_width\", 0)\n        baseline = syn_props.get(\"baseline\", 0)\n        syn_amp = syn_props.get(\"amp\", 0)\n\n    if self.run_train_input:\n        self.tuner._simulate_model(self.train_frequency, self.train_delay)\n        amp = self.tuner._response_amplitude()\n        ppr, induction, recovery, simple_ppr = self.tuner._calc_ppr_induction_recovery(\n            amp, print_math=False\n        )\n        amp = self.tuner._find_max_amp(amp)\n\n    return {\n        \"induction\": float(induction),\n        \"ppr\": float(ppr),\n        \"recovery\": float(recovery),\n        \"simple_ppr\": float(simple_ppr),\n        \"max_amplitude\": float(amp),\n        \"rise_time\": float(rise_time),\n        \"decay_time\": float(decay_time),\n        \"latency\": float(latency),\n        \"half_width\": float(half_width),\n        \"baseline\": float(baseline),\n        \"amp\": float(syn_amp),\n    }\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer._default_cost_function","title":"<code>_default_cost_function(metrics, target_metrics)</code>","text":"<p>Default cost function that minimizes the squared difference between achieved and target induction.</p> Parameters: <p>metrics : Dict[str, float]     Dictionary of calculated metrics from the current simulation. target_metrics : Dict[str, float]     Dictionary of target metrics to optimize towards.</p> Returns: <p>float     The squared error between achieved and target induction.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _default_cost_function(\n    self, metrics: Dict[str, float], target_metrics: Dict[str, float]\n) -&gt; float:\n    \"\"\"\n    Default cost function that minimizes the squared difference between achieved and target induction.\n\n    Parameters:\n    -----------\n    metrics : Dict[str, float]\n        Dictionary of calculated metrics from the current simulation.\n    target_metrics : Dict[str, float]\n        Dictionary of target metrics to optimize towards.\n\n    Returns:\n    --------\n    float\n        The squared error between achieved and target induction.\n    \"\"\"\n    return float((metrics[\"induction\"] - target_metrics[\"induction\"]) ** 2)\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer._objective_function","title":"<code>_objective_function(normalized_params, param_names, cost_function, target_metrics)</code>","text":"<p>Calculate error using provided cost function</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _objective_function(\n    self,\n    normalized_params: np.ndarray,\n    param_names: List[str],\n    cost_function: Callable,\n    target_metrics: Dict[str, float],\n) -&gt; float:\n    \"\"\"\n    Calculate error using provided cost function\n    \"\"\"\n    # Denormalize parameters\n    params = self._denormalize_params(normalized_params, param_names)\n\n    # Set parameters\n    for name, value in zip(param_names, params):\n        setattr(self.tuner.syn, name, value)\n\n    # just do this and have the SingleEvent handle it\n    if self.run_single_event:\n        self.tuner.using_optimizer = True\n        self.tuner.param_names = param_names\n        self.tuner.params = params\n\n    # Calculate metrics and error\n    metrics = self._calculate_metrics()\n    error = float(cost_function(metrics, target_metrics))  # Ensure error is scalar\n\n    # Store history with denormalized values\n    history_entry = {\n        \"params\": dict(zip(param_names, params)),\n        \"metrics\": metrics,\n        \"error\": error,\n    }\n    self.optimization_history.append(history_entry)\n\n    return error\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer.optimize_parameters","title":"<code>optimize_parameters(target_metrics, param_bounds, run_single_event=False, run_train_input=True, train_frequency=50, train_delay=250, cost_function=None, method='SLSQP', init_guess='random')</code>","text":"<p>Optimize synaptic parameters to achieve target metrics.</p> Parameters: <p>target_metrics : Dict[str, float]     Target values for synaptic metrics (e.g., {'induction': 0.2, 'rise_time': 0.5}) param_bounds : Dict[str, Tuple[float, float]]     Bounds for each parameter to optimize (e.g., {'tau_d': (5, 50), 'Use': (0.1, 0.9)}) run_single_event : bool, optional     Whether to run single event simulations during optimization (default: False) run_train_input : bool, optional     Whether to run train input simulations during optimization (default: True) train_frequency : float, optional     Frequency of the stimulus train in Hz (default: 50) train_delay : float, optional     Delay between pulse trains in ms (default: 250) cost_function : Optional[Callable]     Custom cost function for optimization. If None, uses default cost function     that optimizes induction. method : str, optional     Optimization method to use (default: 'SLSQP') init_guess : str, optional     Method for initial parameter guess ('random' or 'middle_guess')</p> Returns: <p>SynapseOptimizationResult     Results of the optimization including optimal parameters, achieved metrics,     target metrics, final error, and optimization path.</p> Notes: <p>This function uses scipy.optimize.minimize to find the optimal parameter values that minimize the difference between achieved and target metrics.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def optimize_parameters(\n    self,\n    target_metrics: Dict[str, float],\n    param_bounds: Dict[str, Tuple[float, float]],\n    run_single_event: bool = False,\n    run_train_input: bool = True,\n    train_frequency: float = 50,\n    train_delay: float = 250,\n    cost_function: Optional[Callable] = None,\n    method: str = \"SLSQP\",\n    init_guess=\"random\",\n) -&gt; SynapseOptimizationResult:\n    \"\"\"\n    Optimize synaptic parameters to achieve target metrics.\n\n    Parameters:\n    -----------\n    target_metrics : Dict[str, float]\n        Target values for synaptic metrics (e.g., {'induction': 0.2, 'rise_time': 0.5})\n    param_bounds : Dict[str, Tuple[float, float]]\n        Bounds for each parameter to optimize (e.g., {'tau_d': (5, 50), 'Use': (0.1, 0.9)})\n    run_single_event : bool, optional\n        Whether to run single event simulations during optimization (default: False)\n    run_train_input : bool, optional\n        Whether to run train input simulations during optimization (default: True)\n    train_frequency : float, optional\n        Frequency of the stimulus train in Hz (default: 50)\n    train_delay : float, optional\n        Delay between pulse trains in ms (default: 250)\n    cost_function : Optional[Callable]\n        Custom cost function for optimization. If None, uses default cost function\n        that optimizes induction.\n    method : str, optional\n        Optimization method to use (default: 'SLSQP')\n    init_guess : str, optional\n        Method for initial parameter guess ('random' or 'middle_guess')\n\n    Returns:\n    --------\n    SynapseOptimizationResult\n        Results of the optimization including optimal parameters, achieved metrics,\n        target metrics, final error, and optimization path.\n\n    Notes:\n    ------\n    This function uses scipy.optimize.minimize to find the optimal parameter values\n    that minimize the difference between achieved and target metrics.\n    \"\"\"\n    self.optimization_history = []\n    self.train_frequency = train_frequency\n    self.train_delay = train_delay\n    self.run_single_event = run_single_event\n    self.run_train_input = run_train_input\n\n    param_names = list(param_bounds.keys())\n    bounds = [param_bounds[name] for name in param_names]\n\n    if cost_function is None:\n        cost_function = self._default_cost_function\n\n    # Calculate scaling factors\n    self.param_scales = {\n        name: max(abs(bounds[i][0]), abs(bounds[i][1])) for i, name in enumerate(param_names)\n    }\n\n    # Normalize bounds\n    normalized_bounds = [\n        (b[0] / self.param_scales[name], b[1] / self.param_scales[name])\n        for name, b in zip(param_names, bounds)\n    ]\n\n    # picks with method of init value we want to use\n    if init_guess == \"random\":\n        x0 = np.array([np.random.uniform(b[0], b[1]) for b in bounds])\n    elif init_guess == \"middle_guess\":\n        x0 = [(b[0] + b[1]) / 2 for b in bounds]\n    else:\n        raise Exception(\"Pick a valid init guess method: either 'random' or 'middle_guess'\")\n    normalized_x0 = self._normalize_params(np.array(x0), param_names)\n\n    # Run optimization\n    result = minimize(\n        self._objective_function,\n        normalized_x0,\n        args=(param_names, cost_function, target_metrics),\n        method=method,\n        bounds=normalized_bounds,\n    )\n\n    # Get final parameters and metrics\n    final_params = dict(zip(param_names, self._denormalize_params(result.x, param_names)))\n    for name, value in final_params.items():\n        setattr(self.tuner.syn, name, value)\n    final_metrics = self._calculate_metrics()\n\n    return SynapseOptimizationResult(\n        optimal_params=final_params,\n        achieved_metrics=final_metrics,\n        target_metrics=target_metrics,\n        error=result.fun,\n        optimization_path=self.optimization_history,\n    )\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.SynapseOptimizer.plot_optimization_results","title":"<code>plot_optimization_results(result)</code>","text":"<p>Plot optimization results including convergence and final traces.</p> Parameters: <p>result : SynapseOptimizationResult     Results from optimization as returned by optimize_parameters()</p> Notes: <p>This method generates three plots: 1. Error convergence plot showing how the error decreased over iterations 2. Parameter convergence plots showing how each parameter changed 3. Final model response with the optimal parameters</p> <p>It also prints a summary of the optimization results including target vs. achieved metrics and the optimal parameter values.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def plot_optimization_results(self, result: SynapseOptimizationResult):\n    \"\"\"\n    Plot optimization results including convergence and final traces.\n\n    Parameters:\n    -----------\n    result : SynapseOptimizationResult\n        Results from optimization as returned by optimize_parameters()\n\n    Notes:\n    ------\n    This method generates three plots:\n    1. Error convergence plot showing how the error decreased over iterations\n    2. Parameter convergence plots showing how each parameter changed\n    3. Final model response with the optimal parameters\n\n    It also prints a summary of the optimization results including target vs. achieved\n    metrics and the optimal parameter values.\n    \"\"\"\n    # Ensure errors are properly shaped for plotting\n    iterations = range(len(result.optimization_path))\n    errors = np.array([float(h[\"error\"]) for h in result.optimization_path]).flatten()\n\n    # Plot error convergence\n    fig1, ax1 = plt.subplots(figsize=(8, 5))\n    ax1.plot(iterations, errors, label=\"Error\")\n    ax1.set_xlabel(\"Iteration\")\n    ax1.set_ylabel(\"Error\")\n    ax1.set_title(\"Error Convergence\")\n    ax1.set_yscale(\"log\")\n    ax1.legend()\n    plt.tight_layout()\n    plt.show()\n\n    # Plot parameter convergence\n    param_names = list(result.optimal_params.keys())\n    num_params = len(param_names)\n    fig2, axs = plt.subplots(nrows=num_params, ncols=1, figsize=(8, 5 * num_params))\n\n    if num_params == 1:\n        axs = [axs]\n\n    for ax, param in zip(axs, param_names):\n        values = [float(h[\"params\"][param]) for h in result.optimization_path]\n        ax.plot(iterations, values, label=f\"{param}\")\n        ax.set_xlabel(\"Iteration\")\n        ax.set_ylabel(\"Parameter Value\")\n        ax.set_title(f\"Convergence of {param}\")\n        ax.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # Print final results\n    print(\"Optimization Results:\")\n    print(f\"Final Error: {float(result.error):.2e}\\n\")\n    print(\"Target Metrics:\")\n    for metric, value in result.target_metrics.items():\n        achieved = result.achieved_metrics.get(metric)\n        if achieved is not None and metric != \"amplitudes\":  # Skip amplitude array\n            print(f\"{metric}: {float(achieved):.3f} (target: {float(value):.3f})\")\n\n    print(\"\\nOptimal Parameters:\")\n    for param, value in result.optimal_params.items():\n        print(f\"{param}: {float(value):.3f}\")\n\n    # Plot final model response\n    if self.run_train_input:\n        self.tuner._plot_model(\n            [\n                self.tuner.general_settings[\"tstart\"] - self.tuner.nstim.interval / 3,\n                self.tuner.tstop,\n            ]\n        )\n        amp = self.tuner._response_amplitude()\n        self.tuner._calc_ppr_induction_recovery(amp)\n    if self.run_single_event:\n        self.tuner.ispk = None\n        self.tuner.SingleEvent(plot_and_print=True)\n</code></pre>"},{"location":"api/synapses/#gap-junction-optimization-results","title":"Gap Junction Optimization Results","text":""},{"location":"api/synapses/#bmtool.synapses.GapOptimizationResult","title":"<code>bmtool.synapses.GapOptimizationResult</code>  <code>dataclass</code>","text":"<p>Container for gap junction optimization results</p> Source code in <code>bmtool/synapses.py</code> <pre><code>@dataclass\nclass GapOptimizationResult:\n    \"\"\"Container for gap junction optimization results\"\"\"\n\n    optimal_resistance: float\n    achieved_cc: float\n    target_cc: float\n    error: float\n    optimization_path: List[Dict[str, float]]\n</code></pre>"},{"location":"api/synapses/#gap-junction-optimization","title":"Gap Junction Optimization","text":""},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer","title":"<code>bmtool.synapses.GapJunctionOptimizer</code>","text":"Source code in <code>bmtool/synapses.py</code> <pre><code>class GapJunctionOptimizer:\n    def __init__(self, tuner):\n        \"\"\"\n        Initialize the gap junction optimizer\n\n        Parameters:\n        -----------\n        tuner : GapJunctionTuner\n            Instance of the GapJunctionTuner class\n        \"\"\"\n        self.tuner = tuner\n        self.optimization_history = []\n\n    def _objective_function(self, resistance: float, target_cc: float) -&gt; float:\n        \"\"\"\n        Calculate error between achieved and target coupling coefficient\n\n        Parameters:\n        -----------\n        resistance : float\n            Gap junction resistance to try\n        target_cc : float\n            Target coupling coefficient to match\n\n        Returns:\n        --------\n        float : Error between achieved and target coupling coefficient\n        \"\"\"\n        # Run model with current resistance\n        self.tuner.model(resistance)\n\n        # Calculate coupling coefficient\n        achieved_cc = self.tuner.coupling_coefficient(\n            self.tuner.t_vec,\n            self.tuner.soma_v_1,\n            self.tuner.soma_v_2,\n            self.tuner.general_settings[\"tstart\"],\n            self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n        )\n\n        # Calculate error\n        error = (achieved_cc - target_cc) ** 2  # MSE\n\n        # Store history\n        self.optimization_history.append(\n            {\"resistance\": resistance, \"achieved_cc\": achieved_cc, \"error\": error}\n        )\n\n        return error\n\n    def optimize_resistance(\n        self, target_cc: float, resistance_bounds: tuple = (1e-4, 1e-2), method: str = \"bounded\"\n    ) -&gt; GapOptimizationResult:\n        \"\"\"\n        Optimize gap junction resistance to achieve a target coupling coefficient.\n\n        Parameters:\n        -----------\n        target_cc : float\n            Target coupling coefficient to achieve (between 0 and 1)\n        resistance_bounds : tuple, optional\n            (min, max) bounds for resistance search in MOhm. Default is (1e-4, 1e-2).\n        method : str, optional\n            Optimization method to use. Default is 'bounded' which works well\n            for single-parameter optimization.\n\n        Returns:\n        --------\n        GapOptimizationResult\n            Container with optimization results including:\n            - optimal_resistance: The optimized resistance value\n            - achieved_cc: The coupling coefficient achieved with the optimal resistance\n            - target_cc: The target coupling coefficient\n            - error: The final error (squared difference between target and achieved)\n            - optimization_path: List of all values tried during optimization\n\n        Notes:\n        ------\n        Uses scipy.optimize.minimize_scalar with bounded method, which is\n        appropriate for this single-parameter optimization problem.\n        \"\"\"\n        self.optimization_history = []\n\n        # Run optimization\n        result = minimize_scalar(\n            self._objective_function, args=(target_cc,), bounds=resistance_bounds, method=method\n        )\n\n        # Run final model with optimal resistance\n        self.tuner.model(result.x)\n        final_cc = self.tuner.coupling_coefficient(\n            self.tuner.t_vec,\n            self.tuner.soma_v_1,\n            self.tuner.soma_v_2,\n            self.tuner.general_settings[\"tstart\"],\n            self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n        )\n\n        # Package up our results\n        optimization_result = GapOptimizationResult(\n            optimal_resistance=result.x,\n            achieved_cc=final_cc,\n            target_cc=target_cc,\n            error=result.fun,\n            optimization_path=self.optimization_history,\n        )\n\n        return optimization_result\n\n    def plot_optimization_results(self, result: GapOptimizationResult):\n        \"\"\"\n        Plot optimization results including convergence and final voltage traces\n\n        Parameters:\n        -----------\n        result : GapOptimizationResult\n            Results from optimization\n        \"\"\"\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n        # Plot voltage traces\n        t_range = [\n            self.tuner.general_settings[\"tstart\"] - 100.0,\n            self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"] + 100.0,\n        ]\n        t = np.array(self.tuner.t_vec)\n        v1 = np.array(self.tuner.soma_v_1)\n        v2 = np.array(self.tuner.soma_v_2)\n        tidx = (t &gt;= t_range[0]) &amp; (t &lt;= t_range[1])\n\n        ax1.plot(t[tidx], v1[tidx], \"b\", label=f\"{self.tuner.cell_name} 1\")\n        ax1.plot(t[tidx], v2[tidx], \"r\", label=f\"{self.tuner.cell_name} 2\")\n        ax1.set_xlabel(\"Time (ms)\")\n        ax1.set_ylabel(\"Membrane Voltage (mV)\")\n        ax1.legend()\n        ax1.set_title(\"Optimized Voltage Traces\")\n\n        # Plot error convergence\n        errors = [h[\"error\"] for h in result.optimization_path]\n        ax2.plot(errors)\n        ax2.set_xlabel(\"Iteration\")\n        ax2.set_ylabel(\"Error\")\n        ax2.set_title(\"Error Convergence\")\n        ax2.set_yscale(\"log\")\n\n        # Plot resistance convergence\n        resistances = [h[\"resistance\"] for h in result.optimization_path]\n        ax3.plot(resistances)\n        ax3.set_xlabel(\"Iteration\")\n        ax3.set_ylabel(\"Resistance\")\n        ax3.set_title(\"Resistance Convergence\")\n        ax3.set_yscale(\"log\")\n\n        # Print final results\n        result_text = (\n            f\"Optimal Resistance: {result.optimal_resistance:.2e}\\n\"\n            f\"Target CC: {result.target_cc:.3f}\\n\"\n            f\"Achieved CC: {result.achieved_cc:.3f}\\n\"\n            f\"Final Error: {result.error:.2e}\"\n        )\n        ax4.text(0.1, 0.7, result_text, transform=ax4.transAxes, fontsize=10)\n        ax4.axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n\n    def parameter_sweep(self, resistance_range: np.ndarray) -&gt; dict:\n        \"\"\"\n        Perform a parameter sweep across different resistance values.\n\n        Parameters:\n        -----------\n        resistance_range : np.ndarray\n            Array of resistance values to test.\n\n        Returns:\n        --------\n        dict\n            Dictionary containing the results of the parameter sweep, with keys:\n            - 'resistance': List of resistance values tested\n            - 'coupling_coefficient': Corresponding coupling coefficients\n\n        Notes:\n        ------\n        This method is useful for understanding the relationship between gap junction\n        resistance and coupling coefficient before attempting optimization.\n        \"\"\"\n        results = {\"resistance\": [], \"coupling_coefficient\": []}\n\n        for resistance in tqdm(resistance_range, desc=\"Sweeping resistance values\"):\n            self.tuner.model(resistance)\n            cc = self.tuner.coupling_coefficient(\n                self.tuner.t_vec,\n                self.tuner.soma_v_1,\n                self.tuner.soma_v_2,\n                self.tuner.general_settings[\"tstart\"],\n                self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n            )\n\n            results[\"resistance\"].append(resistance)\n            results[\"coupling_coefficient\"].append(cc)\n\n        return results\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer.__init__","title":"<code>__init__(tuner)</code>","text":"<p>Initialize the gap junction optimizer</p> Parameters: <p>tuner : GapJunctionTuner     Instance of the GapJunctionTuner class</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def __init__(self, tuner):\n    \"\"\"\n    Initialize the gap junction optimizer\n\n    Parameters:\n    -----------\n    tuner : GapJunctionTuner\n        Instance of the GapJunctionTuner class\n    \"\"\"\n    self.tuner = tuner\n    self.optimization_history = []\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer._objective_function","title":"<code>_objective_function(resistance, target_cc)</code>","text":"<p>Calculate error between achieved and target coupling coefficient</p> Parameters: <p>resistance : float     Gap junction resistance to try target_cc : float     Target coupling coefficient to match</p> Returns: <p>float : Error between achieved and target coupling coefficient</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def _objective_function(self, resistance: float, target_cc: float) -&gt; float:\n    \"\"\"\n    Calculate error between achieved and target coupling coefficient\n\n    Parameters:\n    -----------\n    resistance : float\n        Gap junction resistance to try\n    target_cc : float\n        Target coupling coefficient to match\n\n    Returns:\n    --------\n    float : Error between achieved and target coupling coefficient\n    \"\"\"\n    # Run model with current resistance\n    self.tuner.model(resistance)\n\n    # Calculate coupling coefficient\n    achieved_cc = self.tuner.coupling_coefficient(\n        self.tuner.t_vec,\n        self.tuner.soma_v_1,\n        self.tuner.soma_v_2,\n        self.tuner.general_settings[\"tstart\"],\n        self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n    )\n\n    # Calculate error\n    error = (achieved_cc - target_cc) ** 2  # MSE\n\n    # Store history\n    self.optimization_history.append(\n        {\"resistance\": resistance, \"achieved_cc\": achieved_cc, \"error\": error}\n    )\n\n    return error\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer.optimize_resistance","title":"<code>optimize_resistance(target_cc, resistance_bounds=(0.0001, 0.01), method='bounded')</code>","text":"<p>Optimize gap junction resistance to achieve a target coupling coefficient.</p> Parameters: <p>target_cc : float     Target coupling coefficient to achieve (between 0 and 1) resistance_bounds : tuple, optional     (min, max) bounds for resistance search in MOhm. Default is (1e-4, 1e-2). method : str, optional     Optimization method to use. Default is 'bounded' which works well     for single-parameter optimization.</p> Returns: <p>GapOptimizationResult     Container with optimization results including:     - optimal_resistance: The optimized resistance value     - achieved_cc: The coupling coefficient achieved with the optimal resistance     - target_cc: The target coupling coefficient     - error: The final error (squared difference between target and achieved)     - optimization_path: List of all values tried during optimization</p> Notes: <p>Uses scipy.optimize.minimize_scalar with bounded method, which is appropriate for this single-parameter optimization problem.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def optimize_resistance(\n    self, target_cc: float, resistance_bounds: tuple = (1e-4, 1e-2), method: str = \"bounded\"\n) -&gt; GapOptimizationResult:\n    \"\"\"\n    Optimize gap junction resistance to achieve a target coupling coefficient.\n\n    Parameters:\n    -----------\n    target_cc : float\n        Target coupling coefficient to achieve (between 0 and 1)\n    resistance_bounds : tuple, optional\n        (min, max) bounds for resistance search in MOhm. Default is (1e-4, 1e-2).\n    method : str, optional\n        Optimization method to use. Default is 'bounded' which works well\n        for single-parameter optimization.\n\n    Returns:\n    --------\n    GapOptimizationResult\n        Container with optimization results including:\n        - optimal_resistance: The optimized resistance value\n        - achieved_cc: The coupling coefficient achieved with the optimal resistance\n        - target_cc: The target coupling coefficient\n        - error: The final error (squared difference between target and achieved)\n        - optimization_path: List of all values tried during optimization\n\n    Notes:\n    ------\n    Uses scipy.optimize.minimize_scalar with bounded method, which is\n    appropriate for this single-parameter optimization problem.\n    \"\"\"\n    self.optimization_history = []\n\n    # Run optimization\n    result = minimize_scalar(\n        self._objective_function, args=(target_cc,), bounds=resistance_bounds, method=method\n    )\n\n    # Run final model with optimal resistance\n    self.tuner.model(result.x)\n    final_cc = self.tuner.coupling_coefficient(\n        self.tuner.t_vec,\n        self.tuner.soma_v_1,\n        self.tuner.soma_v_2,\n        self.tuner.general_settings[\"tstart\"],\n        self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n    )\n\n    # Package up our results\n    optimization_result = GapOptimizationResult(\n        optimal_resistance=result.x,\n        achieved_cc=final_cc,\n        target_cc=target_cc,\n        error=result.fun,\n        optimization_path=self.optimization_history,\n    )\n\n    return optimization_result\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer.plot_optimization_results","title":"<code>plot_optimization_results(result)</code>","text":"<p>Plot optimization results including convergence and final voltage traces</p> Parameters: <p>result : GapOptimizationResult     Results from optimization</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def plot_optimization_results(self, result: GapOptimizationResult):\n    \"\"\"\n    Plot optimization results including convergence and final voltage traces\n\n    Parameters:\n    -----------\n    result : GapOptimizationResult\n        Results from optimization\n    \"\"\"\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n    # Plot voltage traces\n    t_range = [\n        self.tuner.general_settings[\"tstart\"] - 100.0,\n        self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"] + 100.0,\n    ]\n    t = np.array(self.tuner.t_vec)\n    v1 = np.array(self.tuner.soma_v_1)\n    v2 = np.array(self.tuner.soma_v_2)\n    tidx = (t &gt;= t_range[0]) &amp; (t &lt;= t_range[1])\n\n    ax1.plot(t[tidx], v1[tidx], \"b\", label=f\"{self.tuner.cell_name} 1\")\n    ax1.plot(t[tidx], v2[tidx], \"r\", label=f\"{self.tuner.cell_name} 2\")\n    ax1.set_xlabel(\"Time (ms)\")\n    ax1.set_ylabel(\"Membrane Voltage (mV)\")\n    ax1.legend()\n    ax1.set_title(\"Optimized Voltage Traces\")\n\n    # Plot error convergence\n    errors = [h[\"error\"] for h in result.optimization_path]\n    ax2.plot(errors)\n    ax2.set_xlabel(\"Iteration\")\n    ax2.set_ylabel(\"Error\")\n    ax2.set_title(\"Error Convergence\")\n    ax2.set_yscale(\"log\")\n\n    # Plot resistance convergence\n    resistances = [h[\"resistance\"] for h in result.optimization_path]\n    ax3.plot(resistances)\n    ax3.set_xlabel(\"Iteration\")\n    ax3.set_ylabel(\"Resistance\")\n    ax3.set_title(\"Resistance Convergence\")\n    ax3.set_yscale(\"log\")\n\n    # Print final results\n    result_text = (\n        f\"Optimal Resistance: {result.optimal_resistance:.2e}\\n\"\n        f\"Target CC: {result.target_cc:.3f}\\n\"\n        f\"Achieved CC: {result.achieved_cc:.3f}\\n\"\n        f\"Final Error: {result.error:.2e}\"\n    )\n    ax4.text(0.1, 0.7, result_text, transform=ax4.transAxes, fontsize=10)\n    ax4.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/synapses/#bmtool.synapses.GapJunctionOptimizer.parameter_sweep","title":"<code>parameter_sweep(resistance_range)</code>","text":"<p>Perform a parameter sweep across different resistance values.</p> Parameters: <p>resistance_range : np.ndarray     Array of resistance values to test.</p> Returns: <p>dict     Dictionary containing the results of the parameter sweep, with keys:     - 'resistance': List of resistance values tested     - 'coupling_coefficient': Corresponding coupling coefficients</p> Notes: <p>This method is useful for understanding the relationship between gap junction resistance and coupling coefficient before attempting optimization.</p> Source code in <code>bmtool/synapses.py</code> <pre><code>def parameter_sweep(self, resistance_range: np.ndarray) -&gt; dict:\n    \"\"\"\n    Perform a parameter sweep across different resistance values.\n\n    Parameters:\n    -----------\n    resistance_range : np.ndarray\n        Array of resistance values to test.\n\n    Returns:\n    --------\n    dict\n        Dictionary containing the results of the parameter sweep, with keys:\n        - 'resistance': List of resistance values tested\n        - 'coupling_coefficient': Corresponding coupling coefficients\n\n    Notes:\n    ------\n    This method is useful for understanding the relationship between gap junction\n    resistance and coupling coefficient before attempting optimization.\n    \"\"\"\n    results = {\"resistance\": [], \"coupling_coefficient\": []}\n\n    for resistance in tqdm(resistance_range, desc=\"Sweeping resistance values\"):\n        self.tuner.model(resistance)\n        cc = self.tuner.coupling_coefficient(\n            self.tuner.t_vec,\n            self.tuner.soma_v_1,\n            self.tuner.soma_v_2,\n            self.tuner.general_settings[\"tstart\"],\n            self.tuner.general_settings[\"tstart\"] + self.tuner.general_settings[\"tdur\"],\n        )\n\n        results[\"resistance\"].append(resistance)\n        results[\"coupling_coefficient\"].append(cc)\n\n    return results\n</code></pre>"},{"location":"api/analysis/entrainment/","title":"Entrainment Analysis","text":"<p>The <code>entrainment</code> module provides tools for analyzing the entrainment of spikes and LFP signals, including phase-locking value (PLV) and pairwise phase consistency (PPC) calculations.</p>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.align_spike_times_with_lfp","title":"<code>bmtool.analysis.entrainment.align_spike_times_with_lfp(lfp, timestamps)</code>","text":"<p>the lfp xarray should have a time axis. use that to align the spike times since the lfp can start at a non-zero time after sliced. Both need to be on same fs for this to be correct.</p> <p>Parameters:</p> Name Type Description Default <code>lfp</code> <code>DataArray</code> <p>LFP data with time coordinates</p> required <code>timestamps</code> <code>ndarray</code> <p>Array of spike timestamps</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Copy of timestamps with adjusted timestamps to align with lfp.</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def align_spike_times_with_lfp(lfp: xr.DataArray, timestamps: np.ndarray) -&gt; np.ndarray:\n    \"\"\"the lfp xarray should have a time axis. use that to align the spike times since the lfp can start at a\n    non-zero time after sliced. Both need to be on same fs for this to be correct.\n\n    Parameters\n    ----------\n    lfp : xarray.DataArray\n        LFP data with time coordinates\n    timestamps : np.ndarray\n        Array of spike timestamps\n\n    Returns\n    -------\n    np.ndarray\n        Copy of timestamps with adjusted timestamps to align with lfp.\n    \"\"\"\n    # print(\"Pairing LFP and Spike Times\")\n    # print(lfp.time.values)\n    # print(f\"LFP starts at {lfp.time.values[0]}ms\")\n    # need to make sure lfp and spikes have the same time axis\n    # align spikes with lfp\n    timestamps = timestamps[\n        (timestamps &gt;= lfp.time.values[0]) &amp; (timestamps &lt;= lfp.time.values[-1])\n    ].copy()\n    # set the time axis of the spikes to match the lfp\n    # timestamps = timestamps - lfp.time.values[0]\n    return timestamps\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_signal_signal_plv","title":"<code>bmtool.analysis.entrainment.calculate_signal_signal_plv(signal1, signal2, fs, freq_of_interest=None, filter_method='wavelet', lowcut=None, highcut=None, bandwidth=2.0)</code>","text":"<p>Calculate Phase Locking Value (PLV) between two signals using wavelet or Hilbert method.</p> <p>Parameters:</p> Name Type Description Default <code>signal1</code> <code>ndarray</code> <p>First input signal (1D array)</p> required <code>signal2</code> <code>ndarray</code> <p>Second input signal (1D array, same length as signal1)</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz</p> required <code>freq_of_interest</code> <code>float</code> <p>Desired frequency for wavelet PLV calculation, required if filter_method='wavelet'</p> <code>None</code> <code>filter_method</code> <code>str</code> <p>Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')</p> <code>'wavelet'</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Phase Locking Value (1D array)</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def calculate_signal_signal_plv(\n    signal1: np.ndarray,\n    signal2: np.ndarray,\n    fs: float,\n    freq_of_interest: float = None,\n    filter_method: str = \"wavelet\",\n    lowcut: float = None,\n    highcut: float = None,\n    bandwidth: float = 2.0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculate Phase Locking Value (PLV) between two signals using wavelet or Hilbert method.\n\n    Parameters\n    ----------\n    signal1 : np.ndarray\n        First input signal (1D array)\n    signal2 : np.ndarray\n        Second input signal (1D array, same length as signal1)\n    fs : float\n        Sampling frequency in Hz\n    freq_of_interest : float, optional\n        Desired frequency for wavelet PLV calculation, required if filter_method='wavelet'\n    filter_method : str, optional\n        Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n\n    Returns\n    -------\n    np.ndarray\n        Phase Locking Value (1D array)\n    \"\"\"\n    if len(signal1) != len(signal2):\n        raise ValueError(\"Input signals must have the same length.\")\n\n    if filter_method == \"wavelet\":\n        if freq_of_interest is None:\n            raise ValueError(\"freq_of_interest must be provided for the wavelet method.\")\n\n        # Apply CWT to both signals\n        theta1 = wavelet_filter(x=signal1, freq=freq_of_interest, fs=fs, bandwidth=bandwidth)\n        theta2 = wavelet_filter(x=signal2, freq=freq_of_interest, fs=fs, bandwidth=bandwidth)\n\n    elif filter_method == \"butter\":\n        if lowcut is None or highcut is None:\n            print(\n                \"Lowcut and/or highcut were not defined, signal will not be filtered and will just take Hilbert transform for PLV calculation\"\n            )\n\n        if lowcut and highcut:\n            # Bandpass filter and get the analytic signal using the Hilbert transform\n            filtered_signal1 = butter_bandpass_filter(\n                data=signal1, lowcut=lowcut, highcut=highcut, fs=fs\n            )\n            filtered_signal2 = butter_bandpass_filter(\n                data=signal2, lowcut=lowcut, highcut=highcut, fs=fs\n            )\n            # Get phase using the Hilbert transform\n            theta1 = signal.hilbert(filtered_signal1)\n            theta2 = signal.hilbert(filtered_signal2)\n        else:\n            # Get phase using the Hilbert transform without filtering\n            theta1 = signal.hilbert(signal1)\n            theta2 = signal.hilbert(signal2)\n\n    else:\n        raise ValueError(\"Invalid method. Choose 'wavelet' or 'butter'.\")\n\n    # Calculate phase difference\n    phase_diff = np.angle(theta1) - np.angle(theta2)\n\n    # Calculate PLV from standard equation from Measuring phase synchrony in brain signals(1999)\n    plv = np.abs(np.mean(np.exp(1j * phase_diff), axis=-1))\n    #avg phase lag\n    phase_lag = np.angle(np.mean(np.exp(1j * phase_diff)))\n\n    return plv\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_spike_lfp_plv","title":"<code>bmtool.analysis.entrainment.calculate_spike_lfp_plv(spike_times=None, lfp_data=None, spike_fs=None, lfp_fs=None, filter_method='butter', freq_of_interest=None, lowcut=None, highcut=None, bandwidth=2.0, filtered_lfp_phase=None)</code>","text":"<p>Calculate spike-lfp unbiased phase locking value</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>ndarray</code> <p>Array of spike times</p> <code>None</code> <code>lfp_data</code> <code>ndarray</code> <p>Local field potential time series data. Not required if filtered_lfp_phase is provided.</p> <code>None</code> <code>spike_fs</code> <code>float</code> <p>Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates</p> <code>None</code> <code>lfp_fs</code> <code>float</code> <p>Sampling frequency in Hz of the LFP data</p> <code>None</code> <code>filter_method</code> <code>str</code> <p>Method to use for filtering, either 'wavelet' or 'butter' (default: 'butter')</p> <code>'butter'</code> <code>freq_of_interest</code> <code>float</code> <p>Desired frequency for wavelet phase extraction, required if filter_method='wavelet'</p> <code>None</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2.0</code> <code>filtered_lfp_phase</code> <code>ndarray</code> <p>Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Phase Locking Value (unbiased)</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def calculate_spike_lfp_plv(\n    spike_times: np.ndarray = None,\n    lfp_data=None,\n    spike_fs: float = None,\n    lfp_fs: float = None,\n    filter_method: str = \"butter\",\n    freq_of_interest: float = None,\n    lowcut: float = None,\n    highcut: float = None,\n    bandwidth: float = 2.0,\n    filtered_lfp_phase: np.ndarray = None,\n) -&gt; float:\n    \"\"\"\n    Calculate spike-lfp unbiased phase locking value\n\n    Parameters\n    ----------\n    spike_times : np.ndarray\n        Array of spike times\n    lfp_data : np.ndarray\n        Local field potential time series data. Not required if filtered_lfp_phase is provided.\n    spike_fs : float, optional\n        Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates\n    lfp_fs : float\n        Sampling frequency in Hz of the LFP data\n    filter_method : str, optional\n        Method to use for filtering, either 'wavelet' or 'butter' (default: 'butter')\n    freq_of_interest : float, optional\n        Desired frequency for wavelet phase extraction, required if filter_method='wavelet'\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n    filtered_lfp_phase : np.ndarray, optional\n        Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.\n\n    Returns\n    -------\n    float\n        Phase Locking Value (unbiased)\n    \"\"\"\n\n    spike_phases = _get_spike_phases(\n        spike_times=spike_times,\n        lfp_data=lfp_data,\n        spike_fs=spike_fs,\n        lfp_fs=lfp_fs,\n        filter_method=filter_method,\n        freq_of_interest=freq_of_interest,\n        lowcut=lowcut,\n        highcut=highcut,\n        bandwidth=bandwidth,\n        filtered_lfp_phase=filtered_lfp_phase,\n    )\n\n    if len(spike_phases) &lt;= 1:\n        return 0\n\n    # Number of spikes\n    N = len(spike_phases)\n\n    # Convert phases to unit vectors in the complex plane\n    unit_vectors = np.exp(1j * spike_phases)\n\n    # Sum of all unit vectors (resultant vector)\n    resultant_vector = np.sum(unit_vectors)\n\n    # Calculate plv^2 * N\n    plv2n = (resultant_vector * resultant_vector.conjugate()).real / N  # plv^2 * N\n    plv = (plv2n / N) ** 0.5\n    ppc = (plv2n - 1) / (N - 1)  # ppc = (plv^2 * N - 1) / (N - 1)\n    plv_unbiased = np.fmax(ppc, 0.0) ** 0.5  # ensure non-negative\n\n    return plv_unbiased\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc","title":"<code>bmtool.analysis.entrainment.calculate_ppc(spike_times=None, lfp_data=None, spike_fs=None, lfp_fs=None, filter_method='wavelet', freq_of_interest=None, lowcut=None, highcut=None, bandwidth=2.0, ppc_method='numpy', filtered_lfp_phase=None)</code>","text":"<p>Calculate Pairwise Phase Consistency (PPC) between spike times and LFP signal. Based on https://www.sciencedirect.com/science/article/pii/S1053811910000959</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>ndarray</code> <p>Array of spike times</p> <code>None</code> <code>lfp_data</code> <code>ndarray</code> <p>Local field potential time series data. Not required if filtered_lfp_phase is provided.</p> <code>None</code> <code>spike_fs</code> <code>float</code> <p>Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates</p> <code>None</code> <code>lfp_fs</code> <code>float</code> <p>Sampling frequency in Hz of the LFP data</p> <code>None</code> <code>filter_method</code> <code>str</code> <p>Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')</p> <code>'wavelet'</code> <code>freq_of_interest</code> <code>float</code> <p>Desired frequency for wavelet phase extraction, required if filter_method='wavelet'</p> <code>None</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2.0</code> <code>ppc_method</code> <code>str</code> <p>Algorithm to use for PPC calculation: 'numpy', 'numba', or 'gpu' (default: 'numpy')</p> <code>'numpy'</code> <code>filtered_lfp_phase</code> <code>ndarray</code> <p>Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Pairwise Phase Consistency value</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def calculate_ppc(\n    spike_times: np.ndarray = None,\n    lfp_data=None,\n    spike_fs: float = None,\n    lfp_fs: float = None,\n    filter_method: str = \"wavelet\",\n    freq_of_interest: float = None,\n    lowcut: float = None,\n    highcut: float = None,\n    bandwidth: float = 2.0,\n    ppc_method: str = \"numpy\",\n    filtered_lfp_phase: np.ndarray = None,\n) -&gt; float:\n    \"\"\"\n    Calculate Pairwise Phase Consistency (PPC) between spike times and LFP signal.\n    Based on https://www.sciencedirect.com/science/article/pii/S1053811910000959\n\n    Parameters\n    ----------\n    spike_times : np.ndarray\n        Array of spike times\n    lfp_data : np.ndarray\n        Local field potential time series data. Not required if filtered_lfp_phase is provided.\n    spike_fs : float, optional\n        Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates\n    lfp_fs : float\n        Sampling frequency in Hz of the LFP data\n    filter_method : str, optional\n        Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')\n    freq_of_interest : float, optional\n        Desired frequency for wavelet phase extraction, required if filter_method='wavelet'\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n    ppc_method : str, optional\n        Algorithm to use for PPC calculation: 'numpy', 'numba', or 'gpu' (default: 'numpy')\n    filtered_lfp_phase : np.ndarray, optional\n        Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.\n\n    Returns\n    -------\n    float\n        Pairwise Phase Consistency value\n    \"\"\"\n\n    spike_phases = _get_spike_phases(\n        spike_times=spike_times,\n        lfp_data=lfp_data,\n        spike_fs=spike_fs,\n        lfp_fs=lfp_fs,\n        filter_method=filter_method,\n        freq_of_interest=freq_of_interest,\n        lowcut=lowcut,\n        highcut=highcut,\n        bandwidth=bandwidth,\n        filtered_lfp_phase=filtered_lfp_phase,\n    )\n\n    if len(spike_phases) &lt;= 1:\n        return 0\n\n    n_spikes = len(spike_phases)\n\n    # Calculate PPC (Pairwise Phase Consistency)\n    # Explicit calculation of pairwise phase consistency\n    # Vectorized computation for efficiency\n    if ppc_method == \"numpy\":\n        i, j = np.triu_indices(n_spikes, k=1)\n        phase_diff = spike_phases[i] - spike_phases[j]\n        sum_cos_diff = np.sum(np.cos(phase_diff))\n        ppc = (2 / (n_spikes * (n_spikes - 1))) * sum_cos_diff\n    elif ppc_method == \"numba\":\n        ppc = _ppc_parallel_numba(spike_phases)\n    elif ppc_method == \"gpu\":\n        ppc = _ppc_gpu(spike_phases)\n    else:\n        raise ValueError(\"Please use a supported ppc method currently that is numpy, numba or gpu\")\n    return ppc\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2","title":"<code>bmtool.analysis.entrainment.calculate_ppc2(spike_times=None, lfp_data=None, spike_fs=None, lfp_fs=None, filter_method='wavelet', freq_of_interest=None, lowcut=None, highcut=None, bandwidth=2.0, filtered_lfp_phase=None)</code>","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2---","title":"-----------------------------------------------------------------------------","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2--ppc2-calculation-vinck-et-al-2010","title":"PPC2 Calculation (Vinck et al., 2010)","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2---_1","title":"-----------------------------------------------------------------------------","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2--equationoriginal","title":"Equation(Original):","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2--ppc-2-n-n-1-sumcos_i-_j-for-all-i-j","title":"PPC = (2 / (n * (n - 1))) * sum(cos(\u03c6_i - \u03c6_j) for all i &lt; j)","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2--optimized-formula-algebraically-equivalent","title":"Optimized Formula (Algebraically Equivalent):","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2--ppc-sumei_j2-n-n-n-1","title":"PPC = (|sum(e^(i*\u03c6_j))|^2 - n) / (n * (n - 1))","text":""},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_ppc2---_2","title":"-----------------------------------------------------------------------------","text":"<p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>ndarray</code> <p>Array of spike times</p> <code>None</code> <code>lfp_data</code> <code>ndarray</code> <p>Local field potential time series data. Not required if filtered_lfp_phase is provided.</p> <code>None</code> <code>spike_fs</code> <code>float</code> <p>Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates</p> <code>None</code> <code>lfp_fs</code> <code>float</code> <p>Sampling frequency in Hz of the LFP data</p> <code>None</code> <code>filter_method</code> <code>str</code> <p>Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')</p> <code>'wavelet'</code> <code>freq_of_interest</code> <code>float</code> <p>Desired frequency for wavelet phase extraction, required if filter_method='wavelet'</p> <code>None</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2.0</code> <code>filtered_lfp_phase</code> <code>ndarray</code> <p>Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Pairwise Phase Consistency 2 (PPC2) value</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def calculate_ppc2(\n    spike_times: np.ndarray = None,\n    lfp_data=None,\n    spike_fs: float = None,\n    lfp_fs: float = None,\n    filter_method: str = \"wavelet\",\n    freq_of_interest: float = None,\n    lowcut: float = None,\n    highcut: float = None,\n    bandwidth: float = 2.0,\n    filtered_lfp_phase: np.ndarray = None,\n) -&gt; float:\n    \"\"\"\n    # -----------------------------------------------------------------------------\n    # PPC2 Calculation (Vinck et al., 2010)\n    # -----------------------------------------------------------------------------\n    # Equation(Original):\n    #   PPC = (2 / (n * (n - 1))) * sum(cos(\u03c6_i - \u03c6_j) for all i &lt; j)\n    # Optimized Formula (Algebraically Equivalent):\n    #   PPC = (|sum(e^(i*\u03c6_j))|^2 - n) / (n * (n - 1))\n    # -----------------------------------------------------------------------------\n\n    Parameters\n    ----------\n    spike_times : np.ndarray\n        Array of spike times\n    lfp_data : np.ndarray\n        Local field potential time series data. Not required if filtered_lfp_phase is provided.\n    spike_fs : float, optional\n        Sampling frequency in Hz of the spike times, only needed if spike times and LFP have different sampling rates\n    lfp_fs : float\n        Sampling frequency in Hz of the LFP data\n    filter_method : str, optional\n        Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')\n    freq_of_interest : float, optional\n        Desired frequency for wavelet phase extraction, required if filter_method='wavelet'\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n    filtered_lfp_phase : np.ndarray, optional\n        Pre-computed instantaneous phase of the filtered LFP. If provided, the function will skip the filtering step.\n\n    Returns\n    -------\n    float\n        Pairwise Phase Consistency 2 (PPC2) value\n    \"\"\"\n\n    spike_phases = _get_spike_phases(\n        spike_times=spike_times,\n        lfp_data=lfp_data,\n        spike_fs=spike_fs,\n        lfp_fs=lfp_fs,\n        filter_method=filter_method,\n        freq_of_interest=freq_of_interest,\n        lowcut=lowcut,\n        highcut=highcut,\n        bandwidth=bandwidth,\n        filtered_lfp_phase=filtered_lfp_phase,\n    )\n\n    if len(spike_phases) &lt;= 1:\n        return 0\n\n    # Calculate PPC2 according to Vinck et al. (2010), Equation 6\n    n = len(spike_phases)\n\n    # Convert phases to unit vectors in the complex plane\n    unit_vectors = np.exp(1j * spike_phases)\n\n    # Calculate the resultant vector\n    resultant_vector = np.sum(unit_vectors)\n\n    # PPC2 = (|\u2211(e^(i*\u03c6_j))|\u00b2 - n) / (n * (n - 1))\n    ppc2 = (np.abs(resultant_vector) ** 2 - n) / (n * (n - 1))\n\n    return ppc2\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.calculate_entrainment_per_cell","title":"<code>bmtool.analysis.entrainment.calculate_entrainment_per_cell(spike_df=None, lfp_data=None, filter_method='wavelet', pop_names=None, entrainment_method='plv', lowcut=None, highcut=None, spike_fs=None, lfp_fs=None, bandwidth=2, freqs=None, ppc_method='numpy')</code>","text":"<p>Calculate neural entrainment (PPC, PLV) per neuron (cell) for specified frequencies across different populations.</p> <p>This function computes the entrainment metrics for each neuron within the specified populations based on their spike times and the provided LFP signal. It returns a nested dictionary structure containing the entrainment values organized by population, node ID, and frequency.</p> <p>Parameters:</p> Name Type Description Default <code>spike_df</code> <code>DataFrame</code> <p>DataFrame containing spike data with columns 'pop_name', 'node_ids', and 'timestamps'</p> <code>None</code> <code>lfp_data</code> <code>ndarray</code> <p>Local field potential (LFP) time series data</p> <code>None</code> <code>filter_method</code> <code>str</code> <p>Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')</p> <code>'wavelet'</code> <code>entrainment_method</code> <code>str</code> <p>Method to use for entrainment calculation, either 'plv', 'ppc', or 'ppc2' (default: 'plv')</p> <code>'plv'</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>spike_fs</code> <code>float</code> <p>Sampling frequency of the spike times in Hz</p> <code>None</code> <code>lfp_fs</code> <code>float</code> <p>Sampling frequency of the LFP signal in Hz</p> <code>None</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)</p> <code>2</code> <code>ppc_method</code> <code>str</code> <p>Algorithm to use for PPC calculation: 'numpy', 'numba', or 'gpu' (default: 'numpy')</p> <code>'numpy'</code> <code>pop_names</code> <code>List[str]</code> <p>List of population names to analyze</p> <code>None</code> <code>freqs</code> <code>List[float]</code> <p>List of frequencies (in Hz) at which to calculate entrainment</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[int, Dict[float, float]]]</code> <p>Nested dictionary where the structure is: {     population_name: {         node_id: {             frequency: entrainment value         }     } } Entrainment values are floats representing the metric (PPC, PLV) at each frequency</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def calculate_entrainment_per_cell(\n    spike_df: pd.DataFrame = None,\n    lfp_data: np.ndarray = None,\n    filter_method: str = \"wavelet\",\n    pop_names: List[str] = None,\n    entrainment_method: str = \"plv\",\n    lowcut: float = None,\n    highcut: float = None,\n    spike_fs: float = None,\n    lfp_fs: float = None,\n    bandwidth: float = 2,\n    freqs: List[float] = None,\n    ppc_method: str = \"numpy\",\n) -&gt; Dict[str, Dict[int, Dict[float, float]]]:\n    \"\"\"\n    Calculate neural entrainment (PPC, PLV) per neuron (cell) for specified frequencies across different populations.\n\n    This function computes the entrainment metrics for each neuron within the specified populations based on their spike times\n    and the provided LFP signal. It returns a nested dictionary structure containing the entrainment values\n    organized by population, node ID, and frequency.\n\n    Parameters\n    ----------\n    spike_df : pd.DataFrame\n        DataFrame containing spike data with columns 'pop_name', 'node_ids', and 'timestamps'\n    lfp_data : np.ndarray\n        Local field potential (LFP) time series data\n    filter_method : str, optional\n        Method to use for filtering, either 'wavelet' or 'butter' (default: 'wavelet')\n    entrainment_method : str, optional\n        Method to use for entrainment calculation, either 'plv', 'ppc', or 'ppc2' (default: 'plv')\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    spike_fs : float\n        Sampling frequency of the spike times in Hz\n    lfp_fs : float\n        Sampling frequency of the LFP signal in Hz\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 2.0)\n    ppc_method : str, optional\n        Algorithm to use for PPC calculation: 'numpy', 'numba', or 'gpu' (default: 'numpy')\n    pop_names : List[str]\n        List of population names to analyze\n    freqs : List[float]\n        List of frequencies (in Hz) at which to calculate entrainment\n\n    Returns\n    -------\n    Dict[str, Dict[int, Dict[float, float]]]\n        Nested dictionary where the structure is:\n        {\n            population_name: {\n                node_id: {\n                    frequency: entrainment value\n                }\n            }\n        }\n        Entrainment values are floats representing the metric (PPC, PLV) at each frequency\n    \"\"\"\n    # pre filter lfp to speed up calculate of entrainment\n    filtered_lfp_phases = {}\n    for freq in range(len(freqs)):\n        phase = get_lfp_phase(\n            lfp_data=lfp_data,\n            freq_of_interest=freqs[freq],\n            fs=lfp_fs,\n            filter_method=filter_method,\n            lowcut=lowcut,\n            highcut=highcut,\n            bandwidth=bandwidth,\n        )\n        filtered_lfp_phases[freqs[freq]] = phase\n\n    entrainment_dict = {}\n    for pop in pop_names:\n        skip_count = 0\n        pop_spikes = spike_df[spike_df[\"pop_name\"] == pop]\n        nodes = sorted(pop_spikes[\"node_ids\"].unique())  # sort so all nodes are processed in order\n        entrainment_dict[pop] = {}\n        print(f\"Processing {pop} population\")\n        for node in tqdm(nodes):\n            node_spikes = pop_spikes[pop_spikes[\"node_ids\"] == node]\n\n            # Skip nodes with less than or equal to 1 spike\n            if len(node_spikes) &lt;= 1:\n                skip_count += 1\n                continue\n\n            entrainment_dict[pop][node] = {}\n            for freq in freqs:\n                # Calculate entrainment based on the selected method using the pre-filtered phases\n                if entrainment_method == \"plv\":\n                    entrainment_dict[pop][node][freq] = calculate_spike_lfp_plv(\n                        node_spikes[\"timestamps\"].values,\n                        lfp_data,\n                        spike_fs=spike_fs,\n                        lfp_fs=lfp_fs,\n                        freq_of_interest=freq,\n                        bandwidth=bandwidth,\n                        lowcut=lowcut,\n                        highcut=highcut,\n                        filter_method=filter_method,\n                        filtered_lfp_phase=filtered_lfp_phases[freq],\n                    )\n                elif entrainment_method == \"ppc2\":\n                    entrainment_dict[pop][node][freq] = calculate_ppc2(\n                        node_spikes[\"timestamps\"].values,\n                        lfp_data,\n                        spike_fs=spike_fs,\n                        lfp_fs=lfp_fs,\n                        freq_of_interest=freq,\n                        bandwidth=bandwidth,\n                        lowcut=lowcut,\n                        highcut=highcut,\n                        filter_method=filter_method,\n                        filtered_lfp_phase=filtered_lfp_phases[freq],\n                    )\n                elif entrainment_method == \"ppc\":\n                    entrainment_dict[pop][node][freq] = calculate_ppc(\n                        node_spikes[\"timestamps\"].values,\n                        lfp_data,\n                        spike_fs=spike_fs,\n                        lfp_fs=lfp_fs,\n                        freq_of_interest=freq,\n                        bandwidth=bandwidth,\n                        lowcut=lowcut,\n                        highcut=highcut,\n                        filter_method=filter_method,\n                        ppc_method=ppc_method,\n                        filtered_lfp_phase=filtered_lfp_phases[freq],\n                    )\n\n        print(\n            f\"Calculated {entrainment_method.upper()} for {pop} population with {len(nodes)-skip_count} valid cells, skipped {skip_count} cells for lack of spikes\"\n        )\n\n    return entrainment_dict\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.get_spikes_in_cycle","title":"<code>bmtool.analysis.entrainment.get_spikes_in_cycle(spike_df, lfp_data, spike_fs=1000, lfp_fs=400, filter_method='butter', lowcut=None, highcut=None, bandwidth=2.0, freq_of_interest=None)</code>","text":"<p>Analyze spike timing relative to oscillation phases.</p> Parameters: <p>spike_df : pd.DataFrame lfp_data : np.array     Raw LFP signal fs : float     Sampling frequency of LFP in Hz gamma_band : tuple     Lower and upper bounds of gamma frequency band in Hz</p> Returns: <p>phase_data : dict     Dictionary containing phase values for each spike and neuron population</p> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def get_spikes_in_cycle(\n    spike_df,\n    lfp_data,\n    spike_fs=1000,\n    lfp_fs=400,\n    filter_method=\"butter\",\n    lowcut=None,\n    highcut=None,\n    bandwidth=2.0,\n    freq_of_interest=None,\n):\n    \"\"\"\n    Analyze spike timing relative to oscillation phases.\n\n    Parameters:\n    -----------\n    spike_df : pd.DataFrame\n    lfp_data : np.array\n        Raw LFP signal\n    fs : float\n        Sampling frequency of LFP in Hz\n    gamma_band : tuple\n        Lower and upper bounds of gamma frequency band in Hz\n\n    Returns:\n    --------\n    phase_data : dict\n        Dictionary containing phase values for each spike and neuron population\n    \"\"\"\n    phase = get_lfp_phase(\n        lfp_data=lfp_data,\n        fs=lfp_fs,\n        filter_method=filter_method,\n        lowcut=lowcut,\n        highcut=highcut,\n        bandwidth=bandwidth,\n        freq_of_interest=freq_of_interest,\n    )\n\n    # Get unique neuron populations\n    neuron_pops = spike_df[\"pop_name\"].unique()\n\n    # Get the phase at each spike time for each neuron population\n    phase_data = {}\n\n    for pop in neuron_pops:\n        # Get spike times for this population\n        pop_spikes = spike_df[spike_df[\"pop_name\"] == pop][\"timestamps\"].values\n\n        # Convert spike times to sample indices\n        spike_times_seconds = pop_spikes / spike_fs\n\n        # Then convert from seconds to samples at the new sampling rate\n        spike_indices = np.round(spike_times_seconds * lfp_fs).astype(int)\n\n        # Ensure spike times are within LFP data range\n        valid_indices = (spike_indices &gt;= 0) &amp; (spike_indices &lt; len(phase))\n\n        if np.any(valid_indices):\n            valid_samples = spike_indices[valid_indices]\n            phase_data[pop] = phase[valid_samples]\n\n    return phase_data\n</code></pre>"},{"location":"api/analysis/entrainment/#bmtool.analysis.entrainment.compute_fr_hist_phase_amplitude","title":"<code>bmtool.analysis.entrainment.compute_fr_hist_phase_amplitude(spike_df, lfp_data, pop_names, freqs, spike_fs=1000, lfp_fs=1000, nbins_pha=16, nbins_amp=16, pop_num=None, duration=None)</code>","text":"<p>Compute firing rate histograms binned by LFP phase and amplitude quantiles.</p> <p>This function computes 2D histograms of spike firing rates as a function of  instantaneous LFP phase and amplitude at multiple frequencies. The output shows percentage change in firing rate relative to the mean across all phase-amplitude bins.</p> <p>Parameters:</p> Name Type Description Default <code>spike_df</code> <code>DataFrame</code> <p>DataFrame with spike data containing 'timestamps', 'pop_name', 'node_ids' columns</p> required <code>lfp_data</code> <code>DataArray</code> <p>LFP data with time coordinate</p> required <code>pop_names</code> <code>List[str]</code> <p>List of population names to analyze</p> required <code>freqs</code> <code>List[float]</code> <p>List of frequencies to analyze (Hz)</p> required <code>spike_fs</code> <code>float</code> <p>Spike sampling frequency (Hz) - should match lfp_fs for proper alignment</p> <code>1000</code> <code>lfp_fs</code> <code>float</code> <p>LFP sampling frequency (Hz)</p> <code>1000</code> <code>nbins_pha</code> <code>int</code> <p>Number of phase bins</p> <code>16</code> <code>nbins_amp</code> <code>int</code> <p>Number of amplitude quantile bins</p> <code>16</code> <code>pop_num</code> <code>Dict[str, int]</code> <p>Number of cells per population. If None, computed from spike_df.</p> <code>None</code> <code>duration</code> <code>float</code> <p>Duration of the data in seconds. If None, computed from lfp_data.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>fr_hist of shape (n_pop, n_freq, nbins_pha, nbins_amp) with % change in firing rate</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage\n&gt;&gt;&gt; fr_hist = compute_fr_hist_phase_amplitude(\n...     spike_df, lfp_data, ['PV', 'SST'], [25, 40], \n...     spike_fs=1000, lfp_fs=1000\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # With custom binning\n&gt;&gt;&gt; fr_hist = compute_fr_hist_phase_amplitude(\n...     spike_df, lfp_data, pop_names, [30], \n...     nbins_pha=16, nbins_amp=16\n... )\n</code></pre> Source code in <code>bmtool/analysis/entrainment.py</code> <pre><code>def compute_fr_hist_phase_amplitude(\n    spike_df: pd.DataFrame, \n    lfp_data: xr.DataArray, \n    pop_names: List[str], \n    freqs: List[float], \n    spike_fs: float = 1000, \n    lfp_fs: float = 1000, \n    nbins_pha: int = 16, \n    nbins_amp: int = 16, \n    pop_num: Optional[Dict[str, int]] = None, \n    duration: Optional[float] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute firing rate histograms binned by LFP phase and amplitude quantiles.\n\n    This function computes 2D histograms of spike firing rates as a function of \n    instantaneous LFP phase and amplitude at multiple frequencies. The output shows\n    percentage change in firing rate relative to the mean across all phase-amplitude bins.\n\n    Parameters\n    ----------\n    spike_df : pd.DataFrame\n        DataFrame with spike data containing 'timestamps', 'pop_name', 'node_ids' columns\n    lfp_data : xr.DataArray\n        LFP data with time coordinate\n    pop_names : List[str]\n        List of population names to analyze\n    freqs : List[float]\n        List of frequencies to analyze (Hz)\n    spike_fs : float, default=1000\n        Spike sampling frequency (Hz) - should match lfp_fs for proper alignment\n    lfp_fs : float, default=1000\n        LFP sampling frequency (Hz)\n    nbins_pha : int, default=16\n        Number of phase bins\n    nbins_amp : int, default=16\n        Number of amplitude quantile bins\n    pop_num : Dict[str, int], optional\n        Number of cells per population. If None, computed from spike_df.\n    duration : float, optional\n        Duration of the data in seconds. If None, computed from lfp_data.\n\n    Returns\n    -------\n    np.ndarray\n        fr_hist of shape (n_pop, n_freq, nbins_pha, nbins_amp) with % change in firing rate\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Basic usage\n    &gt;&gt;&gt; fr_hist = compute_fr_hist_phase_amplitude(\n    ...     spike_df, lfp_data, ['PV', 'SST'], [25, 40], \n    ...     spike_fs=1000, lfp_fs=1000\n    ... )\n\n    &gt;&gt;&gt; # With custom binning\n    &gt;&gt;&gt; fr_hist = compute_fr_hist_phase_amplitude(\n    ...     spike_df, lfp_data, pop_names, [30], \n    ...     nbins_pha=16, nbins_amp=16\n    ... )\n    \"\"\"\n    # Ensure spike and LFP sampling rates are consistent\n    if spike_fs != lfp_fs:\n        print(f\"Warning: spike_fs ({spike_fs}) != lfp_fs ({lfp_fs}). Using lfp_fs for spike indexing.\")\n\n    if pop_num is None:\n        pop_num = {p: len(spike_df[spike_df['pop_name'] == p]['node_ids'].unique()) for p in pop_names}\n    if duration is None:\n        duration = len(lfp_data) / lfp_fs  # in seconds\n\n    pha_bins = np.linspace(-np.pi, np.pi, nbins_pha + 1)\n    quantiles = np.linspace(0, 1, nbins_amp + 1)\n\n    n_pop = len(pop_names)\n    n_freq = len(freqs)\n    fr_hist = np.zeros((n_pop, n_freq, nbins_pha, nbins_amp))\n\n    for i, pop in enumerate(pop_names):\n        pop_spikes = spike_df[spike_df['pop_name'] == pop]\n        for j, freq in enumerate(freqs):\n            # Get filtered LFP and compute phase and amplitude\n            filtered_lfp = wavelet_filter(lfp_data.values, freq, lfp_fs)\n            phase = np.angle(filtered_lfp)\n            amplitude = np.abs(filtered_lfp)\n\n            # Compute amplitude quantiles\n            amp_bins = np.quantile(amplitude, quantiles)\n\n            # Get spike phases and amplitudes\n            spike_times = pop_spikes['timestamps'].values\n            # Convert spike times (ms) to LFP sample indices using lfp_fs\n            spike_indices = np.round(spike_times / 1000 * lfp_fs).astype(int)\n            spike_indices = np.clip(spike_indices, 0, len(phase) - 1)\n\n            spike_phases = phase[spike_indices]\n            spike_amps = amplitude[spike_indices]\n\n            # Bin spikes\n            fr, _, _ = np.histogram2d(spike_phases, spike_amps, bins=(pha_bins, amp_bins))\n\n            # Normalize to firing rate\n            fr /= pop_num[pop] * duration\n\n            # Compute % change from mean\n            fr_mean = fr.mean()\n            if fr_mean &gt; 0:\n                fr_hist[i, j] = 100 * (fr - fr_mean) / fr_mean\n            else:\n                fr_hist[i, j] = 0  # Handle case where mean is zero\n\n    return fr_hist\n</code></pre>"},{"location":"api/analysis/lfp/","title":"LFP/ECP Analysis","text":"<p>The <code>lfp</code> module provides tools for analyzing local field potentials (LFP) and extracellular potentials (ECP).</p>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.load_ecp_to_xarray","title":"<code>bmtool.analysis.lfp.load_ecp_to_xarray(ecp_file, demean=False)</code>","text":"<p>Load ECP data from an HDF5 file (BMTK sim) into an xarray DataArray.</p> Parameters: <p>ecp_file : str     Path to the HDF5 file containing ECP data. demean : bool, optional     If True, the mean of the data will be subtracted (default is False).</p> Returns: <p>xr.DataArray     An xarray DataArray containing the ECP data, with time as one dimension     and channel_id as another.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def load_ecp_to_xarray(ecp_file: str, demean: bool = False) -&gt; xr.DataArray:\n    \"\"\"\n    Load ECP data from an HDF5 file (BMTK sim) into an xarray DataArray.\n\n    Parameters:\n    ----------\n    ecp_file : str\n        Path to the HDF5 file containing ECP data.\n    demean : bool, optional\n        If True, the mean of the data will be subtracted (default is False).\n\n    Returns:\n    -------\n    xr.DataArray\n        An xarray DataArray containing the ECP data, with time as one dimension\n        and channel_id as another.\n    \"\"\"\n    with h5py.File(ecp_file, \"r\") as f:\n        ecp = xr.DataArray(\n            f[\"ecp\"][\"data\"][()].T,\n            coords=dict(\n                channel_id=f[\"ecp\"][\"channel_id\"][()],\n                time=np.arange(*f[\"ecp\"][\"time\"]),  # ms\n            ),\n            attrs=dict(\n                fs=1000 / f[\"ecp\"][\"time\"][2]  # Hz\n            ),\n        )\n    if demean:\n        ecp -= ecp.mean(dim=\"time\")\n    return ecp\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.ecp_to_lfp","title":"<code>bmtool.analysis.lfp.ecp_to_lfp(ecp_data, cutoff=250, fs=10000, downsample_freq=1000)</code>","text":"<p>Apply a low-pass Butterworth filter to an xarray DataArray and optionally downsample. This filters out the high end frequencies turning the ECP into a LFP</p> Parameters: <p>ecp_data : xr.DataArray     The input data array containing LFP data with time as one dimension. cutoff : float     The cutoff frequency for the low-pass filter in Hz (default is 250Hz). fs : float, optional     The sampling frequency of the data (default is 10000 Hz). downsample_freq : float, optional     The frequency to downsample to (default is 1000 Hz).</p> Returns: <p>xr.DataArray     The filtered (and possibly downsampled) data as an xarray DataArray.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def ecp_to_lfp(\n    ecp_data: xr.DataArray, cutoff: float = 250, fs: float = 10000, downsample_freq: float = 1000\n) -&gt; xr.DataArray:\n    \"\"\"\n    Apply a low-pass Butterworth filter to an xarray DataArray and optionally downsample.\n    This filters out the high end frequencies turning the ECP into a LFP\n\n    Parameters:\n    ----------\n    ecp_data : xr.DataArray\n        The input data array containing LFP data with time as one dimension.\n    cutoff : float\n        The cutoff frequency for the low-pass filter in Hz (default is 250Hz).\n    fs : float, optional\n        The sampling frequency of the data (default is 10000 Hz).\n    downsample_freq : float, optional\n        The frequency to downsample to (default is 1000 Hz).\n\n    Returns:\n    -------\n    xr.DataArray\n        The filtered (and possibly downsampled) data as an xarray DataArray.\n    \"\"\"\n    # Bandpass filter design\n    nyq = 0.5 * fs\n    cut = cutoff / nyq\n    b, a = signal.butter(8, cut, btype=\"low\", analog=False)\n\n    # Initialize an array to hold filtered data\n    filtered_data = xr.DataArray(\n        np.zeros_like(ecp_data), coords=ecp_data.coords, dims=ecp_data.dims\n    )\n\n    # Apply the filter to each channel\n    for channel in ecp_data.channel_id:\n        filtered_data.loc[channel, :] = signal.filtfilt(\n            b, a, ecp_data.sel(channel_id=channel).values\n        )\n\n    # Downsample the filtered data if a downsample frequency is provided\n    if downsample_freq is not None:\n        downsample_factor = int(fs / downsample_freq)\n        filtered_data = filtered_data.isel(time=slice(None, None, downsample_factor))\n        # Update the sampling frequency attribute\n        filtered_data.attrs[\"fs\"] = downsample_freq\n\n    return filtered_data\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.slice_time_series","title":"<code>bmtool.analysis.lfp.slice_time_series(data, time_ranges)</code>","text":"<p>Slice the xarray DataArray based on provided time ranges. Can be used to get LFP during certain stimulus times</p> Parameters: <p>data : xr.DataArray     The input xarray DataArray containing time-series data. time_ranges : tuple or list of tuples     One or more tuples representing the (start, stop) time points for slicing.     For example: (start, stop) or [(start1, stop1), (start2, stop2)]</p> Returns: <p>xr.DataArray     A new xarray DataArray containing the concatenated slices.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def slice_time_series(data: xr.DataArray, time_ranges: tuple) -&gt; xr.DataArray:\n    \"\"\"\n    Slice the xarray DataArray based on provided time ranges.\n    Can be used to get LFP during certain stimulus times\n\n    Parameters:\n    ----------\n    data : xr.DataArray\n        The input xarray DataArray containing time-series data.\n    time_ranges : tuple or list of tuples\n        One or more tuples representing the (start, stop) time points for slicing.\n        For example: (start, stop) or [(start1, stop1), (start2, stop2)]\n\n    Returns:\n    -------\n    xr.DataArray\n        A new xarray DataArray containing the concatenated slices.\n    \"\"\"\n    # Ensure time_ranges is a list of tuples\n    if isinstance(time_ranges, tuple) and len(time_ranges) == 2:\n        time_ranges = [time_ranges]\n\n    # List to hold sliced data\n    slices = []\n\n    # Slice the data for each time range\n    for start, stop in time_ranges:\n        sliced_data = data.sel(time=slice(start, stop))\n        slices.append(sliced_data)\n\n    # Concatenate all slices along the time dimension if more than one slice\n    if len(slices) &gt; 1:\n        return xr.concat(slices, dim=\"time\")\n    else:\n        return slices[0]\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.fit_fooof","title":"<code>bmtool.analysis.lfp.fit_fooof(f, pxx, aperiodic_mode='fixed', dB_threshold=3.0, max_n_peaks=10, freq_range=None, peak_width_limits=None, report=False, plot=False, plt_log=False, plt_range=None, figsize=None, title=None)</code>","text":"<p>Fit a FOOOF model to power spectral density data.</p> Parameters: <p>f : array-like     Frequencies corresponding to the power spectral density data. pxx : array-like     Power spectral density data to fit. aperiodic_mode : str, optional     The mode for fitting aperiodic components ('fixed' or 'knee', default is 'fixed'). dB_threshold : float, optional     Minimum peak height in dB (default is 3). max_n_peaks : int, optional     Maximum number of peaks to fit (default is 10). freq_range : tuple, optional     Frequency range to fit (default is None, which uses the full range). peak_width_limits : tuple, optional     Limits on the width of peaks (default is None). report : bool, optional     If True, will print fitting results (default is False). plot : bool, optional     If True, will plot the fitting results (default is False). plt_log : bool, optional     If True, use a logarithmic scale for the y-axis in plots (default is False). plt_range : tuple, optional     Range for plotting (default is None). figsize : tuple, optional     Size of the figure (default is None). title : str, optional     Title for the plot (default is None).</p> Returns: <p>tuple     A tuple containing the fitting results and the FOOOF model object.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def fit_fooof(\n    f: np.ndarray,\n    pxx: np.ndarray,\n    aperiodic_mode: str = \"fixed\",\n    dB_threshold: float = 3.0,\n    max_n_peaks: int = 10,\n    freq_range: tuple = None,\n    peak_width_limits: tuple = None,\n    report: bool = False,\n    plot: bool = False,\n    plt_log: bool = False,\n    plt_range: tuple = None,\n    figsize: tuple = None,\n    title: str = None,\n) -&gt; tuple:\n    \"\"\"\n    Fit a FOOOF model to power spectral density data.\n\n    Parameters:\n    ----------\n    f : array-like\n        Frequencies corresponding to the power spectral density data.\n    pxx : array-like\n        Power spectral density data to fit.\n    aperiodic_mode : str, optional\n        The mode for fitting aperiodic components ('fixed' or 'knee', default is 'fixed').\n    dB_threshold : float, optional\n        Minimum peak height in dB (default is 3).\n    max_n_peaks : int, optional\n        Maximum number of peaks to fit (default is 10).\n    freq_range : tuple, optional\n        Frequency range to fit (default is None, which uses the full range).\n    peak_width_limits : tuple, optional\n        Limits on the width of peaks (default is None).\n    report : bool, optional\n        If True, will print fitting results (default is False).\n    plot : bool, optional\n        If True, will plot the fitting results (default is False).\n    plt_log : bool, optional\n        If True, use a logarithmic scale for the y-axis in plots (default is False).\n    plt_range : tuple, optional\n        Range for plotting (default is None).\n    figsize : tuple, optional\n        Size of the figure (default is None).\n    title : str, optional\n        Title for the plot (default is None).\n\n    Returns:\n    -------\n    tuple\n        A tuple containing the fitting results and the FOOOF model object.\n    \"\"\"\n    if aperiodic_mode != \"knee\":\n        aperiodic_mode = \"fixed\"\n\n    def set_range(x, upper=f[-1]):\n        x = np.array(upper) if x is None else np.array(x)\n        return [f[2], x.item()] if x.size == 1 else x.tolist()\n\n    freq_range = set_range(freq_range)\n    peak_width_limits = set_range(peak_width_limits, np.inf)\n\n    # Initialize a FOOOF object\n    fm = FOOOF(\n        peak_width_limits=peak_width_limits,\n        min_peak_height=dB_threshold / 10,\n        peak_threshold=0.0,\n        max_n_peaks=max_n_peaks,\n        aperiodic_mode=aperiodic_mode,\n    )\n\n    # Fit the model\n    try:\n        fm.fit(f, pxx, freq_range)\n    except Exception as e:\n        fl = np.linspace(f[0], f[-1], int((f[-1] - f[0]) / np.min(np.diff(f))) + 1)\n        fm.fit(fl, np.interp(fl, f, pxx), freq_range)\n\n    results = fm.get_results()\n\n    if report:\n        fm.print_results()\n        if aperiodic_mode == \"knee\":\n            ap_params = results.aperiodic_params\n            if ap_params[1] &lt;= 0:\n                print(\n                    \"Negative value of knee parameter occurred. Suggestion: Fit without knee parameter.\"\n                )\n            knee_freq = np.abs(ap_params[1]) ** (1 / ap_params[2])\n            print(f\"Knee location: {knee_freq:.2f} Hz\")\n\n    if plot:\n        plt_range = set_range(plt_range)\n        fm.plot(ax=plt.gca(), plt_log=plt_log)\n        plt.xlim(np.log10(plt_range) if plt_log else plt_range)\n        # plt.ylim(-8, -5.5)\n        if figsize:\n            plt.gcf().set_size_inches(figsize)\n        if title:\n            plt.title(title)\n        if is_notebook():\n            pass\n        else:\n            plt.show()\n\n    return results, fm\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.generate_resd_from_fooof","title":"<code>bmtool.analysis.lfp.generate_resd_from_fooof(fooof_model)</code>","text":"<p>Generate residuals from a fitted FOOOF model.</p> Parameters: <p>fooof_model : FOOOF     A fitted FOOOF model object.</p> Returns: <p>tuple     A tuple containing the residual power spectral density and the aperiodic fit.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def generate_resd_from_fooof(fooof_model: FOOOF) -&gt; tuple:\n    \"\"\"\n    Generate residuals from a fitted FOOOF model.\n\n    Parameters:\n    ----------\n    fooof_model : FOOOF\n        A fitted FOOOF model object.\n\n    Returns:\n    -------\n    tuple\n        A tuple containing the residual power spectral density and the aperiodic fit.\n    \"\"\"\n    results = fooof_model.get_results()\n    full_fit, _, ap_fit = gen_model(\n        fooof_model.freqs[1:],\n        results.aperiodic_params,\n        results.gaussian_params,\n        return_components=True,\n    )\n\n    full_fit, ap_fit = 10**full_fit, 10**ap_fit  # Convert back from log\n    res_psd = np.insert(\n        (10 ** fooof_model.power_spectrum[1:]) - ap_fit, 0, 0.0\n    )  # Convert back from log\n    res_fit = np.insert(full_fit - ap_fit, 0, 0.0)\n    ap_fit = np.insert(ap_fit, 0, 0.0)\n\n    return res_psd, ap_fit\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.get_fooof_freq_band","title":"<code>bmtool.analysis.lfp.get_fooof_freq_band(gaussian_params, freq_range, width_limit=(0.0, np.inf), top_n_peaks=1, bandwidth_n_sigma=1.5)</code>","text":"<p>Get frequency band of the top N peaks in the FOOOF results within a given band of interest.</p> <p>Parameters:</p> Name Type Description Default <code>gaussian_params</code> <code>NDArray[float]</code> <p>Gaussian parameters from FOOOF results, FOOOFResults.gaussian_params.</p> required <code>freq_range</code> <code>tuple[float, float]</code> <p>Frequency band of interest</p> required <code>width_limit</code> <code>tuple[float, float]</code> <p>Width limit of the peaks in terms of the standard deviation of the Gaussian parameters.</p> <code>(0.0, inf)</code> <code>top_n_peaks</code> <code>int</code> <p>Number of top peaks to include in the band.</p> <code>1</code> <code>bandwidth_n_sigma</code> <code>float</code> <p>Multiplier of sigma of the Gaussian parameters to define the bandwidth of the peak.</p> <code>1.5</code> <p>Returns:</p> Name Type Description <code>band</code> <code>tuple[float, float]</code> <p>Combined frequency band of the top N peaks within the given band of interest. If no peaks are found within the given band of interest, return (np.nan, np.nan).</p> <code>peak_inds</code> <code>array_like of bool</code> <p>Boolean array of the peaks within the given band of interest.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def get_fooof_freq_band(\n    gaussian_params,\n    freq_range,\n    width_limit=(0., np.inf),\n    top_n_peaks=1,\n    bandwidth_n_sigma=1.5\n):\n    \"\"\"Get frequency band of the top N peaks in the FOOOF results within a given band of interest.\n\n    Parameters\n    ----------\n    gaussian_params : NDArray[float]\n        Gaussian parameters from FOOOF results, FOOOFResults.gaussian_params.\n    freq_range : tuple[float, float]\n        Frequency band of interest\n    width_limit : tuple[float, float]\n        Width limit of the peaks in terms of the standard deviation of the Gaussian parameters.\n    top_n_peaks : int\n        Number of top peaks to include in the band.\n    bandwidth_n_sigma : float\n        Multiplier of sigma of the Gaussian parameters to define the bandwidth of the peak.\n\n    Returns\n    -------\n    band : tuple[float, float]\n        Combined frequency band of the top N peaks within the given band of interest.\n        If no peaks are found within the given band of interest, return (np.nan, np.nan).\n    peak_inds : array_like of bool\n        Boolean array of the peaks within the given band of interest.\n    \"\"\"\n    # find peaks within the given band of interest\n    peak_inds = (gaussian_params[:, 0] &gt;= freq_range[0]) &amp; (gaussian_params[:, 0] &lt;= freq_range[1])\n    peak_inds = peak_inds &amp; (gaussian_params[:, 2] &gt;= width_limit[0]) &amp; (gaussian_params[:, 2] &lt;= width_limit[1])\n    peak_inds = np.nonzero(peak_inds)[0]\n    if peak_inds.size == 0:\n        return (np.nan, np.nan), peak_inds\n\n    # find top peaks by height\n    top_n_peaks = max(top_n_peaks, 1)  # at least one peak\n    idx = np.argsort(gaussian_params[peak_inds, 1])[::-1][:top_n_peaks]\n    peak_inds = peak_inds[idx]\n\n    # get the combined frequency band of the top peaks\n    band_peaks = gaussian_params[peak_inds, :]\n    band_widths = bandwidth_n_sigma * band_peaks[:, 2]  # one-sided bandwidth\n    band_freqs = np.fmax(band_peaks[:, [0]] + np.outer(band_widths, [-1, 1]), 0.)\n    band = (band_freqs[:, 0].min(), band_freqs[:, 1].max())  # combined frequency band\n    return band, peak_inds\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.calculate_SNR","title":"<code>bmtool.analysis.lfp.calculate_SNR(fooof_model, freq_band)</code>","text":"<p>Calculate the signal-to-noise ratio (SNR) from a fitted FOOOF model.</p> Parameters: <p>fooof_model : FOOOF     A fitted FOOOF model object. freq_band : tuple     Frequency band (min, max) for SNR calculation.</p> Returns: <p>float     The calculated SNR for the specified frequency band.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def calculate_SNR(fooof_model: FOOOF, freq_band: tuple) -&gt; float:\n    \"\"\"\n    Calculate the signal-to-noise ratio (SNR) from a fitted FOOOF model.\n\n    Parameters:\n    ----------\n    fooof_model : FOOOF\n        A fitted FOOOF model object.\n    freq_band : tuple\n        Frequency band (min, max) for SNR calculation.\n\n    Returns:\n    -------\n    float\n        The calculated SNR for the specified frequency band.\n    \"\"\"\n    periodic, ap = generate_resd_from_fooof(fooof_model)\n    freq = fooof_model.freqs  # Get frequencies from model\n    indices = (freq &gt;= freq_band[0]) &amp; (freq &lt;= freq_band[1])  # Get only the band we care about\n    band_periodic = periodic[indices]  # Filter based on band\n    band_ap = ap[indices]  # Filter\n    band_freq = freq[indices]  # Another filter\n    periodic_power = np.trapz(band_periodic, band_freq)  # Integrate periodic power\n    ap_power = np.trapz(band_ap, band_freq)  # Integrate aperiodic power\n    normalized_power = periodic_power / ap_power  # Compute the SNR\n    return normalized_power\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.wavelet_filter","title":"<code>bmtool.analysis.lfp.wavelet_filter(x, freq, fs, bandwidth=1.0, axis=-1, show_passband=False)</code>","text":"<p>Compute the Continuous Wavelet Transform (CWT) for a specified frequency using a complex Morlet wavelet.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input signal</p> required <code>freq</code> <code>float</code> <p>Target frequency for the wavelet filter</p> required <code>fs</code> <code>float</code> <p>Sampling frequency of the signal</p> required <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter of the wavelet filter (default is 1.0)</p> <code>1.0</code> <code>axis</code> <code>int</code> <p>Axis along which to compute the CWT (default is -1)</p> <code>-1</code> <code>show_passband</code> <code>bool</code> <p>If True, print the passband of the wavelet filter (default is False)</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Continuous Wavelet Transform of the input signal</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def wavelet_filter(\n    x: np.ndarray,\n    freq: float,\n    fs: float,\n    bandwidth: float = 1.0,\n    axis: int = -1,\n    show_passband: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute the Continuous Wavelet Transform (CWT) for a specified frequency using a complex Morlet wavelet.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input signal\n    freq : float\n        Target frequency for the wavelet filter\n    fs : float\n        Sampling frequency of the signal\n    bandwidth : float, optional\n        Bandwidth parameter of the wavelet filter (default is 1.0)\n    axis : int, optional\n        Axis along which to compute the CWT (default is -1)\n    show_passband : bool, optional\n        If True, print the passband of the wavelet filter (default is False)\n\n    Returns\n    -------\n    np.ndarray\n        Continuous Wavelet Transform of the input signal\n    \"\"\"\n    if show_passband:\n        lower_bound, upper_bound, passband_width = calculate_wavelet_passband(\n            freq, bandwidth, threshold=0.3\n        )  # kinda made up threshold gives the rough idea\n        print(f\"Wavelet filter at {freq:.1f} Hz Bandwidth: {bandwidth:.1f} Hz:\")\n        print(\n            f\"  Passband: {lower_bound:.1f} - {upper_bound:.1f} Hz (width: {passband_width:.1f} Hz)\"\n        )\n    wavelet = \"cmor\" + str(2 * bandwidth**2) + \"-1.0\"\n    scale = pywt.scale2frequency(wavelet, 1) * fs / freq\n    x_a = pywt.cwt(x, [scale], wavelet=wavelet, axis=axis)[0][0]\n    return x_a\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.butter_bandpass_filter","title":"<code>bmtool.analysis.lfp.butter_bandpass_filter(data, lowcut, highcut, fs, order=5, axis=-1)</code>","text":"<p>Apply a Butterworth bandpass filter to the input data.</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def butter_bandpass_filter(\n    data: np.ndarray, lowcut: float, highcut: float, fs: float, order: int = 5, axis: int = -1\n) -&gt; np.ndarray:\n    \"\"\"\n    Apply a Butterworth bandpass filter to the input data.\n    \"\"\"\n    sos = signal.butter(order, [lowcut, highcut], fs=fs, btype=\"band\", output=\"sos\")\n    x_a = signal.sosfiltfilt(sos, data, axis=axis)\n    return x_a\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.get_lfp_power","title":"<code>bmtool.analysis.lfp.get_lfp_power(lfp_data, freq_of_interest, fs, filter_method='wavelet', lowcut=None, highcut=None, bandwidth=1.0)</code>","text":"<p>Compute the power of the raw LFP signal in a specified frequency band, preserving xarray structure if input is xarray.</p> <p>Parameters:</p> Name Type Description Default <code>lfp_data</code> <code>ndarray or DataArray</code> <p>Raw local field potential (LFP) time series data</p> required <code>freq_of_interest</code> <code>float</code> <p>Center frequency (Hz) for wavelet filtering method</p> required <code>fs</code> <code>float</code> <p>Sampling frequency (Hz) of the input data</p> required <code>filter_method</code> <code>str</code> <p>Filtering method to use, either 'wavelet' or 'butter' (default: 'wavelet')</p> <code>'wavelet'</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'</p> <code>None</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray or DataArray</code> <p>Power of the filtered signal (magnitude squared) with same structure as input</p> Notes <ul> <li>The 'wavelet' method uses a complex Morlet wavelet centered at the specified frequency</li> <li>The 'butter' method uses a Butterworth bandpass filter with the specified cutoff frequencies</li> <li>When using the 'butter' method, both lowcut and highcut must be provided</li> <li>If input is an xarray DataArray, the output will preserve the same structure with coordinates</li> </ul> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def get_lfp_power(\n    lfp_data,\n    freq_of_interest: float,\n    fs: float,\n    filter_method: str = \"wavelet\",\n    lowcut: float = None,\n    highcut: float = None,\n    bandwidth: float = 1.0,\n):\n    \"\"\"\n    Compute the power of the raw LFP signal in a specified frequency band,\n    preserving xarray structure if input is xarray.\n\n    Parameters\n    ----------\n    lfp_data : np.ndarray or xr.DataArray\n        Raw local field potential (LFP) time series data\n    freq_of_interest : float\n        Center frequency (Hz) for wavelet filtering method\n    fs : float\n        Sampling frequency (Hz) of the input data\n    filter_method : str, optional\n        Filtering method to use, either 'wavelet' or 'butter' (default: 'wavelet')\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth bandpass filter, required if filter_method='butter'\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 1.0)\n\n    Returns\n    -------\n    np.ndarray or xr.DataArray\n        Power of the filtered signal (magnitude squared) with same structure as input\n\n    Notes\n    -----\n    - The 'wavelet' method uses a complex Morlet wavelet centered at the specified frequency\n    - The 'butter' method uses a Butterworth bandpass filter with the specified cutoff frequencies\n    - When using the 'butter' method, both lowcut and highcut must be provided\n    - If input is an xarray DataArray, the output will preserve the same structure with coordinates\n    \"\"\"\n    import xarray as xr\n\n    # Check if input is xarray\n    is_xarray = isinstance(lfp_data, xr.DataArray)\n\n    if is_xarray:\n        # Get the raw data from xarray\n        raw_data = lfp_data.values\n        # Check if 'fs' attribute exists in the xarray and override if necessary\n        if \"fs\" in lfp_data.attrs and fs is None:\n            fs = lfp_data.attrs[\"fs\"]\n    else:\n        raw_data = lfp_data\n\n    if filter_method == \"wavelet\":\n        filtered_signal = wavelet_filter(raw_data, freq_of_interest, fs, bandwidth)\n    elif filter_method == \"butter\":\n        if lowcut is None or highcut is None:\n            raise ValueError(\n                \"Both lowcut and highcut must be specified when using 'butter' method.\"\n            )\n        filtered_signal = butter_bandpass_filter(raw_data, lowcut, highcut, fs)\n    else:\n        raise ValueError(\"Invalid method. Choose 'wavelet' or 'butter'.\")\n\n    # Calculate power (magnitude squared of filtered signal)\n    power = np.abs(filtered_signal) ** 2\n\n    # If the input was an xarray, return an xarray with the same coordinates\n    if is_xarray:\n        power_xarray = xr.DataArray(\n            power,\n            coords=lfp_data.coords,\n            dims=lfp_data.dims,\n            attrs={\n                **lfp_data.attrs,\n                \"filter_method\": filter_method,\n                \"frequency_of_interest\": freq_of_interest,\n                \"bandwidth\": bandwidth,\n                \"lowcut\": lowcut,\n                \"highcut\": highcut,\n                \"power_type\": \"magnitude_squared\",\n            },\n        )\n        return power_xarray\n\n    return power\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.get_lfp_phase","title":"<code>bmtool.analysis.lfp.get_lfp_phase(lfp_data, freq_of_interest, fs, filter_method='wavelet', lowcut=None, highcut=None, bandwidth=1.0)</code>","text":"<p>Calculate the phase of the filtered signal, preserving xarray structure if input is xarray.</p> <p>Parameters:</p> Name Type Description Default <code>lfp_data</code> <code>ndarray or DataArray</code> <p>Input LFP data</p> required <code>freq_of_interest</code> <code>float</code> <p>Frequency of interest (Hz)</p> required <code>fs</code> <code>float</code> <p>Sampling frequency (Hz)</p> required <code>filter_method</code> <code>str</code> <p>Method for filtering the signal ('wavelet' or 'butter')</p> <code>'wavelet'</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter when method='wavelet' (default: 1.0)</p> <code>1.0</code> <code>lowcut</code> <code>float</code> <p>Low cutoff frequency for Butterworth filter when method='butter'</p> <code>None</code> <code>highcut</code> <code>float</code> <p>High cutoff frequency for Butterworth filter when method='butter'</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray or DataArray</code> <p>Phase of the filtered signal with same structure as input</p> Notes <ul> <li>The 'wavelet' method uses a complex Morlet wavelet centered at the specified frequency</li> <li>The 'butter' method uses a Butterworth bandpass filter with the specified cutoff frequencies   followed by Hilbert transform to extract the phase</li> <li>When using the 'butter' method, both lowcut and highcut must be provided</li> <li>If input is an xarray DataArray, the output will preserve the same structure with coordinates</li> </ul> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def get_lfp_phase(\n    lfp_data,\n    freq_of_interest: float,\n    fs: float,\n    filter_method: str = \"wavelet\",\n    lowcut: float = None,\n    highcut: float = None,\n    bandwidth: float = 1.0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the phase of the filtered signal, preserving xarray structure if input is xarray.\n\n    Parameters\n    ----------\n    lfp_data : np.ndarray or xr.DataArray\n        Input LFP data\n    freq_of_interest : float\n        Frequency of interest (Hz)\n    fs : float\n        Sampling frequency (Hz)\n    filter_method : str, optional\n        Method for filtering the signal ('wavelet' or 'butter')\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter when method='wavelet' (default: 1.0)\n    lowcut : float, optional\n        Low cutoff frequency for Butterworth filter when method='butter'\n    highcut : float, optional\n        High cutoff frequency for Butterworth filter when method='butter'\n\n    Returns\n    -------\n    np.ndarray or xr.DataArray\n        Phase of the filtered signal with same structure as input\n\n    Notes\n    -----\n    - The 'wavelet' method uses a complex Morlet wavelet centered at the specified frequency\n    - The 'butter' method uses a Butterworth bandpass filter with the specified cutoff frequencies\n      followed by Hilbert transform to extract the phase\n    - When using the 'butter' method, both lowcut and highcut must be provided\n    - If input is an xarray DataArray, the output will preserve the same structure with coordinates\n    \"\"\"\n    import xarray as xr\n\n    # Check if input is xarray\n    is_xarray = isinstance(lfp_data, xr.DataArray)\n\n    if is_xarray:\n        # Get the raw data from xarray\n        raw_data = lfp_data.values\n        # Check if 'fs' attribute exists in the xarray and override if necessary\n        if \"fs\" in lfp_data.attrs and fs is None:\n            fs = lfp_data.attrs[\"fs\"]\n    else:\n        raw_data = lfp_data\n\n    if filter_method == \"wavelet\":\n        if freq_of_interest is None:\n            raise ValueError(\"freq_of_interest must be provided for the wavelet method.\")\n        # Wavelet filter returns complex values directly\n        filtered_signal = wavelet_filter(raw_data, freq_of_interest, fs, bandwidth)\n        # Phase is the angle of the complex signal\n        phase = np.angle(filtered_signal)\n    elif filter_method == \"butter\":\n        if lowcut is None or highcut is None:\n            raise ValueError(\n                \"Both lowcut and highcut must be specified when using 'butter' method.\"\n            )\n        # Butterworth filter returns real values\n        filtered_signal = butter_bandpass_filter(raw_data, lowcut, highcut, fs)\n        # Apply Hilbert transform to get analytic signal (complex)\n        analytic_signal = signal.hilbert(filtered_signal)\n        # Phase is the angle of the analytic signal\n        phase = np.angle(analytic_signal)\n    else:\n        raise ValueError(f\"Invalid method {filter_method}. Choose 'wavelet' or 'butter'.\")\n\n    # If the input was an xarray, return an xarray with the same coordinates\n    if is_xarray:\n        phase_xarray = xr.DataArray(\n            phase,\n            coords=lfp_data.coords,\n            dims=lfp_data.dims,\n            attrs={\n                **lfp_data.attrs,\n                \"filter_method\": filter_method,\n                \"freq_of_interest\": freq_of_interest,\n                \"bandwidth\": bandwidth,\n                \"lowcut\": lowcut,\n                \"highcut\": highcut,\n            },\n        )\n        return phase_xarray\n\n    return phase\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.cwt_spectrogram","title":"<code>bmtool.analysis.lfp.cwt_spectrogram(x, fs, nNotes=6, nOctaves=np.inf, freq_range=(0, np.inf), bandwidth=1.0, axis=-1, detrend=False, normalize=False)</code>","text":"<p>Calculate spectrogram using continuous wavelet transform</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def cwt_spectrogram(\n    x,\n    fs,\n    nNotes=6,\n    nOctaves=np.inf,\n    freq_range=(0, np.inf),\n    bandwidth=1.0,\n    axis=-1,\n    detrend=False,\n    normalize=False,\n):\n    \"\"\"Calculate spectrogram using continuous wavelet transform\"\"\"\n    x = np.asarray(x)\n    N = x.shape[axis]\n    times = np.arange(N) / fs\n    # detrend and normalize\n    if detrend:\n        x = signal.detrend(x, axis=axis, type=\"linear\")\n    if normalize:\n        x = x / x.std()\n    # Define some parameters of our wavelet analysis.\n    # range of scales (in time) that makes sense\n    # min = 2 (Nyquist frequency)\n    # max = np.floor(N/2)\n    nOctaves = min(nOctaves, np.log2(2 * np.floor(N / 2)))\n    scales = 2 ** np.arange(1, nOctaves, 1 / nNotes)\n    # cwt and the frequencies used.\n    # Use the complex morelet with bw=2*bandwidth^2 and center frequency of 1.0\n    # bandwidth is sigma of the gaussian envelope\n    wavelet = \"cmor\" + str(2 * bandwidth**2) + \"-1.0\"\n    frequencies = pywt.scale2frequency(wavelet, scales) * fs\n    scales = scales[(frequencies &gt;= freq_range[0]) &amp; (frequencies &lt;= freq_range[1])]\n    coef, frequencies = pywt.cwt(\n        x, scales[::-1], wavelet=wavelet, sampling_period=1 / fs, axis=axis\n    )\n    power = np.real(coef * np.conj(coef))  # equivalent to power = np.abs(coef)**2\n    # cone of influence in terms of wavelength\n    coi = N / 2 - np.abs(np.arange(N) - (N - 1) / 2)\n    # cone of influence in terms of frequency\n    coif = COI_FREQ * fs / coi\n    return power, times, frequencies, coif\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.cwt_spectrogram_xarray","title":"<code>bmtool.analysis.lfp.cwt_spectrogram_xarray(x, fs, time=None, axis=-1, downsample_fs=None, channel_coords=None, **cwt_kwargs)</code>","text":"<p>Calculate spectrogram using continuous wavelet transform and return an xarray.Dataset x: input array fs: sampling frequency (Hz) axis: dimension index of time axis in x downsample_fs: downsample to the frequency if specified channel_coords: dictionary of {coordinate name: index} for channels cwt_kwargs: keyword arguments for cwt_spectrogram()</p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def cwt_spectrogram_xarray(\n    x, fs, time=None, axis=-1, downsample_fs=None, channel_coords=None, **cwt_kwargs\n):\n    \"\"\"Calculate spectrogram using continuous wavelet transform and return an xarray.Dataset\n    x: input array\n    fs: sampling frequency (Hz)\n    axis: dimension index of time axis in x\n    downsample_fs: downsample to the frequency if specified\n    channel_coords: dictionary of {coordinate name: index} for channels\n    cwt_kwargs: keyword arguments for cwt_spectrogram()\n    \"\"\"\n    x = np.asarray(x)\n    T = x.shape[axis]  # number of time points\n    t = np.arange(T) / fs if time is None else np.asarray(time)\n    if downsample_fs is None or downsample_fs &gt;= fs:\n        downsample_fs = fs\n        downsampled = x\n    else:\n        num = int(T * downsample_fs / fs)\n        downsample_fs = num / T * fs\n        downsampled, t = signal.resample(x, num=num, t=t, axis=axis)\n    downsampled = np.moveaxis(downsampled, axis, -1)\n    sxx, _, f, coif = cwt_spectrogram(downsampled, downsample_fs, **cwt_kwargs)\n    sxx = np.moveaxis(sxx, 0, -2)  # shape (... , freq, time)\n    if channel_coords is None:\n        channel_coords = {f\"dim_{i:d}\": range(d) for i, d in enumerate(sxx.shape[:-2])}\n    sxx = xr.DataArray(sxx, coords={**channel_coords, \"frequency\": f, \"time\": t}).to_dataset(\n        name=\"PSD\"\n    )\n    sxx.update(dict(cone_of_influence_frequency=xr.DataArray(coif, coords={\"time\": t})))\n    return sxx\n</code></pre>"},{"location":"api/analysis/lfp/#bmtool.analysis.lfp.get_windowed_data","title":"<code>bmtool.analysis.lfp.get_windowed_data(x, windows, win_grp_idx, dim='time', win_dim='cycle', win_coord=None, grp_dim='unique_cycle')</code>","text":"<p>Apply functions of windowing to data x: DataArray windows: <code>windows</code> for <code>windowed_xarray</code> win_grp_idx: <code>win_grp_idx</code> for <code>group_windows</code> dim: dimension along which to divide win_dim: dimension for different windows win_coord: pandas Index object of <code>win_dim</code> coordinates grp_dim: dimension along which to stack average of window groups.     If None or empty or False, do not calculate average. Return: data returned by three functions,     <code>windowed_xarray</code>, <code>group_windows</code>, <code>average_group_windows</code></p> Source code in <code>bmtool/analysis/lfp.py</code> <pre><code>def get_windowed_data(\n    x, windows, win_grp_idx, dim=\"time\", win_dim=\"cycle\", win_coord=None, grp_dim=\"unique_cycle\"\n):\n    \"\"\"Apply functions of windowing to data\n    x: DataArray\n    windows: `windows` for `windowed_xarray`\n    win_grp_idx: `win_grp_idx` for `group_windows`\n    dim: dimension along which to divide\n    win_dim: dimension for different windows\n    win_coord: pandas Index object of `win_dim` coordinates\n    grp_dim: dimension along which to stack average of window groups.\n        If None or empty or False, do not calculate average.\n    Return: data returned by three functions,\n        `windowed_xarray`, `group_windows`, `average_group_windows`\n    \"\"\"\n    x_win = windowed_xarray(x, windows, dim=dim, new_coord_name=win_dim, new_coord=win_coord)\n    x_win_onff = group_windows(x_win, win_grp_idx, win_dim=win_dim)\n    if grp_dim:\n        x_win_avg = [average_group_windows(x, win_dim=win_dim, grp_dim=grp_dim) for x in x_win_onff]\n    else:\n        x_win_avg = None\n    return x_win, x_win_onff, x_win_avg\n</code></pre>"},{"location":"api/analysis/netcon_reports/","title":"Network Connectivity Reports","text":"<p>The <code>netcon_reports</code> module provides tools for analyzing and reporting network connectivity statistics.</p>"},{"location":"api/analysis/spikes/","title":"Spike Analysis","text":"<p>The <code>spikes</code> module provides functions for loading and analyzing spike data from simulations.</p>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes.load_spikes_to_df","title":"<code>bmtool.analysis.spikes.load_spikes_to_df(spike_file, network_name, sort=True, config=None, groupby='pop_name')</code>","text":"<p>Load spike data from an HDF5 file into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spike_file</code> <code>str</code> <p>Path to the HDF5 file containing spike data</p> required <code>network_name</code> <code>str</code> <p>The name of the network within the HDF5 file from which to load spike data</p> required <code>sort</code> <code>bool</code> <p>Whether to sort the DataFrame by 'timestamps' (default: True)</p> <code>True</code> <code>config</code> <code>str</code> <p>Path to configuration file to label the cell type of each spike (default: None)</p> <code>None</code> <code>groupby</code> <code>Union[str, List[str]]</code> <p>The column(s) to group by (default: 'pop_name')</p> <code>'pop_name'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame containing 'node_ids' and 'timestamps' columns from the spike data, with additional columns if a config file is provided</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = load_spikes_to_df(\"spikes.h5\", \"cortex\")\n&gt;&gt;&gt; df = load_spikes_to_df(\"spikes.h5\", \"cortex\", config=\"config.json\", groupby=[\"pop_name\", \"model_type\"])\n</code></pre> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def load_spikes_to_df(\n    spike_file: str,\n    network_name: str,\n    sort: bool = True,\n    config: str = None,\n    groupby: Union[str, List[str]] = \"pop_name\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load spike data from an HDF5 file into a pandas DataFrame.\n\n    Parameters\n    ----------\n    spike_file : str\n        Path to the HDF5 file containing spike data\n    network_name : str\n        The name of the network within the HDF5 file from which to load spike data\n    sort : bool, optional\n        Whether to sort the DataFrame by 'timestamps' (default: True)\n    config : str, optional\n        Path to configuration file to label the cell type of each spike (default: None)\n    groupby : Union[str, List[str]], optional\n        The column(s) to group by (default: 'pop_name')\n\n    Returns\n    -------\n    pd.DataFrame\n        A pandas DataFrame containing 'node_ids' and 'timestamps' columns from the spike data,\n        with additional columns if a config file is provided\n\n    Examples\n    --------\n    &gt;&gt;&gt; df = load_spikes_to_df(\"spikes.h5\", \"cortex\")\n    &gt;&gt;&gt; df = load_spikes_to_df(\"spikes.h5\", \"cortex\", config=\"config.json\", groupby=[\"pop_name\", \"model_type\"])\n    \"\"\"\n    with h5py.File(spike_file) as f:\n        spikes_df = pd.DataFrame(\n            {\n                \"node_ids\": f[\"spikes\"][network_name][\"node_ids\"],\n                \"timestamps\": f[\"spikes\"][network_name][\"timestamps\"],\n            }\n        )\n\n        if sort:\n            spikes_df.sort_values(by=\"timestamps\", inplace=True, ignore_index=True)\n\n        if config:\n            nodes = load_nodes_from_config(config)\n            nodes = nodes[network_name]\n\n            # Convert single string to a list for uniform handling\n            if isinstance(groupby, str):\n                groupby = [groupby]\n\n            # Ensure all requested columns exist\n            missing_cols = [col for col in groupby if col not in nodes.columns]\n            if missing_cols:\n                raise KeyError(f\"Columns {missing_cols} not found in nodes DataFrame.\")\n\n            spikes_df = spikes_df.merge(\n                nodes[groupby], left_on=\"node_ids\", right_index=True, how=\"left\"\n            )\n\n    return spikes_df\n</code></pre>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes.compute_firing_rate_stats","title":"<code>bmtool.analysis.spikes.compute_firing_rate_stats(df, groupby='pop_name', start_time=None, stop_time=None)</code>","text":"<p>Computes the firing rates of individual nodes and the mean and standard deviation of firing rates per group.</p> <p>Args:     df (pd.DataFrame): Dataframe containing spike timestamps and node IDs.     groupby (str or list of str, optional): Column(s) to group by (e.g., 'pop_name' or ['pop_name', 'layer']).     start_time (float, optional): Start time for the analysis window. Defaults to the minimum timestamp in the data.     stop_time (float, optional): Stop time for the analysis window. Defaults to the maximum timestamp in the data.</p> <p>Returns:     Tuple[pd.DataFrame, pd.DataFrame]:         - The first DataFrame (<code>pop_stats</code>) contains the mean and standard deviation of firing rates per group.         - The second DataFrame (<code>individual_stats</code>) contains the firing rate of each individual node.</p> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def compute_firing_rate_stats(\n    df: pd.DataFrame,\n    groupby: Union[str, List[str]] = \"pop_name\",\n    start_time: float = None,\n    stop_time: float = None,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Computes the firing rates of individual nodes and the mean and standard deviation of firing rates per group.\n\n    Args:\n        df (pd.DataFrame): Dataframe containing spike timestamps and node IDs.\n        groupby (str or list of str, optional): Column(s) to group by (e.g., 'pop_name' or ['pop_name', 'layer']).\n        start_time (float, optional): Start time for the analysis window. Defaults to the minimum timestamp in the data.\n        stop_time (float, optional): Stop time for the analysis window. Defaults to the maximum timestamp in the data.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]:\n            - The first DataFrame (`pop_stats`) contains the mean and standard deviation of firing rates per group.\n            - The second DataFrame (`individual_stats`) contains the firing rate of each individual node.\n    \"\"\"\n\n    # Ensure groupby is a list\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    # Ensure all columns exist in the dataframe\n    for col in groupby:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in dataframe.\")\n\n    # Filter dataframe based on start/stop time\n    if start_time is not None:\n        df = df[df[\"timestamps\"] &gt;= start_time]\n    if stop_time is not None:\n        df = df[df[\"timestamps\"] &lt;= stop_time]\n\n    # Compute total duration for firing rate calculation\n    if start_time is None:\n        min_time = df[\"timestamps\"].min()\n    else:\n        min_time = start_time\n\n    if stop_time is None:\n        max_time = df[\"timestamps\"].max()\n    else:\n        max_time = stop_time\n\n    duration = max_time - min_time  # Avoid division by zero\n\n    if duration &lt;= 0:\n        raise ValueError(\"Invalid time window: Stop time must be greater than start time.\")\n\n    # Compute firing rate for each node\n\n    # Compute spike counts per node\n    spike_counts = df[\"node_ids\"].value_counts().reset_index()\n    spike_counts.columns = [\"node_ids\", \"spike_count\"]  # Rename columns\n\n    # Merge with original dataframe to get corresponding labels (e.g., 'pop_name')\n    spike_counts = spike_counts.merge(\n        df[[\"node_ids\"] + groupby].drop_duplicates(), on=\"node_ids\", how=\"left\"\n    )\n\n    # Compute firing rate\n    spike_counts[\"firing_rate\"] = spike_counts[\"spike_count\"] / duration * 1000  # scale to Hz\n    indivdual_stats = spike_counts\n\n    # Compute mean and standard deviation per group\n    pop_stats = spike_counts.groupby(groupby)[\"firing_rate\"].agg([\"mean\", \"std\"]).reset_index()\n\n    # Rename columns\n    pop_stats.rename(columns={\"mean\": \"firing_rate_mean\", \"std\": \"firing_rate_std\"}, inplace=True)\n\n    return pop_stats, indivdual_stats\n</code></pre>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes._pop_spike_rate","title":"<code>bmtool.analysis.spikes._pop_spike_rate(spike_times, time=None, time_points=None, frequency=False)</code>","text":"<p>Calculate the spike count or frequency histogram over specified time intervals.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>Union[ndarray, list]</code> <p>Array or list of spike times in milliseconds</p> required <code>time</code> <code>Optional[Tuple[float, float, float]]</code> <p>Tuple specifying (start, stop, step) in milliseconds. Used to create evenly spaced time points if <code>time_points</code> is not provided. Default is None.</p> <code>None</code> <code>time_points</code> <code>Optional[Union[ndarray, list]]</code> <p>Array or list of specific time points for binning. If provided, <code>time</code> is ignored. Default is None.</p> <code>None</code> <code>frequency</code> <code>bool</code> <p>If True, returns spike frequency in Hz; otherwise, returns spike count. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of spike counts or frequencies, depending on the <code>frequency</code> flag.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>time</code> and <code>time_points</code> are None.</p> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def _pop_spike_rate(\n    spike_times: Union[np.ndarray, list],\n    time: Optional[Tuple[float, float, float]] = None,\n    time_points: Optional[Union[np.ndarray, list]] = None,\n    frequency: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the spike count or frequency histogram over specified time intervals.\n\n    Parameters\n    ----------\n    spike_times : Union[np.ndarray, list]\n        Array or list of spike times in milliseconds\n    time : Optional[Tuple[float, float, float]], optional\n        Tuple specifying (start, stop, step) in milliseconds. Used to create evenly spaced time points\n        if `time_points` is not provided. Default is None.\n    time_points : Optional[Union[np.ndarray, list]], optional\n        Array or list of specific time points for binning. If provided, `time` is ignored. Default is None.\n    frequency : bool, optional\n        If True, returns spike frequency in Hz; otherwise, returns spike count. Default is False.\n\n    Returns\n    -------\n    np.ndarray\n        Array of spike counts or frequencies, depending on the `frequency` flag.\n\n    Raises\n    ------\n    ValueError\n        If both `time` and `time_points` are None.\n    \"\"\"\n    if time_points is None:\n        if time is None:\n            raise ValueError(\"Either `time` or `time_points` must be provided.\")\n        time_points = np.arange(*time)\n        dt = time[2]\n    else:\n        time_points = np.asarray(time_points).ravel()\n        dt = (time_points[-1] - time_points[0]) / (time_points.size - 1)\n\n    bins = np.append(time_points, time_points[-1] + dt)\n    spike_rate, _ = np.histogram(np.asarray(spike_times), bins)\n\n    if frequency:\n        spike_rate = 1000 / dt * spike_rate\n\n    return spike_rate\n</code></pre>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes.get_population_spike_rate","title":"<code>bmtool.analysis.spikes.get_population_spike_rate(spike_data, fs=400.0, t_start=0, t_stop=None, config=None, network_name=None, save=False, save_path=None, normalize=False, smooth_window=50, smooth_method='gaussian')</code>","text":"<p>Calculate the population spike rate for each population in the given spike data.</p> <p>Parameters:</p> Name Type Description Default <code>spike_data</code> <code>DataFrame</code> <p>A DataFrame containing spike data with columns 'pop_name', 'timestamps', and 'node_ids'</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz, which determines the time bin size for calculating the spike rate (default: 400.0)</p> <code>400.0</code> <code>t_start</code> <code>float</code> <p>Start time (in milliseconds) for spike rate calculation (default: 0)</p> <code>0</code> <code>t_stop</code> <code>Optional[float]</code> <p>Stop time (in milliseconds) for spike rate calculation. If None, defaults to the maximum timestamp in the data</p> <code>None</code> <code>config</code> <code>Optional[str]</code> <p>Path to a configuration file containing node information, used to determine the correct number of nodes per population. If None, node count is estimated from unique node spikes (default: None)</p> <code>None</code> <code>network_name</code> <code>Optional[str]</code> <p>Name of the network used in the configuration file, allowing selection of nodes for that network. Required if <code>config</code> is provided (default: None)</p> <code>None</code> <code>save</code> <code>bool</code> <p>Whether to save the calculated population spike rate to a file (default: False)</p> <code>False</code> <code>save_path</code> <code>Optional[str]</code> <p>Directory path where the file should be saved if <code>save</code> is True (default: None)</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the spike rates for each population to a range of [0, 1] (default: False)</p> <code>False</code> <code>smooth_window</code> <code>int</code> <p>Window size for smoothing in number of time bins (default: 50)</p> <code>50</code> <code>smooth_method</code> <code>str</code> <p>Smoothing method to use: 'gaussian', 'boxcar', or 'exponential' (default: 'gaussian')</p> <code>'gaussian'</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>An xarray DataArray containing the spike rates with dimensions of time, population, and type. The 'type' dimension includes 'raw' and 'smoothed' values. The DataArray includes sampling frequency (fs) as an attribute. If normalize is True, each population's spike rate is scaled to [0, 1].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>save</code> is True but <code>save_path</code> is not provided. If an invalid smooth_method is specified.</p> Notes <ul> <li>If <code>config</code> is None, the function assumes all cells in each population have fired at least once;   otherwise, the node count may be inaccurate.</li> <li>If normalization is enabled, each population's spike rate is scaled using Min-Max normalization.</li> <li>Smoothing is applied using scipy.ndimage's filters based on the specified method.</li> </ul> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def get_population_spike_rate(\n    spike_data: pd.DataFrame,\n    fs: float = 400.0,\n    t_start: float = 0,\n    t_stop: Optional[float] = None,\n    config: Optional[str] = None,\n    network_name: Optional[str] = None,\n    save: bool = False,\n    save_path: Optional[str] = None,\n    normalize: bool = False,\n    smooth_window: int = 50,  # Window size for smoothing (in time bins)\n    smooth_method: str = \"gaussian\",  # Smoothing method: 'gaussian', 'boxcar', or 'exponential'\n) -&gt; xr.DataArray:\n    \"\"\"\n    Calculate the population spike rate for each population in the given spike data.\n\n    Parameters\n    ----------\n    spike_data : pd.DataFrame\n        A DataFrame containing spike data with columns 'pop_name', 'timestamps', and 'node_ids'\n    fs : float, optional\n        Sampling frequency in Hz, which determines the time bin size for calculating the spike rate (default: 400.0)\n    t_start : float, optional\n        Start time (in milliseconds) for spike rate calculation (default: 0)\n    t_stop : Optional[float], optional\n        Stop time (in milliseconds) for spike rate calculation. If None, defaults to the maximum timestamp in the data\n    config : Optional[str], optional\n        Path to a configuration file containing node information, used to determine the correct number of nodes per population.\n        If None, node count is estimated from unique node spikes (default: None)\n    network_name : Optional[str], optional\n        Name of the network used in the configuration file, allowing selection of nodes for that network.\n        Required if `config` is provided (default: None)\n    save : bool, optional\n        Whether to save the calculated population spike rate to a file (default: False)\n    save_path : Optional[str], optional\n        Directory path where the file should be saved if `save` is True (default: None)\n    normalize : bool, optional\n        Whether to normalize the spike rates for each population to a range of [0, 1] (default: False)\n    smooth_window : int, optional\n        Window size for smoothing in number of time bins (default: 50)\n    smooth_method : str, optional\n        Smoothing method to use: 'gaussian', 'boxcar', or 'exponential' (default: 'gaussian')\n\n    Returns\n    -------\n    xr.DataArray\n        An xarray DataArray containing the spike rates with dimensions of time, population, and type.\n        The 'type' dimension includes 'raw' and 'smoothed' values.\n        The DataArray includes sampling frequency (fs) as an attribute.\n        If normalize is True, each population's spike rate is scaled to [0, 1].\n\n    Raises\n    ------\n    ValueError\n        If `save` is True but `save_path` is not provided.\n        If an invalid smooth_method is specified.\n\n    Notes\n    -----\n    - If `config` is None, the function assumes all cells in each population have fired at least once;\n      otherwise, the node count may be inaccurate.\n    - If normalization is enabled, each population's spike rate is scaled using Min-Max normalization.\n    - Smoothing is applied using scipy.ndimage's filters based on the specified method.\n    \"\"\"\n    import numpy as np\n    from scipy import ndimage\n\n    # Validate smoothing method\n    if smooth_method not in [\"gaussian\", \"boxcar\", \"exponential\"]:\n        raise ValueError(\n            f\"Invalid smooth_method: {smooth_method}. Choose from 'gaussian', 'boxcar', or 'exponential'.\"\n        )\n\n    pop_spikes = {}\n    node_number = {}\n\n    if config is None:\n        pass\n        # print(\n        #     \"Note: Node number is obtained by counting unique node spikes in the network.\\nIf the network did not run for a sufficient duration, or not all cells fired,\\nthen this count will not include all nodes so the firing rate will not be of the whole population!\"\n        # )\n        # print(\n        #     \"You can provide a config to calculate the correct amount of nodes! for a true population rate.\"\n        # )\n\n    if config:\n        if not network_name:\n            print(\n                \"Grabbing first network; specify a network name to ensure correct node population is selected.\"\n            )\n\n    # Get t_stop if not provided\n    if t_stop is None:\n        t_stop = spike_data[\"timestamps\"].max()\n\n    # Get population names and prepare data\n    populations = spike_data[\"pop_name\"].unique()\n    for pop_name in populations:\n        ps = spike_data[spike_data[\"pop_name\"] == pop_name]\n\n        if config:\n            nodes = load_nodes_from_config(config)\n            if network_name:\n                nodes = nodes[network_name]\n            else:\n                nodes = list(nodes.values())[0] if nodes else {}\n            nodes = nodes[nodes[\"pop_name\"] == pop_name]\n            node_number[pop_name] = nodes.index.nunique()\n\n        else:\n            node_number[pop_name] = ps[\"node_ids\"].nunique()\n\n        filtered_spikes = spike_data[\n            (spike_data[\"pop_name\"] == pop_name)\n            &amp; (spike_data[\"timestamps\"] &gt; t_start)\n            &amp; (spike_data[\"timestamps\"] &lt; t_stop)\n        ]\n        pop_spikes[pop_name] = filtered_spikes\n\n    # Calculate time points\n    time = np.arange(t_start, t_stop, 1000 / fs)  # Convert sampling frequency to time steps\n\n    # Calculate spike rates for each population\n    spike_rates = []\n    for p in populations:\n        raw_rate = _pop_spike_rate(pop_spikes[p][\"timestamps\"], (t_start, t_stop, 1000 / fs))\n        rate = fs / node_number[p] * raw_rate\n        spike_rates.append(rate)\n\n    spike_rates_array = np.array(spike_rates).T  # Transpose to have time as first dimension\n\n    # Calculate smoothed version for each population\n    smoothed_rates = []\n\n    for i in range(spike_rates_array.shape[1]):\n        pop_rate = spike_rates_array[:, i]\n\n        if smooth_method == \"gaussian\":\n            # Gaussian smoothing (sigma is approximately window/6 for a Gaussian filter)\n            sigma = smooth_window / 6\n            smoothed_pop_rate = ndimage.gaussian_filter1d(pop_rate, sigma=sigma)\n        elif smooth_method == \"boxcar\":\n            # Boxcar/uniform smoothing\n            kernel = np.ones(smooth_window) / smooth_window\n            smoothed_pop_rate = ndimage.convolve1d(pop_rate, kernel, mode=\"nearest\")\n        elif smooth_method == \"exponential\":\n            # Exponential smoothing\n            alpha = 2 / (smooth_window + 1)  # Equivalent to window size in exponential smoothing\n            smoothed_pop_rate = np.zeros_like(pop_rate)\n            smoothed_pop_rate[0] = pop_rate[0]\n            for t in range(1, len(pop_rate)):\n                smoothed_pop_rate[t] = alpha * pop_rate[t] + (1 - alpha) * smoothed_pop_rate[t - 1]\n\n        smoothed_rates.append(smoothed_pop_rate)\n\n    smoothed_rates_array = np.array(smoothed_rates).T  # Transpose to have time as first dimension\n\n    # Stack raw and smoothed data\n    combined_data = np.stack([spike_rates_array, smoothed_rates_array], axis=2)\n\n    # Create DataArray with the additional 'type' dimension\n    spike_rate_array = xr.DataArray(\n        combined_data,\n        coords={\"time\": time, \"population\": populations, \"type\": [\"raw\", \"smoothed\"]},\n        dims=[\"time\", \"population\", \"type\"],\n        attrs={\n            \"fs\": fs,\n            \"normalized\": False,\n            \"smooth_method\": smooth_method,\n            \"smooth_window\": smooth_window,\n        },\n    )\n\n    # Normalize if requested\n    if normalize:\n        # Apply normalization for each population and each type (raw/smoothed)\n        for pop_idx in range(len(populations)):\n            for type_idx, type_name in enumerate([\"raw\", \"smoothed\"]):\n                pop_data = spike_rate_array.sel(population=populations[pop_idx], type=type_name)\n                min_val = pop_data.min(dim=\"time\")\n                max_val = pop_data.max(dim=\"time\")\n\n                # Handle case where min == max (constant signal)\n                if max_val != min_val:\n                    spike_rate_array.loc[:, populations[pop_idx], type_name] = (\n                        pop_data - min_val\n                    ) / (max_val - min_val)\n\n        spike_rate_array.attrs[\"normalized\"] = True\n\n    # Save if requested\n    if save:\n        if save_path is None:\n            raise ValueError(\"save_path must be provided if save is True.\")\n\n        os.makedirs(save_path, exist_ok=True)\n        save_file = os.path.join(save_path, \"spike_rate.h5\")\n        spike_rate_array.to_netcdf(save_file)\n\n    return spike_rate_array\n</code></pre>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes.average_spike_rate_over_windows","title":"<code>bmtool.analysis.spikes.average_spike_rate_over_windows(spike_rate, windows)</code>","text":"<p>Calculate the average spike rate over multiple time windows.</p> <p>Parameters:</p> Name Type Description Default <code>spike_rate</code> <code>DataArray</code> <p>The spike rate data array with dimensions (time, population, type) where 'type' can be 'raw' or 'smoothed'</p> required <code>windows</code> <code>List[Tuple[float, float]]</code> <p>List of (start, end) times in milliseconds defining the windows to average over</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>Averaged spike rate with time normalized to start at 0, preserving all original dimensions (time, population, type)</p> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def average_spike_rate_over_windows(\n    spike_rate: xr.DataArray, windows: List[Tuple[float, float]]\n) -&gt; xr.DataArray:\n    \"\"\"\n    Calculate the average spike rate over multiple time windows.\n\n    Parameters\n    ----------\n    spike_rate : xr.DataArray\n        The spike rate data array with dimensions (time, population, type)\n        where 'type' can be 'raw' or 'smoothed'\n    windows : List[Tuple[float, float]]\n        List of (start, end) times in milliseconds defining the windows to average over\n\n    Returns\n    -------\n    xr.DataArray\n        Averaged spike rate with time normalized to start at 0,\n        preserving all original dimensions (time, population, type)\n    \"\"\"\n    # Check if the DataArray has a 'type' dimension (compatible with new format)\n    has_type_dim = \"type\" in spike_rate.dims\n\n    # Initialize list to store data from each window\n    window_data = []\n\n    # Get data for each window\n    for start, end in windows:\n        # Select data points within the window\n        window = spike_rate.sel(time=slice(start, end))\n\n        # Normalize time to start at 0 for this window\n        window = window.assign_coords(time=window.time - start)\n        window_data.append(window)\n\n    # Align and average windows\n    # First window determines the time coordinates\n    aligned_data = xr.concat(window_data, dim=\"window\")\n    averaged_data = aligned_data.mean(dim=\"window\")\n\n    # Create new DataArray with the averaged data\n    if has_type_dim:\n        # Create result with time, population, and type dimensions\n        result = xr.DataArray(\n            averaged_data.values,\n            coords={\n                \"time\": averaged_data.time.values,\n                \"population\": averaged_data.population,\n                \"type\": averaged_data.type,\n            },\n            dims=[\"time\", \"population\", \"type\"],\n        )\n    else:\n        # Handle older format without 'type' dimension (for backward compatibility)\n        result = xr.DataArray(\n            averaged_data.values,\n            coords={\"time\": averaged_data.time.values, \"population\": averaged_data.population},\n            dims=[\"time\", \"population\"],\n        )\n\n    # Preserve attributes\n    result.attrs = spike_rate.attrs\n\n    return result\n</code></pre>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes.compare_firing_over_times","title":"<code>bmtool.analysis.spikes.compare_firing_over_times(spike_df, group_by, time_window_1, time_window_2)</code>","text":"<p>Compares the firing rates of a population during two different time windows and performs a statistical test to determine if there is a significant difference.</p> <p>Parameters:</p> Name Type Description Default <code>spike_df</code> <code>DataFrame</code> <p>DataFrame containing spike data with columns for timestamps, node_ids, and grouping variable</p> required <code>group_by</code> <code>str</code> <p>Column name to group spikes by (e.g., 'pop_name')</p> required <code>time_window_1</code> <code>List[float]</code> <p>First time window as [start, stop] in milliseconds</p> required <code>time_window_2</code> <code>List[float]</code> <p>Second time window as [start, stop] in milliseconds</p> required <p>Returns:</p> Type Description <code>None</code> <p>Results are printed to the console</p> Notes <p>Uses Mann-Whitney U test (non-parametric) to compare firing rates between the two windows</p> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def compare_firing_over_times(\n    spike_df: pd.DataFrame, group_by: str, time_window_1: List[float], time_window_2: List[float]\n) -&gt; None:\n    \"\"\"\n    Compares the firing rates of a population during two different time windows and performs\n    a statistical test to determine if there is a significant difference.\n\n    Parameters\n    ----------\n    spike_df : pd.DataFrame\n        DataFrame containing spike data with columns for timestamps, node_ids, and grouping variable\n    group_by : str\n        Column name to group spikes by (e.g., 'pop_name')\n    time_window_1 : List[float]\n        First time window as [start, stop] in milliseconds\n    time_window_2 : List[float]\n        Second time window as [start, stop] in milliseconds\n\n    Returns\n    -------\n    None\n        Results are printed to the console\n\n    Notes\n    -----\n    Uses Mann-Whitney U test (non-parametric) to compare firing rates between the two windows\n    \"\"\"\n    # Filter spikes for the population of interest\n    for pop_name in spike_df[group_by].unique():\n        print(f\"Population: {pop_name}\")\n        pop_spikes = spike_df[spike_df[group_by] == pop_name]\n\n        # Filter by time windows\n        pop_spikes_1 = pop_spikes[\n            (pop_spikes[\"timestamps\"] &gt;= time_window_1[0])\n            &amp; (pop_spikes[\"timestamps\"] &lt;= time_window_1[1])\n        ]\n        pop_spikes_2 = pop_spikes[\n            (pop_spikes[\"timestamps\"] &gt;= time_window_2[0])\n            &amp; (pop_spikes[\"timestamps\"] &lt;= time_window_2[1])\n        ]\n\n        # Get unique neuron IDs\n        unique_neurons = pop_spikes[\"node_ids\"].unique()\n\n        # Calculate firing rates per neuron for each time window in Hz\n        neuron_rates_1 = []\n        neuron_rates_2 = []\n\n        for neuron in unique_neurons:\n            # Count spikes for this neuron in each window\n            n_spikes_1 = len(pop_spikes_1[pop_spikes_1[\"node_ids\"] == neuron])\n            n_spikes_2 = len(pop_spikes_2[pop_spikes_2[\"node_ids\"] == neuron])\n\n            # Calculate firing rate in Hz (convert ms to seconds by dividing by 1000)\n            rate_1 = n_spikes_1 / ((time_window_1[1] - time_window_1[0]) / 1000)\n            rate_2 = n_spikes_2 / ((time_window_2[1] - time_window_2[0]) / 1000)\n\n            neuron_rates_1.append(rate_1)\n            neuron_rates_2.append(rate_2)\n\n        # Calculate average firing rates\n        avg_firing_rate_1 = np.mean(neuron_rates_1) if neuron_rates_1 else 0\n        avg_firing_rate_2 = np.mean(neuron_rates_2) if neuron_rates_2 else 0\n\n        # Perform Mann-Whitney U test\n        # Handle the case when one or both arrays are empty\n        if len(neuron_rates_1) &gt; 0 and len(neuron_rates_2) &gt; 0:\n            u_stat, p_val = mannwhitneyu(neuron_rates_1, neuron_rates_2, alternative=\"two-sided\")\n        else:\n            u_stat, p_val = np.nan, np.nan\n\n        print(f\"    Average firing rate in window 1: {avg_firing_rate_1:.2f} Hz\")\n        print(f\"    Average firing rate in window 2: {avg_firing_rate_2:.2f} Hz\")\n        print(f\"    U-statistic: {u_stat:.2f}\")\n        print(f\"    p-value: {p_val}\")\n        print(f\"    Significant difference (p&lt;0.05): {'Yes' if p_val &lt; 0.05 else 'No'}\")\n    return\n</code></pre>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes.find_bursting_cells","title":"<code>bmtool.analysis.spikes.find_bursting_cells(df, isi_threshold=10, burst_count_threshold=1)</code>","text":"<p>Finds bursting cells in a population based on a time difference threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing spike data with columns for timestamps, node_ids, and pop_name</p> required <code>isi_threshold</code> <code>float</code> <p>Time difference threshold in milliseconds to identify bursts</p> <code>10</code> <code>burst_count_threshold</code> <code>int</code> <p>Number of bursts required to identify a bursting cell</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with bursting cells renamed in their pop_name column</p> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def find_bursting_cells(\n    df: pd.DataFrame, isi_threshold: float = 10, burst_count_threshold: int = 1\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Finds bursting cells in a population based on a time difference threshold.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing spike data with columns for timestamps, node_ids, and pop_name\n    isi_threshold : float, optional\n        Time difference threshold in milliseconds to identify bursts\n    burst_count_threshold : int, optional\n        Number of bursts required to identify a bursting cell\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with bursting cells renamed in their pop_name column\n    \"\"\"\n    # Create a new DataFrame with the time differences\n    diff_df = df.copy()\n    diff_df[\"time_diff\"] = df.groupby(\"node_ids\")[\"timestamps\"].diff()\n\n    # Create a column indicating whether each time difference is a burst\n    diff_df[\"is_burst_instance\"] = diff_df[\"time_diff\"] &lt; isi_threshold\n\n    # Group by node_ids and check if any row has a burst instance\n    # check if there are enough bursts\n    burst_summary = diff_df.groupby(\"node_ids\")[\"is_burst_instance\"].sum() &gt;= burst_count_threshold\n\n    # Convert to a DataFrame with reset index\n    burst_cells = burst_summary.reset_index(name=\"is_burst\")\n\n    # merge with original df to get timestamps\n    burst_cells = pd.merge(burst_cells, df, on=\"node_ids\")\n\n    # Create a mask for burst cells that don't already have \"_bursters\" in their name\n    burst_mask = burst_cells[\"is_burst\"] &amp; ~burst_cells[\"pop_name\"].str.contains(\n        \"_bursters\", na=False\n    )\n\n    # Add \"_bursters\" suffix only to those cells\n    burst_cells.loc[burst_mask, \"pop_name\"] = burst_cells.loc[burst_mask, \"pop_name\"] + \"_bursters\"\n\n    for pop in sorted(burst_cells[\"pop_name\"].unique()):\n        print(\n            f\"Number of cells in {pop}: {burst_cells[burst_cells['pop_name'] == pop]['node_ids'].nunique()}\"\n        )\n\n    return burst_cells\n</code></pre>"},{"location":"api/analysis/spikes/#bmtool.analysis.spikes.find_highest_firing_cells","title":"<code>bmtool.analysis.spikes.find_highest_firing_cells(df, upper_quantile, groupby='pop_name')</code>","text":"<p>Identifies and returns spikes from cells with firing rates above a specified upper quantile, grouped by a population label.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing spike data with at least the following columns: - 'timestamps': Time of each spike event - 'node_ids': Identifier for each neuron - groupby (e.g., 'pop_name'): Population labels or grouping identifiers for neurons</p> required <code>upper_quantile</code> <code>float</code> <p>The upper quantile threshold (between 0 and 1). Cells with firing rates in the top (1 - upper_quantile) fraction are selected. For example, upper_quantile=0.8 selects the top 20% of high-firing cells.</p> required <code>groupby</code> <code>str</code> <p>The column name used to group neurons by population. Default is 'pop_name'.</p> <code>'pop_name'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing only the spikes from the high-firing cells across all groupbys.</p> Source code in <code>bmtool/analysis/spikes.py</code> <pre><code>def find_highest_firing_cells(\n    df: pd.DataFrame, upper_quantile: float, groupby: str = \"pop_name\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Identifies and returns spikes from cells with firing rates above a specified upper quantile,\n    grouped by a population label.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing spike data with at least the following columns:\n        - 'timestamps': Time of each spike event\n        - 'node_ids': Identifier for each neuron\n        - groupby (e.g., 'pop_name'): Population labels or grouping identifiers for neurons\n\n    upper_quantile : float\n        The upper quantile threshold (between 0 and 1).\n        Cells with firing rates in the top (1 - upper_quantile) fraction are selected.\n        For example, upper_quantile=0.8 selects the top 20% of high-firing cells.\n\n    groupby : str, optional\n        The column name used to group neurons by population. Default is 'pop_name'.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing only the spikes from the high-firing cells across all groupbys.\n    \"\"\"\n    if upper_quantile == 0:\n        return df\n    df_list = []\n    for pop in df[groupby].unique():\n        pop_df = df[df[groupby] == pop]\n        _, pop_fr = compute_firing_rate_stats(pop_df, groupby=groupby)\n\n        # Identify high firing cells\n        threshold = pop_fr[\"firing_rate\"].quantile(upper_quantile)\n        high_firing_cells = pop_fr[pop_fr[\"firing_rate\"] &gt;= threshold][\"node_ids\"]\n\n        # Filter spikes for high firing cells\n        pop_spikes = pop_df[pop_df[\"node_ids\"].isin(high_firing_cells)]\n        df_list.append(pop_spikes)\n\n    # Combine all high firing spikes into one DataFrame\n    result_df = pd.concat(df_list, ignore_index=True)\n    return result_df\n</code></pre>"},{"location":"api/bmplot/connections/","title":"Network Connections Plotting API","text":""},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.is_notebook","title":"<code>bmtool.bmplot.connections.is_notebook()</code>","text":"<p>Detect if code is running in a Jupyter notebook environment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if running in a Jupyter notebook, False otherwise.</p> Notes <p>This is used to determine whether to call plt.show() explicitly or rely on Jupyter's automatic display functionality.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; if is_notebook():\n...     plt.show()\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def is_notebook() -&gt; bool:\n    \"\"\"\n    Detect if code is running in a Jupyter notebook environment.\n\n    Returns\n    -------\n    bool\n        True if running in a Jupyter notebook, False otherwise.\n\n    Notes\n    -----\n    This is used to determine whether to call plt.show() explicitly or\n    rely on Jupyter's automatic display functionality.\n\n    Examples\n    --------\n    &gt;&gt;&gt; if is_notebook():\n    ...     plt.show()\n    \"\"\"\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == \"ZMQInteractiveShell\":\n            return True  # Jupyter notebook or qtconsole\n        elif shell == \"TerminalInteractiveShell\":\n            return False  # Terminal running IPython\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False  # Probably standard Python interpreter\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.total_connection_matrix","title":"<code>bmtool.bmplot.connections.total_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, synaptic_info='0', include_gap=True)</code>","text":"<p>Generate a plot displaying total connections or other synaptic statistics.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network names to use as sources.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network names to use as targets.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifiers to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifiers to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, don't display population name before sid or tid in the plot. Default is False.</p> <code>False</code> <code>synaptic_info</code> <code>str</code> <p>Type of information to display. Options: - '0': Total connections (default) - '1': Mean and standard deviation of connections - '2': All synapse .mod files used - '3': All synapse .json files used</p> <code>'0'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions and chemical synapses in the analysis. If False, only include chemical synapses. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple of (Figure, Axes)</code> <p>The matplotlib Figure and Axes objects for further customization or saving.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; total_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     title='PN to LN Connections'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def total_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    synaptic_info: str = \"0\",\n    include_gap: bool = True,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generate a plot displaying total connections or other synaptic statistics.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network names to use as sources.\n    targets : str, optional\n        Comma-separated string of network names to use as targets.\n    sids : str, optional\n        Comma-separated string of source node identifiers to filter.\n    tids : str, optional\n        Comma-separated string of target node identifiers to filter.\n    no_prepend_pop : bool, optional\n        If True, don't display population name before sid or tid in the plot. Default is False.\n    synaptic_info : str, optional\n        Type of information to display. Options:\n        - '0': Total connections (default)\n        - '1': Mean and standard deviation of connections\n        - '2': All synapse .mod files used\n        - '3': All synapse .json files used\n    include_gap : bool, optional\n        If True, include gap junctions and chemical synapses in the analysis.\n        If False, only include chemical synapses. Default is True.\n\n    Returns\n    -------\n    tuple of (Figure, Axes)\n        The matplotlib Figure and Axes objects for further customization or saving.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; total_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     title='PN to LN Connections'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    text, num, source_labels, target_labels = util.connection_totals(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        synaptic_info=synaptic_info,\n        include_gap=include_gap,\n    )\n\n    if title is None or title == \"\":\n        title = \"Total Connections\"\n    if synaptic_info == \"1\":\n        title = \"Mean and Stdev # of Conn on Target\"\n    if synaptic_info == \"2\":\n        title = \"All Synapse .mod Files Used\"\n    if synaptic_info == \"3\":\n        title = \"All Synapse .json Files Used\"\n\n    return plot_connection_info(\n        text, num, source_labels, target_labels, title, syn_info=synaptic_info\n    )\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.percent_connection_matrix","title":"<code>bmtool.bmplot.connections.percent_connection_matrix(config, nodes=None, edges=None, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, method='total', include_gap=True, return_dict=False)</code>","text":"<p>Generates a plot showing the percent connectivity of a network.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid in the plot. Default is False.</p> <code>False</code> <code>method</code> <code>str</code> <p>Method for calculating percent connectivity. Options: 'total', 'uni', 'bi'. Default is 'total'.</p> <code>'total'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. If False, only include chemical synapses. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tuple[Figure, Axes], Dict]</code> <p>If return_dict=True, returns a dictionary of connection information. Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = percent_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     return_dict=True\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def percent_connection_matrix(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    method: str = \"total\",\n    include_gap: bool = True,\n    return_dict: bool = False,\n) -&gt; Union[Tuple[Any, Any], Dict]:\n    \"\"\"\n    Generates a plot showing the percent connectivity of a network.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid in the plot. Default is False.\n    method : str, optional\n        Method for calculating percent connectivity. Options: 'total', 'uni', 'bi'.\n        Default is 'total'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. If False, only include chemical synapses.\n        Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is False.\n\n    Returns\n    -------\n    Union[Tuple[Figure, Axes], Dict]\n        If return_dict=True, returns a dictionary of connection information.\n        Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = percent_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     return_dict=True\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    text, num, source_labels, target_labels = util.percent_connections(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        method=method,\n        include_gap=include_gap,\n    )\n    if title is None or title == \"\":\n        title = \"Percent Connectivity\"\n\n    if return_dict:\n        result_dict = plot_connection_info(\n            text, num, source_labels, target_labels, title, return_dict=return_dict\n        )\n        return result_dict\n    else:\n        return plot_connection_info(text, num, source_labels, target_labels, title)\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.probability_connection_matrix","title":"<code>bmtool.bmplot.connections.probability_connection_matrix(config, nodes=None, edges=None, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, dist_X=True, dist_Y=True, dist_Z=True, bins=8, line_plot=False, verbose=False, include_gap=True)</code>","text":"<p>Generates probability graphs showing connectivity as a function of distance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> required <code>dist_X</code> <code>bool</code> <p>If True, include X distance in calculations. Default is True.</p> <code>True</code> <code>dist_Y</code> <code>bool</code> <p>If True, include Y distance in calculations. Default is True.</p> <code>True</code> <code>dist_Z</code> <code>bool</code> <p>If True, include Z distance in calculations. Default is True.</p> <code>True</code> <code>bins</code> <code>int</code> <p>Number of distance bins for the probability calculation. Default is 8.</p> <code>8</code> <code>line_plot</code> <code>bool</code> <p>If True, plot lines instead of bars. Default is False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, print debugging information. Default is False.</p> <code>False</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> Notes <p>This function needs model_template to be defined to work properly.</p> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def probability_connection_matrix(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    dist_X: bool = True,\n    dist_Y: bool = True,\n    dist_Z: bool = True,\n    bins: int = 8,\n    line_plot: bool = False,\n    verbose: bool = False,\n    include_gap: bool = True,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generates probability graphs showing connectivity as a function of distance.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    dist_X : bool, optional\n        If True, include X distance in calculations. Default is True.\n    dist_Y : bool, optional\n        If True, include Y distance in calculations. Default is True.\n    dist_Z : bool, optional\n        If True, include Z distance in calculations. Default is True.\n    bins : int, optional\n        Number of distance bins for the probability calculation. Default is 8.\n    line_plot : bool, optional\n        If True, plot lines instead of bars. Default is False.\n    verbose : bool, optional\n        If True, print debugging information. Default is False.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Notes\n    -----\n    This function needs model_template to be defined to work properly.\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    throwaway, data, source_labels, target_labels = util.connection_probabilities(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        dist_X=dist_X,\n        dist_Y=dist_Y,\n        dist_Z=dist_Z,\n        num_bins=bins,\n        include_gap=include_gap,\n    )\n    if not data.any():\n        return\n    if data[0][0] == -1:\n        return\n    # plot_connection_info(data,source_labels,target_labels,title, save_file=save_file)\n\n    # plt.clf()# clears previous plots\n    np.seterr(divide=\"ignore\", invalid=\"ignore\")\n    num_src, num_tar = data.shape\n    fig, axes = plt.subplots(nrows=num_src, ncols=num_tar, figsize=(12, 12))\n    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n\n    for x in range(num_src):\n        for y in range(num_tar):\n            ns = data[x][y][\"ns\"]\n            bins_data = data[x][y][\"bins\"]\n\n            XX = bins_data[:-1]\n            YY = ns[0] / ns[1]\n\n            if line_plot:\n                axes[x, y].plot(XX, YY)\n            else:\n                axes[x, y].bar(XX, YY)\n\n            if x == num_src - 1:\n                axes[x, y].set_xlabel(target_labels[y])\n            if y == 0:\n                axes[x, y].set_ylabel(source_labels[x])\n\n            if verbose:\n                print(\"Source: [\" + source_labels[x] + \"] | Target: [\" + target_labels[y] + \"]\")\n                print(\"X:\")\n                print(XX)\n                print(\"Y:\")\n                print(YY)\n\n    tt = \"Distance Probability Matrix\"\n    if title:\n        tt = title\n    st = fig.suptitle(tt, fontsize=14)\n    fig.text(0.5, 0.04, \"Target\", ha=\"center\")\n    fig.text(0.04, 0.5, \"Source\", va=\"center\", rotation=\"vertical\")\n\n    return fig, axes\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.convergence_connection_matrix","title":"<code>bmtool.bmplot.connections.convergence_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, convergence=True, method='mean+std', include_gap=True, return_dict=None)</code>","text":"<p>Generates connection plot displaying synaptic convergence data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> required <code>convergence</code> <code>bool</code> <p>If True, compute convergence; if False, compute divergence. Default is True.</p> <code>True</code> <code>method</code> <code>str</code> <p>Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'. Default is 'mean+std'.</p> <code>'mean+std'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Figure, Axes], Dict, None]</code> <p>If return_dict=True, returns a dictionary of connection information. Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = convergence_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='mean+std'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def convergence_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    convergence: bool = True,\n    method: str = \"mean+std\",\n    include_gap: bool = True,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[Tuple[Any, Any], Dict, None]:\n    \"\"\"\n    Generates connection plot displaying synaptic convergence data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    convergence : bool, optional\n        If True, compute convergence; if False, compute divergence. Default is True.\n    method : str, optional\n        Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'.\n        Default is 'mean+std'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is None.\n\n    Returns\n    -------\n    Union[Tuple[Figure, Axes], Dict, None]\n        If return_dict=True, returns a dictionary of connection information.\n        Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = convergence_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='mean+std'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    return divergence_connection_matrix(\n        config,\n        title,\n        sources,\n        targets,\n        sids,\n        tids,\n        no_prepend_pop,\n        convergence,\n        method,\n        include_gap=include_gap,\n        return_dict=return_dict,\n    )\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.divergence_connection_matrix","title":"<code>bmtool.bmplot.connections.divergence_connection_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, convergence=False, method='mean+std', include_gap=True, return_dict=None)</code>","text":"<p>Generates connection plot displaying synaptic divergence data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> required <code>convergence</code> <code>bool</code> <p>If True, compute convergence; if False, compute divergence. Default is False.</p> <code>False</code> <code>method</code> <code>str</code> <p>Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'. Default is 'mean+std'.</p> <code>'mean+std'</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <code>return_dict</code> <code>bool</code> <p>If True, return connection information as a dictionary. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Figure, Axes], Dict, None]</code> <p>If return_dict=True, returns a dictionary of connection information. Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined or sources/targets are not defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = divergence_connection_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='mean+std'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def divergence_connection_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    convergence: bool = False,\n    method: str = \"mean+std\",\n    include_gap: bool = True,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[Tuple[Any, Any], Dict, None]:\n    \"\"\"\n    Generates connection plot displaying synaptic divergence data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    convergence : bool, optional\n        If True, compute convergence; if False, compute divergence. Default is False.\n    method : str, optional\n        Statistical method for display. Options: 'mean', 'min', 'max', 'stdev', 'mean+std'.\n        Default is 'mean+std'.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n    return_dict : bool, optional\n        If True, return connection information as a dictionary. Default is None.\n\n    Returns\n    -------\n    Union[Tuple[Figure, Axes], Dict, None]\n        If return_dict=True, returns a dictionary of connection information.\n        Otherwise, returns a tuple of (Figure, Axes) for further customization or saving.\n\n    Raises\n    ------\n    Exception\n        If config is not defined or sources/targets are not defined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = divergence_connection_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='mean+std'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    syn_info, data, source_labels, target_labels = util.connection_divergence(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        convergence=convergence,\n        method=method,\n        include_gap=include_gap,\n    )\n\n    # data, labels = util.connection_divergence_average(config=config,nodes=nodes,edges=edges,populations=populations)\n\n    if title is None or title == \"\":\n        if method == \"min\":\n            title = \"Minimum \"\n        elif method == \"max\":\n            title = \"Maximum \"\n        elif method == \"std\":\n            title = \"Standard Deviation \"\n        elif method == \"mean\":\n            title = \"Mean \"\n        else:\n            title = \"Mean + Std \"\n\n        if convergence:\n            title = title + \"Synaptic Convergence\"\n        else:\n            title = title + \"Synaptic Divergence\"\n    if return_dict:\n        result_dict = plot_connection_info(\n            syn_info,\n            data,\n            source_labels,\n            target_labels,\n            title,\n            return_dict=return_dict,\n        )\n        return result_dict\n    else:\n        return plot_connection_info(\n            syn_info, data, source_labels, target_labels, title\n        )\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.gap_junction_matrix","title":"<code>bmtool.bmplot.connections.gap_junction_matrix(config, title=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=False, method='convergence')</code>","text":"<p>Generates connection plot displaying gap junction data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>title</code> <code>str</code> <p>Title for the plot. If None, a default title will be used.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not displayed before sid or tid. Default is False.</p> <code>False</code> <code>save_file</code> <code>str</code> <p>Path to save the plot. If None, plot is not saved.</p> required <code>method</code> <code>str</code> <p>Method for computing gap junction statistics. Options: 'convergence', 'percent'. Default is 'convergence'.</p> <code>'convergence'</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If config is not defined, sources/targets are not defined, or method is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gap_junction_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     method='convergence'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def gap_junction_matrix(\n    config: str,\n    title: Optional[str] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = False,\n    method: str = \"convergence\",\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generates connection plot displaying gap junction data.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    title : str, optional\n        Title for the plot. If None, a default title will be used.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter.\n    no_prepend_pop : bool, optional\n        If True, population name is not displayed before sid or tid. Default is False.\n    save_file : str, optional\n        Path to save the plot. If None, plot is not saved.\n    method : str, optional\n        Method for computing gap junction statistics. Options: 'convergence', 'percent'.\n        Default is 'convergence'.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If config is not defined, sources/targets are not defined, or method is invalid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; gap_junction_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     method='convergence'\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    if method != \"convergence\" and method != \"percent\":\n        raise Exception(\"type must be 'convergence' or 'percent'\")\n    sources = sources.split(\",\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n    syn_info, data, source_labels, target_labels = util.gap_junction_connections(\n        config=config,\n        nodes=None,\n        edges=None,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        method=method,\n    )\n\n    def filter_rows(\n        syn_info: np.ndarray,\n        data: np.ndarray,\n        source_labels: List,\n        target_labels: List,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, List]:\n        \"\"\"\n        Filters out rows in a connectivity matrix that contain only NaN or zero values.\n\n        This function is used to clean up connection matrices by removing rows that have\n        no meaningful data, which helps create more informative visualizations of network connectivity.\n\n        Parameters\n        ----------\n        syn_info : np.ndarray\n            Array containing synaptic information corresponding to the data matrix.\n        data : np.ndarray\n            2D matrix containing connectivity data with rows representing sources\n            and columns representing targets.\n        source_labels : list\n            List of labels for the source populations corresponding to rows in the data matrix.\n        target_labels : list\n            List of labels for the target populations corresponding to columns in the data matrix.\n\n        Returns\n        -------\n        tuple\n            A tuple containing (syn_info, data, source_labels, target_labels) with invalid rows removed.\n        \"\"\"\n        # Identify rows with all NaN or all zeros\n        valid_rows = ~np.all(np.isnan(data), axis=1) &amp; ~np.all(data == 0, axis=1)\n\n        # Filter rows based on valid_rows mask\n        new_syn_info = syn_info[valid_rows]\n        new_data = data[valid_rows]\n        new_source_labels = np.array(source_labels)[valid_rows]\n\n        return new_syn_info, new_data, new_source_labels, target_labels\n\n    def filter_rows_and_columns(\n        syn_info: np.ndarray,\n        data: np.ndarray,\n        source_labels: List,\n        target_labels: List,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, List, List]:\n        \"\"\"\n        Filters out both rows and columns in a connectivity matrix that contain only NaN or zero values.\n\n        This function performs a two-step filtering process: first removing rows with no data,\n        then transposing the matrix and removing columns with no data (by treating them as rows).\n        This creates a cleaner, more informative connectivity matrix visualization.\n\n        Parameters\n        ----------\n        syn_info : np.ndarray\n            Array containing synaptic information corresponding to the data matrix.\n        data : np.ndarray\n            2D matrix containing connectivity data with rows representing sources\n            and columns representing targets.\n        source_labels : list\n            List of labels for the source populations corresponding to rows in the data matrix.\n        target_labels : list\n            List of labels for the target populations corresponding to columns in the data matrix.\n\n        Returns\n        -------\n        tuple\n            A tuple containing (syn_info, data, source_labels, target_labels) with both\n            invalid rows and columns removed.\n        \"\"\"\n        # Filter rows first\n        syn_info, data, source_labels, target_labels = filter_rows(\n            syn_info, data, source_labels, target_labels\n        )\n\n        # Transpose data to filter columns\n        transposed_syn_info = np.transpose(syn_info)\n        transposed_data = np.transpose(data)\n        transposed_source_labels = target_labels\n        transposed_target_labels = source_labels\n\n        # Filter columns (by treating them as rows in transposed data)\n        (\n            transposed_syn_info,\n            transposed_data,\n            transposed_source_labels,\n            transposed_target_labels,\n        ) = filter_rows(\n            transposed_syn_info, transposed_data, transposed_source_labels, transposed_target_labels\n        )\n\n        # Transpose back to original orientation\n        filtered_syn_info = np.transpose(transposed_syn_info)\n        filtered_data = np.transpose(transposed_data)\n        filtered_source_labels = transposed_target_labels  # Back to original source_labels\n        filtered_target_labels = transposed_source_labels  # Back to original target_labels\n\n        return filtered_syn_info, filtered_data, filtered_source_labels, filtered_target_labels\n\n    syn_info, data, source_labels, target_labels = filter_rows_and_columns(\n        syn_info, data, source_labels, target_labels\n    )\n\n    if title is None or title == \"\":\n        title = \"Gap Junction\"\n        if method == \"convergence\":\n            title += \" Syn Convergence\"\n        elif method == \"percent\":\n            title += \" Percent Connectivity\"\n    return plot_connection_info(syn_info, data, source_labels, target_labels, title)\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.connection_histogram","title":"<code>bmtool.bmplot.connections.connection_histogram(config, nodes=None, edges=None, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=True, synaptic_info='0', source_cell=None, target_cell=None, include_gap=True)</code>","text":"<p>Generates histogram of the number of connections individual cells receive from another population.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data. If None, will be loaded from config.</p> <code>None</code> <code>edges</code> <code>DataFrame</code> <p>Pre-loaded edge data. If None, will be loaded from config.</p> <code>None</code> <code>sources</code> <code>str</code> <p>Comma-separated string of network name(s) to plot as sources.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated string of network name(s) to plot as targets.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated string of source node identifier(s) to filter by.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated string of target node identifier(s) to filter by.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population name is not prepended to sid or tid. Default is True.</p> <code>True</code> <code>synaptic_info</code> <code>str</code> <p>Type of synaptic information to display. Default is '0'.</p> <code>'0'</code> <code>source_cell</code> <code>str</code> <p>Specific source cell type to plot connections from.</p> <code>None</code> <code>target_cell</code> <code>str</code> <p>Specific target cell type to plot connections onto.</p> <code>None</code> <code>include_gap</code> <code>bool</code> <p>If True, include gap junctions in analysis. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(matplotlib.figure.Figure, matplotlib.axes.Axes) containing the histogram.</p> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def connection_histogram(\n    config: str,\n    nodes: Optional[pd.DataFrame] = None,\n    edges: Optional[pd.DataFrame] = None,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: bool = True,\n    synaptic_info: str = \"0\",\n    source_cell: Optional[str] = None,\n    target_cell: Optional[str] = None,\n    include_gap: bool = True,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generates histogram of the number of connections individual cells receive from another population.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data. If None, will be loaded from config.\n    edges : pd.DataFrame, optional\n        Pre-loaded edge data. If None, will be loaded from config.\n    sources : str, optional\n        Comma-separated string of network name(s) to plot as sources.\n    targets : str, optional\n        Comma-separated string of network name(s) to plot as targets.\n    sids : str, optional\n        Comma-separated string of source node identifier(s) to filter by.\n    tids : str, optional\n        Comma-separated string of target node identifier(s) to filter by.\n    no_prepend_pop : bool, optional\n        If True, population name is not prepended to sid or tid. Default is True.\n    synaptic_info : str, optional\n        Type of synaptic information to display. Default is '0'.\n    source_cell : str, optional\n        Specific source cell type to plot connections from.\n    target_cell : str, optional\n        Specific target cell type to plot connections onto.\n    include_gap : bool, optional\n        If True, include gap junctions in analysis. Default is True.\n\n    Returns\n    -------\n    tuple\n        (matplotlib.figure.Figure, matplotlib.axes.Axes) containing the histogram.\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    sources_list = sources.split(\",\") if sources else []\n    targets_list = targets.split(\",\") if targets else []\n    if sids:\n        sids_list = sids.split(\",\")\n    else:\n        sids_list = []\n    if tids:\n        tids_list = tids.split(\",\")\n    else:\n        tids_list = []\n\n    def connection_pair_histogram(ax=None, **kwargs: Dict) -&gt; None:\n        \"\"\"\n        Creates a histogram showing the distribution of connection counts between specific cell types.\n\n        This function is designed to be used with the relation_matrix utility and will only\n        create histograms for the specified source and target cell types.\n\n        Parameters\n        ----------\n        ax : matplotlib.axes.Axes, optional\n            The axes object on which to create the histogram. If None, uses current axes.\n        kwargs : dict\n            Dictionary containing edge data and filtering information.\n            - edges: DataFrame containing edge information\n            - sid: Column name for source ID type in the edges DataFrame\n            - tid: Column name for target ID type in the edges DataFrame\n            - source_id: Value to filter edges by source ID type\n            - target_id: Value to filter edges by target ID type\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if ax is None:\n            ax = plt.gca()\n        edges_data = kwargs[\"edges\"]\n        source_id_type = kwargs[\"sid\"]\n        target_id_type = kwargs[\"tid\"]\n        source_id = kwargs[\"source_id\"]\n        target_id = kwargs[\"target_id\"]\n        if source_id == source_cell and target_id == target_cell:\n            temp = edges_data[\n                (edges_data[source_id_type] == source_id) &amp; (edges_data[target_id_type] == target_id)\n            ]\n            if not include_gap:\n                gap_col = temp[\"is_gap_junction\"].fillna(False).astype(bool)\n                temp = temp[~gap_col]\n            node_pairs = temp.groupby(\"target_node_id\")[\"source_node_id\"].count()\n            try:\n                conn_mean = statistics.mean(node_pairs.values)\n                conn_std = statistics.stdev(node_pairs.values)\n                conn_median = statistics.median(node_pairs.values)\n                label = \"mean {:.2f} std {:.2f} median {:.2f}\".format(\n                    conn_mean, conn_std, conn_median\n                )\n            except (statistics.StatisticsError, ValueError):  # lazy fix for std not calculated with 1 node\n                conn_mean = statistics.mean(node_pairs.values)\n                conn_median = statistics.median(node_pairs.values)\n                label = \"mean {:.2f} median {:.2f}\".format(conn_mean, conn_median)\n            ax.hist(node_pairs.values, density=False, bins=\"auto\", stacked=True, label=label)\n            ax.legend()\n            ax.set_xlabel(\"# of conns from {} to {}\".format(source_cell, target_cell))\n            ax.set_ylabel(\"# of cells\")\n        else:  # dont care about other cell pairs so pass\n            pass\n\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n\n    # Create figure for the histogram\n    fig, ax = plt.subplots()\n\n    # Wrapper to pass ax to the connection_pair_histogram function\n    def relation_func_wrapper(**kwargs):\n        return connection_pair_histogram(ax=ax, **kwargs)\n\n    util.relation_matrix(\n        config,\n        nodes,\n        edges,\n        sources_list,\n        targets_list,\n        sids_list,\n        tids_list,\n        not no_prepend_pop,\n        relation_func=relation_func_wrapper,\n        synaptic_info=synaptic_info,\n    )\n\n    return fig, ax\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.connection_distance","title":"<code>bmtool.bmplot.connections.connection_distance(config, sources, targets, source_cell_id, target_id_type, ignore_z=False)</code>","text":"<p>Plots the 3D spatial distribution of target nodes relative to a source node and a histogram of distances from the source node to each target node.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>sources</code> <code>str</code> <p>Network name(s) to plot as sources.</p> required <code>targets</code> <code>str</code> <p>Network name(s) to plot as targets.</p> required <code>source_cell_id</code> <code>int</code> <p>ID of the source cell for calculating distances to target nodes.</p> required <code>target_id_type</code> <code>str</code> <p>String to filter target nodes based off the target_query.</p> required <code>ignore_z</code> <code>bool</code> <p>If True, ignore Z axis when calculating distance. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Two tuples, each containing (matplotlib.figure.Figure, matplotlib.axes.Axes): - First tuple: 3D/2D scatter plot of node positions - Second tuple: Histogram of distances</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; (fig1, ax1), (fig2, ax2) = connection_distance(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     source_cell_id=0,\n...     target_id_type='LN',\n...     ignore_z=False\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def connection_distance(\n    config: str,\n    sources: str,\n    targets: str,\n    source_cell_id: int,\n    target_id_type: str,\n    ignore_z: bool = False,\n) -&gt; Tuple[Tuple[Any, Any], Tuple[Any, Any]]:\n    \"\"\"\n    Plots the 3D spatial distribution of target nodes relative to a source node\n    and a histogram of distances from the source node to each target node.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    sources : str\n        Network name(s) to plot as sources.\n    targets : str\n        Network name(s) to plot as targets.\n    source_cell_id : int\n        ID of the source cell for calculating distances to target nodes.\n    target_id_type : str\n        String to filter target nodes based off the target_query.\n    ignore_z : bool, optional\n        If True, ignore Z axis when calculating distance. Default is False.\n\n    Returns\n    -------\n    tuple\n        Two tuples, each containing (matplotlib.figure.Figure, matplotlib.axes.Axes):\n        - First tuple: 3D/2D scatter plot of node positions\n        - Second tuple: Histogram of distances\n\n    Examples\n    --------\n    &gt;&gt;&gt; (fig1, ax1), (fig2, ax2) = connection_distance(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     source_cell_id=0,\n    ...     target_id_type='LN',\n    ...     ignore_z=False\n    ... )\n    \"\"\"\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    # if source != target:\n    # raise Exception(\"Code is setup for source and target to be the same! Look at source code for function to add feature\")\n\n    # Load nodes and edges based on config file\n    nodes, edges = util.load_nodes_edges_from_config(config)\n\n    edge_network = sources + \"_to_\" + targets\n    node_network = sources\n\n    # Filter edges to obtain connections originating from the source node\n    edge = edges[edge_network]\n    edge = edge[edge[\"source_node_id\"] == source_cell_id]\n    if target_id_type:\n        edge = edge[edge[\"target_query\"].str.contains(target_id_type, na=False)]\n\n    target_node_ids = edge[\"target_node_id\"]\n\n    # Filter nodes to obtain only the target and source nodes\n    node = nodes[node_network]\n    target_nodes = node.loc[node.index.isin(target_node_ids)]\n    source_node = node.loc[node.index == source_cell_id]\n\n    # Calculate distances between source node and each target node\n    if ignore_z:\n        target_positions = target_nodes[[\"pos_x\", \"pos_y\"]].values\n        source_position = np.array(\n            [source_node[\"pos_x\"], source_node[\"pos_y\"]]\n        ).ravel()  # Ensure 1D shape\n    else:\n        target_positions = target_nodes[[\"pos_x\", \"pos_y\", \"pos_z\"]].values\n        source_position = np.array(\n            [source_node[\"pos_x\"], source_node[\"pos_y\"], source_node[\"pos_z\"]]\n        ).ravel()  # Ensure 1D shape\n    distances = np.linalg.norm(target_positions - source_position, axis=1)\n\n    # Plot positions of source and target nodes in 3D space or 2D\n    if ignore_z:\n        fig = plt.figure(figsize=(8, 6))\n        ax = fig.add_subplot(111)\n        ax.scatter(target_nodes[\"pos_x\"], target_nodes[\"pos_y\"], c=\"blue\", label=\"target cells\")\n        ax.scatter(source_node[\"pos_x\"], source_node[\"pos_y\"], c=\"red\", label=\"source cell\")\n    else:\n        fig = plt.figure(figsize=(8, 6))\n        ax = fig.add_subplot(111, projection=\"3d\")\n        ax.scatter(\n            target_nodes[\"pos_x\"],\n            target_nodes[\"pos_y\"],\n            target_nodes[\"pos_z\"],\n            c=\"blue\",\n            label=\"target cells\",\n        )\n        ax.scatter(\n            source_node[\"pos_x\"],\n            source_node[\"pos_y\"],\n            source_node[\"pos_z\"],\n            c=\"red\",\n            label=\"source cell\",\n        )\n\n    # Optional: Add text annotations for distances\n    # for i, distance in enumerate(distances):\n    #     ax.text(target_nodes['pos_x'].iloc[i], target_nodes['pos_y'].iloc[i], target_nodes['pos_z'].iloc[i],\n    #             f'{distance:.2f}', color='black', fontsize=8, ha='center')\n\n    plt.legend()\n\n    # Plot distances in a separate 2D plot\n    fig2, ax2 = plt.subplots(figsize=(8, 6))\n    ax2.hist(distances, bins=20, color=\"blue\", edgecolor=\"black\")\n    ax2.set_xlabel(\"Distance\")\n    ax2.set_ylabel(\"Count\")\n    ax2.set_title(\"Distance from Source Node to Each Target Node\")\n    ax2.grid(True)\n\n    return (fig, ax), (fig2, ax2)\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.edge_histogram_matrix","title":"<code>bmtool.bmplot.connections.edge_histogram_matrix(config, sources=None, targets=None, sids=None, tids=None, no_prepend_pop=None, edge_property=None, time=None, time_compare=None, report=None, title=None)</code>","text":"<p>Generates a matrix of histograms showing the distribution of edge properties between populations.</p> <p>This function creates a grid of histograms where each cell represents the distribution of a specific edge property between source and target populations.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to a BMTK simulation config file.</p> required <code>sources</code> <code>str</code> <p>Comma-separated list of source network names.</p> <code>None</code> <code>targets</code> <code>str</code> <p>Comma-separated list of target network names.</p> <code>None</code> <code>sids</code> <code>str</code> <p>Comma-separated list of source node identifiers to filter by.</p> <code>None</code> <code>tids</code> <code>str</code> <p>Comma-separated list of target node identifiers to filter by.</p> <code>None</code> <code>no_prepend_pop</code> <code>bool</code> <p>If True, population names are not prepended to node identifiers.</p> <code>None</code> <code>edge_property</code> <code>str</code> <p>The edge property to analyze (e.g., 'syn_weight', 'delay').</p> <code>None</code> <code>time</code> <code>int</code> <p>Time point to analyze from a time series report.</p> <code>None</code> <code>time_compare</code> <code>int</code> <p>Second time point for comparison with time.</p> <code>None</code> <code>report</code> <code>str</code> <p>Name of the report to analyze.</p> <code>None</code> <code>title</code> <code>str</code> <p>Custom title for the plot.</p> <code>None</code> <code>save_file</code> <code>str</code> <p>Path to save the generated plot.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(matplotlib.figure.Figure, matplotlib.axes.Axes) containing the histogram matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, axes = edge_histogram_matrix(\n...     config='config.json',\n...     sources='PN',\n...     targets='LN',\n...     edge_property='syn_weight'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def edge_histogram_matrix(\n    config: str,\n    sources: Optional[str] = None,\n    targets: Optional[str] = None,\n    sids: Optional[str] = None,\n    tids: Optional[str] = None,\n    no_prepend_pop: Optional[bool] = None,\n    edge_property: Optional[str] = None,\n    time: Optional[int] = None,\n    time_compare: Optional[int] = None,\n    report: Optional[str] = None,\n    title: Optional[str] = None,\n\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generates a matrix of histograms showing the distribution of edge properties between populations.\n\n    This function creates a grid of histograms where each cell represents the distribution\n    of a specific edge property between source and target populations.\n\n    Parameters\n    ----------\n    config : str\n        Path to a BMTK simulation config file.\n    sources : str, optional\n        Comma-separated list of source network names.\n    targets : str, optional\n        Comma-separated list of target network names.\n    sids : str, optional\n        Comma-separated list of source node identifiers to filter by.\n    tids : str, optional\n        Comma-separated list of target node identifiers to filter by.\n    no_prepend_pop : bool, optional\n        If True, population names are not prepended to node identifiers.\n    edge_property : str, optional\n        The edge property to analyze (e.g., 'syn_weight', 'delay').\n    time : int, optional\n        Time point to analyze from a time series report.\n    time_compare : int, optional\n        Second time point for comparison with time.\n    report : str, optional\n        Name of the report to analyze.\n    title : str, optional\n        Custom title for the plot.\n    save_file : str, optional\n        Path to save the generated plot.\n\n    Returns\n    -------\n    tuple\n        (matplotlib.figure.Figure, matplotlib.axes.Axes) containing the histogram matrix.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, axes = edge_histogram_matrix(\n    ...     config='config.json',\n    ...     sources='PN',\n    ...     targets='LN',\n    ...     edge_property='syn_weight'\n    ... )\n    \"\"\"\n\n    if not config:\n        raise Exception(\"config not defined\")\n    if not sources or not targets:\n        raise Exception(\"Sources or targets not defined\")\n    targets = targets.split(\",\")\n    if sids:\n        sids = sids.split(\",\")\n    else:\n        sids = []\n    if tids:\n        tids = tids.split(\",\")\n    else:\n        tids = []\n\n    if time_compare:\n        time_compare = int(time_compare)\n\n    data, source_labels, target_labels = util.edge_property_matrix(\n        edge_property,\n        nodes=None,\n        edges=None,\n        config=config,\n        sources=sources,\n        targets=targets,\n        sids=sids,\n        tids=tids,\n        prepend_pop=not no_prepend_pop,\n        report=report,\n        time=time,\n        time_compare=time_compare,\n    )\n\n    # Fantastic resource\n    # https://stackoverflow.com/questions/7941207/is-there-a-function-to-make-scatterplot-matrices-in-matplotlib\n    num_src, num_tar = data.shape\n    fig, axes = plt.subplots(nrows=num_src, ncols=num_tar, figsize=(12, 12))\n    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n\n    for x in range(num_src):\n        for y in range(num_tar):\n            axes[x, y].hist(data[x][y])\n\n            if x == num_src - 1:\n                axes[x, y].set_xlabel(target_labels[y])\n            if y == 0:\n                axes[x, y].set_ylabel(source_labels[x])\n\n    tt = edge_property + \" Histogram Matrix\"\n    if title:\n        tt = title\n    st = fig.suptitle(tt, fontsize=14)\n    fig.text(0.5, 0.04, \"Target\", ha=\"center\")\n    fig.text(0.04, 0.5, \"Source\", va=\"center\", rotation=\"vertical\")\n    plt.draw()\n\n    return fig, axes\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.distance_delay_plot","title":"<code>bmtool.bmplot.connections.distance_delay_plot(simulation_config, source, target, group_by, sid, tid)</code>","text":"<p>Plots the relationship between the distance and delay of connections between nodes in a neural network.</p> <p>This function loads node and edge data from a simulation configuration file, filters nodes by population, identifies connections (edges) between source and target node populations, calculates the Euclidean distance between connected nodes, and plots the delay as a function of distance.</p> <p>Parameters:</p> Name Type Description Default <code>simulation_config</code> <code>str</code> <p>Path to the simulation config file.</p> required <code>source</code> <code>str</code> <p>The name of the source population in the edge data.</p> required <code>target</code> <code>str</code> <p>The name of the target population in the edge data.</p> required <code>group_by</code> <code>str</code> <p>Column name to group nodes by (e.g., population name).</p> required <code>sid</code> <code>str</code> <p>Identifier for the source group (e.g., 'PN').</p> required <code>tid</code> <code>str</code> <p>Identifier for the target group (e.g., 'PN').</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(matplotlib.figure.Figure, matplotlib.axes.Axes) containing the scatter plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, ax = distance_delay_plot(\n...     'config.json',\n...     'cortex',\n...     'cortex',\n...     'node_type_id',\n...     'E',\n...     'E'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def distance_delay_plot(\n    simulation_config: str, source: str, target: str, group_by: str, sid: str, tid: str\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Plots the relationship between the distance and delay of connections between nodes in a neural network.\n\n    This function loads node and edge data from a simulation configuration file, filters nodes by population,\n    identifies connections (edges) between source and target node populations, calculates the Euclidean distance\n    between connected nodes, and plots the delay as a function of distance.\n\n    Parameters\n    ----------\n    simulation_config : str\n        Path to the simulation config file.\n    source : str\n        The name of the source population in the edge data.\n    target : str\n        The name of the target population in the edge data.\n    group_by : str\n        Column name to group nodes by (e.g., population name).\n    sid : str\n        Identifier for the source group (e.g., 'PN').\n    tid : str\n        Identifier for the target group (e.g., 'PN').\n\n    Returns\n    -------\n    tuple\n        (matplotlib.figure.Figure, matplotlib.axes.Axes) containing the scatter plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = distance_delay_plot(\n    ...     'config.json',\n    ...     'cortex',\n    ...     'cortex',\n    ...     'node_type_id',\n    ...     'E',\n    ...     'E'\n    ... )\n    \"\"\"\n    nodes, edges = util.load_nodes_edges_from_config(simulation_config)\n    nodes = nodes[target]\n    # node id is index of nodes df\n    node_id_source = nodes[nodes[group_by] == sid].index\n    node_id_target = nodes[nodes[group_by] == tid].index\n\n    edges = edges[f\"{source}_to_{target}\"]\n    edges = edges[\n        edges[\"source_node_id\"].isin(node_id_source) &amp; edges[\"target_node_id\"].isin(node_id_target)\n    ]\n\n    stuff_to_plot = []\n    for index, row in edges.iterrows():\n        try:\n            source_node = row[\"source_node_id\"]\n            target_node = row[\"target_node_id\"]\n\n            source_pos = nodes.loc[[source_node], [\"pos_x\", \"pos_y\", \"pos_z\"]]\n            target_pos = nodes.loc[[target_node], [\"pos_x\", \"pos_y\", \"pos_z\"]]\n\n            distance = np.linalg.norm(source_pos.values - target_pos.values)\n\n            delay = row[\"delay\"]  # This line may raise KeyError\n            stuff_to_plot.append([distance, delay])\n\n        except KeyError as e:\n            print(f\"KeyError: Missing key {e} in either edge properties or node positions.\")\n        except IndexError as e:\n            print(f\"IndexError: Node ID {source_node} or {target_node} not found in nodes.\")\n        except Exception as e:\n            print(f\"Unexpected error at edge index {index}: {e}\")\n\n    fig, ax = plt.subplots()\n    ax.scatter([x[0] for x in stuff_to_plot], [x[1] for x in stuff_to_plot])\n    ax.set_xlabel(\"Distance\")\n    ax.set_ylabel(\"Delay\")\n    ax.set_title(f\"Distance vs Delay for edge between {sid} and {tid}\")\n\n    return fig, ax\n</code></pre>"},{"location":"api/bmplot/connections/#bmtool.bmplot.connections.plot_synapse_location","title":"<code>bmtool.bmplot.connections.plot_synapse_location(config, source, target, sids, tids, syn_feature='afferent_section_id')</code>","text":"<p>Generates a connectivity matrix showing synaptic distribution across different cell sections.</p> <p>Note: Excludes gap junctions since they don't have an afferent id stored in the h5 file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to BMTK config file.</p> required <code>source</code> <code>str</code> <p>The source BMTK network name.</p> required <code>target</code> <code>str</code> <p>The target BMTK network name.</p> required <code>sids</code> <code>str</code> <p>Column name in nodes file containing source population identifiers.</p> required <code>tids</code> <code>str</code> <p>Column name in nodes file containing target population identifiers.</p> required <code>syn_feature</code> <code>str</code> <p>Synaptic feature to analyze. Default is 'afferent_section_id'. Options: 'afferent_section_id' or 'afferent_section_pos'.</p> <code>'afferent_section_id'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(matplotlib.figure.Figure, matplotlib.axes.Axes) containing the plot.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required parameters are missing or invalid.</p> <code>RuntimeError</code> <p>If template loading or cell instantiation fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, ax = plot_synapse_location(\n...     config='config.json',\n...     source='LGN',\n...     target='cortex',\n...     sids='node_type_id',\n...     tids='node_type_id'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/connections.py</code> <pre><code>def plot_synapse_location(\n    config: str,\n    source: str,\n    target: str,\n    sids: str,\n    tids: str,\n    syn_feature: str = \"afferent_section_id\",\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Generates a connectivity matrix showing synaptic distribution across different cell sections.\n\n    Note: Excludes gap junctions since they don't have an afferent id stored in the h5 file.\n\n    Parameters\n    ----------\n    config : str\n        Path to BMTK config file.\n    source : str\n        The source BMTK network name.\n    target : str\n        The target BMTK network name.\n    sids : str\n        Column name in nodes file containing source population identifiers.\n    tids : str\n        Column name in nodes file containing target population identifiers.\n    syn_feature : str, optional\n        Synaptic feature to analyze. Default is 'afferent_section_id'.\n        Options: 'afferent_section_id' or 'afferent_section_pos'.\n\n    Returns\n    -------\n    tuple\n        (matplotlib.figure.Figure, matplotlib.axes.Axes) containing the plot.\n\n    Raises\n    ------\n    ValueError\n        If required parameters are missing or invalid.\n    RuntimeError\n        If template loading or cell instantiation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = plot_synapse_location(\n    ...     config='config.json',\n    ...     source='LGN',\n    ...     target='cortex',\n    ...     sids='node_type_id',\n    ...     tids='node_type_id'\n    ... )\n    \"\"\"\n    # Validate inputs\n    if not all([config, source, target, sids, tids]):\n        raise ValueError(\n            \"Missing required parameters: config, source, target, sids, and tids must be provided\"\n        )\n\n    # Fix the validation logic - it was using 'or' instead of 'and'\n    #if syn_feature not in [\"afferent_section_id\", \"afferent_section_pos\"]:\n    #    raise ValueError(\"Currently only syn features supported are afferent_section_id or afferent_section_pos\")\n\n    try:\n        # Load mechanisms and template\n        util.load_templates_from_config(config)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load templates from config: {str(e)}\")\n\n    try:\n        # Load node and edge data\n        nodes, edges = util.load_nodes_edges_from_config(config)\n        if source not in nodes or f\"{source}_to_{target}\" not in edges:\n            raise ValueError(f\"Source '{source}' or target '{target}' networks not found in data\")\n\n        target_nodes = nodes[target]\n        source_nodes = nodes[source]\n        edges = edges[f\"{source}_to_{target}\"]\n\n        # Find edges with NaN values in the specified feature\n        nan_edges = edges[edges[syn_feature].isna()]\n        # Print information about removed edges\n        if not nan_edges.empty:\n            unique_indices = sorted(list(set(nan_edges.index.tolist())))\n            print(f\"Removing {len(nan_edges)} edges with missing {syn_feature}\")\n            print(f\"Unique indices removed: {unique_indices}\")\n\n        # Filter out edges with NaN values in the specified feature\n        edges = edges[edges[syn_feature].notna()]\n\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load nodes and edges: {str(e)}\")\n\n    # Map identifiers while checking for missing values\n    edges[\"target_model_template\"] = edges[\"target_node_id\"].map(target_nodes[\"model_template\"])\n    edges[\"target_pop_name\"] = edges[\"target_node_id\"].map(target_nodes[tids])\n    edges[\"source_pop_name\"] = edges[\"source_node_id\"].map(source_nodes[sids])\n\n    if edges[\"target_model_template\"].isnull().any():\n        print(\"Warning: Some target nodes missing model template\")\n    if edges[\"target_pop_name\"].isnull().any():\n        print(\"Warning: Some target nodes missing population name\")\n    if edges[\"source_pop_name\"].isnull().any():\n        print(\"Warning: Some source nodes missing population name\")\n\n    # Get unique populations\n    source_pops = edges[\"source_pop_name\"].unique()\n    target_pops = edges[\"target_pop_name\"].unique()\n\n    # Initialize matrices\n    num_connections = np.zeros((len(source_pops), len(target_pops)))\n    text_data = np.empty((len(source_pops), len(target_pops)), dtype=object)\n\n    # Create mappings for indices\n    source_pop_to_idx = {pop: idx for idx, pop in enumerate(source_pops)}\n    target_pop_to_idx = {pop: idx for idx, pop in enumerate(target_pops)}\n\n    # Cache for section mappings to avoid recreating cells\n    section_mappings = {}\n\n    # Calculate connectivity statistics\n    for source_pop in source_pops:\n        for target_pop in target_pops:\n            # Filter edges for this source-target pair\n            filtered_edges = edges[\n                (edges[\"source_pop_name\"] == source_pop) &amp; (edges[\"target_pop_name\"] == target_pop)\n            ]\n\n            source_idx = source_pop_to_idx[source_pop]\n            target_idx = target_pop_to_idx[target_pop]\n\n            if len(filtered_edges) == 0:\n                num_connections[source_idx, target_idx] = 0\n                text_data[source_idx, target_idx] = \"No connections\"\n                continue\n\n            total_connections = len(filtered_edges)\n            target_model_template = filtered_edges[\"target_model_template\"].iloc[0]\n\n            try:\n                # Get or create section mapping for this model\n                if target_model_template not in section_mappings:\n                    cell_class_name = (\n                        target_model_template.split(\":\")[1]\n                        if \":\" in target_model_template\n                        else target_model_template\n                    )\n                    cell = getattr(h, cell_class_name)()\n\n                    # Create section mapping\n                    section_mapping = {}\n                    for idx, sec in enumerate(cell.all):\n                        section_mapping[idx] = sec.name().split(\".\")[-1]  # Clean name\n                    section_mappings[target_model_template] = section_mapping\n\n                section_mapping = section_mappings[target_model_template]\n\n                # Calculate section distribution\n                section_counts = filtered_edges[syn_feature].value_counts()\n                section_percentages = (section_counts / total_connections * 100).round(1)\n\n                # Format section distribution text - show all sections\n                section_display = []\n                for section_id, percentage in section_percentages.items():\n                    section_name = section_mapping.get(section_id, f\"sec_{section_id}\")\n                    section_display.append(f\"{section_name}:{percentage}%\")\n\n\n                num_connections[source_idx, target_idx] = total_connections\n                text_data[source_idx, target_idx] = \"\\n\".join(section_display)\n\n            except Exception as e:\n                print(f\"Warning: Error processing {target_model_template}: {str(e)}\")\n                num_connections[source_idx, target_idx] = total_connections\n                text_data[source_idx, target_idx] = \"Feature info N/A\"\n\n    # Create the plot\n    title = f\"Synaptic Distribution by {syn_feature.replace('_', ' ').title()}: {source} to {target}\"\n    fig, ax = plot_connection_info(\n        text=text_data,\n        num=num_connections,\n        source_labels=list(source_pops),\n        target_labels=list(target_pops),\n        title=title,\n        syn_info=\"1\",\n    )\n    return fig, ax\n</code></pre>"},{"location":"api/bmplot/entrainment/","title":"Entrainment Plotting API","text":""},{"location":"api/bmplot/entrainment/#bmtool.bmplot.entrainment.calculate_trial_statistics","title":"<code>bmtool.bmplot.entrainment.calculate_trial_statistics(data, error_type='ci')</code>","text":"<p>Calculate mean and error statistics across trials.</p> <p>Computes trial-averaged statistics with proper handling of NaN values. Supports three error types: 95% confidence intervals (via t-distribution), standard error of the mean (SEM), and standard deviation (SD).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>2D array of shape (n_trials, n_values) containing trial-wise data. Can contain NaN values which are ignored in calculations.</p> required <code>error_type</code> <code>str</code> <p>Type of error to compute: \"ci\" for 95% confidence interval, \"sem\" for standard error of the mean, or \"std\" for standard deviation (default: \"ci\").</p> <code>'ci'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <ul> <li>mean_values : 1D array of mean values across trials (one per value).</li> <li>error_values : 1D array of error bounds/bars corresponding to each mean.   For \"ci\", represents the half-width of the 95% confidence interval.   For \"sem\" and \"std\", represents the error magnitude.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If error_type is not 'ci', 'sem', or 'std'. If data is not 2D or is empty.</p> Notes <ul> <li>NaN values are ignored using numpy's nanmean and nanstd functions.</li> <li>For \"ci\", uses the t-distribution with degrees of freedom = min(valid_counts) - 1.</li> <li>Confidence intervals are computed at 95% (\u03b1 = 0.05, two-tailed).</li> <li>If fewer than 2 valid trials exist for a value, error is set to NaN.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = np.array([[1, 2, 3], [1.1, 2.2, 3.1], [0.9, 1.9, 3.2]])  # 3 trials, 3 values\n&gt;&gt;&gt; mean, error = calculate_trial_statistics(data, error_type='ci')\n&gt;&gt;&gt; print(mean, error)\n</code></pre> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def calculate_trial_statistics(\n    data: np.ndarray,\n    error_type: str = \"ci\",\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate mean and error statistics across trials.\n\n    Computes trial-averaged statistics with proper handling of NaN values. Supports\n    three error types: 95% confidence intervals (via t-distribution), standard error\n    of the mean (SEM), and standard deviation (SD).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2D array of shape (n_trials, n_values) containing trial-wise data.\n        Can contain NaN values which are ignored in calculations.\n    error_type : str, optional\n        Type of error to compute: \"ci\" for 95% confidence interval, \"sem\" for\n        standard error of the mean, or \"std\" for standard deviation (default: \"ci\").\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        - mean_values : 1D array of mean values across trials (one per value).\n        - error_values : 1D array of error bounds/bars corresponding to each mean.\n          For \"ci\", represents the half-width of the 95% confidence interval.\n          For \"sem\" and \"std\", represents the error magnitude.\n\n    Raises\n    ------\n    ValueError\n        If error_type is not 'ci', 'sem', or 'std'.\n        If data is not 2D or is empty.\n\n    Notes\n    -----\n    - NaN values are ignored using numpy's nanmean and nanstd functions.\n    - For \"ci\", uses the t-distribution with degrees of freedom = min(valid_counts) - 1.\n    - Confidence intervals are computed at 95% (\u03b1 = 0.05, two-tailed).\n    - If fewer than 2 valid trials exist for a value, error is set to NaN.\n\n    Examples\n    --------\n    &gt;&gt;&gt; data = np.array([[1, 2, 3], [1.1, 2.2, 3.1], [0.9, 1.9, 3.2]])  # 3 trials, 3 values\n    &gt;&gt;&gt; mean, error = calculate_trial_statistics(data, error_type='ci')\n    &gt;&gt;&gt; print(mean, error)\n    \"\"\"\n    if error_type not in [\"ci\", \"sem\", \"std\"]:\n        raise ValueError(\n            \"error_type must be 'ci' for confidence interval, 'sem' for standard error, \"\n            \"or 'std' for standard deviation.\"\n        )\n\n    if data.ndim != 2 or data.size == 0:\n        raise ValueError(\"data must be a non-empty 2D array of shape (n_trials, n_values).\")\n\n    # Calculate mean across trials, ignoring NaNs\n    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n        mean_values = np.nanmean(data, axis=0)\n\n        # Count valid trials per value (trials without NaN)\n        valid_counts = np.sum(~np.isnan(data), axis=0)\n\n        # Calculate standard deviation across trials, ignoring NaNs\n        std_values = np.nanstd(data, axis=0, ddof=1)\n\n        if error_type == \"ci\":\n            # Calculate 95% confidence interval using t-distribution\n            # SEM = std / sqrt(n_valid)\n            sem_values = std_values / np.sqrt(np.maximum(valid_counts, 1))\n\n            # Find the minimum valid count to use a conservative t-value\n            # (all values use the same t-value for consistency)\n            min_valid = np.min(valid_counts[valid_counts &gt; 1])\n\n            if min_valid &gt; 1:\n                # Two-tailed t-distribution at 95% confidence level\n                t_value = stats.t.ppf(0.975, min_valid - 1)\n                error_values = t_value * sem_values\n            else:\n                # Insufficient data for meaningful CI\n                error_values = np.full_like(mean_values, np.nan)\n\n        elif error_type == \"sem\":\n            # Standard error of the mean\n            sem_values = std_values / np.sqrt(np.maximum(valid_counts, 1))\n            error_values = sem_values\n\n        else:  # error_type == \"std\"\n            # Standard deviation (no division by sqrt(n))\n            error_values = std_values\n\n    return mean_values, error_values\n</code></pre>"},{"location":"api/bmplot/entrainment/#bmtool.bmplot.entrainment.plot_spike_power_correlation","title":"<code>bmtool.bmplot.entrainment.plot_spike_power_correlation(spike_df, lfp_data, fs, pop_names, filter_method='wavelet', bandwidth=2.0, lowcut=None, highcut=None, freq_range=(10, 100), freq_step=5, type_name='raw', figsize=(12, 8))</code>","text":"<p>Calculate and plot spike rate-LFP power correlation across frequencies for full signal.</p> <p>Analyzes the relationship between population spike rates and LFP power across a range of frequencies, using Spearman correlation for the entire signal duration.</p> <p>Parameters:</p> Name Type Description Default <code>spike_df</code> <code>DataFrame</code> <p>DataFrame containing spike data with columns 'timestamps', 'node_ids', and 'pop_name'.</p> required <code>lfp_data</code> <code>DataArray</code> <p>LFP data with time dimension.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> required <code>pop_names</code> <code>List[str]</code> <p>List of population names to analyze.</p> required <code>filter_method</code> <code>str</code> <p>Filtering method: 'wavelet' or 'butter' (default: 'wavelet').</p> <code>'wavelet'</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter (default: 2.0).</p> <code>2.0</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth filter. Required if filter_method='butter'.</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth filter. Required if filter_method='butter'.</p> <code>None</code> <code>freq_range</code> <code>Tuple[float, float]</code> <p>Min and max frequency to analyze in Hz (default: (10, 100)).</p> <code>(10, 100)</code> <code>freq_step</code> <code>float</code> <p>Step size for frequency analysis in Hz (default: 5).</p> <code>5</code> <code>type_name</code> <code>str</code> <p>Which type of spike rate to use (default: 'raw').</p> <code>'raw'</code> <code>figsize</code> <code>Tuple[float, float]</code> <p>Figure size (width, height) in inches (default: (12, 8)).</p> <code>(12, 8)</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Figure containing the correlation plot.</p> Notes <ul> <li>Uses Spearman correlation (rank-based, robust to outliers).</li> <li>Pre-computes LFP power at all frequencies for efficiency.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig = plot_spike_power_correlation(\n...     spike_df=spike_df,\n...     lfp_data=lfp,\n...     fs=400,\n...     pop_names=['PV', 'SST'],\n...     freq_range=(10, 100),\n...     freq_step=5\n... )\n</code></pre> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def plot_spike_power_correlation(\n    spike_df: pd.DataFrame,\n    lfp_data: xr.DataArray,\n    fs: float,\n    pop_names: List[str],\n    filter_method: str = \"wavelet\",\n    bandwidth: float = 2.0,\n    lowcut: Optional[float] = None,\n    highcut: Optional[float] = None,\n    freq_range: Tuple[float, float] = (10, 100),\n    freq_step: float = 5,\n    type_name: str = \"raw\",\n    figsize: Tuple[float, float] = (12, 8),\n) -&gt; Figure:\n    \"\"\"\n    Calculate and plot spike rate-LFP power correlation across frequencies for full signal.\n\n    Analyzes the relationship between population spike rates and LFP power across a range\n    of frequencies, using Spearman correlation for the entire signal duration.\n\n    Parameters\n    ----------\n    spike_df : pd.DataFrame\n        DataFrame containing spike data with columns 'timestamps', 'node_ids', and 'pop_name'.\n    lfp_data : xr.DataArray\n        LFP data with time dimension.\n    fs : float\n        Sampling frequency in Hz.\n    pop_names : List[str]\n        List of population names to analyze.\n    filter_method : str, optional\n        Filtering method: 'wavelet' or 'butter' (default: 'wavelet').\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter (default: 2.0).\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth filter. Required if filter_method='butter'.\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth filter. Required if filter_method='butter'.\n    freq_range : Tuple[float, float], optional\n        Min and max frequency to analyze in Hz (default: (10, 100)).\n    freq_step : float, optional\n        Step size for frequency analysis in Hz (default: 5).\n    type_name : str, optional\n        Which type of spike rate to use (default: 'raw').\n    figsize : Tuple[float, float], optional\n        Figure size (width, height) in inches (default: (12, 8)).\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure containing the correlation plot.\n\n    Notes\n    -----\n    - Uses Spearman correlation (rank-based, robust to outliers).\n    - Pre-computes LFP power at all frequencies for efficiency.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = plot_spike_power_correlation(\n    ...     spike_df=spike_df,\n    ...     lfp_data=lfp,\n    ...     fs=400,\n    ...     pop_names=['PV', 'SST'],\n    ...     freq_range=(10, 100),\n    ...     freq_step=5\n    ... )\n    \"\"\"\n    # Compute spike rate for all spikes\n    spike_rate = bmspikes.get_population_spike_rate(spike_df, fs=fs)\n\n    # Setup frequencies for analysis\n    frequencies = np.arange(freq_range[0], freq_range[1] + 1, freq_step)\n\n    # Pre-calculate LFP power for all frequencies\n    power_by_freq = {}\n    for freq in frequencies:\n        power_by_freq[freq] = get_lfp_power(\n            lfp_data, freq, fs, filter_method, lowcut=lowcut, highcut=highcut, bandwidth=bandwidth\n        )\n\n    # Calculate correlations for each population and frequency\n    results = {}\n    for pop in pop_names:\n        results[pop] = {}\n        pop_spike_rate = spike_rate.sel(population=pop, type=type_name)\n\n        for freq in frequencies:\n            lfp_power = power_by_freq[freq]\n\n            if len(pop_spike_rate) != len(lfp_power):\n                print(f\"Warning: Length mismatch for {pop} at {freq} Hz\")\n                print(f\"{len(pop_spike_rate)} {len(lfp_power)}\")\n                continue\n\n            corr, p_val = stats.spearmanr(pop_spike_rate.values, lfp_power.values)\n            results[pop][freq] = {\"correlation\": corr, \"p_value\": p_val}\n\n    # Create plot\n    sns.set_style(\"whitegrid\")\n    fig = plt.figure(figsize=figsize)\n\n    colors = plt.get_cmap(\"tab10\")\n    for i, pop in enumerate(pop_names):\n        plot_freqs = []\n        plot_corrs = []\n\n        for freq in frequencies:\n            if freq in results[pop] and not np.isnan(results[pop][freq][\"correlation\"]):\n                plot_freqs.append(freq)\n                plot_corrs.append(results[pop][freq][\"correlation\"])\n\n        if len(plot_freqs) == 0:\n            continue\n\n        plot_freqs = np.array(plot_freqs)\n        plot_corrs = np.array(plot_corrs)\n        color = colors(i)\n\n        plt.plot(\n            plot_freqs, plot_corrs, marker=\"o\", label=pop, linewidth=2, markersize=6, color=color\n        )\n\n    # Formatting\n    plt.xlabel(\"Frequency (Hz)\", fontsize=12)\n    plt.ylabel(\"Spike Rate-Power Correlation\", fontsize=12)\n\n    plt.title(\n        \"Spike Rate-LFP Power Correlation\",\n        fontsize=14,\n    )\n    plt.grid(True, alpha=0.3)\n    plt.axhline(y=0, color=\"gray\", linestyle=\"-\", alpha=0.5)\n\n    # Setup legend\n    from matplotlib.lines import Line2D\n\n    legend_elements = [\n        Line2D([0], [0], color=colors(i), marker=\"o\", linestyle=\"-\", label=pop)\n        for i, pop in enumerate(pop_names)\n    ]\n    plt.legend(handles=legend_elements, fontsize=10, loc=\"best\")\n\n    # Axis formatting\n    if len(frequencies) &gt; 10:\n        plt.xticks(frequencies[::2])\n    else:\n        plt.xticks(frequencies)\n    plt.xlim(frequencies[0], frequencies[-1])\n\n    y_min, y_max = plt.ylim()\n    plt.ylim(min(y_min, -0.1), max(y_max, 0.1))\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/entrainment/#bmtool.bmplot.entrainment.plot_trial_avg_spike_power_correlation","title":"<code>bmtool.bmplot.entrainment.plot_trial_avg_spike_power_correlation(spike_df, lfp_data, time_windows, fs, pop_names, filter_method='wavelet', bandwidth=2.0, lowcut=None, highcut=None, freq_range=(10, 100), freq_step=5, type_name='raw', error_type='ci', figsize=(12, 8))</code>","text":"<p>Calculate and plot trial-averaged spike rate-LFP power correlation across frequencies.</p> <p>Computes spike rate-LFP power correlation for each trial separately, then averages results across trials with optional error bands.</p> <p>Parameters:</p> Name Type Description Default <code>spike_df</code> <code>DataFrame</code> <p>DataFrame containing spike data with columns 'timestamps', 'node_ids', and 'pop_name'.</p> required <code>lfp_data</code> <code>DataArray</code> <p>LFP data with time dimension.</p> required <code>time_windows</code> <code>List[Tuple[float, float]]</code> <p>List of (start, end) time tuples in milliseconds for each trial.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> required <code>pop_names</code> <code>List[str]</code> <p>List of population names to analyze.</p> required <code>filter_method</code> <code>str</code> <p>Filtering method: 'wavelet' or 'butter' (default: 'wavelet').</p> <code>'wavelet'</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter (default: 2.0).</p> <code>2.0</code> <code>lowcut</code> <code>float</code> <p>Lower frequency bound (Hz) for butterworth filter.</p> <code>None</code> <code>highcut</code> <code>float</code> <p>Upper frequency bound (Hz) for butterworth filter.</p> <code>None</code> <code>freq_range</code> <code>Tuple[float, float]</code> <p>Min and max frequency to analyze in Hz (default: (10, 100)).</p> <code>(10, 100)</code> <code>freq_step</code> <code>float</code> <p>Step size for frequency analysis in Hz (default: 5).</p> <code>5</code> <code>type_name</code> <code>str</code> <p>Which type of spike rate to use (default: 'raw').</p> <code>'raw'</code> <code>error_type</code> <code>str</code> <p>Type of error bars: \"ci\" for 95% CI, \"sem\" for SEM, or \"std\" for SD (default: \"ci\").</p> <code>'ci'</code> <code>figsize</code> <code>Tuple[float, float]</code> <p>Figure size (width, height) in inches (default: (12, 8)).</p> <code>(12, 8)</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Figure containing the trial-averaged correlation plot.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If error_type is invalid.</p> Notes <ul> <li>Uses calculate_trial_statistics helper for consistent error computation.</li> <li>NaN values are handled gracefully with warnings for problematic trials.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; time_windows = [(1000, 2000), (2500, 3500), (4000, 5000)]\n&gt;&gt;&gt; fig = plot_trial_avg_spike_power_correlation(\n...     spike_df=spike_df,\n...     lfp_data=lfp,\n...     time_windows=time_windows,\n...     fs=400,\n...     pop_names=['PV', 'SST'],\n...     error_type='ci'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def plot_trial_avg_spike_power_correlation(\n    spike_df: pd.DataFrame,\n    lfp_data: xr.DataArray,\n    time_windows: List[Tuple[float, float]],\n    fs: float,\n    pop_names: List[str],\n    filter_method: str = \"wavelet\",\n    bandwidth: float = 2.0,\n    lowcut: Optional[float] = None,\n    highcut: Optional[float] = None,\n    freq_range: Tuple[float, float] = (10, 100),\n    freq_step: float = 5,\n    type_name: str = \"raw\",\n    error_type: str = \"ci\",\n    figsize: Tuple[float, float] = (12, 8),\n) -&gt; Figure:\n    \"\"\"\n    Calculate and plot trial-averaged spike rate-LFP power correlation across frequencies.\n\n    Computes spike rate-LFP power correlation for each trial separately, then averages\n    results across trials with optional error bands.\n\n    Parameters\n    ----------\n    spike_df : pd.DataFrame\n        DataFrame containing spike data with columns 'timestamps', 'node_ids', and 'pop_name'.\n    lfp_data : xr.DataArray\n        LFP data with time dimension.\n    time_windows : List[Tuple[float, float]]\n        List of (start, end) time tuples in milliseconds for each trial.\n    fs : float\n        Sampling frequency in Hz.\n    pop_names : List[str]\n        List of population names to analyze.\n    filter_method : str, optional\n        Filtering method: 'wavelet' or 'butter' (default: 'wavelet').\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter (default: 2.0).\n    lowcut : float, optional\n        Lower frequency bound (Hz) for butterworth filter.\n    highcut : float, optional\n        Upper frequency bound (Hz) for butterworth filter.\n    freq_range : Tuple[float, float], optional\n        Min and max frequency to analyze in Hz (default: (10, 100)).\n    freq_step : float, optional\n        Step size for frequency analysis in Hz (default: 5).\n    type_name : str, optional\n        Which type of spike rate to use (default: 'raw').\n    error_type : str, optional\n        Type of error bars: \"ci\" for 95% CI, \"sem\" for SEM, or \"std\" for SD (default: \"ci\").\n    figsize : Tuple[float, float], optional\n        Figure size (width, height) in inches (default: (12, 8)).\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure containing the trial-averaged correlation plot.\n\n    Raises\n    ------\n    ValueError\n        If error_type is invalid.\n\n    Notes\n    -----\n    - Uses calculate_trial_statistics helper for consistent error computation.\n    - NaN values are handled gracefully with warnings for problematic trials.\n\n    Examples\n    --------\n    &gt;&gt;&gt; time_windows = [(1000, 2000), (2500, 3500), (4000, 5000)]\n    &gt;&gt;&gt; fig = plot_trial_avg_spike_power_correlation(\n    ...     spike_df=spike_df,\n    ...     lfp_data=lfp,\n    ...     time_windows=time_windows,\n    ...     fs=400,\n    ...     pop_names=['PV', 'SST'],\n    ...     error_type='ci'\n    ... )\n    \"\"\"\n    if error_type not in [\"ci\", \"sem\", \"std\"]:\n        raise ValueError(\n            \"error_type must be 'ci' for confidence interval, 'sem' for standard error, \"\n            \"or 'std' for standard deviation\"\n        )\n\n    # Validate that fs matches LFP data sampling rate if available\n    if hasattr(lfp_data, 'fs') and lfp_data.fs != fs:\n        raise ValueError(\n            f\"Provided fs ({fs} Hz) does not match LFP data sampling rate ({lfp_data.fs} Hz). \"\n        )\n\n    # Setup frequencies for analysis\n    frequencies = np.arange(freq_range[0], freq_range[1] + 1, freq_step)\n\n    # Pre-calculate LFP power for all frequencies (same for all trials)\n    power_by_freq = {}\n    for freq in frequencies:\n        power_by_freq[freq] = get_lfp_power(\n            lfp_data, freq, fs, filter_method, lowcut=lowcut, highcut=highcut, bandwidth=bandwidth\n        )\n\n    # Storage: dict of pop_name -&gt; list of trial_correlations per frequency\n    all_correlations = {pop: {freq: [] for freq in frequencies} for pop in pop_names}\n\n    # Process each trial\n    for trial_idx, (start_time, end_time) in enumerate(time_windows):\n        # Extract spikes for this trial\n        trial_spikes = spike_df[\n            (spike_df[\"timestamps\"] &gt;= start_time) &amp; (spike_df[\"timestamps\"] &lt;= end_time)\n        ].copy()\n\n        if len(trial_spikes) == 0:\n            print(f\"Warning: No spikes found in trial {trial_idx} ({start_time}-{end_time} ms)\")\n            continue\n\n        # Compute spike rate for this trial\n        trial_spike_rate = bmspikes.get_population_spike_rate(\n            trial_spikes, fs=fs, t_start=start_time, t_stop=end_time\n        )\n\n        # Calculate correlations for each population and frequency\n        for pop in pop_names:\n            if pop not in trial_spike_rate.population.values:\n                print(f\"Warning: Population {pop} not found in trial {trial_idx}\")\n                continue\n\n            pop_spike_rate = trial_spike_rate.sel(population=pop, type=type_name)\n\n            for freq in frequencies:\n                try:\n                    lfp_power = power_by_freq[freq]\n                    trial_lfp_power = lfp_power.sel(time=slice(start_time, end_time))\n\n                    if len(trial_lfp_power) &lt; 2 or len(pop_spike_rate) &lt; 2:\n                        continue\n\n                    # Align time coordinates\n                    common_times = np.intersect1d(pop_spike_rate.time.values,\n                                                 trial_lfp_power.time.values)\n                    if len(common_times) &lt; 2:\n                        continue\n\n                    trial_sr = pop_spike_rate.sel(time=common_times).values\n                    trial_lfp = trial_lfp_power.sel(time=common_times).values\n\n                    # Compute correlation\n                    corr, _ = stats.spearmanr(trial_sr, trial_lfp)\n                    if not np.isnan(corr):\n                        all_correlations[pop][freq].append(corr)\n\n                except Exception as e:\n                    print(\n                        f\"Warning: Error computing correlation for {pop} at {freq} Hz \"\n                        f\"in trial {trial_idx}: {e}\"\n                    )\n                    continue\n\n    # Calculate trial statistics for each population/frequency\n    results = {pop: {} for pop in pop_names}\n    for pop in pop_names:\n        for freq in frequencies:\n            if len(all_correlations[pop][freq]) &gt; 0:\n                freq_data = np.array(all_correlations[pop][freq])\n                mean_corr, error_corr = calculate_trial_statistics(\n                    freq_data.reshape(-1, 1), error_type=error_type\n                )\n                results[pop][freq] = {\n                    \"mean\": mean_corr[0],\n                    \"error\": error_corr[0],\n                    \"n_trials\": len(freq_data),\n                }\n\n    # Create plot\n    sns.set_style(\"whitegrid\")\n    fig = plt.figure(figsize=figsize)\n\n    colors = plt.get_cmap(\"tab10\")\n    for i, pop in enumerate(pop_names):\n        plot_freqs = []\n        plot_means = []\n        plot_errors = []\n\n        for freq in frequencies:\n            if freq in results[pop] and \"mean\" in results[pop][freq]:\n                plot_freqs.append(freq)\n                plot_means.append(results[pop][freq][\"mean\"])\n                plot_errors.append(results[pop][freq][\"error\"])\n\n        if len(plot_freqs) == 0:\n            continue\n\n        plot_freqs = np.array(plot_freqs)\n        plot_means = np.array(plot_means)\n        plot_errors = np.array(plot_errors)\n        color = colors(i)\n\n        # Plot line\n        plt.plot(\n            plot_freqs, plot_means, marker=\"o\", label=pop, linewidth=2, markersize=6, color=color\n        )\n\n        # Plot error band\n        plt.fill_between(\n            plot_freqs,\n            plot_means - plot_errors,\n            plot_means + plot_errors,\n            alpha=0.2,\n            color=color,\n        )\n\n    # Formatting\n    plt.xlabel(\"Frequency (Hz)\", fontsize=12)\n    plt.ylabel(\"Spike Rate-Power Correlation\", fontsize=12)\n\n    error_labels = {\"ci\": \"95% CI\", \"sem\": \"\u00b1SEM\", \"std\": \"\u00b11 SD\"}\n    error_label = error_labels[error_type]\n    plt.title(\n        f\"Trial-Averaged Spike Rate-LFP Power Correlation ({error_label})\",\n        fontsize=14,\n    )\n    plt.grid(True, alpha=0.3)\n    plt.axhline(y=0, color=\"gray\", linestyle=\"-\", alpha=0.5)\n\n    # Setup legend\n    from matplotlib.lines import Line2D\n\n    legend_elements = [\n        Line2D([0], [0], color=colors(i), marker=\"o\", linestyle=\"-\", label=pop)\n        for i, pop in enumerate(pop_names)\n    ]\n    legend_elements.append(Line2D([0], [0], color=\"gray\", alpha=0.3, linewidth=10, label=error_label))\n    plt.legend(handles=legend_elements, fontsize=10, loc=\"best\")\n\n    # Axis formatting\n    if len(frequencies) &gt; 10:\n        plt.xticks(frequencies[::2])\n    else:\n        plt.xticks(frequencies)\n    plt.xlim(frequencies[0], frequencies[-1])\n\n    y_min, y_max = plt.ylim()\n    plt.ylim(min(y_min, -0.1), max(y_max, 0.1))\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/entrainment/#bmtool.bmplot.entrainment.plot_cycle_with_spike_histograms","title":"<code>bmtool.bmplot.entrainment.plot_cycle_with_spike_histograms(phase_data, pop_names, bins=36)</code>","text":"<p>Plot an idealized cycle with spike histograms for different neuron populations.</p> <p>Parameters:</p> Name Type Description Default <code>phase_data</code> <code>dict</code> <p>Dictionary containing phase values for each spike and neuron population</p> required <code>pop_names</code> <code>List[str]</code> <p>List of population names to be plotted</p> required <code>bins</code> <code>int</code> <p>Number of bins for the phase histogram (default 36 gives 10-degree bins)</p> <code>36</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The figure containing the cycle and histograms</p> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def plot_cycle_with_spike_histograms(phase_data, pop_names: List[str], bins: int = 36):\n    \"\"\"\n    Plot an idealized cycle with spike histograms for different neuron populations.\n\n    Parameters\n    -----------\n    phase_data : dict\n        Dictionary containing phase values for each spike and neuron population\n    pop_names : List[str]\n        List of population names to be plotted\n    bins : int, optional\n        Number of bins for the phase histogram (default 36 gives 10-degree bins)\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The figure containing the cycle and histograms\n    \"\"\"\n    sns.set_style(\"whitegrid\")\n    # Create a figure with subplots\n    fig = plt.figure(figsize=(12, 8))\n    gs = GridSpec(len(pop_names) + 1, 1, height_ratios=[1.5] + [1] * len(pop_names))\n\n    # Top subplot: Idealized gamma cycle\n    ax_gamma = fig.add_subplot(gs[0])\n\n    # Create an idealized gamma cycle\n    x = np.linspace(-np.pi, np.pi, 1000)\n    y = np.sin(x)\n\n    ax_gamma.plot(x, y, \"b-\", linewidth=2)\n    ax_gamma.set_title(\"Cycle with Neuron Population Spike Distributions\", fontsize=14)\n    ax_gamma.set_ylabel(\"Amplitude\", fontsize=12)\n    ax_gamma.set_xlim(-np.pi, np.pi)\n    ax_gamma.set_xticks(np.linspace(-np.pi, np.pi, 9))\n    ax_gamma.set_xticklabels([\"-180\u00b0\", \"-135\u00b0\", \"-90\u00b0\", \"-45\u00b0\", \"0\u00b0\", \"45\u00b0\", \"90\u00b0\", \"135\u00b0\", \"180\u00b0\"])\n    ax_gamma.grid(True)\n    ax_gamma.axhline(y=0, color=\"k\", linestyle=\"-\", alpha=0.3)\n    ax_gamma.axvline(x=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n\n    # Generate a color map for the different populations\n    colors = plt.cm.tab10(np.linspace(0, 1, len(pop_names)))\n\n    # Add histograms for each neuron population\n    for i, pop_name in enumerate(pop_names):\n        ax_hist = fig.add_subplot(gs[i + 1], sharex=ax_gamma)\n\n        # Compute histogram\n        hist, bin_edges = np.histogram(phase_data[pop_name], bins=bins, range=(-np.pi, np.pi))\n        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n        # Normalize histogram\n        if np.sum(hist) &gt; 0:\n            hist = hist / np.sum(hist) * 100  # Convert to percentage\n\n        # Plot histogram\n        ax_hist.bar(bin_centers, hist, width=2 * np.pi / bins, alpha=0.7, color=colors[i])\n        ax_hist.set_ylabel(f\"{pop_name}\\nSpikes (%)\", fontsize=10)\n\n        # Add grid to align with gamma cycle\n        ax_hist.grid(True, alpha=0.3)\n        ax_hist.set_ylim(0, max(hist) * 1.2)  # Add some headroom\n\n    # Set x-label for the last subplot\n    ax_hist.set_xlabel(\"Phase (degrees)\", fontsize=12)\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/entrainment/#bmtool.bmplot.entrainment.plot_entrainment_by_population","title":"<code>bmtool.bmplot.entrainment.plot_entrainment_by_population(ppc_dict, pop_names, freqs, figsize=(15, 8), title=None)</code>","text":"<p>Plot PPC for all node populations on one graph with mean and standard error.</p> Parameters: <p>ppc_dict : Dict[str, Dict[str, Dict[float, float]]]     Dictionary containing PPC data organized by population, node, and frequency pop_names : List[str]     List of population names to plot data for freqs : List[float]     List of frequencies to plot figsize : Tuple[float, float], optional     Figure size for the plot title : str, optional     Title for the plot</p> <p>Returns:</p> Type Description <code>Figure</code> <p>The figure containing the bar plot</p> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def plot_entrainment_by_population(ppc_dict: Dict[str, Dict[str, Dict[float, float]]], pop_names: List[str], freqs: List[float], figsize: Tuple[float, float] = (15, 8), title: Optional[str] = None):\n    \"\"\"\n    Plot PPC for all node populations on one graph with mean and standard error.\n\n    Parameters:\n    -----------\n    ppc_dict : Dict[str, Dict[str, Dict[float, float]]]\n        Dictionary containing PPC data organized by population, node, and frequency\n    pop_names : List[str]\n        List of population names to plot data for\n    freqs : List[float]\n        List of frequencies to plot\n    figsize : Tuple[float, float], optional\n        Figure size for the plot\n    title : str, optional\n        Title for the plot\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The figure containing the bar plot\n    \"\"\"\n    # Set up the visualization style\n    sns.set_style(\"whitegrid\")\n    fig = plt.figure(figsize=figsize)\n\n    # Calculate the width of each group of bars\n    n_groups = len(freqs)\n    n_populations = len(pop_names)\n    group_width = 0.8\n    bar_width = group_width / n_populations\n\n    # Color palette for different populations\n    pop_colors = sns.color_palette(n_colors=n_populations)\n\n    # For tracking x-axis positions and labels\n    x_centers = np.arange(n_groups)\n    tick_labels = [str(freq) for freq in freqs]\n\n    # Process and plot data for each population\n    for i, pop in enumerate(pop_names):\n        # Store mean and SE for each frequency in this population\n        means = []\n        errors = []\n        valid_freqs_idx = []\n\n        # Collect and process data for all frequencies in this population\n        for freq_idx, freq in enumerate(freqs):\n            freq_values = []\n\n            # Collect values across all nodes for this frequency\n            for node in ppc_dict[pop]:\n                try:\n                    ppc_value = ppc_dict[pop][node][freq]\n                    freq_values.append(ppc_value)\n                except KeyError:\n                    continue\n\n            # If we have data for this frequency\n            if freq_values:\n                mean_val = np.mean(freq_values)\n                se_val = stats.sem(freq_values)\n                means.append(mean_val)\n                errors.append(se_val)\n                valid_freqs_idx.append(freq_idx)\n\n        # Calculate x positions for this population's bars\n        # Each population's bars are offset within their frequency group\n        x_positions = x_centers[valid_freqs_idx] + (i - n_populations / 2 + 0.5) * bar_width\n\n        # Plot bars with error bars\n        plt.bar(\n            x_positions, means, width=bar_width * 0.9, color=pop_colors[i], alpha=0.7, label=pop\n        )\n        plt.errorbar(x_positions, means, yerr=errors, fmt=\"none\", ecolor=\"black\", capsize=4)\n\n    # Set up the plot labels and legend\n    plt.xlabel(\"Frequency\")\n    plt.ylabel(\"PPC Value\")\n    if title:\n        plt.title(title)\n    plt.xticks(x_centers, tick_labels)\n    plt.legend(title=\"Population\")\n\n    # Adjust layout and save\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/entrainment/#bmtool.bmplot.entrainment.plot_entrainment_swarm_plot","title":"<code>bmtool.bmplot.entrainment.plot_entrainment_swarm_plot(ppc_dict, pop_names, freq, save_path=None, title=None)</code>","text":"<p>Plot a swarm plot of the entrainment for different populations at a single frequency.</p> Parameters: <p>ppc_dict : Dict[str, Dict[str, Dict[float, float]]]     Dictionary containing PPC values organized by population, node, and frequency pop_names : List[str]     List of population names to include in the plot freq : Union[float, int]     The specific frequency to plot save_path : str, optional     Path to save the figure. If None, figure is just displayed. title : str, optional     Title for the plot</p> Returns: <p>matplotlib.figure.Figure     The figure object for further customization if needed</p> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def plot_entrainment_swarm_plot(ppc_dict: Dict[str, Dict[str, Dict[float, float]]], pop_names: List[str], freq: Union[float, int], save_path: Optional[str] = None, title: Optional[str] = None):\n    \"\"\"\n    Plot a swarm plot of the entrainment for different populations at a single frequency.\n\n    Parameters:\n    -----------\n    ppc_dict : Dict[str, Dict[str, Dict[float, float]]]\n        Dictionary containing PPC values organized by population, node, and frequency\n    pop_names : List[str]\n        List of population names to include in the plot\n    freq : Union[float, int]\n        The specific frequency to plot\n    save_path : str, optional\n        Path to save the figure. If None, figure is just displayed.\n    title : str, optional\n        Title for the plot\n\n    Returns:\n    --------\n    matplotlib.figure.Figure\n        The figure object for further customization if needed\n    \"\"\"\n    # Set the style\n    sns.set_style(\"whitegrid\")\n\n    # Prepare data for the swarm plot\n    data_list = []\n\n    for pop in pop_names:\n        values = []\n        node_ids = []\n\n        for node in ppc_dict[pop]:\n            if freq in ppc_dict[pop][node] and ppc_dict[pop][node][freq] is not None:\n                data_list.append(\n                    {\"Population\": pop, \"Node\": node, \"PPC Difference\": ppc_dict[pop][node][freq]}\n                )\n\n    # Create DataFrame in long format\n    df = pd.DataFrame(data_list)\n\n    if df.empty:\n        print(f\"No data available for frequency {freq}.\")\n        return None\n\n    # Print mean PPC change for each population)\n    for pop in pop_names:\n        subset = df[df[\"Population\"] == pop]\n        if not subset.empty:\n            mean_val = subset[\"PPC Difference\"].mean()\n            std_val = subset[\"PPC Difference\"].std()\n            n = len(subset)\n            sem_val = std_val / np.sqrt(n)  # Standard error of the mean\n            print(f\"{pop}: {mean_val:.4f} \u00b1 {sem_val:.4f} (n={n})\")\n\n    # Create figure\n    fig = plt.figure(figsize=(max(8, len(pop_names) * 1.5), 8))\n\n    # Create swarm plot\n    ax = sns.swarmplot(\n        x=\"Population\",\n        y=\"PPC Difference\",\n        data=df,\n        size=3,\n        # palette='Set2'\n    )\n\n    # Add sample size annotations\n    for i, pop in enumerate(pop_names):\n        subset = df[df[\"Population\"] == pop]\n        if not subset.empty:\n            n = len(subset)\n            y_min = subset[\"PPC Difference\"].min()\n            y_max = subset[\"PPC Difference\"].max()\n\n            # Position annotation below the lowest point\n            plt.annotate(\n                f\"n={n}\", (i, y_min - 0.05 * (y_max - y_min) - 0.05), ha=\"center\", fontsize=10\n            )\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5, alpha=0.7)\n\n    # Add horizontal lines for mean values\n    for i, pop in enumerate(pop_names):\n        subset = df[df[\"Population\"] == pop]\n        if not subset.empty:\n            mean_val = subset[\"PPC Difference\"].mean()\n            plt.plot([i - 0.25, i + 0.25], [mean_val, mean_val], \"r-\", linewidth=2)\n\n    # Calculate and display statistics\n    if len(pop_names) &gt; 1:\n        # Print statistical test results\n        print(f\"\\nMann-Whitney U Test Results at {freq} Hz:\")\n        print(\"-\" * 60)\n\n        # Add p-values for pairwise comparisons\n        y_max = df[\"PPC Difference\"].max()\n        y_min = df[\"PPC Difference\"].min()\n        y_range = y_max - y_min\n\n        # Perform t-tests between populations if there are at least 2\n        for i in range(len(pop_names)):\n            for j in range(i + 1, len(pop_names)):\n                pop1 = pop_names[i]\n                pop2 = pop_names[j]\n\n                vals1 = df[df[\"Population\"] == pop1][\"PPC Difference\"].values\n                vals2 = df[df[\"Population\"] == pop2][\"PPC Difference\"].values\n\n                if len(vals1) &gt; 1 and len(vals2) &gt; 1:\n                    # Perform Mann-Whitney U test (non-parametric)\n                    u_stat, p_val = stats.mannwhitneyu(vals1, vals2, alternative=\"two-sided\")\n\n                    # Add significance markers\n                    sig_str = \"ns\"\n                    if p_val &lt; 0.05:\n                        sig_str = \"*\"\n                    if p_val &lt; 0.01:\n                        sig_str = \"**\"\n                    if p_val &lt; 0.001:\n                        sig_str = \"***\"\n\n                    # Position the significance bar\n                    bar_height = y_max + 0.1 * y_range * (1 + (j - i - 1) * 0.5)\n\n                    # Draw the bar\n                    plt.plot([i, j], [bar_height, bar_height], \"k-\")\n                    plt.plot([i, i], [bar_height - 0.02 * y_range, bar_height], \"k-\")\n                    plt.plot([j, j], [bar_height - 0.02 * y_range, bar_height], \"k-\")\n\n                    # Add significance marker\n                    plt.text(\n                        (i + j) / 2,\n                        bar_height + 0.01 * y_range,\n                        sig_str,\n                        ha=\"center\",\n                        va=\"bottom\",\n                        fontsize=12,\n                    )\n\n                    # Print the statistical comparison\n                    print(f\"{pop1} vs {pop2}: U={u_stat:.1f}, p={p_val:.4f} {sig_str}\")\n\n    # Add labels and title\n    plt.xlabel(\"Population\", fontsize=14)\n    plt.ylabel(\"PPC\", fontsize=14)\n    if title:\n        plt.title(title, fontsize=16)\n\n    # Adjust y-axis limits to make room for annotations\n    y_min, y_max = plt.ylim()\n    plt.ylim(y_min - 0.15 * (y_max - y_min), y_max + 0.25 * (y_max - y_min))\n\n    # Add gridlines\n    plt.grid(True, linestyle=\"--\", alpha=0.7, axis=\"y\")\n\n    # Adjust layout\n    plt.tight_layout()\n\n    # Save figure if path is provided\n    if save_path:\n        plt.savefig(f\"{save_path}/ppc_change_swarm_plot_{freq}Hz.png\", dpi=300, bbox_inches=\"tight\")\n\n    return fig\n</code></pre>"},{"location":"api/bmplot/entrainment/#bmtool.bmplot.entrainment.plot_trial_avg_entrainment","title":"<code>bmtool.bmplot.entrainment.plot_trial_avg_entrainment(spike_df, lfp, time_windows, entrainment_method, pop_names, freqs, firing_quantile, spike_fs=1000, error_type='ci')</code>","text":"<p>Plot trial-averaged entrainment for specified population names. Only supports wavelet filter current, could easily add other support</p> Parameters: <p>spike_df : pd.DataFrame     Spike data containing timestamps, node_ids, and pop_name columns lfp : xr.DataArray     Xarray for a channel of the lfp data time_windows : List[Tuple[float, float]]     List of windows to analysis with start and stp time [(start_time, end_time), ...] for each trial entrainment_method : str     Method for entrainment calculation ('ppc', 'ppc2' or 'plv') pop_names : List[str]     List of population names to process (e.g., ['FSI', 'LTS']) freqs : Union[List[float], np.ndarray]     Array of frequencies to analyze (Hz) firing_quantile : float     Upper quantile threshold for selecting high-firing cells (e.g., 0.8 for top 20%) spike_fs : float, optional     fs for spike data. Default is 1000 error_type : str, optional     Type of error bars to plot: \"ci\" for 95% confidence interval, \"sem\" for standard error, \"std\" for standard deviation</p> Raises: <p>ValueError     If entrainment_method is not 'ppc', 'ppc2' or 'plv'     If error_type is not 'ci', 'sem', or 'std'     If no spikes found for a population in a trial</p> Returns: <p>matplotlib.figure.Figure     The figure containing the plot</p> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def plot_trial_avg_entrainment(\n    spike_df: pd.DataFrame,\n    lfp: xr.DataArray,\n    time_windows: List[Tuple[float, float]],\n    entrainment_method: str,\n    pop_names: List[str],\n    freqs: Union[List[float], np.ndarray],\n    firing_quantile: float,\n    spike_fs: float = 1000,\n    error_type: str = \"ci\",\n) -&gt; Figure:\n    \"\"\"\n    Plot trial-averaged entrainment for specified population names. Only supports wavelet filter current, could easily add other support\n\n    Parameters:\n    -----------\n    spike_df : pd.DataFrame\n        Spike data containing timestamps, node_ids, and pop_name columns\n    lfp : xr.DataArray\n        Xarray for a channel of the lfp data\n    time_windows : List[Tuple[float, float]]\n        List of windows to analysis with start and stp time [(start_time, end_time), ...] for each trial\n    entrainment_method : str\n        Method for entrainment calculation ('ppc', 'ppc2' or 'plv')\n    pop_names : List[str]\n        List of population names to process (e.g., ['FSI', 'LTS'])\n    freqs : Union[List[float], np.ndarray]\n        Array of frequencies to analyze (Hz)\n    firing_quantile : float\n        Upper quantile threshold for selecting high-firing cells (e.g., 0.8 for top 20%)\n    spike_fs : float, optional\n        fs for spike data. Default is 1000\n    error_type : str, optional\n        Type of error bars to plot: \"ci\" for 95% confidence interval, \"sem\" for standard error, \"std\" for standard deviation\n\n    Raises:\n    -------\n    ValueError\n        If entrainment_method is not 'ppc', 'ppc2' or 'plv'\n        If error_type is not 'ci', 'sem', or 'std'\n        If no spikes found for a population in a trial\n\n    Returns:\n    --------\n    matplotlib.figure.Figure\n        The figure containing the plot\n    \"\"\"\n    sns.set_style(\"whitegrid\")\n    # Validate inputs\n    if entrainment_method not in [\"ppc\", \"plv\", \"ppc2\"]:\n        raise ValueError(\"entrainment_method must be 'ppc', ppc2 or 'plv'\")\n\n    if error_type not in [\"ci\", \"sem\", \"std\"]:\n        raise ValueError(\n            \"error_type must be 'ci' for confidence interval, 'sem' for standard error, or 'std' for standard deviation\"\n        )\n\n    if not (0 &lt;= firing_quantile &lt; 1):\n        raise ValueError(\"firing_quantile must be between 0 and 1\")\n\n    # Convert freqs to numpy array for easier indexing\n    freqs = np.array(freqs)\n\n    # Collect all PPC/PLV values across trials for each population\n    all_plv_data = {}  # Dictionary to store results for each population\n\n    # Initialize storage for each population\n    for pop_name in pop_names:\n        all_plv_data[pop_name] = []  # Will be shape (n_trials, n_freqs)\n\n    # Loop through all pulse groups to collect data\n    for trial_idx in range(len(time_windows)):\n        plv_lists = {}  # Store PLV lists for this trial\n\n        # Initialize PLV lists for each population\n        for pop_name in pop_names:\n            plv_lists[pop_name] = []\n\n        # Filter spikes for this trial\n        network_spikes = spike_df[\n            (spike_df[\"timestamps\"] &gt;= time_windows[trial_idx][0])\n            &amp; (spike_df[\"timestamps\"] &lt;= time_windows[trial_idx][1])\n        ].copy()\n\n        # Process each population\n        pop_spike_data = {}\n        for pop_name in pop_names:\n            # Get spikes for this population\n            pop_spikes = network_spikes[network_spikes[\"pop_name\"] == pop_name]\n\n            if len(pop_spikes) == 0:\n                print(f\"Warning: No spikes found for population {pop_name} in trial {trial_idx}\")\n                # Add NaN values for this trial/population\n                plv_lists[pop_name] = [np.nan] * len(freqs)\n                continue\n\n            # Filter to get the top firing cells\n            # firing_quantile of 0.8 gets the top 20% of firing cells to use\n            pop_spikes = bmspikes.find_highest_firing_cells(\n                pop_spikes, upper_quantile=firing_quantile\n            )\n\n            if len(pop_spikes) == 0:\n                print(\n                    f\"Warning: No high-firing spikes found for population {pop_name} in trial {trial_idx}\"\n                )\n                plv_lists[pop_name] = [np.nan] * len(freqs)\n                continue\n\n            pop_spike_data[pop_name] = pop_spikes\n\n        # Calculate PPC/PLV for each frequency and each population\n        for freq_idx, freq in enumerate(freqs):\n            for pop_name in pop_names:\n                if pop_name not in pop_spike_data:\n                    continue  # Skip if no data for this population\n\n                pop_spikes = pop_spike_data[pop_name]\n\n                try:\n                    if entrainment_method == \"ppc\":\n                        result = bmentr.calculate_ppc(\n                            pop_spikes[\"timestamps\"].values,\n                            lfp,\n                            spike_fs=spike_fs,\n                            lfp_fs=lfp.fs,\n                            freq_of_interest=freq,\n                            filter_method=\"wavelet\",\n                            ppc_method=\"gpu\",\n                        )\n                    elif entrainment_method == \"plv\":\n                        result = bmentr.calculate_spike_lfp_plv(\n                            pop_spikes[\"timestamps\"].values,\n                            lfp,\n                            spike_fs=spike_fs,\n                            lfp_fs=lfp.fs,\n                            freq_of_interest=freq,\n                            filter_method=\"wavelet\",\n                        )\n                    elif entrainment_method == \"ppc2\":\n                        result = bmentr.calculate_ppc2(\n                            pop_spikes[\"timestamps\"].values,\n                            lfp,\n                            spike_fs=spike_fs,\n                            lfp_fs=lfp.fs,\n                            freq_of_interest=freq,\n                            filter_method=\"wavelet\",\n                        )\n\n                    plv_lists[pop_name].append(result)\n\n                except Exception as e:\n                    print(\n                        f\"Warning: Error calculating {entrainment_method} for {pop_name} at {freq}Hz in trial {trial_idx}: {e}\"\n                    )\n                    plv_lists[pop_name].append(np.nan)\n\n        # Store this trial's results for each population\n        for pop_name in pop_names:\n            if pop_name in plv_lists and len(plv_lists[pop_name]) == len(freqs):\n                all_plv_data[pop_name].append(plv_lists[pop_name])\n            else:\n                # Fill with NaNs if data is missing\n                all_plv_data[pop_name].append([np.nan] * len(freqs))\n\n    # Convert to numpy arrays and calculate statistics\n    mean_plv = {}\n    error_plv = {}\n\n    for pop_name in pop_names:\n        all_plv_data[pop_name] = np.array(all_plv_data[pop_name])  # Shape: (n_trials, n_freqs)\n\n        # Use helper function to calculate trial statistics\n        mean_plv[pop_name], error_plv[pop_name] = calculate_trial_statistics(\n            all_plv_data[pop_name], error_type=error_type\n        )\n\n    # Create the combined plot\n    fig = plt.figure(figsize=(12, 8))\n\n    # Define markers and colors for different populations\n    markers = [\"o-\", \"s-\", \"^-\", \"D-\", \"v-\", \"&lt;-\", \"&gt;-\", \"p-\"]\n    colors = sns.color_palette(n_colors=len(pop_names))\n\n    # Plot each population\n    for i, pop_name in enumerate(pop_names):\n        marker = markers[i % len(markers)]  # Cycle through markers if more populations than markers\n        color = colors[i]\n\n        # Only plot if we have valid data\n        valid_mask = ~np.isnan(mean_plv[pop_name])\n        if np.any(valid_mask):\n            plt.plot(\n                freqs[valid_mask],\n                mean_plv[pop_name][valid_mask],\n                marker,\n                linewidth=2,\n                label=pop_name,\n                color=color,\n                markersize=6,\n            )\n\n            # Add error bars/shading if available\n            if not np.all(np.isnan(error_plv[pop_name])):\n                plt.fill_between(\n                    freqs[valid_mask],\n                    (mean_plv[pop_name] - error_plv[pop_name])[valid_mask],\n                    (mean_plv[pop_name] + error_plv[pop_name])[valid_mask],\n                    alpha=0.3,\n                    color=color,\n                )\n\n    plt.xlabel(\"Frequency (Hz)\", fontsize=12)\n    plt.ylabel(f\"{entrainment_method.upper()}\", fontsize=12)\n\n    # Calculate percentage for title and update title based on error type\n    firing_percentage = round(float((1 - firing_quantile) * 100), 1)\n    error_labels = {\"ci\": \"95% CI\", \"sem\": \"\u00b1SEM\", \"std\": \"\u00b11 SD\"}\n    error_label = error_labels[error_type]\n    plt.title(\n        f\"{entrainment_method.upper()} Across Trials for Top {firing_percentage}% Firing Cells ({error_label})\",\n        fontsize=14,\n    )\n\n    plt.legend(fontsize=10)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/entrainment/#bmtool.bmplot.entrainment.plot_fr_hist_phase_amplitude","title":"<code>bmtool.bmplot.entrainment.plot_fr_hist_phase_amplitude(fr_hist, pop_names, freq_labels, nbins_pha=16, nbins_amp=16, common_clim=True, figsize=(3, 2), cmap='viridis', title=None)</code>","text":"<p>Plot firing rate histograms binned by LFP phase and amplitude.  Check out the bmtool/bmtool/analysis/entrainment.py function compute_fr_hist_phase_amplitude</p> <p>Parameters:</p> Name Type Description Default <code>fr_hist</code> <code>ndarray</code> <p>Firing rate histogram of shape (n_pop, n_freq, nbins_pha, nbins_amp)</p> required <code>pop_names</code> <code>List[str]</code> <p>List of population names</p> required <code>freq_labels</code> <code>List[str]</code> <p>List of frequency labels for subplot titles (e.g., ['Beta', 'Gamma'])</p> required <code>nbins_pha</code> <code>int</code> <p>Number of phase bins</p> <code>16</code> <code>nbins_amp</code> <code>int</code> <p>Number of amplitude bins</p> <code>16</code> <code>common_clim</code> <code>bool</code> <p>Whether to use common color limits across all subplots</p> <code>True</code> <code>figsize</code> <code>Tuple[float, float]</code> <p>Size of each subplot</p> <code>(3, 2)</code> <code>cmap</code> <code>str</code> <p>Colormap to use</p> <code>'RdBu_r'</code> <code>title</code> <code>Optional[str]</code> <p>Overall title for the figure</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Figure, ndarray]</code> <p>Figure and axes objects</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, axs = plot_fr_hist_phase_amplitude(\n...     fr_hist, ['PV', 'SST'], ['Beta', 'Gamma'], \n...     common_clim=True, cmap='RdBu_r', title='LFP Phase-Amplitude Coupling'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def plot_fr_hist_phase_amplitude(\n    fr_hist: np.ndarray, \n    pop_names: List[str], \n    freq_labels: List[str], \n    nbins_pha: int = 16, \n    nbins_amp: int = 16,\n    common_clim: bool = True, \n    figsize: Tuple[float, float] = (3, 2),\n    cmap: str = 'viridis',\n    title: Optional[str] = None\n) -&gt; Tuple[plt.Figure, np.ndarray]:\n    \"\"\"\n    Plot firing rate histograms binned by LFP phase and amplitude. \n    Check out the bmtool/bmtool/analysis/entrainment.py function\n    compute_fr_hist_phase_amplitude\n\n    Parameters\n    ----------\n    fr_hist : np.ndarray\n        Firing rate histogram of shape (n_pop, n_freq, nbins_pha, nbins_amp)\n    pop_names : List[str]\n        List of population names\n    freq_labels : List[str]\n        List of frequency labels for subplot titles (e.g., ['Beta', 'Gamma'])\n    nbins_pha : int, default=16\n        Number of phase bins\n    nbins_amp : int, default=16\n        Number of amplitude bins\n    common_clim : bool, default=True\n        Whether to use common color limits across all subplots\n    figsize : Tuple[float, float], default=(3, 2)\n        Size of each subplot\n    cmap : str, default='RdBu_r'\n        Colormap to use\n    title : Optional[str], default=None\n        Overall title for the figure\n\n    Returns\n    -------\n    Tuple[plt.Figure, np.ndarray]\n        Figure and axes objects\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, axs = plot_fr_hist_phase_amplitude(\n    ...     fr_hist, ['PV', 'SST'], ['Beta', 'Gamma'], \n    ...     common_clim=True, cmap='RdBu_r', title='LFP Phase-Amplitude Coupling'\n    ... )\n    \"\"\"\n    pha_bins = np.linspace(-np.pi, np.pi, nbins_pha + 1)\n    quantiles = np.linspace(0, 1, nbins_amp + 1)\n\n    n_pop = len(pop_names)\n    n_freq = len(freq_labels)\n\n    fig, axs = plt.subplots(n_pop, n_freq, \n                           figsize=(figsize[0] * n_freq, figsize[1] * n_pop),\n                           squeeze=False)\n\n\n    # Add overall title if provided\n    if title:\n        fig.suptitle(title, fontsize=14, y=0.98)\n\n    for i, p in enumerate(pop_names):\n        if common_clim:\n            vmin, vmax = fr_hist.min(), fr_hist.max()\n        else:\n            # Calculate min/max for this population across all frequencies\n            vmin, vmax = fr_hist[i].min(), fr_hist[i].max()\n\n        for j, freq_label in enumerate(freq_labels):\n            ax = axs[i, j]\n            pcm = ax.pcolormesh(pha_bins, quantiles, fr_hist[i, j].T, \n                               vmin=vmin, vmax=vmax, cmap=cmap)\n            ax.set_title(p)\n\n            if i &lt; n_pop - 1:\n                ax.get_xaxis().set_visible(False)\n            else:\n                ax.set_xlabel(freq_label.title() + ' Phase')\n                ax.set_xticks((-np.pi, 0, np.pi))\n                ax.set_xticklabels([r'$-\\pi$', '0', r'$\\pi$'])\n\n            if j &gt; 0:\n                ax.get_yaxis().set_visible(False)\n            else:\n                ax.set_ylabel('Amplitude (quantile)')\n\n        # Add single colorbar for the entire population (all frequencies)\n        plt.colorbar(mappable=pcm, ax=axs[i], \n                    label='Firing rate (% Change)', pad=0.02)\n\n    return fig, axs\n</code></pre>"},{"location":"api/bmplot/entrainment/#bmtool.bmplot.entrainment.plot_trial_avg_spike_rate_plv","title":"<code>bmtool.bmplot.entrainment.plot_trial_avg_spike_rate_plv(spike_rate, time_windows, pop_names, freqs, pop_pairs=None, error_type='ci', figsize=(12, 8), filter_method='wavelet', bandwidth=1.0)</code>","text":"<p>Plot trial-averaged Phase Locking Value (PLV) between spike rate pairs across frequencies.</p> <p>This function computes PLV between spike rates of different population pairs for each trial, then averages the results across trials with optional error bars/bands.</p> <p>Parameters:</p> Name Type Description Default <code>spike_rate</code> <code>DataArray</code> <p>Pre-computed spike rate data with dimensions (time, population, type). Must have an 'fs' attribute (sampling frequency in Hz). Time dimension should be in milliseconds.</p> required <code>time_windows</code> <code>List[Tuple[float, float]]</code> <p>List of (start, end) time tuples in milliseconds for each trial window.</p> required <code>pop_names</code> <code>List[str]</code> <p>List of population names to consider (must match 'population' coords in spike_rate).</p> required <code>freqs</code> <code>Union[List[float], ndarray]</code> <p>Array of frequencies (Hz) to analyze.</p> required <code>pop_pairs</code> <code>Optional[List[Tuple[str, str]]]</code> <p>List of (pop1, pop2) tuples specifying population pairs to analyze. If None, all unique pairs from pop_names are generated (default: None).</p> <code>None</code> <code>error_type</code> <code>str</code> <p>Type of error bars to plot: \"ci\" for 95% confidence interval, \"sem\" for standard error, or \"std\" for standard deviation (default: \"ci\").</p> <code>'ci'</code> <code>figsize</code> <code>Tuple[float, float]</code> <p>Figure size (width, height) in inches (default: (12, 8)).</p> <code>(12, 8)</code> <code>filter_method</code> <code>str</code> <p>Filtering method for PLV calculation: 'wavelet' or 'butter' (default: 'wavelet').</p> <code>'wavelet'</code> <code>bandwidth</code> <code>float</code> <p>Bandwidth parameter for wavelet filter (default: 1.0).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The figure containing the trial-averaged PLV plot.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If error_type is not 'ci', 'sem', or 'std'. If pop_pairs contains invalid population names. If spike_rate does not have required dimensions/attributes.</p> Notes <ul> <li>Uses 'raw' type from spike_rate dimension (hardcoded for spike rate analysis).</li> <li>Time units in time_windows must match spike_rate.time units (milliseconds).</li> <li>Generates all unique population pairs if pop_pairs is None.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig = plot_trial_avg_spike_rate_plv(\n...     spike_rate=spike_rate,\n...     time_windows=[(1000, 2000), (2500, 3500), (4000, 5000)],\n...     pop_names=['ET', 'IT', 'PV', 'SST'],\n...     freqs=[10, 20, 30, 40, 50],\n...     error_type='ci'\n... )\n</code></pre> Source code in <code>bmtool/bmplot/entrainment.py</code> <pre><code>def plot_trial_avg_spike_rate_plv(\n    spike_rate: xr.DataArray,\n    time_windows: List[Tuple[float, float]],\n    pop_names: List[str],\n    freqs: Union[List[float], np.ndarray],\n    pop_pairs: Optional[List[Tuple[str, str]]] = None,\n    error_type: str = \"ci\",\n    figsize: Tuple[float, float] = (12, 8),\n    filter_method: str = \"wavelet\",\n    bandwidth: float = 1.0,\n) -&gt; Figure:\n    \"\"\"\n    Plot trial-averaged Phase Locking Value (PLV) between spike rate pairs across frequencies.\n\n    This function computes PLV between spike rates of different population pairs for each trial,\n    then averages the results across trials with optional error bars/bands.\n\n    Parameters\n    ----------\n    spike_rate : xr.DataArray\n        Pre-computed spike rate data with dimensions (time, population, type).\n        Must have an 'fs' attribute (sampling frequency in Hz).\n        Time dimension should be in milliseconds.\n    time_windows : List[Tuple[float, float]]\n        List of (start, end) time tuples in milliseconds for each trial window.\n    pop_names : List[str]\n        List of population names to consider (must match 'population' coords in spike_rate).\n    freqs : Union[List[float], np.ndarray]\n        Array of frequencies (Hz) to analyze.\n    pop_pairs : Optional[List[Tuple[str, str]]], optional\n        List of (pop1, pop2) tuples specifying population pairs to analyze.\n        If None, all unique pairs from pop_names are generated (default: None).\n    error_type : str, optional\n        Type of error bars to plot: \"ci\" for 95% confidence interval, \"sem\" for standard error,\n        or \"std\" for standard deviation (default: \"ci\").\n    figsize : Tuple[float, float], optional\n        Figure size (width, height) in inches (default: (12, 8)).\n    filter_method : str, optional\n        Filtering method for PLV calculation: 'wavelet' or 'butter' (default: 'wavelet').\n    bandwidth : float, optional\n        Bandwidth parameter for wavelet filter (default: 1.0).\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The figure containing the trial-averaged PLV plot.\n\n    Raises\n    ------\n    ValueError\n        If error_type is not 'ci', 'sem', or 'std'.\n        If pop_pairs contains invalid population names.\n        If spike_rate does not have required dimensions/attributes.\n\n    Notes\n    -----\n    - Uses 'raw' type from spike_rate dimension (hardcoded for spike rate analysis).\n    - Time units in time_windows must match spike_rate.time units (milliseconds).\n    - Generates all unique population pairs if pop_pairs is None.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = plot_trial_avg_spike_rate_plv(\n    ...     spike_rate=spike_rate,\n    ...     time_windows=[(1000, 2000), (2500, 3500), (4000, 5000)],\n    ...     pop_names=['ET', 'IT', 'PV', 'SST'],\n    ...     freqs=[10, 20, 30, 40, 50],\n    ...     error_type='ci'\n    ... )\n    \"\"\"\n    sns.set_style(\"whitegrid\")\n\n    # Validate inputs\n    if error_type not in [\"ci\", \"sem\", \"std\"]:\n        raise ValueError(\n            \"error_type must be 'ci' for confidence interval, 'sem' for standard error, or 'std' for standard deviation\"\n        )\n\n    if \"fs\" not in spike_rate.attrs:\n        raise ValueError(\"spike_rate must have 'fs' attribute (sampling frequency).\")\n\n    if \"time\" not in spike_rate.dims or \"population\" not in spike_rate.dims:\n        raise ValueError(\"spike_rate must have 'time' and 'population' dimensions.\")\n\n    # Convert freqs to numpy array\n    freqs = np.array(freqs)\n\n    # Generate population pairs if not provided\n    if pop_pairs is None:\n        n_pops = len(pop_names)\n        pop_pairs = [\n            (pop_names[i], pop_names[j]) for i in range(n_pops) for j in range(i + 1, n_pops)\n        ]\n\n    # Validate population pairs\n    available_pops = set(spike_rate.population.values)\n    for pop1, pop2 in pop_pairs:\n        if pop1 not in available_pops or pop2 not in available_pops:\n            raise ValueError(\n                f\"Population pair ({pop1}, {pop2}) contains names not in spike_rate. \"\n                f\"Available: {available_pops}\"\n            )\n\n    # Get sampling frequency\n    fs = spike_rate.fs\n\n    # Storage for PLV data: dict of pair -&gt; list of trials -&gt; list of freqs\n    all_plv_data = {f\"{p1}-{p2}\": [] for p1, p2 in pop_pairs}\n\n    # Loop through each trial window\n    for trial_idx, (start_time, end_time) in enumerate(time_windows):\n        try:\n            # Slice spike rate data for this trial window\n            trial_data = spike_rate.sel(time=slice(start_time, end_time), type=\"raw\")\n\n            if trial_data.time.size == 0:\n                print(f\"Warning: No data in trial {trial_idx} for window ({start_time}, {end_time})\")\n                for pair_key in all_plv_data.keys():\n                    all_plv_data[pair_key].append([np.nan] * len(freqs))\n                continue\n\n            # Calculate PLV for each population pair at each frequency\n            trial_plv_values = {}\n            for pop1, pop2 in pop_pairs:\n                pair_key = f\"{pop1}-{pop2}\"\n                trial_plv_values[pair_key] = []\n\n                try:\n                    # Extract spike rate signals for this pair\n                    sr1 = trial_data.sel(population=pop1).values\n                    sr2 = trial_data.sel(population=pop2).values\n\n                    if len(sr1) &lt; 2 or len(sr2) &lt; 2:\n                        print(\n                            f\"Warning: Insufficient data for {pair_key} in trial {trial_idx}\"\n                        )\n                        trial_plv_values[pair_key] = [np.nan] * len(freqs)\n                        continue\n\n                    # Calculate PLV for each frequency\n                    for freq in freqs:\n                        try:\n                            plv = bmentr.calculate_signal_signal_plv(\n                                sr1,\n                                sr2,\n                                fs=fs,\n                                freq_of_interest=freq,\n                                filter_method=filter_method,\n                                bandwidth=bandwidth,\n                            )\n                            trial_plv_values[pair_key].append(plv)\n                        except Exception as e:\n                            print(\n                                f\"Warning: Error calculating PLV for {pair_key} at {freq} Hz in trial {trial_idx}: {e}\"\n                            )\n                            trial_plv_values[pair_key].append(np.nan)\n\n                except Exception as e:\n                    print(f\"Warning: Error processing {pair_key} in trial {trial_idx}: {e}\")\n                    trial_plv_values[pair_key] = [np.nan] * len(freqs)\n\n            # Store trial results\n            for pair_key in all_plv_data.keys():\n                if pair_key in trial_plv_values and len(trial_plv_values[pair_key]) == len(freqs):\n                    all_plv_data[pair_key].append(trial_plv_values[pair_key])\n                else:\n                    all_plv_data[pair_key].append([np.nan] * len(freqs))\n\n        except Exception as e:\n            print(f\"Warning: Error processing trial {trial_idx}: {e}\")\n            for pair_key in all_plv_data.keys():\n                all_plv_data[pair_key].append([np.nan] * len(freqs))\n\n    # Convert to numpy arrays and calculate statistics\n    mean_plv = {}\n    error_plv = {}\n\n    for pair_key in all_plv_data.keys():\n        all_plv_data[pair_key] = np.array(all_plv_data[pair_key])  # Shape: (n_trials, n_freqs)\n\n        # Use helper function to calculate trial statistics\n        mean_plv[pair_key], error_plv[pair_key] = calculate_trial_statistics(\n            all_plv_data[pair_key], error_type=error_type\n        )\n\n    # Create plot\n    fig = plt.figure(figsize=figsize)\n\n    # Determine subplot layout if many pairs\n    n_pairs = len(pop_pairs)\n    if n_pairs &lt;= 3:\n        # Single row for up to 3 pairs\n        n_rows, n_cols = 1, n_pairs\n    else:\n        # Multiple rows for more pairs\n        n_cols = min(n_pairs, 3)\n        n_rows = (n_pairs + n_cols - 1) // n_cols\n\n    # Define markers and colors for different pairs\n    markers = [\"o-\", \"s-\", \"^-\", \"D-\", \"v-\", \"&lt;-\", \"&gt;-\", \"p-\"]\n    colors = sns.color_palette(n_colors=n_pairs)\n\n    # Plot each population pair\n    if n_rows == 1 and n_cols == 1:\n        axes = [plt.subplot(n_rows, n_cols, 1)]\n    elif n_rows == 1:\n        axes = [plt.subplot(n_rows, n_cols, i + 1) for i in range(n_cols)]\n    else:\n        axes = [plt.subplot(n_rows, n_cols, i + 1) for i in range(n_pairs)]\n\n    for pair_idx, (pop1, pop2) in enumerate(pop_pairs):\n        pair_key = f\"{pop1}-{pop2}\"\n        marker = markers[pair_idx % len(markers)]\n        color = colors[pair_idx]\n        ax = axes[pair_idx] if isinstance(axes, list) else axes\n\n        # Only plot if we have valid data\n        valid_mask = ~np.isnan(mean_plv[pair_key])\n        if np.any(valid_mask):\n            ax.plot(\n                freqs[valid_mask],\n                mean_plv[pair_key][valid_mask],\n                marker,\n                linewidth=2,\n                label=pair_key,\n                color=color,\n                markersize=6,\n            )\n\n            # Add error bars/shading if available\n            if not np.all(np.isnan(error_plv[pair_key])):\n                ax.fill_between(\n                    freqs[valid_mask],\n                    (mean_plv[pair_key] - error_plv[pair_key])[valid_mask],\n                    (mean_plv[pair_key] + error_plv[pair_key])[valid_mask],\n                    alpha=0.3,\n                    color=color,\n                )\n\n        ax.set_xlabel(\"Frequency (Hz)\", fontsize=11)\n        ax.set_ylabel(\"PLV\", fontsize=11)\n        ax.set_title(f\"{pair_key}\", fontsize=12)\n        ax.grid(True, alpha=0.3)\n\n    # Add overall title\n    error_labels = {\"ci\": \"95% CI\", \"sem\": \"\u00b1SEM\", \"std\": \"\u00b11 SD\"}\n    error_label = error_labels[error_type]\n    fig.suptitle(f\"Trial-Averaged Spike Rate PLV ({error_label})\", fontsize=14, y=0.995)\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/lfp/","title":"LFP/ECP Plotting API","text":""},{"location":"api/bmplot/lfp/#bmtool.bmplot.lfp.plot_spectrogram","title":"<code>bmtool.bmplot.lfp.plot_spectrogram(sxx_xarray, remove_aperiodic=None, log_power=False, plt_range=None, clr_freq_range=None, pad=0.03, ax=None, vmin=None, vmax=None)</code>","text":"<p>Plot a power spectrogram with optional aperiodic removal and frequency-based coloring.</p> <p>Parameters:</p> Name Type Description Default <code>sxx_xarray</code> <code>array - like</code> <p>Spectrogram data as an xarray DataArray with PSD values.</p> required <code>remove_aperiodic</code> <code>optional</code> <p>FOOOF model object for aperiodic subtraction. If None, raw spectrum is displayed.</p> <code>None</code> <code>log_power</code> <code>bool or str</code> <p>If True or 'dB', convert power to log scale. Default is False.</p> <code>False</code> <code>plt_range</code> <code>tuple of float</code> <p>Frequency range to display as (f_min, f_max). If None, displays full range.</p> <code>None</code> <code>clr_freq_range</code> <code>tuple of float</code> <p>Frequency range to use for determining color limits. If None, uses full range.</p> <code>None</code> <code>pad</code> <code>float</code> <p>Padding for colorbar. Default is 0.03.</p> <code>0.03</code> <code>ax</code> <code>Axes</code> <p>Axes to plot on. If None, creates a new figure and axes.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>Minimum value for colorbar scaling. If None, computed from data.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>Maximum value for colorbar scaling. If None, computed from data.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The figure object containing the spectrogram.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig = plot_spectrogram(\n...     sxx_xarray, log_power='dB',\n...     plt_range=(10, 100), clr_freq_range=(20, 50)\n... )\n</code></pre> Source code in <code>bmtool/bmplot/lfp.py</code> <pre><code>def plot_spectrogram(\n    sxx_xarray: Any,\n    remove_aperiodic: Optional[Any] = None,\n    log_power: bool = False,\n    plt_range: Optional[Tuple[float, float]] = None,\n    clr_freq_range: Optional[Tuple[float, float]] = None,\n    pad: float = 0.03,\n    ax: Optional[plt.Axes] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n) -&gt; Figure:\n    \"\"\"\n    Plot a power spectrogram with optional aperiodic removal and frequency-based coloring.\n\n    Parameters\n    ----------\n    sxx_xarray : array-like\n        Spectrogram data as an xarray DataArray with PSD values.\n    remove_aperiodic : optional\n        FOOOF model object for aperiodic subtraction. If None, raw spectrum is displayed.\n    log_power : bool or str, optional\n        If True or 'dB', convert power to log scale. Default is False.\n    plt_range : tuple of float, optional\n        Frequency range to display as (f_min, f_max). If None, displays full range.\n    clr_freq_range : tuple of float, optional\n        Frequency range to use for determining color limits. If None, uses full range.\n    pad : float, optional\n        Padding for colorbar. Default is 0.03.\n    ax : matplotlib.axes.Axes, optional\n        Axes to plot on. If None, creates a new figure and axes.\n    vmin : float, optional\n        Minimum value for colorbar scaling. If None, computed from data.\n    vmax : float, optional\n        Maximum value for colorbar scaling. If None, computed from data.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The figure object containing the spectrogram.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = plot_spectrogram(\n    ...     sxx_xarray, log_power='dB',\n    ...     plt_range=(10, 100), clr_freq_range=(20, 50)\n    ... )\n    \"\"\"\n    sxx = sxx_xarray.PSD.values.copy()\n    t = sxx_xarray.time.values.copy()\n    f = sxx_xarray.frequency.values.copy()\n\n    cbar_label = \"PSD\" if remove_aperiodic is None else \"PSD Residual\"\n    if log_power:\n        with np.errstate(divide=\"ignore\"):\n            sxx = np.log10(sxx)\n        cbar_label += \" dB\" if log_power == \"dB\" else \" log(power)\"\n\n    if remove_aperiodic is not None:\n        f1_idx = 0 if f[0] else 1\n        ap_fit = gen_aperiodic(f[f1_idx:], remove_aperiodic.aperiodic_params)\n        sxx[f1_idx:, :] -= (ap_fit if log_power else 10**ap_fit)[:, None]\n        sxx[:f1_idx, :] = 0.0\n\n    if log_power == \"dB\":\n        sxx *= 10\n\n    if ax is None:\n        _, ax = plt.subplots(1, 1)\n    plt_range = np.array(f[-1]) if plt_range is None else np.array(plt_range)\n    if plt_range.size == 1:\n        plt_range = [f[0 if f[0] else 1] if log_power else 0.0, plt_range.item()]\n    f_idx = (f &gt;= plt_range[0]) &amp; (f &lt;= plt_range[1])\n\n    # Determine vmin and vmax: explicit parameters take precedence, then clr_freq_range, then None\n    if vmin is None:\n        if clr_freq_range is not None:\n            c_idx = (f &gt;= clr_freq_range[0]) &amp; (f &lt;= clr_freq_range[1])\n            vmin = sxx[c_idx, :].min()\n\n    if vmax is None:\n        if clr_freq_range is not None:\n            c_idx = (f &gt;= clr_freq_range[0]) &amp; (f &lt;= clr_freq_range[1])\n            vmax = sxx[c_idx, :].max()\n\n    f = f[f_idx]\n    pcm = ax.pcolormesh(t, f, sxx[f_idx, :], shading=\"gouraud\", vmin=vmin, vmax=vmax, rasterized=True,cmap='viridis')\n    if \"cone_of_influence_frequency\" in sxx_xarray:\n        coif = sxx_xarray.cone_of_influence_frequency\n        ax.plot(t, coif)\n        ax.fill_between(t, coif, step=\"mid\", alpha=0.2)\n    ax.set_xlim(t[0], t[-1])\n    # ax.set_xlim(t[0],0.2)\n    ax.set_ylim(f[0], f[-1])\n    plt.colorbar(mappable=pcm, ax=ax, label=cbar_label, pad=pad)\n    ax.set_xlabel(\"Time (sec)\")\n    ax.set_ylabel(\"Frequency (Hz)\")\n    return ax.figure\n</code></pre>"},{"location":"api/bmplot/lfp/#bmtool.bmplot.lfp.plot_population_spike_rates_with_lfp","title":"<code>bmtool.bmplot.lfp.plot_population_spike_rates_with_lfp(spikes_df, lfp, freq_of_interest, freq_labels, freq_colors, time_range, pop_names, pop_color, pop_groups=None, FR_type='smoothed', stimulus_time=None)</code>","text":"<p>Plot population spike rates with LFP power overlays, with optional trial averaging.</p> <p>Parameters:</p> Name Type Description Default <code>spikes_df</code> <code>DataFrame</code> <p>DataFrame with spike data.</p> required <code>lfp</code> <code>array - like</code> <p>LFP data (xarray or similar format).</p> required <code>freq_of_interest</code> <code>list of float</code> <p>List of frequencies for LFP power analysis (required).</p> required <code>freq_labels</code> <code>list of str</code> <p>Labels for the frequencies (required).</p> required <code>freq_colors</code> <code>list of str</code> <p>Colors for the frequency plots (required).</p> required <code>time_range</code> <code>tuple of float or list of tuple</code> <p>If tuple (start, end): plots continuous data in that time range. If list of tuples: trial times for averaging. E.g., [(1000,2000), (2500,3500)]. For trial averaging, mean is computed across trials (required).</p> required <code>pop_names</code> <code>list of str</code> <p>List of population names (required).</p> required <code>pop_color</code> <code>dict</code> <p>Dictionary mapping population names to colors (required).</p> required <code>pop_groups</code> <code>list of list of str</code> <p>List of population groups to plot on the same subplot.  E.g., [['PV', 'SST'], ['ET', 'IT']] plots PV and SST on one plot, ET and IT on another. If None, each population gets its own subplot (default).</p> <code>None</code> <code>FR_type</code> <code>str</code> <p>Type of firing rate to plot ('raw', 'smoothed', etc.). Default is 'smoothed'.</p> <code>'smoothed'</code> <code>stimulus_time</code> <code>float</code> <p>Time of stimulus onset.  For trial averaging: relative to the start of the trial window (e.g., stimulus_time=200 means 200ms after trial start). For continuous plots: absolute time value (e.g., stimulus_time=2500 means stimulus at 2500ms). When provided, the x-axis will be relative to stimulus time (0 = stimulus onset). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure or None</code> <p>Figure object containing the plot, or None if no data to plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Continuous plot\n&gt;&gt;&gt; fig = plot_population_spike_rates_with_lfp(\n...     spikes_df, lfp, [40, 80], ['Beta', 'Gamma'],\n...     ['blue', 'red'], (0, 10), ['PV', 'SST'],\n...     {'PV': 'blue', 'SST': 'red'},\n...     pop_groups=[['PV', 'SST']]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Trial-averaged plot\n&gt;&gt;&gt; fig = plot_population_spike_rates_with_lfp(\n...     spikes_df, lfp, [40, 80], ['Beta', 'Gamma'],\n...     ['blue', 'red'], [(1000,2000), (2500,3500)], ['PV', 'SST'],\n...     {'PV': 'blue', 'SST': 'red'},\n...     pop_groups=[['PV', 'SST']]\n... )\n</code></pre> Source code in <code>bmtool/bmplot/lfp.py</code> <pre><code>def plot_population_spike_rates_with_lfp(\n    spikes_df: pd.DataFrame,\n    lfp: Any,\n    freq_of_interest: List[float],\n    freq_labels: List[str],\n    freq_colors: List[str],\n    time_range: Any,\n    pop_names: List[str],\n    pop_color: Dict[str, str],\n    pop_groups: Optional[List[List[str]]] = None,\n    FR_type: str = 'smoothed',\n    stimulus_time: Optional[float] = None,\n) -&gt; Optional[Figure]:\n    \"\"\"\n    Plot population spike rates with LFP power overlays, with optional trial averaging.\n\n    Parameters\n    ----------\n    spikes_df : pd.DataFrame\n        DataFrame with spike data.\n    lfp : array-like\n        LFP data (xarray or similar format).\n    freq_of_interest : list of float\n        List of frequencies for LFP power analysis (required).\n    freq_labels : list of str\n        Labels for the frequencies (required).\n    freq_colors : list of str\n        Colors for the frequency plots (required).\n    time_range : tuple of float or list of tuple\n        If tuple (start, end): plots continuous data in that time range.\n        If list of tuples: trial times for averaging. E.g., [(1000,2000), (2500,3500)].\n        For trial averaging, mean is computed across trials (required).\n    pop_names : list of str\n        List of population names (required).\n    pop_color : dict\n        Dictionary mapping population names to colors (required).\n    pop_groups : list of list of str, optional\n        List of population groups to plot on the same subplot. \n        E.g., [['PV', 'SST'], ['ET', 'IT']] plots PV and SST on one plot, ET and IT on another.\n        If None, each population gets its own subplot (default).\n    FR_type : str, optional\n        Type of firing rate to plot ('raw', 'smoothed', etc.). Default is 'smoothed'.\n    stimulus_time : float, optional\n        Time of stimulus onset. \n        For trial averaging: relative to the start of the trial window (e.g., stimulus_time=200 means 200ms after trial start).\n        For continuous plots: absolute time value (e.g., stimulus_time=2500 means stimulus at 2500ms).\n        When provided, the x-axis will be relative to stimulus time (0 = stimulus onset). Default is None.\n\n    Returns\n    -------\n    matplotlib.figure.Figure or None\n        Figure object containing the plot, or None if no data to plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Continuous plot\n    &gt;&gt;&gt; fig = plot_population_spike_rates_with_lfp(\n    ...     spikes_df, lfp, [40, 80], ['Beta', 'Gamma'],\n    ...     ['blue', 'red'], (0, 10), ['PV', 'SST'],\n    ...     {'PV': 'blue', 'SST': 'red'},\n    ...     pop_groups=[['PV', 'SST']]\n    ... )\n\n    &gt;&gt;&gt; # Trial-averaged plot\n    &gt;&gt;&gt; fig = plot_population_spike_rates_with_lfp(\n    ...     spikes_df, lfp, [40, 80], ['Beta', 'Gamma'],\n    ...     ['blue', 'red'], [(1000,2000), (2500,3500)], ['PV', 'SST'],\n    ...     {'PV': 'blue', 'SST': 'red'},\n    ...     pop_groups=[['PV', 'SST']]\n    ... )\n    \"\"\"\n    # Compute spike rates\n    spike_rate = get_population_spike_rate(spikes_df, fs=400, network_name='cortex')\n\n    # Compute power for each frequency of interest\n    powers = [\n        get_lfp_power(lfp, freq_of_interest=freq, fs=lfp.fs, filter_method=\"wavelet\", bandwidth=1.0)\n        for freq in freq_of_interest\n    ]\n\n    # Determine if we're doing trial averaging\n    is_trial_avg = isinstance(time_range, list) and len(time_range) &gt; 0 and isinstance(time_range[0], tuple)\n\n    # Extract and align trials if needed\n    spike_rate_trials: Optional[List] = None\n    power_trials: Optional[List] = None\n    target_length: Optional[int] = None\n    trial_start: float = 0.0\n    trial_duration: float = 0.0\n    if is_trial_avg:\n        spike_rate_trials, power_trials, trial_times = _extract_trials(\n            spike_rate, powers, time_range\n        )\n        # trial_times from _extract_trials is normalized (0 to 1)\n        # Convert to actual milliseconds based on first trial duration\n        trial_start = float(time_range[0][0])\n        trial_end = float(time_range[0][1])\n        trial_duration = trial_end - trial_start\n\n        # Convert normalized times to milliseconds\n        plot_time = trial_times * trial_duration\n\n        # Adjust for stimulus if provided (stimulus_time is relative to trial start)\n        if stimulus_time is not None:\n            plot_time = plot_time - stimulus_time\n\n        target_length = len(trial_times)\n    else:\n        # For continuous plots, time_range is a tuple (start, end)\n        # We'll just pass the time_range for now; actual shifting happens during plotting\n        plot_time = time_range\n\n    # Determine plot groups\n    if pop_groups is None:\n        # Default: each population gets its own subplot\n        plot_groups = [[pop] for pop in pop_names]\n    else:\n        plot_groups = pop_groups\n\n    # Plotting\n    num_subplots = len(plot_groups)\n    fig, axes = plt.subplots(num_subplots, 1, figsize=(12, 3.5 * num_subplots))\n    if num_subplots == 1:\n        axes = [axes]\n\n    for ax_idx, group in enumerate(plot_groups):\n        ax = axes[ax_idx]\n\n        # Filter valid populations in this group\n        valid_pops = [pop for pop in group if pop in spike_rate.population.values]\n\n        if not valid_pops:\n            continue\n\n        # Plot spike rates for each population in the group\n        fr_handles = []\n        if is_trial_avg and spike_rate_trials is not None:\n            # Plot trial-averaged firing rates with SEM shading\n            for pop in valid_pops:\n                fr_mean, fr_sem = _compute_trial_average(spike_rate_trials, pop, FR_type, target_length=target_length)\n                line, = ax.plot(plot_time, fr_mean,\n                               color=pop_color[pop], \n                               label=f'{pop} FR',\n                               linewidth=2)\n                ax.fill_between(plot_time, fr_mean - fr_sem, fr_mean + fr_sem,\n                               color=pop_color[pop], alpha=0.2)\n                fr_handles.append(line)\n        else:\n            # Plot continuous firing rates\n            plot_time_values = spike_rate.time.values\n            if stimulus_time is not None and not is_trial_avg:\n                # Shift time axis relative to stimulus\n                plot_time_values = plot_time_values - stimulus_time\n\n            for pop in valid_pops:\n                line, = ax.plot(plot_time_values, \n                               spike_rate.sel(type=FR_type, population=pop).values,\n                               color=pop_color[pop], \n                               label=f'{pop} FR',\n                               linewidth=2)\n                fr_handles.append(line)\n\n        # Set labels and title\n        group_title = ' + '.join(valid_pops)\n        avg_text = ' (Trial Avg)' if is_trial_avg else ''\n        ax.set_title(group_title + avg_text, fontsize=12)\n        ax.set_ylabel('Spike Rate (Hz)', fontsize=11)\n        ax.tick_params(axis='y')\n\n        # Twin axis for LFP power\n        ax2 = ax.twinx()\n        lfp_handles = []\n        if is_trial_avg and power_trials is not None:\n            # Plot trial-averaged LFP power with SEM shading\n            for power_trial, label, color in zip(power_trials, freq_labels, freq_colors):\n                power_mean, power_sem = _compute_trial_average_power(power_trial, target_length=target_length)\n                line, = ax2.plot(plot_time, power_mean,\n                                color=color, label=label, linestyle='--', linewidth=2)\n                ax2.fill_between(plot_time, power_mean - power_sem, power_mean + power_sem,\n                                color=color, alpha=0.1)\n                lfp_handles.append(line)\n        else:\n            # Plot continuous LFP power\n            for power, label, color in zip(powers, freq_labels, freq_colors):\n                plot_time_lfp = power['time'].values\n                if stimulus_time is not None and not is_trial_avg:\n                    # Shift time axis relative to stimulus\n                    plot_time_lfp = plot_time_lfp - stimulus_time\n\n                line, = ax2.plot(plot_time_lfp, power.values.squeeze(), \n                                color=color, label=label, linestyle='--', linewidth=2)\n                lfp_handles.append(line)\n\n        ax2.set_ylabel('LFP Power', fontsize=11)\n        ax2.tick_params(axis='y')\n\n        # Combined legend\n        all_handles = fr_handles + lfp_handles\n        all_labels = [h.get_label() for h in all_handles]\n        ax.legend(all_handles, all_labels, loc='upper right', fontsize=10)\n\n        if is_trial_avg:\n            ax.set_xlim(plot_time[0], plot_time[-1])\n            if stimulus_time is not None:\n                ax.set_xlabel('Time relative to stimulus (ms)', fontsize=11)\n            else:\n                ax.set_xlabel('Time from trial start (ms)', fontsize=11)\n        else:\n            # For continuous plots\n            if stimulus_time is not None:\n                # Shift xlim by stimulus time\n                xlim = (plot_time[0] - stimulus_time, plot_time[1] - stimulus_time)\n                ax.set_xlim(xlim)\n                ax.set_xlabel('Time relative to stimulus (ms)', fontsize=11)\n            else:\n                ax.set_xlim(plot_time)\n                ax.set_xlabel('Time (ms)', fontsize=11)\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/lfp/#bmtool.bmplot.lfp.plot_spike_rate_coherence","title":"<code>bmtool.bmplot.lfp.plot_spike_rate_coherence(spike_rates, fooof_params=None, plt_range=None, plt_log=False, plt_db=True, figsize=(10, 3), ax=None)</code>","text":"<p>Plot coherence between spike rate populations.</p> <p>Computes coherence exactly like Analyze_PSD_ziao notebook: calculates coherence between population pairs and applies FOOOF fitting to coherence spectra.</p> <p>Parameters:</p> Name Type Description Default <code>spike_rates</code> <code>DataArray</code> <p>Spike rate data with dimensions (population, time) and 'fs' attribute</p> required <code>fooof_params</code> <code>dict</code> <p>Parameters for FOOOF fitting. If None, uses default parameters</p> <code>None</code> <code>plt_range</code> <code>tuple</code> <p>Frequency range to display (default: [2, 100])</p> <code>None</code> <code>plt_log</code> <code>bool</code> <p>Use log scale for frequency axis, default: False</p> <code>False</code> <code>plt_db</code> <code>bool</code> <p>Plot power in dB, default: True</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size, default: (10, 3)</p> <code>(10, 3)</code> <code>ax</code> <code>Axes</code> <p>Axes to plot on. If None, creates new figure</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Figure object containing the coherence plots</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig = plot_spike_rate_coherence(spike_rates=spike_rate_data)\n</code></pre> Source code in <code>bmtool/bmplot/lfp.py</code> <pre><code>def plot_spike_rate_coherence(\n    spike_rates: Any,\n    fooof_params: Optional[Dict] = None,\n    plt_range: Optional[Tuple[float, float]] = None,\n    plt_log: bool = False,\n    plt_db: bool = True,\n    figsize: Tuple[int, int] = (10, 3),\n    ax: Optional[plt.Axes] = None,\n) -&gt; Figure:\n    \"\"\"\n    Plot coherence between spike rate populations.\n\n    Computes coherence exactly like Analyze_PSD_ziao notebook: calculates coherence\n    between population pairs and applies FOOOF fitting to coherence spectra.\n\n    Parameters\n    ----------\n    spike_rates : xr.DataArray\n        Spike rate data with dimensions (population, time) and 'fs' attribute\n    fooof_params : dict, optional\n        Parameters for FOOOF fitting. If None, uses default parameters\n    plt_range : tuple, optional\n        Frequency range to display (default: [2, 100])\n    plt_log : bool\n        Use log scale for frequency axis, default: False\n    plt_db : bool\n        Plot power in dB, default: True\n    figsize : tuple\n        Figure size, default: (10, 3)\n    ax : matplotlib.axes.Axes, optional\n        Axes to plot on. If None, creates new figure\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure object containing the coherence plots\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = plot_spike_rate_coherence(spike_rates=spike_rate_data)\n    \"\"\"\n    from scipy import signal\n\n    from ..analysis.lfp import fit_fooof\n\n    # Extract fs from spike_rates attributes\n    if not hasattr(spike_rates, 'attrs') or 'fs' not in spike_rates.attrs:\n        raise ValueError(\"spike_rates must have 'fs' attribute\")\n    fs = spike_rates.attrs['fs']\n\n    # Set default parameters\n    if fooof_params is None:\n        fooof_params = dict(aperiodic_mode='knee', freq_range=(1, 100), \n                           peak_width_limits=100., max_n_peaks=1, dB_threshold=0.05)\n\n    if plt_range is None:\n        plt_range = [2., 100.]\n\n    # Get population pairs like in Analyze_PSD_ziao\n    pop_names = spike_rates.population.values\n    n_pops = len(pop_names)\n    grp_pairs = [[i, j] for i in range(n_pops) for j in range(i+1, n_pops)]\n    npairs = len(grp_pairs)\n\n    if npairs == 0:\n        raise ValueError(\"Need at least 2 populations for coherence analysis\")\n\n    # Create figure with max 3 plots per row\n    if ax is None:\n        ncols = min(npairs, 3)\n        nrows = (npairs + ncols - 1) // ncols\n        fig, axes = plt.subplots(nrows, ncols, figsize=(figsize[0], figsize[1]))\n    else:\n        fig = ax.get_figure()\n        axes = [ax]\n\n    if npairs == 1:\n        axes = [axes] if not isinstance(axes, (list, np.ndarray)) else axes\n\n    # Calculate coherence for each pair\n    for i, grp_pair in enumerate(grp_pairs):\n        if isinstance(axes, np.ndarray):\n            ax = axes.flat[i]\n        else:\n            ax = axes[i]\n\n        pop1_name = pop_names[grp_pair[0]]\n        pop2_name = pop_names[grp_pair[1]]\n\n        # Get spike rate data for the pair\n        signal1 = spike_rates.sel(type='smoothed', population=pop1_name).values\n        signal2 = spike_rates.sel(type='smoothed', population=pop2_name).values\n\n        # Check if both populations have non-zero std\n        if np.std(signal1) == 0 or np.std(signal2) == 0:\n            ax.text(0.5, 0.5, 'No variation in data', ha='center', va='center',\n                   transform=ax.transAxes)\n            ax.set_title(f'Coherence {pop1_name}-{pop2_name}')\n            continue\n\n        # Calculate coherence over entire time series\n        f, cxy = signal.coherence(signal1, signal2, fs=fs)\n\n        # Filter valid coherence values (positive and not NaN)\n        idx = (cxy &gt; 0) &amp; np.isfinite(cxy)\n        if not np.any(idx):\n            ax.text(0.5, 0.5, 'No valid coherence', ha='center', va='center',\n                   transform=ax.transAxes)\n            ax.set_title(f'Coherence {pop1_name}-{pop2_name}')\n            continue\n\n        # Apply FOOOF to coherence (exactly like Analyze_PSD_ziao)\n        f_filtered = f[idx]\n        cxy_filtered = cxy[idx]\n\n        plt.sca(ax)\n        fooof_results, fm = fit_fooof(f_filtered, cxy_filtered, **fooof_params, \n                                   report=False, plot=True)\n\n        # Formatting like Analyze_PSD_ziao\n        ax.set_xlabel('Frequency (Hz)')\n        ax.set_ylabel('Coherence')\n        ax.set_title(f'Coherence {pop1_name}-{pop2_name}')\n        ax.set_xlim(plt_range)\n        ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/spikes/","title":"Spike Plotting API","text":""},{"location":"api/bmplot/spikes/#bmtool.bmplot.spikes.raster","title":"<code>bmtool.bmplot.spikes.raster(spikes_df=None, config=None, network_name=None, groupby='pop_name', sortby=None, ax=None, tstart=None, tstop=None, color_map=None, dot_size=0.3)</code>","text":"<p>Plots a raster plot of neural spikes, with different colors for each population.</p> Parameters: <p>spikes_df : pd.DataFrame, optional     DataFrame containing spike data with columns 'timestamps', 'node_ids', and optional 'pop_name'. config : str, optional     Path to the configuration file used to load node data. network_name : str, optional     Specific network name to select from the configuration; if not provided, uses the first network. groupby : str, optional     Column name to group spikes by for coloring. Default is 'pop_name'. sortby : str, optional     Column name to sort node_ids within each group. If provided, nodes within each population will be sorted by this column. ax : matplotlib.axes.Axes, optional     Axes on which to plot the raster; if None, a new figure and axes are created. tstart : float, optional     Start time for filtering spikes; only spikes with timestamps greater than <code>tstart</code> will be plotted. tstop : float, optional     Stop time for filtering spikes; only spikes with timestamps less than <code>tstop</code> will be plotted. color_map : dict, optional     Dictionary specifying colors for each population. Keys should be population names, and values should be color values. dot_size: float, optional     Size of the dot to display on the scatterplot</p> Returns: <p>matplotlib.axes.Axes     Axes with the raster plot.</p> Notes: <ul> <li>If <code>config</code> is provided, the function merges population names from the node data with <code>spikes_df</code>.</li> <li>Each unique population from groupby in <code>spikes_df</code> will be represented by a different color if <code>color_map</code> is not specified.</li> <li>If <code>color_map</code> is provided, it should contain colors for all unique <code>pop_name</code> values in <code>spikes_df</code>.</li> </ul> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def raster(\n    spikes_df: Optional[pd.DataFrame] = None,\n    config: Optional[str] = None,\n    network_name: Optional[str] = None,\n    groupby: str = \"pop_name\",\n    sortby: Optional[str] = None,\n    ax: Optional[Axes] = None,\n    tstart: Optional[float] = None,\n    tstop: Optional[float] = None,\n    color_map: Optional[Dict[str, str]] = None,\n    dot_size: float = 0.3,\n) -&gt; Axes:\n    \"\"\"\n    Plots a raster plot of neural spikes, with different colors for each population.\n\n    Parameters:\n    ----------\n    spikes_df : pd.DataFrame, optional\n        DataFrame containing spike data with columns 'timestamps', 'node_ids', and optional 'pop_name'.\n    config : str, optional\n        Path to the configuration file used to load node data.\n    network_name : str, optional\n        Specific network name to select from the configuration; if not provided, uses the first network.\n    groupby : str, optional\n        Column name to group spikes by for coloring. Default is 'pop_name'.\n    sortby : str, optional\n        Column name to sort node_ids within each group. If provided, nodes within each population will be sorted by this column.\n    ax : matplotlib.axes.Axes, optional\n        Axes on which to plot the raster; if None, a new figure and axes are created.\n    tstart : float, optional\n        Start time for filtering spikes; only spikes with timestamps greater than `tstart` will be plotted.\n    tstop : float, optional\n        Stop time for filtering spikes; only spikes with timestamps less than `tstop` will be plotted.\n    color_map : dict, optional\n        Dictionary specifying colors for each population. Keys should be population names, and values should be color values.\n    dot_size: float, optional\n        Size of the dot to display on the scatterplot\n\n    Returns:\n    -------\n    matplotlib.axes.Axes\n        Axes with the raster plot.\n\n    Notes:\n    -----\n    - If `config` is provided, the function merges population names from the node data with `spikes_df`.\n    - Each unique population from groupby in `spikes_df` will be represented by a different color if `color_map` is not specified.\n    - If `color_map` is provided, it should contain colors for all unique `pop_name` values in `spikes_df`.\n    \"\"\"\n    # Initialize axes if none provided\n    sns.set_style(\"whitegrid\")\n    if ax is None:\n        _, ax = plt.subplots(1, 1)\n\n    # Filter spikes by time range if specified\n    if tstart is not None:\n        spikes_df = spikes_df[spikes_df[\"timestamps\"] &gt; tstart]\n    if tstop is not None:\n        spikes_df = spikes_df[spikes_df[\"timestamps\"] &lt; tstop]\n\n    # Load and merge node population data if config is provided\n    if config:\n        nodes = load_nodes_from_config(config)\n        if network_name:\n            nodes = nodes.get(network_name, {})\n        else:\n            nodes = list(nodes.values())[0] if nodes else {}\n            print(\n                \"Grabbing first network; specify a network name to ensure correct node population is selected.\"\n            )\n\n        # Find common columns, but exclude the join key from the list\n        common_columns = spikes_df.columns.intersection(nodes.columns).tolist()\n        common_columns = [\n            col for col in common_columns if col != \"node_ids\"\n        ]  # Remove our join key from the common list\n\n        # Drop all intersecting columns except the join key column from df2\n        spikes_df = spikes_df.drop(columns=common_columns)\n        # merge nodes and spikes df\n        spikes_df = spikes_df.merge(\n            nodes[groupby], left_on=\"node_ids\", right_index=True, how=\"left\"\n        )\n\n    # Get unique population names\n    unique_pop_names = spikes_df[groupby].unique()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"tab10\")  # Default colormap\n        color_map = {\n            pop_name: cmap(i / len(unique_pop_names)) for i, pop_name in enumerate(unique_pop_names)\n        }\n    else:\n        # Ensure color_map contains all population names\n        missing_colors = [pop for pop in unique_pop_names if pop not in color_map]\n        if missing_colors:\n            raise ValueError(f\"color_map is missing colors for populations: {missing_colors}\")\n\n    # Plot each population with its specified or generated color\n    legend_handles = []\n    y_offset = 0  # Track y-position offset for stacking populations\n\n    for pop_name, group in spikes_df.groupby(groupby):\n        if sortby:\n            # Sort by the specified column, putting NaN values at the end\n            group_sorted = group.sort_values(by=sortby, na_position='last')\n            # Create a mapping from node_ids to consecutive y-positions based on sorted order\n            # Use the sorted order to maintain the same sequence for all spikes from same node\n            unique_nodes_sorted = group_sorted['node_ids'].drop_duplicates()\n            node_to_y = {node_id: y_offset + i for i, node_id in enumerate(unique_nodes_sorted)}\n            # Map node_ids to new y-positions for ALL spikes (not just the sorted group)\n            y_positions = group['node_ids'].map(node_to_y)\n            # Verify no data was lost\n            assert len(y_positions) == len(group), f\"Data loss detected in population {pop_name}\"\n            assert y_positions.isna().sum() == 0, f\"Unmapped node_ids found in population {pop_name}\"\n        else:\n            y_positions = group['node_ids']\n\n        ax.scatter(group[\"timestamps\"], y_positions, color=color_map[pop_name], s=dot_size)\n        # Dummy scatter for consistent legend appearance\n        handle = ax.scatter([], [], color=color_map[pop_name], label=pop_name, s=20)\n        legend_handles.append(handle)\n\n        # Update y_offset for next population if sortby is used\n        if sortby:\n            y_offset += len(unique_nodes_sorted)\n\n    # Label axes\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Node ID\")\n    ax.legend(handles=legend_handles, title=\"Population\", loc=\"upper right\", framealpha=0.9)\n\n    return ax\n</code></pre>"},{"location":"api/bmplot/spikes/#bmtool.bmplot.spikes.plot_firing_rate_pop_stats","title":"<code>bmtool.bmplot.spikes.plot_firing_rate_pop_stats(firing_stats, groupby, ax=None, color_map=None)</code>","text":"<p>Plots a bar graph of mean firing rates with error bars (standard deviation).</p> Parameters: <p>firing_stats : pd.DataFrame     Dataframe containing 'firing_rate_mean' and 'firing_rate_std'. groupby : str or list of str     Column(s) used for grouping. ax : matplotlib.axes.Axes, optional     Axes on which to plot the bar chart; if None, a new figure and axes are created. color_map : dict, optional     Dictionary specifying colors for each group. Keys should be group names, and values should be color values.</p> Returns: <p>matplotlib.axes.Axes     Axes with the bar plot.</p> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def plot_firing_rate_pop_stats(\n    firing_stats: pd.DataFrame,\n    groupby: Union[str, List[str]],\n    ax: Optional[Axes] = None,\n    color_map: Optional[Dict[str, str]] = None,\n) -&gt; Axes:\n    \"\"\"\n    Plots a bar graph of mean firing rates with error bars (standard deviation).\n\n    Parameters:\n    ----------\n    firing_stats : pd.DataFrame\n        Dataframe containing 'firing_rate_mean' and 'firing_rate_std'.\n    groupby : str or list of str\n        Column(s) used for grouping.\n    ax : matplotlib.axes.Axes, optional\n        Axes on which to plot the bar chart; if None, a new figure and axes are created.\n    color_map : dict, optional\n        Dictionary specifying colors for each group. Keys should be group names, and values should be color values.\n\n    Returns:\n    -------\n    matplotlib.axes.Axes\n        Axes with the bar plot.\n    \"\"\"\n    # Ensure groupby is a list for consistent handling\n    sns.set_style(\"whitegrid\")\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    # Create a categorical column for grouping\n    firing_stats[\"group\"] = firing_stats[groupby].astype(str).agg(\"_\".join, axis=1)\n\n    # Get unique group names\n    unique_groups = firing_stats[\"group\"].unique()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"viridis\")\n        color_map = {group: cmap(i / len(unique_groups)) for i, group in enumerate(unique_groups)}\n    else:\n        # Ensure color_map contains all groups\n        missing_colors = [group for group in unique_groups if group not in color_map]\n        if missing_colors:\n            raise ValueError(f\"color_map is missing colors for groups: {missing_colors}\")\n\n    # Create new figure and axes if ax is not provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Sort data for consistent plotting\n    firing_stats = firing_stats.sort_values(by=\"group\")\n\n    # Extract values for plotting\n    x_labels = firing_stats[\"group\"]\n    means = firing_stats[\"firing_rate_mean\"]\n    std_devs = firing_stats[\"firing_rate_std\"]\n\n    # Get colors for each group\n    colors = [color_map[group] for group in x_labels]\n\n    # Create bar plot\n    bars = ax.bar(x_labels, means, yerr=std_devs, capsize=5, color=colors, edgecolor=\"black\")\n\n    # Add error bars manually with caps\n    _, caps, _ = ax.errorbar(\n        x=np.arange(len(x_labels)),\n        y=means,\n        yerr=std_devs,\n        fmt=\"none\",\n        capsize=5,\n        capthick=2,\n        color=\"black\",\n    )\n\n    # Formatting\n    ax.set_xticks(np.arange(len(x_labels)))\n    ax.set_xticklabels(x_labels, rotation=45, ha=\"right\")\n    ax.set_xlabel(\"Population Group\")\n    ax.set_ylabel(\"Mean Firing Rate (spikes/s)\")\n    ax.set_title(\"Firing Rate Statistics by Population\")\n    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n    return ax\n</code></pre>"},{"location":"api/bmplot/spikes/#bmtool.bmplot.spikes.plot_firing_rate_distribution","title":"<code>bmtool.bmplot.spikes.plot_firing_rate_distribution(individual_stats, groupby, ax=None, color_map=None, plot_type='box', swarm_alpha=0.6, logscale=False)</code>","text":"<p>Plots a distribution of individual firing rates using one or more plot types (box plot, violin plot, or swarm plot), overlaying them on top of each other.</p> Parameters: <p>individual_stats : pd.DataFrame     Dataframe containing individual firing rates and corresponding group labels. groupby : str or list of str     Column(s) used for grouping. ax : matplotlib.axes.Axes, optional     Axes on which to plot the graph; if None, a new figure and axes are created. color_map : dict, optional     Dictionary specifying colors for each group. Keys should be group names, and values should be color values. plot_type : str or list of str, optional     List of plot types to generate. Options: \"box\", \"violin\", \"swarm\". Default is \"box\". swarm_alpha : float, optional     Transparency of swarm plot points. Default is 0.6. logscale : bool, optional     If True, use logarithmic scale for the y-axis (default is False).</p> Returns: <p>matplotlib.axes.Axes     Axes with the selected plot type(s) overlayed.</p> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def plot_firing_rate_distribution(\n    individual_stats: pd.DataFrame,\n    groupby: Union[str, List[str]],\n    ax: Optional[Axes] = None,\n    color_map: Optional[Dict[str, str]] = None,\n    plot_type: Union[str, List[str]] = \"box\",\n    swarm_alpha: float = 0.6,\n    logscale: bool = False,\n) -&gt; Axes:\n    \"\"\"\n    Plots a distribution of individual firing rates using one or more plot types\n    (box plot, violin plot, or swarm plot), overlaying them on top of each other.\n\n    Parameters:\n    ----------\n    individual_stats : pd.DataFrame\n        Dataframe containing individual firing rates and corresponding group labels.\n    groupby : str or list of str\n        Column(s) used for grouping.\n    ax : matplotlib.axes.Axes, optional\n        Axes on which to plot the graph; if None, a new figure and axes are created.\n    color_map : dict, optional\n        Dictionary specifying colors for each group. Keys should be group names, and values should be color values.\n    plot_type : str or list of str, optional\n        List of plot types to generate. Options: \"box\", \"violin\", \"swarm\". Default is \"box\".\n    swarm_alpha : float, optional\n        Transparency of swarm plot points. Default is 0.6.\n    logscale : bool, optional\n        If True, use logarithmic scale for the y-axis (default is False).\n\n    Returns:\n    -------\n    matplotlib.axes.Axes\n        Axes with the selected plot type(s) overlayed.\n    \"\"\"\n    sns.set_style(\"whitegrid\")\n    # Ensure groupby is a list for consistent handling\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    # Create a categorical column for grouping\n    individual_stats[\"group\"] = individual_stats[groupby].astype(str).agg(\"_\".join, axis=1)\n\n    # Validate plot_type (it can be a list or a single type)\n    if isinstance(plot_type, str):\n        plot_type = [plot_type]\n\n    for pt in plot_type:\n        if pt not in [\"box\", \"violin\", \"swarm\"]:\n            raise ValueError(\"plot_type must be one of: 'box', 'violin', 'swarm'.\")\n\n    # Get unique groups for coloring\n    unique_groups = individual_stats[\"group\"].unique()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"viridis\")\n        color_map = {group: cmap(i / len(unique_groups)) for i, group in enumerate(unique_groups)}\n\n    # Ensure color_map contains all groups\n    missing_colors = [group for group in unique_groups if group not in color_map]\n    if missing_colors:\n        raise ValueError(f\"color_map is missing colors for groups: {missing_colors}\")\n\n    # Create new figure and axes if ax is not provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Sort data for consistent plotting\n    individual_stats = individual_stats.sort_values(by=\"group\")\n\n    # Loop over each plot type and overlay them\n    for pt in plot_type:\n        if pt == \"box\":\n            sns.boxplot(\n                data=individual_stats,\n                x=\"group\",\n                y=\"firing_rate\",\n                ax=ax,\n                palette=color_map,\n                width=0.5,\n            )\n        elif pt == \"violin\":\n            sns.violinplot(\n                data=individual_stats,\n                x=\"group\",\n                y=\"firing_rate\",\n                ax=ax,\n                palette=color_map,\n                inner=\"box\",\n                alpha=0.4,\n                cut=0,  # This prevents the KDE from extending beyond the data range\n            )\n        elif pt == \"swarm\":\n            sns.swarmplot(\n                data=individual_stats,\n                x=\"group\",\n                y=\"firing_rate\",\n                ax=ax,\n                palette=color_map,\n                alpha=swarm_alpha,\n            )\n\n    # Formatting\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_xlabel(\"Population Group\")\n    ax.set_ylabel(\"Firing Rate (spikes/s)\")\n    ax.set_title(\"Firing Rate Distribution for individual cells\")\n    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n    if logscale:\n        ax.set_yscale('log')\n\n    return ax\n</code></pre>"},{"location":"api/bmplot/spikes/#bmtool.bmplot.spikes.plot_firing_rate_vs_node_attribute","title":"<code>bmtool.bmplot.spikes.plot_firing_rate_vs_node_attribute(individual_stats, groupby, attribute, config=None, nodes=None, network_name=None, figsize=(12, 8), dot_size=3, color_map=None)</code>","text":"<p>Plot firing rate vs node attribute for each group in separate subplots.</p> <p>Parameters:</p> Name Type Description Default <code>individual_stats</code> <code>DataFrame</code> <p>DataFrame containing individual cell firing rates from compute_firing_rate_stats</p> required <code>groupby</code> <code>str</code> <p>Column name in individual_stats to group plots by</p> required <code>attribute</code> <code>str</code> <p>Node attribute column name to plot against firing rate</p> required <code>config</code> <code>str</code> <p>Path to configuration file for loading node data</p> <code>None</code> <code>nodes</code> <code>DataFrame</code> <p>Pre-loaded node data as alternative to loading from config</p> <code>None</code> <code>network_name</code> <code>str</code> <p>Name of network to load from config file</p> <code>None</code> <code>figsize</code> <code>Tuple[float, float]</code> <p>Figure dimensions (width, height) in inches</p> <code>(12, 8)</code> <code>dot_size</code> <code>float</code> <p>Size of scatter plot points</p> <code>3</code> <code>color_map</code> <code>dict</code> <p>Dictionary specifying colors for each group. Keys should be group names, and values should be color values.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Figure containing the subplots</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither config nor nodes is provided If network_name is missing when using config If attribute is not found in nodes DataFrame If node_ids column is missing If nodes index is not unique</p> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def plot_firing_rate_vs_node_attribute(\n    individual_stats: pd.DataFrame,\n    groupby: str,\n    attribute: str,\n    config: Optional[str] = None,\n    nodes: Optional[pd.DataFrame] = None,\n    network_name: Optional[str] = None,\n    figsize: Tuple[float, float] = (12, 8),\n    dot_size: float = 3,\n    color_map: Optional[Dict[str, str]] = None,\n) -&gt; plt.Figure:\n    \"\"\"\n    Plot firing rate vs node attribute for each group in separate subplots.\n\n    Parameters\n    ----------\n    individual_stats : pd.DataFrame\n        DataFrame containing individual cell firing rates from compute_firing_rate_stats\n    groupby : str\n        Column name in individual_stats to group plots by\n    attribute : str\n        Node attribute column name to plot against firing rate\n    config : str, optional\n        Path to configuration file for loading node data\n    nodes : pd.DataFrame, optional\n        Pre-loaded node data as alternative to loading from config\n    network_name : str, optional\n        Name of network to load from config file\n    figsize : Tuple[float, float], optional\n        Figure dimensions (width, height) in inches\n    dot_size : float, optional\n        Size of scatter plot points\n    color_map : dict, optional\n        Dictionary specifying colors for each group. Keys should be group names, and values should be color values.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure containing the subplots\n\n    Raises\n    ------\n    ValueError\n        If neither config nor nodes is provided\n        If network_name is missing when using config\n        If attribute is not found in nodes DataFrame\n        If node_ids column is missing\n        If nodes index is not unique\n    \"\"\"\n    # Input validation\n    if config is None and nodes is None:\n        raise ValueError(\"Must provide either config or nodes\")\n    if config is not None and nodes is None:\n        if network_name is None:\n            raise ValueError(\"network_name required when using config\")\n        nodes = load_nodes_from_config(config)\n    if attribute not in nodes.columns:\n        raise ValueError(f\"Attribute '{attribute}' not found in nodes DataFrame\")\n\n    # Extract node attribute data\n    node_attribute = nodes[attribute]\n\n    # Validate data structure\n    if \"node_ids\" not in individual_stats.columns:\n        raise ValueError(\"individual_stats missing required 'node_ids' column\")\n    if not nodes.index.is_unique:\n        raise ValueError(\"nodes DataFrame must have unique index for merging\")\n\n    # Merge firing rate data with node attributes\n    merged_df = individual_stats.merge(\n        node_attribute, left_on=\"node_ids\", right_index=True, how=\"left\"\n    )\n\n    # Setup subplot layout\n    max_groups = 15  # Maximum number of subplots to avoid overcrowding\n    unique_groups = merged_df[groupby].unique()\n    n_groups = min(len(unique_groups), max_groups)\n\n    if len(unique_groups) &gt; max_groups:\n        print(f\"Warning: Limiting display to {max_groups} groups out of {len(unique_groups)}\")\n        unique_groups = unique_groups[:max_groups]\n\n    n_cols = min(3, n_groups)\n    n_rows = (n_groups + n_cols - 1) // n_cols\n\n    # Create subplots\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n    if n_groups == 1:\n        axes = np.array([axes])\n    axes = axes.flatten()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"tab10\")\n        color_map = {group: cmap(i / len(unique_groups)) for i, group in enumerate(unique_groups)}\n    else:\n        # Ensure color_map contains all groups\n        missing_colors = [group for group in unique_groups if group not in color_map]\n        if missing_colors:\n            raise ValueError(f\"color_map is missing colors for groups: {missing_colors}\")\n\n    # Plot each group\n    for i, group in enumerate(unique_groups):\n        group_df = merged_df[merged_df[groupby] == group]\n        axes[i].scatter(group_df[\"firing_rate\"], group_df[attribute], s=dot_size, color=color_map[group])\n        axes[i].set_xlabel(\"Firing Rate (Hz)\")\n        axes[i].set_ylabel(attribute)\n\n        # Calculate and display mean firing rate in legend\n        mean_fr = group_df[\"firing_rate\"].mean()\n        axes[i].legend([f\"Mean FR: {mean_fr:.2f} Hz\"], loc=\"upper right\")\n        axes[i].set_title(f\"{groupby}: {group}\")\n\n    # Hide unused subplots\n    for j in range(i + 1, len(axes)):\n        axes[j].set_visible(False)\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/bmplot/spikes/#bmtool.bmplot.spikes.plot_firing_rate_histogram","title":"<code>bmtool.bmplot.spikes.plot_firing_rate_histogram(individual_stats, groupby='pop_name', ax=None, color_map=None, bins=30, alpha=0.7, figsize=(12, 8), stacked=False, logscale=False, min_fr=None)</code>","text":"<p>Plot histograms of firing rates for each population group.</p> Parameters: <p>individual_stats : pd.DataFrame     DataFrame containing individual firing rates with group labels. groupby : str, optional     Column name to group by (default is \"pop_name\"). ax : matplotlib.axes.Axes, optional     Axes on which to plot; if None, a new figure is created. color_map : dict, optional     Dictionary specifying colors for each group. Keys should be group names, and values should be color values. bins : int, optional     Number of bins for the histogram (default is 30). alpha : float, optional     Transparency level for the histograms (default is 0.7). figsize : Tuple[float, float], optional     Figure size if creating a new figure (default is (12, 8)). stacked : bool, optional     If True, plot all histograms on a single axes stacked (default is False). logscale : bool, optional     If True, use logarithmic scale for the x-axis (default is False). min_fr : float, optional     Minimum firing rate for log scale bins (default is None).</p> Returns: <p>matplotlib.figure.Figure     Figure containing the histogram subplots.</p> Source code in <code>bmtool/bmplot/spikes.py</code> <pre><code>def plot_firing_rate_histogram(\n    individual_stats: pd.DataFrame,\n    groupby: str = \"pop_name\",\n    ax: Optional[Axes] = None,\n    color_map: Optional[Dict[str, str]] = None,\n    bins: int = 30,\n    alpha: float = 0.7,\n    figsize: Tuple[float, float] = (12, 8),\n    stacked: bool = False,\n    logscale: bool = False,\n    min_fr: Optional[float] = None,\n) -&gt; plt.Figure:\n    \"\"\"\n    Plot histograms of firing rates for each population group.\n\n    Parameters:\n    ----------\n    individual_stats : pd.DataFrame\n        DataFrame containing individual firing rates with group labels.\n    groupby : str, optional\n        Column name to group by (default is \"pop_name\").\n    ax : matplotlib.axes.Axes, optional\n        Axes on which to plot; if None, a new figure is created.\n    color_map : dict, optional\n        Dictionary specifying colors for each group. Keys should be group names, and values should be color values.\n    bins : int, optional\n        Number of bins for the histogram (default is 30).\n    alpha : float, optional\n        Transparency level for the histograms (default is 0.7).\n    figsize : Tuple[float, float], optional\n        Figure size if creating a new figure (default is (12, 8)).\n    stacked : bool, optional\n        If True, plot all histograms on a single axes stacked (default is False).\n    logscale : bool, optional\n        If True, use logarithmic scale for the x-axis (default is False).\n    min_fr : float, optional\n        Minimum firing rate for log scale bins (default is None).\n\n    Returns:\n    -------\n    matplotlib.figure.Figure\n        Figure containing the histogram subplots.\n    \"\"\"\n    sns.set_style(\"whitegrid\")\n\n    # Get unique groups\n    unique_groups = individual_stats[groupby].unique()\n\n    # Generate colors if no color_map is provided\n    if color_map is None:\n        cmap = plt.get_cmap(\"tab10\")\n        color_map = {group: cmap(i / len(unique_groups)) for i, group in enumerate(unique_groups)}\n    else:\n        # Ensure color_map contains all groups\n        missing_colors = [group for group in unique_groups if group not in color_map]\n        if missing_colors:\n            raise ValueError(f\"color_map is missing colors for groups: {missing_colors}\")\n\n    # Group data by population\n    pop_fr = {}\n    for group in unique_groups:\n        pop_fr[group] = individual_stats[individual_stats[groupby] == group][\"firing_rate\"].values\n\n    if logscale and min_fr is not None:\n        pop_fr = {p: np.fmax(fr, min_fr) for p, fr in pop_fr.items()}\n    fr = np.concatenate(list(pop_fr.values()))\n    if logscale:\n        fr = fr[fr &gt; 0]\n        bins_array = np.geomspace(fr.min(), fr.max(), bins + 1)\n    else:\n        bins_array = np.linspace(fr.min(), fr.max(), bins + 1)\n\n    # Setup subplot layout or single plot\n    n_groups = len(unique_groups)\n    if stacked or not stacked:  # Always use single ax for now, since stacked means overlaid\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        # If not stacked, but since overlaid is default, perhaps keep as is\n        fig, ax = plt.subplots(figsize=figsize)\n\n    if stacked:\n        ax.hist(pop_fr.values(), bins=bins_array, label=list(pop_fr.keys()),\n                color=[color_map[p] for p in pop_fr.keys()], stacked=True)\n    else:\n        for p, fr_vals in pop_fr.items():\n            ax.hist(fr_vals, bins=bins_array, label=p, color=color_map[p], alpha=alpha)\n\n    if logscale:\n        ax.set_xscale('log')\n        plt.draw()\n        xt = ax.get_xticks()\n        xtl = [f'{x:g}' for x in xt]\n        if min_fr is not None:\n            xt = np.append(xt, min_fr)\n            xtl.append('0')\n        ax.set_xticks(xt)\n        ax.set_xticklabels(xtl)\n\n    ax.set_xlim(bins_array[0], bins_array[-1])\n    ax.legend(loc='upper right')\n    ax.set_title('Firing Rate Histogram')\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Count')\n    return fig\n</code></pre>"},{"location":"examples/analysis/","title":"Analysis Tutorials","text":"<p>The Analysis module provides tools for processing and analyzing simulation results from BMTK models, including spike data and other output reports.</p>"},{"location":"examples/analysis/#features","title":"Features","text":"<ul> <li>Load and analyze spike data from simulations</li> <li>Calculate population statistics and metrics</li> <li>Analyze LFP/ECP data with spectrograms and phase locking</li> <li>Visualize results with various plotting functions</li> </ul> <p>The Using Spikes tutorial demonstrates how to work with spike data from simulations. In this notebook, you'll learn:</p> <ul> <li>How to load spike data from BMTK simulations</li> <li>How to calculate firing rate statistics</li> <li>How to visualize spike patterns using raster plots</li> <li>How to compute population metrics</li> </ul>"},{"location":"examples/analysis/#other-tutorials","title":"Other Tutorials","text":"<ul> <li>Plot Spectrogram: Learn to create and visualize spectrograms from LFP/ECP data</li> <li>Phase Locking: Analyze the relationship between spike times and oscillatory phase</li> </ul>"},{"location":"examples/analysis/#basic-api-usage","title":"Basic API Usage","text":"<p>Here are some basic examples of how to use the Analysis module in your code:</p>"},{"location":"examples/analysis/#spike-analysis","title":"Spike Analysis","text":"<pre><code>from bmtool.analysis.spikes import load_spikes_to_df, compute_firing_rate_stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load spike data from a simulation\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network',\n    config='config.json'  # Optional, for cell type labeling\n)\n\n# Get basic spike statistics\npop_stats, individual_stats = compute_firing_rate_stats(\n    df=spikes_df,\n    groupby='pop_name',\n    start_time=500,\n    stop_time=1500\n)\n\nprint(\"Population firing rate statistics:\")\nprint(pop_stats)\n</code></pre>"},{"location":"examples/analysis/#raster-plots","title":"Raster Plots","text":"<pre><code>from bmtool.analysis.spikes import load_spikes_to_df\nfrom bmtool.bmplot import raster\nimport matplotlib.pyplot as plt\n\n# Load spike data\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network'\n)\n\n# Create a basic raster plot\nfig, ax = plt.subplots(figsize=(10, 6))\nraster(\n    spikes_df=spikes_df,\n    groupby='pop_name',\n    time_range=(0, 2000),\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"examples/analysis/#population-statistics","title":"Population Statistics","text":"<pre><code>from bmtool.analysis.spikes import load_spikes_to_df, get_population_spike_rate\nimport matplotlib.pyplot as plt\n\n# Load spike data\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network'\n)\n\n# Calculate population firing rates over time\npopulation_rates = get_population_spike_rate(\n    spikes=spikes_df,\n    fs=400.0,  # Sampling frequency in Hz\n    t_start=0,\n    t_stop=2000\n)\n\n# Plot population rates\nfor pop_name, rates in population_rates.items():\n    plt.plot(rates, label=pop_name)\nplt.xlabel('Time (ms)')\nplt.ylabel('Firing Rate (Hz)')\nplt.title('Population Firing Rates')\nplt.legend()\nplt.show()\n</code></pre> <p>For more advanced examples and detailed usage, please refer to the Jupyter notebook tutorials above.</p>"},{"location":"examples/bmplot/","title":"BMPlot Examples","text":"<p>This page provides examples of how to use the BMPlot module for visualizing network connectivity and structure.</p>"},{"location":"examples/bmplot/#jupyter-notebook-tutorial","title":"Jupyter Notebook Tutorial","text":"<p>For a comprehensive guide with visualizations, check out our Jupyter notebook tutorial:</p>"},{"location":"examples/bmplot/#plotting-examples","title":"Plotting Examples","text":"<p>The BMPlot Tutorial demonstrates the various plotting functions available in BMTool. In this notebook, you'll learn:</p> <ul> <li>How to create connection matrices to visualize network connectivity</li> <li>How to plot cell positions in 3D space</li> <li>How to analyze connection distances and distributions</li> </ul> <pre><code>import bmtool.bmplot.connections as connections\n\n# Generate a table showing the total number of connections\nconnections.total_connection_matrix(config='config.json')\n\n# Generate a table showing the percent connectivity\nconnections.percent_connection_matrix(config='config.json')\n\n# Generate a table showing the mean convergence\nconnections.convergence_connection_matrix(config='config.json')\n\n# Generate a table showing the mean divergence\nconnections.divergence_connection_matrix(config='config.json')\n\n# Generate a matrix specifically for gap junctions\nconnections.gap_junction_matrix(config='config.json', method='percent')\n</code></pre>"},{"location":"examples/bmplot/#spatial-analysis","title":"Spatial Analysis","text":"<pre><code>import bmtool.bmplot.connections as connections\n\n# Generate a 3D plot with the source and target cells location and connection distance histogram\nconnections.connection_distance(config='config.json', source='PopA', target='PopB')\n\n# Generate a histogram of connection distributions\nconnections.connection_histogram(config='config.json', source='PopA', target='PopB')\n</code></pre>"},{"location":"examples/bmplot/#3d-visualization","title":"3D Visualization","text":"<pre><code>import bmtool.bmplot.connections as connections\n\n# Generate a plot of cell positions in 3D space\nconnections.plot_3d_positions(config='config.json', populations=['PopA', 'PopB'])\n\n# Generate a plot showing cell locations and orientation in 3D\nconnections.plot_3d_cell_rotation(config='config.json', populations=['PopA'])\n</code></pre>"},{"location":"examples/bmplot/#network-graph","title":"Network Graph","text":"<pre><code>import bmtool.bmplot.connections as connections\n\n# Plot a network connection diagram\nconnections.plot_network_graph(\n    config='config.json',\n    sources='LA',\n    targets='LA',\n    tids='pop_name',\n    sids='pop_name',\n    no_prepend_pop=True\n)\n</code></pre>"},{"location":"examples/bmplot/#spike-analysis","title":"Spike Analysis","text":"<pre><code>import bmtool.bmplot.spikes as spikes\n\n# Create a raster plot from spike data\nspikes.raster(spikes_df=spikes_df, groupby='pop_name')\n\n# Plot firing rate statistics\nspikes.plot_firing_rate_pop_stats(firing_stats=firing_stats_df, groupby='pop_name')\n</code></pre>"},{"location":"examples/bmplot/#lfp-analysis","title":"LFP Analysis","text":"<pre><code>import bmtool.bmplot.lfp as lfp\n\n# Plot a spectrogram\nlfp.plot_spectrogram(sxx_xarray=spectrogram_data, log_power=True)\n</code></pre> <p>For more advanced examples and detailed visualizations, please refer to the Jupyter notebook tutorial above.</p>"},{"location":"examples/connectors/","title":"Connectors Examples","text":"<p>This page provides examples of how to use the Connectors module for building complex network connectivity patterns in BMTK models.</p>"},{"location":"examples/connectors/#basic-setup","title":"Basic Setup","text":"<p>All connector examples use the following network node structure:</p> <pre><code>from bmtk.builder import NetworkBuilder\n\n# Create main network\nnet = NetworkBuilder('example_net')\nnet.add_nodes(N=100, pop_name='PopA', model_type='biophysical')\nnet.add_nodes(N=100, pop_name='PopB', model_type='biophysical')\n\n# Create background inputs\nbackground = NetworkBuilder('background')\nbackground.add_nodes(N=300, pop_name='tON', potential='exc', model_type='virtual')\n</code></pre>"},{"location":"examples/connectors/#unidirectional-connector","title":"Unidirectional Connector","text":"<p>The <code>UnidirectionConnector</code> allows you to build unidirectional connections with a given probability:</p> <pre><code>from bmtool.connectors import UnidirectionConnector\n\n# Create connector with 15% connection probability and 1 synapse per connection\nconnector = UnidirectionConnector(p=0.15, n_syn=1)\n\n# Set up source and target nodes\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopB'))\n\n# Add the edges to the network\nnet.add_edges(**connector.edge_params())\n</code></pre> <p>This creates connections from neurons in population 'PopA' to neurons in population 'PopB' with a 15% probability of connection.</p>"},{"location":"examples/connectors/#reciprocal-connector","title":"Reciprocal Connector","text":"<p>The <code>ReciprocalConnector</code> enables you to build connections with a specific reciprocal probability:</p> <pre><code>from bmtool.connectors import ReciprocalConnector\n\n# Create connector with 15% base probability and 6.7% reciprocal probability\nconnector = ReciprocalConnector(\n    p0=0.15,           # Base connection probability\n    pr=0.06767705087,  # Reciprocal connection probability\n    n_syn0=1,          # Number of synapses for base connection\n    n_syn1=1,          # Number of synapses for reciprocal connection\n    estimate_rho=False # Whether to estimate rho value\n)\n\n# Setup for recurrent connections within PopA\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\n\n# Add the edges to the network\nnet.add_edges(**connector.edge_params())\n</code></pre> <p>This creates connections within population 'PopA' where: - Any two neurons have a 15% probability of having a unidirectional connection - If a unidirectional connection exists from neuron A to neuron B, there's a 6.7% probability of also having a connection from B to A</p>"},{"location":"examples/connectors/#correlated-gap-junction","title":"Correlated Gap Junction","text":"<p>The <code>CorrelatedGapJunction</code> connector creates gap junction connections that can be correlated with chemical synapses:</p> <pre><code>from bmtool.connectors import ReciprocalConnector, CorrelatedGapJunction\n\n# First create a chemical synapse connectivity pattern\nconnector = ReciprocalConnector(p0=0.15, pr=0.06, n_syn0=1, n_syn1=1, estimate_rho=False)\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\nnet.add_edges(**connector.edge_params())\n\n# Then create gap junctions that are correlated with chemical synapses\ngap_junc = CorrelatedGapJunction(\n    p_non=0.1228,  # Probability for pairs with no chemical synapse\n    p_uni=0.56,    # Probability for pairs with unidirectional chemical synapse\n    p_rec=1,       # Probability for pairs with reciprocal chemical synapses\n    connector=connector  # Use the chemical synapse connector for correlation\n)\ngap_junc.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\n\n# Add gap junction edges\nconn = net.add_edges(\n    is_gap_junction=True,\n    syn_weight=0.0000495,\n    target_sections=None,\n    afferent_section_id=0,\n    afferent_section_pos=0.5,\n    **gap_junc.edge_params()\n)\n</code></pre> <p>This creates gap junctions with probabilities that depend on the existing chemical synapse connections.</p>"},{"location":"examples/connectors/#one-to-one-sequential-connector","title":"One-to-One Sequential Connector","text":"<p>The <code>OneToOneSequentialConnector</code> creates one-to-one mappings between populations:</p> <pre><code>from bmtool.connectors import OneToOneSequentialConnector\n\n# Create the connector\nconnector = OneToOneSequentialConnector()\n\n# Connect background to PopA\nconnector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name='PopA'))\nnet.add_edges(**connector.edge_params())\n\n# Connect background to PopB\nconnector.setup_nodes(target=net.nodes(pop_name='PopB'))\nnet.add_edges(**connector.edge_params())\n</code></pre> <p>This creates one-to-one connections where each neuron in the background population connects to exactly one neuron in PopA and one in PopB.</p>"},{"location":"examples/connectors/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/connectors/#distance-dependent-connectivity","title":"Distance-Dependent Connectivity","text":"<p>You can create distance-dependent connections by providing a connection probability function:</p> <pre><code>import numpy as np\nfrom bmtool.connectors import UnidirectionConnector\n\n# Define a distance-dependent probability function\ndef connection_probability(source, target, distance_range=500.0, p_max=0.15):\n    \"\"\"Probability decreases with distance\"\"\"\n    dist = np.sqrt(np.sum((source['positions'] - target['positions'])**2, axis=1))\n    return p_max * np.exp(-dist/distance_range)\n\n# Create connector with the probability function\nconnector = UnidirectionConnector(p=connection_probability, n_syn=1)\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopB'))\nnet.add_edges(**connector.edge_params())\n</code></pre> <p>This creates connections where the probability decreases exponentially with distance between neurons.</p>"},{"location":"examples/single-cell/","title":"Single Cell Examples","text":"<p>This page provides examples of how to use the Single Cell module for various analyses.</p>"},{"location":"examples/single-cell/#python-api-examples","title":"Python API Examples","text":"<p>The examples below demonstrate how to use the Single Cell module through its Python API:</p>"},{"location":"examples/single-cell/#basic-setup","title":"Basic Setup","text":"<p>All single cell examples require initializing the Profiler:</p> <pre><code>from bmtool.singlecell import Profiler\n\n# Initialize with paths to templates and mechanisms\nprofiler = Profiler(template_dir='templates', mechanism_dir='mechanisms', dt=0.1)\n</code></pre>"},{"location":"examples/single-cell/#passive-properties-analysis","title":"Passive Properties Analysis","text":"<p>Calculating passive properties (V-rest, input resistance, and time constant):</p> <pre><code>from bmtool.singlecell import Passive, run_and_plot\nimport matplotlib.pyplot as plt\n\n# Create a Passive simulation object\nsim = Passive('Cell_Cf', inj_amp=-100., inj_delay=1500., inj_dur=1000.,\n              tstop=2500., method='exp2')\n\n# Run the simulation and plot the results\ntitle = 'Passive Cell Current Injection'\nxlabel = 'Time (ms)'\nylabel = 'Membrane Potential (mV)'\nX, Y = run_and_plot(sim, title, xlabel, ylabel, plot_injection_only=True)\n\n# Plot the double exponential fit\nplt.gca().plot(*sim.double_exponential_fit(), 'r:', label='double exponential fit')\nplt.legend()\nplt.show()\n</code></pre> <p>This will output the passive properties:</p> <pre><code>Injection location: Cell_Cf[0].soma[0](0.5)\nRecording: Cell_Cf[0].soma[0](0.5)._ref_v\nRunning simulation for passive properties...\n\nV Rest: -70.21 (mV)\nResistance: 128.67 (MOhms)\nMembrane time constant: 55.29 (ms)\n</code></pre>"},{"location":"examples/single-cell/#current-clamp","title":"Current Clamp","text":"<p>Running a current clamp to observe spiking behavior:</p> <pre><code>from bmtool.singlecell import CurrentClamp, run_and_plot\nimport matplotlib.pyplot as plt\n\n# Create a CurrentClamp simulation object\nsim = CurrentClamp('Cell_Cf', inj_amp=350., inj_delay=1500., inj_dur=1000.,\n                   tstop=3000., threshold=-15.)\n\n# Run the simulation and plot the results\nX, Y = run_and_plot(sim, title='Current Injection', xlabel='Time (ms)',\n                    ylabel='Membrane Potential (mV)', plot_injection_only=True)\nplt.show()\n</code></pre>"},{"location":"examples/single-cell/#fi-curve","title":"FI Curve","text":"<p>Generating a frequency-current (FI) curve:</p> <pre><code>from bmtool.singlecell import FI, run_and_plot\nimport matplotlib.pyplot as plt\n\n# Create an FI simulation object\nsim = FI('Cell_Cf', i_start=0., i_stop=1000., i_increment=50.,\n          tstart=1500., threshold=-15.)\n\n# Run the simulation and plot the results\nX, Y = run_and_plot(sim, title='FI Curve', xlabel='Injection (nA)',\n                    ylabel='# Spikes')\nplt.show()\n</code></pre>"},{"location":"examples/single-cell/#zap-protocol","title":"ZAP Protocol","text":"<p>Analyzing frequency response using a chirp current (ZAP):</p> <pre><code>from bmtool.singlecell import ZAP, run_and_plot\nimport matplotlib.pyplot as plt\n\n# Create a ZAP simulation object\nsim = ZAP('Cell_Cf')\n\n# Run the simulation and plot the results\nX, Y = run_and_plot(sim)\nplt.show()\n</code></pre>"},{"location":"examples/single-cell/#jupyter-notebook-tutorials","title":"Jupyter Notebook Tutorials","text":"<p>For more detailed examples with rich output and visualizations, check out our Jupyter notebook tutorials:</p>"},{"location":"examples/single-cell/#comprehensive-single-cell-analysis-tutorial","title":"Comprehensive Single Cell Analysis Tutorial","text":"<p>The Comprehensive Single Cell Analysis Tutorial provides a thorough guide to using BMTool's single cell analysis module. This tutorial covers:</p> <ul> <li>Multiple methods for loading neurons: Allen Database, NEURON HOC templates, and Python class-based models</li> <li>Various electrophysiological analysis techniques: Passive properties, Current clamp, Impedance (ZAP), and Frequency-Intensity (FI) curves</li> <li>Consistent analysis methods that work regardless of how the cell was loaded</li> </ul>"},{"location":"examples/single-cell/#other-examples","title":"Other Examples","text":"<p>You can also access other examples from the command line:</p> <pre><code># Cell Tuning via CLI\nbmtool util cell tune --builder\n\n# VHalf Segregation\nbmtool util cell vhseg\n</code></pre>"},{"location":"examples/slurm/","title":"SLURM Tutorials","text":"<p>The SLURM module provides tools for managing and running simulations on SLURM-based high-performance computing clusters.</p>"},{"location":"examples/slurm/#features","title":"Features","text":"<ul> <li>Automate simulation job submission to SLURM clusters</li> <li>Manage simulation parameters and configurations</li> <li>Track simulation status and results</li> <li>Parallelize parameter sweeps and batch runs</li> </ul> <p>The Block Runner Tutorial demonstrates how to manage simulations on SLURM clusters. In this notebook, you'll learn:</p> <ul> <li>How to set up simulation configurations for SLURM</li> <li>How to submit and monitor jobs</li> <li>How to parallelize parameter sweeps</li> <li>How to collect and analyze results from distributed simulations</li> </ul>"},{"location":"examples/slurm/#basic-api-usage","title":"Basic API Usage","text":"<p>Here are some basic examples of how to use the SLURM module in your code:</p>"},{"location":"examples/slurm/#block-runner","title":"Block Runner","text":"<pre><code>from bmtool.SLURM import BlockRunner\n\n# Initialize a block runner for a BMTK model\nrunner = BlockRunner(\n    model_dir='/path/to/model',\n    config='simulation_config.json',\n    steps_per_block=10,  # Number of simulation steps per SLURM job\n    total_steps=100      # Total simulation steps\n)\n\n# Submit the jobs to SLURM\nrunner.run()\n\n# Check the status of submitted jobs\nstatus = runner.check_status()\nprint(status)\n\n# Collect results from completed jobs\nresults = runner.collect_results()\n</code></pre>"},{"location":"examples/slurm/#parameter-sweeps","title":"Parameter Sweeps","text":"<pre><code>from bmtool.SLURM import ParameterSweep\n\n# Create a parameter sweep\nsweep = ParameterSweep(\n    base_config='simulation_config.json',\n    model_dir='/path/to/model',\n    parameter_specs={\n        'syn_weight': [0.001, 0.002, 0.003, 0.004],\n        'conn_prob': [0.1, 0.2, 0.3],\n        'input_rate': [10, 20, 30, 40, 50]\n    }\n)\n\n# Generate configurations\nconfigs = sweep.generate_configs()\n\n# Run the parameter sweep\nsweep.run(time_limit='2:00:00', memory='16G')\n</code></pre>"},{"location":"examples/slurm/#custom-slurm-runner","title":"Custom SLURM Runner","text":"<pre><code>from bmtool.SLURM import SLURMRunner\n\n# Create a custom SLURM runner\nrunner = SLURMRunner(\n    job_name='bmtk_simulation',\n    partition='normal',\n    nodes=1,\n    cores_per_node=16,\n    memory_gb=32,\n    time_limit='08:00:00',\n    email='user@example.com',\n    email_options=['END', 'FAIL']\n)\n\n# Submit a BMTK simulation\nrunner.submit(\n    model_dir='/path/to/model',\n    config='simulation_config.json',\n    modules_to_load=['neuron', 'python']\n)\n</code></pre> <p>For more advanced examples and detailed usage, please refer to the Jupyter notebook tutorial above.</p>"},{"location":"examples/synapses/","title":"Synapses Tutorials","text":"<p>The Synapses module provides tools for creating and tuning chemical and electrical synapses in NEURON and BMTK models.</p>"},{"location":"examples/synapses/#features","title":"Features","text":"<ul> <li>Interactive tuning of synapse parameters</li> <li>Support for both chemical and electrical (gap junction) synapses</li> <li>Visualization of synaptic responses</li> <li>Parameter fitting to match experimental data</li> </ul> <p>The Synapses module provides two different tutorials for chemical synapse tuning:</p> <p>The BMTK Chemical Synapse Tuner tutorial demonstrates how to use BMTool to interactively tune chemical synapses within BMTK networks. In this notebook, you'll learn:</p> <ul> <li>How to set up and configure chemical synapses in BMTK models</li> <li>How to switch between different network connections for tuning</li> <li>How to adjust synapse parameters and observe responses in a network context</li> <li>How to use the optimizer to automatically fit synaptic parameters</li> </ul> <p>The Neuron Chemical Synapse Tuner tutorial shows how to tune chemical synapses using pure NEURON models. This notebook covers:</p> <ul> <li>How to set up chemical synapses with detailed configuration</li> <li>How to manually tune synapse parameters</li> <li>How to use the optimizer to automatically fit synaptic parameters</li> </ul> <p>The Gap Junction Tuner tutorial shows how to configure and optimize electrical synapses. This notebook covers:</p> <ul> <li>Setting up gap junctions in NEURON models</li> <li>Adjusting gap junction conductance</li> <li>Visualizing current flow through gap junctions</li> <li>Implementing gap junctions in network models</li> </ul>"},{"location":"examples/synapses/#basic-api-usage","title":"Basic API Usage","text":"<p>If you prefer to use the Synapses module directly in your code, here are some basic examples:</p>"},{"location":"examples/synapses/#synapsetuner-with-bmtk-networks","title":"SynapseTuner with BMTK Networks","text":"<pre><code>from bmtool.synapses import SynapseTuner\n\n# Create a tuner for BMTK networks\ntuner = SynapseTuner(\n    config='simulation_config.json',  # Path to BMTK config\n    current_name='i',                 # Synaptic current to record\n    slider_vars=['initW','Dep','Fac','Use','tau1','tau2']  # Parameters for sliders\n)\n\n# Display the interactive tuner\ntuner.InteractiveTuner()\n\n# Switch between different connections in your network\ntuner._switch_connection('PV2Exc')\n</code></pre>"},{"location":"examples/synapses/#synapsetuner-with-pure-neuron-models","title":"SynapseTuner with Pure NEURON Models","text":"<pre><code>from bmtool.synapses import SynapseTuner\n\n# Define general settings\ngeneral_settings = {\n    'vclamp': True,\n    'rise_interval': (0.1, 0.9),\n    'tstart': 500.,\n    'tdur': 100.,\n    'threshold': -15.,\n    'delay': 1.3,\n    'weight': 1.,\n    'dt': 0.025,\n    'celsius': 20\n}\n\n# Define connection-specific settings\nconn_settings = {\n    'Exc2FSI': {\n        'spec_settings': {\n            'post_cell': 'FSI_Cell',\n            'vclamp_amp': -70.,\n            'sec_x': 0.5,\n            'sec_id': 1,\n            \"level_of_detail\": \"AMPA_NMDA_STP\",\n        },\n        'spec_syn_param': {\n            'initW': 0.76,\n            'tau_r_AMPA': 0.45,\n            'tau_d_AMPA': 7.5,\n            'Use': 0.13,\n            'Dep': 0.,\n            'Fac': 200.\n        },\n    }\n}\n\n# Create tuner with custom settings\ntuner = SynapseTuner(\n    general_settings=general_settings,\n    conn_type_settings=conn_settings\n)\n\n# Display the interactive tuner\ntuner.InteractiveTuner()\n</code></pre>"},{"location":"examples/synapses/#gapjunctiontuner","title":"GapJunctionTuner","text":"<pre><code>from bmtool.synapses import GapJunctionTuner\n\n# Create a tuner for gap junctions\ntuner = GapJunctionTuner(\n    cell1_template='Interneuron',\n    cell2_template='Interneuron',\n    template_dir='path/to/templates',\n    mod_dir='path/to/mechanisms'\n)\n\n# Display the interactive tuner\ntuner.show()\n\n# Use the optimizer to find resistance for a target coupling coefficient\noptimal_resistance = tuner.optimize(target_cc=0.05)\nprint(f\"Optimal gap junction resistance: {optimal_resistance} MOhm\")\n</code></pre> <p>For more advanced usage, please refer to the Jupyter notebook tutorials above.</p>"},{"location":"examples/notebooks/SLURM/using_BlockRunner/","title":"Block Runner","text":"In\u00a0[2]: Copied! <pre>import shutil\nfrom pathlib import Path\n\nfrom bmtool.SLURM import BlockRunner, SimulationBlock\n</pre> import shutil from pathlib import Path  from bmtool.SLURM import BlockRunner, SimulationBlock In\u00a0[4]: Copied! <pre>output_name = \"example_output_name\" # can be any string\nbasePath = Path.cwd()  # Equivalent to os.getcwd() gets our current working directory\n\n# Path to 'Run-Storage' directory this is just setting up where i want things stored so this would go ../Run-Storage/example_output_name dir\nrun_storage_dir = basePath.parent / 'Run-Storage'  \ntarget_dir = run_storage_dir / output_name  # Path to 'Run-Storage/CurrentResults'\ndisplay(target_dir)\n</pre> output_name = \"example_output_name\" # can be any string basePath = Path.cwd()  # Equivalent to os.getcwd() gets our current working directory  # Path to 'Run-Storage' directory this is just setting up where i want things stored so this would go ../Run-Storage/example_output_name dir run_storage_dir = basePath.parent / 'Run-Storage'   target_dir = run_storage_dir / output_name  # Path to 'Run-Storage/CurrentResults' display(target_dir) <pre>PosixPath('/home/gjgpb9/cortex_modeling/bmtool/examples/Run-Storage/example')</pre> In\u00a0[5]: Copied! <pre>from pathlib import PosixPath\n\nmanual_path = \"gjgpb9/bmtool/examples/Run-Storage/example\"\n\n# Create a PosixPath object\ntarget_dir = PosixPath(manual_path)\ndisplay(target_dir)\n</pre> from pathlib import PosixPath  manual_path = \"gjgpb9/bmtool/examples/Run-Storage/example\"  # Create a PosixPath object target_dir = PosixPath(manual_path) display(target_dir) <pre>PosixPath('gjgpb9/bmtool/examples/Run-Storage/example')</pre> In\u00a0[\u00a0]: Copied! <pre>if target_dir.exists() and target_dir.is_dir():\n    shutil.rmtree(target_dir)\n</pre> if target_dir.exists() and target_dir.is_dir():     shutil.rmtree(target_dir) In\u00a0[\u00a0]: Copied! <pre>simulation_cases = {\n    \"baseline\": \"mpirun nrniv -mpi -python run_network.py simulation_config_baseline.json False\",\n    \"short\": \"mpirun nrniv -mpi -python run_network.py simulation_config_short.json False\",\n    \"long\": \"mpirun nrniv -mpi -python run_network.py simulation_config_long.json False\"\n}\n</pre> simulation_cases = {     \"baseline\": \"mpirun nrniv -mpi -python run_network.py simulation_config_baseline.json False\",     \"short\": \"mpirun nrniv -mpi -python run_network.py simulation_config_short.json False\",     \"long\": \"mpirun nrniv -mpi -python run_network.py simulation_config_long.json False\" } In\u00a0[\u00a0]: Copied! <pre>block_params = {\n'time': '04:00:00',\n'partition': 'batch',\n'nodes': 1,\n'ntasks': 40,\n'mem': '80G',\n'output_base_dir': target_dir\n}\n</pre> block_params = { 'time': '04:00:00', 'partition': 'batch', 'nodes': 1, 'ntasks': 40, 'mem': '80G', 'output_base_dir': target_dir } In\u00a0[\u00a0]: Copied! <pre>additional_commands = [\n    \"module purge\",\n    \"module load slurm\",\n    \"module load cpu/0.17.3b\",\n    \"module load gcc/10.2.0/npcyll4\",\n    \"module load openmpi/4.1.1\",\n    \"export HDF5_USE_FILE_LOCKING=FALSE\"\n]\n</pre> additional_commands = [     \"module purge\",     \"module load slurm\",     \"module load cpu/0.17.3b\",     \"module load gcc/10.2.0/npcyll4\",     \"module load openmpi/4.1.1\",     \"export HDF5_USE_FILE_LOCKING=FALSE\" ] In\u00a0[\u00a0]: Copied! <pre>#full_path = 'components/synaptic_models/synapses_STP/FSI2LTS.json'\ncomponent_path = 'components'\njson_file_path = 'synaptic_models/synapses_STP/FSI2LTS.json'\nparam_name = 'initW' # key in json\nparam_values = [3.1,5] # values in json\n</pre> #full_path = 'components/synaptic_models/synapses_STP/FSI2LTS.json' component_path = 'components' json_file_path = 'synaptic_models/synapses_STP/FSI2LTS.json' param_name = 'initW' # key in json param_values = [3.1,5] # values in json In\u00a0[\u00a0]: Copied! <pre>num_blocks = len(param_values)\n# Create a list to hold the blocks\nblocks = []\nfor i in range(1, num_blocks + 1):\n    block_name = f'block{i}'\n    block = SimulationBlock(block_name, **block_params, simulation_cases=simulation_cases,\n                            additional_commands=additional_commands,component_path=component_path)\n    blocks.append(block)\n</pre> num_blocks = len(param_values) # Create a list to hold the blocks blocks = [] for i in range(1, num_blocks + 1):     block_name = f'block{i}'     block = SimulationBlock(block_name, **block_params, simulation_cases=simulation_cases,                             additional_commands=additional_commands,component_path=component_path)     blocks.append(block) In\u00a0[\u00a0]: Copied! <pre>flow_url = None\nrunner = BlockRunner(blocks=blocks,json_file_path=json_file_path,\n                     param_name=param_name,param_values=param_values,webhook=flow_url)\n</pre> flow_url = None runner = BlockRunner(blocks=blocks,json_file_path=json_file_path,                      param_name=param_name,param_values=param_values,webhook=flow_url) In\u00a0[\u00a0]: Copied! <pre>runner.submit_blocks_parallel()\n</pre> runner.submit_blocks_parallel()"},{"location":"examples/notebooks/SLURM/using_BlockRunner/#this-is-a-notebook-to-show-how-to-use-the-bmtoolslurm-module","title":"This is a notebook to show how to use the BMTOOL.SLURM module\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#by-gregory-glickert","title":"By Gregory Glickert\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#first-we-will-import-the-packages-we-need","title":"First we will import the packages we need\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#next-we-will-set-up-the-pathing-the-runner-will-use","title":"Next we will set up the pathing the runner will use\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#you-can-also-manually-define-the-target-path-like-this","title":"You can also manually define the target path like this\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#it-is-then-good-practice-to-check-if-that-path-already-exists-and-if-so-delete-it","title":"It is then good practice to check if that path already exists and if so delete it\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#now-we-can-define-out-simulation_cases-this-is-a-dictionary-with-the-key-being-the-name-of-the-case-and-the-value-is-the-command-used-to-run-that-simulation-below-is-an-example-of-what-that-could-look-like","title":"Now we can define out simulation_cases. This is a dictionary with the key being the name of the case and the value is the command used to run that simulation. Below is an example of what that could look like.\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#next-we-define-our-block-parameters-these-are-parameters-that-will-be-used-to-allocate-the-slurm-resources-the-below-example-will-allocate-resources-for-4-hours-on-the-partition-named-batch-the-partition-will-be-different-depending-on-your-system-setup-the-job-will-use-40-cores-and-80g-of-memory-the-output_base_dir-will-be-the-directory-where-the-output-of-the-simulation-will-be-stored","title":"Next we define our block parameters. These are parameters that will be used to allocate the SLURM resources. The below example will allocate resources for 4 hours on the partition named batch. The partition will be different depending on your system setup. The job will use 40 cores and 80G of memory. The output_base_dir will be the directory where the output of the simulation will be stored\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#there-may-also-be-some-additional-commands-you-to-run-in-the-script-before-running-your-model-an-example-could-be-loading-modules-you-can-add-those-like-this-if-your-model-doesnt-need-any-additional-commands-you-can-just-not-include-this","title":"There may also be some additional commands you to run in the script before running your model. An example could be loading modules. You can add those like this. If your model doesn't need any additional commands you can just not include this.\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#next-we-set-up-the-paths-for-changing-our-json-file-the-module-supports-changing-any-key-in-the-json-file-the-component_path-should-be-the-relative-path-to-the-bmtk-components-directory-the-json_file_path-should-be-the-path-from-the-components-folder-to-the-json-file-you-wish-to-edit","title":"Next we set up the paths for changing our json file. The module supports changing any key in the json file. The component_path should be the relative path to the BMTK components directory. The json_file_path should be the path FROM the components folder to the json file you wish to edit.\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#now-we-set-up-the-blocks","title":"Now we set up the blocks.\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#then-we-can-get-the-runner-ready-to-submit-you-can-also-have-the-option-to-put-a-webhook-link-to-send-messages-on-progress-of-the-jobs-currently-only-microsoft-teams-has-been-tested-if-you-want-to-only-do-one-run-of-the-network-then-you-can-just-set-param_values-to-be-none-instead","title":"Then we can get the Runner ready to submit. You can also have the option to put a webhook link to send messages on progress of the jobs. Currently, only Microsoft Teams has been tested. If you want to only do one run of the network then you can just set param_values to be None instead.\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#then-finally-we-submit-the-run","title":"Then finally we submit the run\u00b6","text":""},{"location":"examples/notebooks/SLURM/using_BlockRunner/#something-to-note-is-if-you-are-using-a-webhook-and-run-this-last-line-the-code-run-until-the-last-job-is-finished-this-can-take-some-time-so-it-may-be-easier-to-run-the-file-on-slurm-so-you-dont-have-to-keep-your-code-running-on-your-end-you-could-also-just-close-the-code-after-all-the-jobs-are-submitted-and-just-not-worry-about-knowing-when-the-jobs-are-done","title":"Something to note is if you are using a webhook and run this last line the code run until the last job is finished. This can take some time, so it may be easier to run the file on SLURM so you don't have to keep your code running on your end. You could also just close the code after all the jobs are submitted and just not worry about knowing when the jobs are done.\u00b6","text":""},{"location":"examples/notebooks/analysis/netcon_report/netcon_example/","title":"Synapse Report","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n\nfrom bmtool.analysis.netcon_reports import load_synapse_report\n\nh5_path = \"/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/lower_gamma_final_1_lower_gap_syn_test/block1/baseline/syn_test2.h5\"\nconfig_path = \"/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json\"\n\n# Load and process the data\nsyn_data = load_synapse_report(h5_path, config_path)\nprint(syn_data)\n\n# Example of how to get data for specific populations\nprint(\"\\nExample queries:\")\n\n# Get mean synapse values for each connection type\nprint(\"\\nMean values by connection type:\")\nfor src_pop in np.unique(syn_data.source_pop.values):\n    for trg_pop in np.unique(syn_data.target_pop.values):\n        conn_data = syn_data.where(\n            (syn_data.source_pop == src_pop) &amp; \n            (syn_data.target_pop == trg_pop),\n            drop=True\n        )\n        if len(conn_data.synapse) &gt; 0:\n            mean_val = conn_data.synapse_value.mean().values\n            print(f\"{src_pop}-&gt;{trg_pop}: {mean_val:.6f}\")\n\n# Count synapses by connection type\nprint(\"\\nSynapse counts by connection type:\")\nfor src_pop in np.unique(syn_data.source_pop.values):\n    for trg_pop in np.unique(syn_data.target_pop.values):\n        count = np.sum((syn_data.source_pop == src_pop) &amp; (syn_data.target_pop == trg_pop))\n        if count.values &gt; 0:\n            print(f\"{src_pop}-&gt;{trg_pop}: {count.values} synapses\")\n</pre> import numpy as np  from bmtool.analysis.netcon_reports import load_synapse_report  h5_path = \"/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/lower_gamma_final_1_lower_gap_syn_test/block1/baseline/syn_test2.h5\" config_path = \"/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json\"  # Load and process the data syn_data = load_synapse_report(h5_path, config_path) print(syn_data)  # Example of how to get data for specific populations print(\"\\nExample queries:\")  # Get mean synapse values for each connection type print(\"\\nMean values by connection type:\") for src_pop in np.unique(syn_data.source_pop.values):     for trg_pop in np.unique(syn_data.target_pop.values):         conn_data = syn_data.where(             (syn_data.source_pop == src_pop) &amp;              (syn_data.target_pop == trg_pop),             drop=True         )         if len(conn_data.synapse) &gt; 0:             mean_val = conn_data.synapse_value.mean().values             print(f\"{src_pop}-&gt;{trg_pop}: {mean_val:.6f}\")  # Count synapses by connection type print(\"\\nSynapse counts by connection type:\") for src_pop in np.unique(syn_data.source_pop.values):     for trg_pop in np.unique(syn_data.target_pop.values):         count = np.sum((syn_data.source_pop == src_pop) &amp; (syn_data.target_pop == trg_pop))         if count.values &gt; 0:             print(f\"{src_pop}-&gt;{trg_pop}: {count.values} synapses\") <pre>&lt;xarray.Dataset&gt;\nDimensions:           (time: 100, synapse: 1365)\nCoordinates:\n  * time              (time) float64 0.0 5.0 10.0 15.0 ... 485.0 490.0 495.0\n  * synapse           (synapse) int64 0 1 2 3 4 5 ... 1360 1361 1362 1363 1364\n    source_pop        (synapse) &lt;U10 'CP' 'CP' ... 'unknown_-1' 'unknown_-1'\n    target_pop        (synapse) &lt;U2 'CP' 'CP' 'CP' 'CP' ... 'CP' 'CP' 'CP' 'CP'\n    source_id         (synapse) int64 623 2578 8153 3692 3400 ... -1 -1 -1 -1 -1\n    target_id         (synapse) int64 1 1 1 1 1 1 1 1 1 1 ... 5 5 5 5 5 5 5 5 5\n    sec_id            (synapse) uint64 1 2 1 1 1 1 1 1 1 1 ... 1 1 2 1 1 1 1 1 1\n    sec_x             (synapse) float64 0.5 0.5 0.5 0.5 0.5 ... 0.5 0.5 0.5 0.5\n    connection_label  (synapse) &lt;U14 'CP-&gt;CP' 'CP-&gt;CP' ... 'unknown_-1-&gt;CP'\nData variables:\n    synapse_value     (time, synapse) float64 2.101 2.101 1.834 ... 1.834 1.834\nAttributes:\n    description:  Synapse report data from bmtk simulation\n\nExample queries:\n\nMean values by connection type:\nCP-&gt;CP: 2.101000\nCS-&gt;CP: 1.834000\nunknown_-1-&gt;CP: 1.953314\n\nSynapse counts by connection type:\nCP-&gt;CP: 684 synapses\nCS-&gt;CP: 378 synapses\nunknown_-1-&gt;CP: 303 synapses\n</pre> In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\n\nCP2CP = syn_data.where(\n    (syn_data.source_pop == 'CP') &amp; \n    (syn_data.target_pop == 'CP'),\n    drop=True\n)\n\nplt.plot(CP2CP.time, CP2CP.synapse_value)\nplt.show()\n</pre> import matplotlib.pyplot as plt  CP2CP = syn_data.where(     (syn_data.source_pop == 'CP') &amp;      (syn_data.target_pop == 'CP'),     drop=True )  plt.plot(CP2CP.time, CP2CP.synapse_value) plt.show()"},{"location":"examples/notebooks/analysis/netcon_report/netcon_example/#netcon-report-module","title":"Netcon report module\u00b6","text":"<p>This module was written to help with the analysis of complex netcon reports generated by BMTK. Some example reports could look like</p> <pre>{\n    \"reports\": {\n        \"syn_report\": {\n            \"cells\": \"all\",\n            \"variable_name\": \"tau_r_AMPA\",\n            \"module\": \"netcon_report\",\n            \"sections\": \"all\",\n            \"syn_type\": \"AMPA_NMDA_STP\",\n            \"file_name\": \"syn_test1.h5\",\n            \"start_time\": 0,\n            \"dt\": 5\n        },\n        \"syn_report2\": {\n            \"cells\": {\n                \"node_ids\": [1,2,3,4,5]\n            },\n            \"variable_name\": \"tau_r_AMPA\",\n            \"module\": \"netcon_report\",\n            \"sections\": \"all\",\n            \"syn_type\": \"AMPA_NMDA_STP\",\n            \"file_name\": \"syn_test2.h5\",\n            \"start_time\": 0,\n            \"dt\": 5\n        },\n        \"syn_report3\": {\n            \"cells\": {\n                \"pop_name\": \"CS\"\n            },\n            \"variable_name\": \"tau_r_AMPA\",\n            \"module\": \"netcon_report\",\n            \"sections\": \"all\",\n            \"syn_type\": \"AMPA_NMDA_STP\",\n            \"file_name\": \"syn_test3.h5\",\n            \"start_time\": 0,\n            \"dt\": 5\n        }\n    }\n}\n</pre>"},{"location":"examples/notebooks/analysis/netcon_report/netcon_example/#get-an-example-tone-recording-here","title":"get an example tone recording here\u00b6","text":"<p>plot how the synapse parameter changes over time</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/","title":"Phase Locking","text":"In\u00a0[1]: Copied! <pre>RunningInCOLAB = 'google.colab' in str(get_ipython())\nif RunningInCOLAB:\n    %pip install bmtool &amp;&gt; /dev/null\n</pre> RunningInCOLAB = 'google.colab' in str(get_ipython()) if RunningInCOLAB:     %pip install bmtool &amp;&gt; /dev/null In\u00a0[2]: Copied! <pre>import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import signal as ss\n\nfrom bmtool.analysis.entrainment import calculate_ppc, calculate_ppc2, calculate_spike_lfp_plv\nfrom bmtool.analysis.lfp import fit_fooof\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings  import matplotlib.pyplot as plt import numpy as np from scipy import signal as ss  from bmtool.analysis.entrainment import calculate_ppc, calculate_ppc2, calculate_spike_lfp_plv from bmtool.analysis.lfp import fit_fooof  warnings.filterwarnings(\"ignore\")  <pre>/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/bmtool/analysis/lfp.py:11: DeprecationWarning: \nThe `fooof` package is being deprecated and replaced by the `specparam` (spectral parameterization) package.\nThis version of `fooof` (1.1) is fully functional, but will not be further updated.\nNew projects are recommended to update to using `specparam` (see Changelog for details).\n  from fooof import FOOOF\nWarning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> In\u00a0[3]: Copied! <pre>np.random.seed(9) # lucky number 9\nfs = 1000  # 1 kHz sampling rate\nduration = 200  # 200 seconds\nt = np.arange(0, duration, 1/fs)\n\n# Define oscillation frequencies\nbeta_freq = 15  # Hz\ngamma_freq = 40  # Hz\n\n# Create a simulated LFP with multiple frequency components\nlfp = (0.5 * np.sin(2 * np.pi * gamma_freq * t) + 0.2 * np.sin(2 * np.pi * beta_freq * t) +  0.3 * np.random.randn(len(t)))\n\n# Generate phase information for each frequency\nbeta_phase = np.angle(ss.hilbert(np.sin(2 * np.pi * beta_freq * t)))\ngamma_phase = np.angle(ss.hilbert(np.sin(2 * np.pi * gamma_freq * t)))\n\n# Generate beta-synchronized spikes\n# Strong preference for specific beta phase, weak response to gamma\nbeta_spike_probability = (0.8 * (1 + np.cos(beta_phase - np.pi/4)) + \n                            0.1 * (1 + np.cos(gamma_phase - np.pi/3)))\n\nbeta_spike_train = np.random.rand(len(t)) &lt; beta_spike_probability * 0.02\nbeta_spike_times = t[beta_spike_train]\n\n# Generate gamma-synchronized spikes\n# Strong preference for specific gamma phase, weak response to beta\ngamma_spike_probability = (0.1 * (1 + np.cos(beta_phase - np.pi/4)) + \n                            0.8 * (1 + np.cos(gamma_phase - np.pi/3)))\n\ngamma_spike_train = np.random.rand(len(t)) &lt; gamma_spike_probability * 0.02\ngamma_spike_times = t[gamma_spike_train]\n\nprint(f\"Generated {len(beta_spike_times)} beta-synchronized spikes\")\nprint(f\"Generated {len(gamma_spike_times)} gamma-synchronized spikes\")\n</pre> np.random.seed(9) # lucky number 9 fs = 1000  # 1 kHz sampling rate duration = 200  # 200 seconds t = np.arange(0, duration, 1/fs)  # Define oscillation frequencies beta_freq = 15  # Hz gamma_freq = 40  # Hz  # Create a simulated LFP with multiple frequency components lfp = (0.5 * np.sin(2 * np.pi * gamma_freq * t) + 0.2 * np.sin(2 * np.pi * beta_freq * t) +  0.3 * np.random.randn(len(t)))  # Generate phase information for each frequency beta_phase = np.angle(ss.hilbert(np.sin(2 * np.pi * beta_freq * t))) gamma_phase = np.angle(ss.hilbert(np.sin(2 * np.pi * gamma_freq * t)))  # Generate beta-synchronized spikes # Strong preference for specific beta phase, weak response to gamma beta_spike_probability = (0.8 * (1 + np.cos(beta_phase - np.pi/4)) +                              0.1 * (1 + np.cos(gamma_phase - np.pi/3)))  beta_spike_train = np.random.rand(len(t)) &lt; beta_spike_probability * 0.02 beta_spike_times = t[beta_spike_train]  # Generate gamma-synchronized spikes # Strong preference for specific gamma phase, weak response to beta gamma_spike_probability = (0.1 * (1 + np.cos(beta_phase - np.pi/4)) +                              0.8 * (1 + np.cos(gamma_phase - np.pi/3)))  gamma_spike_train = np.random.rand(len(t)) &lt; gamma_spike_probability * 0.02 gamma_spike_times = t[gamma_spike_train]  print(f\"Generated {len(beta_spike_times)} beta-synchronized spikes\") print(f\"Generated {len(gamma_spike_times)} gamma-synchronized spikes\") <pre>Generated 3596 beta-synchronized spikes\nGenerated 3661 gamma-synchronized spikes\n</pre> In\u00a0[4]: Copied! <pre>hz,pxx = ss.welch(lfp,fs)\n_,_ = fit_fooof(hz,pxx,plot=True,report=True,plt_range=(1,100),freq_range=[1,100])\n</pre> hz,pxx = ss.welch(lfp,fs) _,_ = fit_fooof(hz,pxx,plot=True,report=True,plt_range=(1,100),freq_range=[1,100]) <pre>==================================================================================================\n                                                                                                  \n                                   FOOOF - POWER SPECTRUM MODEL                                   \n                                                                                                  \n                        The model was run on the frequency range 3 - 98 Hz                        \n                                 Frequency Resolution is 3.91 Hz                                  \n                                                                                                  \n                            Aperiodic Parameters (offset, exponent):                              \n                                         -3.8986, -0.0807                                         \n                                                                                                  \n                                       2 peaks were found:                                        \n                                CF:  15.00, PW:  1.239, BW:  7.81                                 \n                                CF:  39.85, PW:  2.120, BW:  7.81                                 \n                                                                                                  \n                                     Goodness of fit metrics:                                     \n                                    R^2 of model fit is 0.9808                                    \n                                    Error of the fit is 0.0539                                    \n                                                                                                  \n==================================================================================================\n</pre> In\u00a0[5]: Copied! <pre># Demonstrate phase locking calculations for gamma-entrained spikes at 40 Hz\n# Note: spikes were generated on second time scale, so spike_fs=1\n\nprint(\"=== Phase Locking Results for Gamma-Entrained Spikes (40 Hz) ===\")\n\n# 1. Unbiased PLV (equivalent to square root of PPC)\nplv_result = calculate_spike_lfp_plv(\n    spike_times=gamma_spike_times, \n    lfp_data=lfp, \n    spike_fs=1, \n    lfp_fs=fs, \n    filter_method='wavelet', \n    freq_of_interest=40\n)\nprint(f\"PLV (unbiased): {plv_result:.4f}\")\n\n# 2. PPC with different computational methods (all should give identical results)\nppc_numpy = calculate_ppc(\n    spike_times=gamma_spike_times, \n    lfp_data=lfp, \n    spike_fs=1, \n    lfp_fs=fs, \n    filter_method='wavelet', \n    freq_of_interest=40, \n    ppc_method='numpy'\n)\nprint(f\"\u221aPPC (numpy):   {np.sqrt(ppc_numpy):.4f}\")\n\nppc_numba = calculate_ppc(\n    spike_times=gamma_spike_times, \n    lfp_data=lfp, \n    spike_fs=1, \n    lfp_fs=fs, \n    filter_method='wavelet', \n    freq_of_interest=40, \n    ppc_method='numba'\n)\nprint(f\"\u221aPPC (numba):   {np.sqrt(ppc_numba):.4f}\")\n\n# 3. GPU-accelerated PPC (requires CUDA toolkit and NVIDIA GPU)\ntry:\n    ppc_gpu = calculate_ppc(\n        spike_times=gamma_spike_times, \n        lfp_data=lfp, \n        spike_fs=1, \n        lfp_fs=fs, \n        filter_method='wavelet', \n        freq_of_interest=40, \n        ppc_method='gpu'\n    )\n    print(f\"\u221aPPC (GPU):     {np.sqrt(ppc_gpu):.4f}\")\nexcept:\n    print(\"\u221aPPC (GPU):     Not available (requires NVIDIA GPU + CUDA)\")\n\n# 4. Optimized PPC2 (most efficient for large datasets)\nppc2_result = calculate_ppc2(\n    spike_times=gamma_spike_times, \n    lfp_data=lfp, \n    spike_fs=1, \n    lfp_fs=fs, \n    filter_method='wavelet', \n    freq_of_interest=40\n)\nprint(f\"\u221aPPC2:          {np.sqrt(ppc2_result):.4f}\")\n\nprint(f\"\\nAll methods should produce similar values (~{plv_result:.3f})\")\n</pre> # Demonstrate phase locking calculations for gamma-entrained spikes at 40 Hz # Note: spikes were generated on second time scale, so spike_fs=1  print(\"=== Phase Locking Results for Gamma-Entrained Spikes (40 Hz) ===\")  # 1. Unbiased PLV (equivalent to square root of PPC) plv_result = calculate_spike_lfp_plv(     spike_times=gamma_spike_times,      lfp_data=lfp,      spike_fs=1,      lfp_fs=fs,      filter_method='wavelet',      freq_of_interest=40 ) print(f\"PLV (unbiased): {plv_result:.4f}\")  # 2. PPC with different computational methods (all should give identical results) ppc_numpy = calculate_ppc(     spike_times=gamma_spike_times,      lfp_data=lfp,      spike_fs=1,      lfp_fs=fs,      filter_method='wavelet',      freq_of_interest=40,      ppc_method='numpy' ) print(f\"\u221aPPC (numpy):   {np.sqrt(ppc_numpy):.4f}\")  ppc_numba = calculate_ppc(     spike_times=gamma_spike_times,      lfp_data=lfp,      spike_fs=1,      lfp_fs=fs,      filter_method='wavelet',      freq_of_interest=40,      ppc_method='numba' ) print(f\"\u221aPPC (numba):   {np.sqrt(ppc_numba):.4f}\")  # 3. GPU-accelerated PPC (requires CUDA toolkit and NVIDIA GPU) try:     ppc_gpu = calculate_ppc(         spike_times=gamma_spike_times,          lfp_data=lfp,          spike_fs=1,          lfp_fs=fs,          filter_method='wavelet',          freq_of_interest=40,          ppc_method='gpu'     )     print(f\"\u221aPPC (GPU):     {np.sqrt(ppc_gpu):.4f}\") except:     print(\"\u221aPPC (GPU):     Not available (requires NVIDIA GPU + CUDA)\")  # 4. Optimized PPC2 (most efficient for large datasets) ppc2_result = calculate_ppc2(     spike_times=gamma_spike_times,      lfp_data=lfp,      spike_fs=1,      lfp_fs=fs,      filter_method='wavelet',      freq_of_interest=40 ) print(f\"\u221aPPC2:          {np.sqrt(ppc2_result):.4f}\")  print(f\"\\nAll methods should produce similar values (~{plv_result:.3f})\") <pre>=== Phase Locking Results for Gamma-Entrained Spikes (40 Hz) ===\nPLV (unbiased): 0.4253\n\u221aPPC (numpy):   0.4253\n</pre> <pre>OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n</pre> <pre>\u221aPPC (numba):   0.4253\n\u221aPPC (GPU):     Not available (requires NVIDIA GPU + CUDA)\n\u221aPPC2:          0.4253\n\nAll methods should produce similar values (~0.425)\n</pre> In\u00a0[6]: Copied! <pre># Test entrainment across a range of frequencies\nfreqs = [1, 5, 15, 25, 30, 40, 50, 60, 70, 80]\nppc_gamma = []\nppc_beta = []\n\nprint(\"Calculating PPC across frequencies...\")\nfor freq in freqs:\n    # Calculate PPC for gamma-entrained spikes\n    ppc_gamma.append(calculate_ppc2(\n        spike_times=gamma_spike_times, \n        lfp_data=lfp, \n        spike_fs=1, \n        lfp_fs=fs, \n        filter_method='wavelet', \n        freq_of_interest=freq\n    ))\n    \n    # Calculate PPC for beta-entrained spikes\n    ppc_beta.append(calculate_ppc2(\n        spike_times=beta_spike_times, \n        lfp_data=lfp, \n        spike_fs=1, \n        lfp_fs=fs, \n        filter_method='wavelet', \n        freq_of_interest=freq\n    ))\n\n# Create the frequency entrainment plot\nplt.figure(figsize=(10, 6))\nplt.plot(freqs, ppc_gamma, 'o-', linewidth=2, markersize=8, \n         label=\"Gamma-entrained spikes\", color='red')\nplt.plot(freqs, ppc_beta, 'o-', linewidth=2, markersize=8, \n         label='Beta-entrained spikes', color='blue')\n\n# Add vertical lines to highlight target frequencies\nplt.axvline(x=15, color='blue', linestyle='--', alpha=0.5, label='Beta frequency (15 Hz)')\nplt.axvline(x=40, color='red', linestyle='--', alpha=0.5, label='Gamma frequency (40 Hz)')\n\nplt.xlabel('Frequency (Hz)', fontsize=12)\nplt.ylabel('PPC Value', fontsize=12)\nplt.title('Phase Locking Strength Across Frequencies', fontsize=14, fontweight='bold')\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Print peak frequencies for verification\ngamma_peak_idx = np.argmax(ppc_gamma)\nbeta_peak_idx = np.argmax(ppc_beta)\nprint(\"\\nPeak entrainment frequencies:\")\nprint(f\"Gamma spikes: {freqs[gamma_peak_idx]} Hz (PPC = {ppc_gamma[gamma_peak_idx]:.4f})\")\nprint(f\"Beta spikes:  {freqs[beta_peak_idx]} Hz (PPC = {ppc_beta[beta_peak_idx]:.4f})\")\n</pre> # Test entrainment across a range of frequencies freqs = [1, 5, 15, 25, 30, 40, 50, 60, 70, 80] ppc_gamma = [] ppc_beta = []  print(\"Calculating PPC across frequencies...\") for freq in freqs:     # Calculate PPC for gamma-entrained spikes     ppc_gamma.append(calculate_ppc2(         spike_times=gamma_spike_times,          lfp_data=lfp,          spike_fs=1,          lfp_fs=fs,          filter_method='wavelet',          freq_of_interest=freq     ))          # Calculate PPC for beta-entrained spikes     ppc_beta.append(calculate_ppc2(         spike_times=beta_spike_times,          lfp_data=lfp,          spike_fs=1,          lfp_fs=fs,          filter_method='wavelet',          freq_of_interest=freq     ))  # Create the frequency entrainment plot plt.figure(figsize=(10, 6)) plt.plot(freqs, ppc_gamma, 'o-', linewidth=2, markersize=8,           label=\"Gamma-entrained spikes\", color='red') plt.plot(freqs, ppc_beta, 'o-', linewidth=2, markersize=8,           label='Beta-entrained spikes', color='blue')  # Add vertical lines to highlight target frequencies plt.axvline(x=15, color='blue', linestyle='--', alpha=0.5, label='Beta frequency (15 Hz)') plt.axvline(x=40, color='red', linestyle='--', alpha=0.5, label='Gamma frequency (40 Hz)')  plt.xlabel('Frequency (Hz)', fontsize=12) plt.ylabel('PPC Value', fontsize=12) plt.title('Phase Locking Strength Across Frequencies', fontsize=14, fontweight='bold') plt.legend(fontsize=10) plt.grid(True, alpha=0.3) plt.tight_layout() plt.show()  # Print peak frequencies for verification gamma_peak_idx = np.argmax(ppc_gamma) beta_peak_idx = np.argmax(ppc_beta) print(\"\\nPeak entrainment frequencies:\") print(f\"Gamma spikes: {freqs[gamma_peak_idx]} Hz (PPC = {ppc_gamma[gamma_peak_idx]:.4f})\") print(f\"Beta spikes:  {freqs[beta_peak_idx]} Hz (PPC = {ppc_beta[beta_peak_idx]:.4f})\") <pre>Calculating PPC across frequencies...\n</pre> <pre>\nPeak entrainment frequencies:\nGamma spikes: 40 Hz (PPC = 0.1809)\nBeta spikes:  15 Hz (PPC = 0.2066)\n</pre>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#calculating-phase-locking-with-bmtool","title":"Calculating Phase Locking with BMTool\u00b6","text":"<p>By Gregory Glickert</p> <p>This notebook demonstrates how to use BMTool to calculate phase locking between spike times and local field potential (LFP) oscillations. We'll explore three different metrics: Phase-Locking Value (PLV), Pairwise Phase Consistency (PPC), and an optimized version (PPC2).</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#data-generation","title":"Data Generation\u00b6","text":"<p>Let's create synthetic data to demonstrate phase locking analysis. We'll generate:</p> <ol> <li>Simulated LFP with beta (15 Hz) and gamma (40 Hz) oscillations plus noise</li> <li>Beta-entrained spikes that preferentially fire at specific beta phases</li> <li>Gamma-entrained spikes that preferentially fire at specific gamma phases</li> </ol> <p>This controlled setup allows us to verify that our analysis methods correctly identify the known phase relationships.</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#power-spectral-analysis","title":"Power Spectral Analysis\u00b6","text":"<p>We can visualize the frequency content of our simulated LFP to confirm it contains the expected beta (15 Hz) and gamma (40 Hz) oscillations:</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#phase-locking-metrics","title":"Phase Locking Metrics\u00b6","text":"<p>Now we can analyze how our spike times are entrained to the LFP we created. BMTool provides three different metrics for quantifying phase locking:</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#1-phase-locking-value-plv-calculate_spike_lfp_plv","title":"1. Phase-Locking Value (PLV) - <code>calculate_spike_lfp_plv</code>\u00b6","text":"<p>The unbiased Phase Locking Value is calculated through the following steps:</p> <ol> <li><p>Standard PLV calculation: $$PLV = \\left| \\frac{1}{N} \\sum_{j=1}^{N} e^{i\\phi_j} \\right|$$ where $\\phi_j$ is the phase at each spike time.</p> </li> <li><p>Pairwise Phase Consistency (PPC) calculation: $$PPC = \\frac{PLV^2 \\cdot N - 1}{N - 1}$$</p> </li> <li><p>Unbiased PLV calculation: $$PLV_{unbiased} = \\sqrt{\\max(PPC, 0)}$$</p> </li> </ol>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#2-pairwise-phase-consistency-ppc-calculate_ppc","title":"2. Pairwise Phase Consistency (PPC) - <code>calculate_ppc</code>\u00b6","text":"<p>PPC calculates phase consistency through pairwise comparisons of spike-phase differences:</p> <p>$$PPC = \\frac{2}{n(n-1)} \\sum_{j=1}^{n-1} \\sum_{k=j+1}^{n} \\cos(\\phi_j - \\phi_k)$$</p> <p>Where:</p> <ul> <li>$\\phi_j, \\phi_k$ = Phases at spike times $j$ and $k$</li> <li>$n$ = Number of spikes</li> </ul>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#3-optimized-pairwise-phase-consistency-ppc2-calculate_ppc2","title":"3. Optimized Pairwise Phase Consistency (PPC2) - <code>calculate_ppc2</code>\u00b6","text":"<p>PPC2 is an algebraically equivalent but computationally optimized version of PPC.</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#derivation-from-ppc","title":"Derivation from PPC:\u00b6","text":"<ol> <li><p>Original PPC: $$PPC = \\frac{2}{n(n-1)} \\sum_{i &lt; j} \\cos(\\phi_i - \\phi_j)$$</p> </li> <li><p>Rewrite using Euler's formula: $$\\cos(\\phi_i - \\phi_j) = \\text{Re}\\left( e^{i(\\phi_i - \\phi_j)} \\right)$$</p> </li> <li><p>Sum over all pairs: $$\\sum_{i &lt; j} \\cos(\\phi_i - \\phi_j) = \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\cos(\\phi_i - \\phi_j) - \\frac{n}{2}$$</p> </li> <li><p>Express in terms of complex exponentials: $$\\sum_{i &lt; j} \\cos(\\phi_i - \\phi_j) = \\frac{1}{2} \\text{Re} \\left( \\left| \\sum_{j=1}^n e^{i\\phi_j} \\right|^2 \\right) - \\frac{n}{2}$$</p> </li> <li><p>Final PPC2 formula: $$PPC2 = \\frac{\\left|\\sum_{j=1}^{n} e^{i\\phi_j}\\right|^2 - n}{n(n-1)}$$</p> </li> </ol>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#computational-advantages-of-ppc2","title":"Computational advantages of PPC2:\u00b6","text":"<ul> <li>Converts phases to complex unit vectors</li> <li>Uses vector magnitude properties to avoid explicit pairwise comparisons</li> <li>Significantly faster for large spike datasets</li> </ul>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#key-points","title":"Key Points:\u00b6","text":"<ul> <li>All three metrics quantify phase locking between spike times and LFP oscillations</li> <li>With sufficient spike times, all metrics should converge to similar values</li> <li>PPC2 offers the best computational performance for large datasets</li> </ul>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#demonstration-calculating-phase-locking-metrics","title":"Demonstration: Calculating Phase Locking Metrics\u00b6","text":"<p>Let's test all three methods on our gamma-entrained spikes to verify they produce consistent results:</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#frequency-specific-entrainment-analysis","title":"Frequency-Specific Entrainment Analysis\u00b6","text":"<p>We can also examine entrainment strength across different frequencies to see which oscillations our spike trains are most strongly locked to:</p>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#summary-and-conclusions","title":"Summary and Conclusions\u00b6","text":"<p>This notebook demonstrated how to use BMTool to:</p> <ol> <li>Generate simulated data with known phase relationships between spikes and LFP oscillations</li> <li>Calculate phase locking metrics using three different methods (PLV, PPC, PPC2)</li> <li>Analyze frequency-specific entrainment to identify which oscillations drive neural activity</li> </ol>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#key-results","title":"Key Results:\u00b6","text":"<ul> <li>Gamma-entrained spikes showed strongest phase locking at 40 Hz (gamma frequency)</li> <li>Beta-entrained spikes showed strongest phase locking at 15 Hz (beta frequency)</li> <li>All three metrics (PLV, PPC, PPC2) produced consistent results</li> <li>PPC2 provides the most computationally efficient approach for large datasets</li> </ul>"},{"location":"examples/notebooks/analysis/phase_locking_value/spike_phase_entrainment/#when-to-use-each-method","title":"When to Use Each Method:\u00b6","text":"<ul> <li>PLV: Good general-purpose metric, corrected for bias</li> <li>PPC: Gold standard for phase consistency, computationally intensive</li> <li>PPC2: Recommended for large datasets due to computational efficiency</li> </ul> <p>BMTool makes it easy to quantify neural entrainment with just a few function calls!</p>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool/","title":"Plot Spectrogram","text":"In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom bmtool.analysis.lfp import (\n    cwt_spectrogram_xarray,\n    ecp_to_lfp,\n    fit_fooof,\n    get_windowed_data,\n    load_ecp_to_xarray,\n)\nfrom bmtool.bmplot.lfp import plot_spectrogram\n\necp = load_ecp_to_xarray('/home/gjgpb9/cortex_modeling/bmtool/examples/analysis/network_to_analysis/long/ecp.h5')\nlfp = ecp_to_lfp(ecp)\n</pre> import matplotlib.pyplot as plt import numpy as np  from bmtool.analysis.lfp import (     cwt_spectrogram_xarray,     ecp_to_lfp,     fit_fooof,     get_windowed_data,     load_ecp_to_xarray, ) from bmtool.bmplot.lfp import plot_spectrogram  ecp = load_ecp_to_xarray('/home/gjgpb9/cortex_modeling/bmtool/examples/analysis/network_to_analysis/long/ecp.h5') lfp = ecp_to_lfp(ecp) In\u00a0[6]: Copied! <pre>downsample_freq = 200 # if you want to further down sample\ntseg = 0.5  # time segment length for PSD\naxis = lfp.dims.index('time') # dimension index of time axis in x\n\nlfp_sxx = cwt_spectrogram_xarray(lfp, lfp.fs, axis=axis, downsample_fs = downsample_freq,\n                                        channel_coords={'channel_id': lfp.channel_id}, freq_range=(1 / tseg, np.inf))\n</pre> downsample_freq = 200 # if you want to further down sample tseg = 0.5  # time segment length for PSD axis = lfp.dims.index('time') # dimension index of time axis in x  lfp_sxx = cwt_spectrogram_xarray(lfp, lfp.fs, axis=axis, downsample_fs = downsample_freq,                                         channel_coords={'channel_id': lfp.channel_id}, freq_range=(1 / tseg, np.inf))  In\u00a0[7]: Copied! <pre>fooof_params = dict(aperiodic_mode='fixed', freq_range=(1,100), peak_width_limits=100.)\nplt_range = [2., 100.]\nclr_freq_range = None\nlog_power = 'dB'\n\nfig, ax = plt.subplots(1,1,figsize=(10,6))\nsxx = lfp_sxx.sel(channel_id=0)\nsxx_tot = sxx.PSD.mean(dim='time')\n\nfooof_results, _ = fit_fooof(sxx_tot.frequency.values, sxx_tot.values, **fooof_params,plot=False)\n_ = plot_spectrogram(sxx, remove_aperiodic=fooof_results, log_power=log_power,\n                        plt_range=plt_range, clr_freq_range=clr_freq_range, pad=0.01, ax=ax)\n</pre> fooof_params = dict(aperiodic_mode='fixed', freq_range=(1,100), peak_width_limits=100.) plt_range = [2., 100.] clr_freq_range = None log_power = 'dB'  fig, ax = plt.subplots(1,1,figsize=(10,6)) sxx = lfp_sxx.sel(channel_id=0) sxx_tot = sxx.PSD.mean(dim='time')  fooof_results, _ = fit_fooof(sxx_tot.frequency.values, sxx_tot.values, **fooof_params,plot=False) _ = plot_spectrogram(sxx, remove_aperiodic=fooof_results, log_power=log_power,                         plt_range=plt_range, clr_freq_range=clr_freq_range, pad=0.01, ax=ax) In\u00a0[8]: Copied! <pre># in this example there are 9 pulses given for 1 second with a 500ms break before the next pulse. The pulse starts 1 second into the simulation \non_time = 1 # how long in seconds is our stimulus\noff_time = 0.5 # how long is the break between stim is \nt_start = 1 # when starts in sec\nstimulus_count = 9\n\ntimes_to_avg = []\nfor i in range(stimulus_count):\n    end_time = t_start+on_time+off_time\n    times_to_avg.append([t_start,end_time])\n    t_start = end_time \n\n# make list into np array \ntimes_to_avg = np.array(times_to_avg)\n\n_, _, lfp_sxx_avg = get_windowed_data(lfp_sxx.PSD, times_to_avg, {0: np.arange(times_to_avg.shape[0])})\nlfp_sxx_avg = lfp_sxx_avg[0].mean_.sel(unique_cycle=0).to_dataset(name='PSD')\n\nfig, ax = plt.subplots(1,1,figsize=(10,6))\nsxx = lfp_sxx_avg.sel(channel_id=0)\nsxx_tot = sxx.PSD.mean(dim='time')\nfooof_results, _ = fit_fooof(sxx_tot.frequency.values, sxx_tot.values, **fooof_params,plot=False)\n_ = plot_spectrogram(sxx, remove_aperiodic=fooof_results, log_power=log_power,\n                        plt_range=plt_range, clr_freq_range=clr_freq_range, pad=0.01, ax=ax)\n</pre> # in this example there are 9 pulses given for 1 second with a 500ms break before the next pulse. The pulse starts 1 second into the simulation  on_time = 1 # how long in seconds is our stimulus off_time = 0.5 # how long is the break between stim is  t_start = 1 # when starts in sec stimulus_count = 9  times_to_avg = [] for i in range(stimulus_count):     end_time = t_start+on_time+off_time     times_to_avg.append([t_start,end_time])     t_start = end_time   # make list into np array  times_to_avg = np.array(times_to_avg)  _, _, lfp_sxx_avg = get_windowed_data(lfp_sxx.PSD, times_to_avg, {0: np.arange(times_to_avg.shape[0])}) lfp_sxx_avg = lfp_sxx_avg[0].mean_.sel(unique_cycle=0).to_dataset(name='PSD')  fig, ax = plt.subplots(1,1,figsize=(10,6)) sxx = lfp_sxx_avg.sel(channel_id=0) sxx_tot = sxx.PSD.mean(dim='time') fooof_results, _ = fit_fooof(sxx_tot.frequency.values, sxx_tot.values, **fooof_params,plot=False) _ = plot_spectrogram(sxx, remove_aperiodic=fooof_results, log_power=log_power,                         plt_range=plt_range, clr_freq_range=clr_freq_range, pad=0.01, ax=ax)  <pre>/home/gjgpb9/miniconda3/envs/bmtk/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom &lt;= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n</pre>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool/#example-of-how-to-generate-a-spectrogram-using-bmtool","title":"Example of how to generate a spectrogram using bmtool\u00b6","text":"<p>By Gregory Glickert</p> <p>First we read in our data and process it into an LFP</p>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool/#calculate-spectrogram","title":"calculate spectrogram\u00b6","text":"<p>Then we need to calculate the spectrogram. This will just make an array of the data and then we will process it further to make our plot</p>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool/#plot-spectrogram","title":"Plot spectrogram\u00b6","text":"<p>Whole simulation spectrogram with the aperiodic removed can be done like this. It may be good to check the aperiodic fit of the model before completely trusting the plot. This can be done by setting the plot argument in the fit_fooof function to True.</p>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool/#average-spectrogram-and-plot","title":"Average spectrogram and plot\u00b6","text":"<p>For this simulation be analyzed there were 9 pulses given. It may be easier to see the pulse response if we average the times together</p>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool_updated/","title":"Spectrogram with bmtool updated","text":"In\u00a0[1]: Copied! <pre># Suppress all warnings for cleaner output\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Suppress all warnings for cleaner output import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre># get environment set up if running in a Google Colab\nRunningInCOLAB = 'google.colab' in str(get_ipython())  # checks to see if we are in google colab\nif RunningInCOLAB:                                     # installs packages if in colab \n    %pip install bmtool &amp;&gt; /dev/null\n</pre> # get environment set up if running in a Google Colab RunningInCOLAB = 'google.colab' in str(get_ipython())  # checks to see if we are in google colab if RunningInCOLAB:                                     # installs packages if in colab      %pip install bmtool &amp;&gt; /dev/null In\u00a0[3]: Copied! <pre># Standard scientific Python libraries\nimport matplotlib.pyplot as plt  # For creating plots and figures\nimport numpy as np               # For numerical operations and arrays\nimport xarray as xr              # For labeled, multi-dimensional arrays\n\n# BMTool-specific functions for LFP analysis\nfrom bmtool.analysis.lfp import (\n    cwt_spectrogram_xarray,  # Continuous wavelet transform for spectrograms\n    fit_fooof,               # Fits 1/f background to power spectra\n    get_lfp_power            # Calculates power in specific frequency bands\n)\nfrom bmtool.bmplot.lfp import plot_spectrogram  # For plotting spectrograms\n</pre> # Standard scientific Python libraries import matplotlib.pyplot as plt  # For creating plots and figures import numpy as np               # For numerical operations and arrays import xarray as xr              # For labeled, multi-dimensional arrays  # BMTool-specific functions for LFP analysis from bmtool.analysis.lfp import (     cwt_spectrogram_xarray,  # Continuous wavelet transform for spectrograms     fit_fooof,               # Fits 1/f background to power spectra     get_lfp_power            # Calculates power in specific frequency bands ) from bmtool.bmplot.lfp import plot_spectrogram  # For plotting spectrograms <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> In\u00a0[4]: Copied! <pre>def create_lfp_with_gamma_bursts(\n    duration=10,                          # Total length of signal in seconds\n    sampling_rate=1000,                   # Number of samples per second (Hz)\n    n_bursts=15,                          # How many gamma bursts to create\n    burst_duration_range=(0.2, 0.6),      # Min and max duration of each burst (seconds)\n    gamma_freq_range=(45, 65),            # Min and max frequency of gamma (Hz)\n    amplitude_range=(0.5, 1.2),           # Min and max amplitude of bursts\n    noise_amplitude=0.4,                  # Amount of random noise to add\n    channel_noise_amplitude=0.1,          # Additional per-channel noise\n    random_seed=42                        # For reproducible results\n):\n    \"\"\"\n    Create synthetic LFP-like signal with random gamma bursts.\n    \n    This function simulates a brain signal (LFP) that contains brief periods \n    of high-frequency activity (gamma bursts) mixed with background noise.\n    \n    Returns\n    -------\n    lfp : xr.DataArray\n        LFP data with dimensions [channel_id, time]\n    \"\"\"\n    # Hardcoded single channel (can be extended to multiple channels)\n    n_channels = 1\n    \n    # Set random seed for reproducibility (same seed = same signal every time)\n    np.random.seed(random_seed)\n    \n    # Create time array: from 0 to duration with steps of 1/sampling_rate\n    t = np.arange(0, duration, 1/sampling_rate)\n    \n    # Start with a signal of all zeros\n    signal = np.zeros_like(t)\n    \n    # Generate random start times for bursts (avoid placing them too close to the end)\n    burst_times = np.sort(np.random.uniform(0, duration - burst_duration_range[1], n_bursts))\n    \n    # Create each gamma burst\n    for burst_time in burst_times:\n        # Random burst duration within specified range\n        burst_duration = np.random.uniform(*burst_duration_range)\n        burst_end = burst_time + burst_duration\n        \n        # Create a mask for time points within this burst\n        burst_mask = (t &gt;= burst_time) &amp; (t &lt; burst_end)\n        \n        # Random gamma frequency and amplitude for this burst\n        gamma_freq = np.random.uniform(*gamma_freq_range)\n        amplitude = np.random.uniform(*amplitude_range)\n        \n        # Create smooth envelope: burst rises and falls smoothly (like a bell curve)\n        burst_relative_t = t[burst_mask] - burst_time\n        envelope = np.sin(np.pi * burst_relative_t / burst_duration) ** 2\n        \n        # Add the gamma oscillation to the signal\n        # Includes main frequency + harmonic (2x frequency) for realism\n        signal[burst_mask] += (\n            amplitude * envelope * np.sin(2 * np.pi * gamma_freq * t[burst_mask]) +\n            0.3 * envelope * np.sin(2 * np.pi * (gamma_freq * 2) * t[burst_mask])\n        )\n    \n    # Add random white noise to make it more realistic\n    signal += noise_amplitude * np.random.randn(len(t))\n    \n    # Create channel data (single channel with a bit of extra noise)\n    data = signal[np.newaxis, :] + channel_noise_amplitude * np.random.randn(n_channels, len(t))\n    \n    # Convert to xarray format (makes it easier to work with labeled data)\n    lfp = xr.DataArray(\n        data,\n        dims=['channel_id', 'time'],\n        coords={'channel_id': np.arange(n_channels), 'time': t},\n        attrs={'fs': sampling_rate}  # Store sampling rate as metadata\n    )\n    \n    return lfp\n\n\n# ===== CREATE THE SIGNAL =====\n# Adjust these parameters to change the signal characteristics\nlfp = create_lfp_with_gamma_bursts(\n    duration=25,                      # 25 seconds of data\n    sampling_rate=1000,               # 1000 samples per second\n    n_bursts=15,                      # 15 gamma bursts\n    burst_duration_range=(0.2, 0.6),  # Each burst lasts 0.2-0.6 seconds\n    gamma_freq_range=(45, 65),        # Gamma frequencies between 45-65 Hz\n    amplitude_range=(0.1, 1.2),       # Variable burst amplitudes\n    noise_amplitude=0.4,              # Moderate background noise\n    channel_noise_amplitude=0.1,      # Small per-channel noise\n    random_seed=42                    # Change this for different random signals\n)\n\n# Extract useful variables for later use\nt = lfp.time.values                   # Time points\nsampling_rate = lfp.attrs['fs']       # Sampling frequency\nn_channels = len(lfp.channel_id)      # Number of channels\n</pre> def create_lfp_with_gamma_bursts(     duration=10,                          # Total length of signal in seconds     sampling_rate=1000,                   # Number of samples per second (Hz)     n_bursts=15,                          # How many gamma bursts to create     burst_duration_range=(0.2, 0.6),      # Min and max duration of each burst (seconds)     gamma_freq_range=(45, 65),            # Min and max frequency of gamma (Hz)     amplitude_range=(0.5, 1.2),           # Min and max amplitude of bursts     noise_amplitude=0.4,                  # Amount of random noise to add     channel_noise_amplitude=0.1,          # Additional per-channel noise     random_seed=42                        # For reproducible results ):     \"\"\"     Create synthetic LFP-like signal with random gamma bursts.          This function simulates a brain signal (LFP) that contains brief periods      of high-frequency activity (gamma bursts) mixed with background noise.          Returns     -------     lfp : xr.DataArray         LFP data with dimensions [channel_id, time]     \"\"\"     # Hardcoded single channel (can be extended to multiple channels)     n_channels = 1          # Set random seed for reproducibility (same seed = same signal every time)     np.random.seed(random_seed)          # Create time array: from 0 to duration with steps of 1/sampling_rate     t = np.arange(0, duration, 1/sampling_rate)          # Start with a signal of all zeros     signal = np.zeros_like(t)          # Generate random start times for bursts (avoid placing them too close to the end)     burst_times = np.sort(np.random.uniform(0, duration - burst_duration_range[1], n_bursts))          # Create each gamma burst     for burst_time in burst_times:         # Random burst duration within specified range         burst_duration = np.random.uniform(*burst_duration_range)         burst_end = burst_time + burst_duration                  # Create a mask for time points within this burst         burst_mask = (t &gt;= burst_time) &amp; (t &lt; burst_end)                  # Random gamma frequency and amplitude for this burst         gamma_freq = np.random.uniform(*gamma_freq_range)         amplitude = np.random.uniform(*amplitude_range)                  # Create smooth envelope: burst rises and falls smoothly (like a bell curve)         burst_relative_t = t[burst_mask] - burst_time         envelope = np.sin(np.pi * burst_relative_t / burst_duration) ** 2                  # Add the gamma oscillation to the signal         # Includes main frequency + harmonic (2x frequency) for realism         signal[burst_mask] += (             amplitude * envelope * np.sin(2 * np.pi * gamma_freq * t[burst_mask]) +             0.3 * envelope * np.sin(2 * np.pi * (gamma_freq * 2) * t[burst_mask])         )          # Add random white noise to make it more realistic     signal += noise_amplitude * np.random.randn(len(t))          # Create channel data (single channel with a bit of extra noise)     data = signal[np.newaxis, :] + channel_noise_amplitude * np.random.randn(n_channels, len(t))          # Convert to xarray format (makes it easier to work with labeled data)     lfp = xr.DataArray(         data,         dims=['channel_id', 'time'],         coords={'channel_id': np.arange(n_channels), 'time': t},         attrs={'fs': sampling_rate}  # Store sampling rate as metadata     )          return lfp   # ===== CREATE THE SIGNAL ===== # Adjust these parameters to change the signal characteristics lfp = create_lfp_with_gamma_bursts(     duration=25,                      # 25 seconds of data     sampling_rate=1000,               # 1000 samples per second     n_bursts=15,                      # 15 gamma bursts     burst_duration_range=(0.2, 0.6),  # Each burst lasts 0.2-0.6 seconds     gamma_freq_range=(45, 65),        # Gamma frequencies between 45-65 Hz     amplitude_range=(0.1, 1.2),       # Variable burst amplitudes     noise_amplitude=0.4,              # Moderate background noise     channel_noise_amplitude=0.1,      # Small per-channel noise     random_seed=42                    # Change this for different random signals )  # Extract useful variables for later use t = lfp.time.values                   # Time points sampling_rate = lfp.attrs['fs']       # Sampling frequency n_channels = len(lfp.channel_id)      # Number of channels  In\u00a0[5]: Copied! <pre># ===== SPECTROGRAM CALCULATION PARAMETERS =====\ndownsample_freq = 200   # Downsample to 200 Hz for faster processing\ntseg = 0.5              # Time segment length for spectral estimation\n\n# Settings for removing aperiodic (1/f) background from spectrogram\nfooof_params = dict(\n    aperiodic_mode='fixed',      # Assumes 1/f background without a \"knee\"\n    freq_range=(1, 100),         # Fit the model from 1-100 Hz\n    peak_width_limits=100.       # Allow peaks of any width\n)\n\nplt_range = [2., 100.]  # Frequency range to display in plots (2-100 Hz)\n\n# Get the dimension index for the time axis\naxis = lfp.dims.index('time')\n\n# ===== COMPUTE THE SPECTROGRAM =====\nlfp_sxx = cwt_spectrogram_xarray(\n    lfp, \n    lfp.fs,                      # Sampling frequency from LFP data\n    axis=axis,                   # Which dimension is time\n    downsample_fs=downsample_freq,\n    channel_coords={'channel_id': lfp.channel_id},\n    freq_range=(1 / tseg, np.inf)  # Minimum frequency to analyze\n)\n</pre> # ===== SPECTROGRAM CALCULATION PARAMETERS ===== downsample_freq = 200   # Downsample to 200 Hz for faster processing tseg = 0.5              # Time segment length for spectral estimation  # Settings for removing aperiodic (1/f) background from spectrogram fooof_params = dict(     aperiodic_mode='fixed',      # Assumes 1/f background without a \"knee\"     freq_range=(1, 100),         # Fit the model from 1-100 Hz     peak_width_limits=100.       # Allow peaks of any width )  plt_range = [2., 100.]  # Frequency range to display in plots (2-100 Hz)  # Get the dimension index for the time axis axis = lfp.dims.index('time')  # ===== COMPUTE THE SPECTROGRAM ===== lfp_sxx = cwt_spectrogram_xarray(     lfp,      lfp.fs,                      # Sampling frequency from LFP data     axis=axis,                   # Which dimension is time     downsample_fs=downsample_freq,     channel_coords={'channel_id': lfp.channel_id},     freq_range=(1 / tseg, np.inf)  # Minimum frequency to analyze )  In\u00a0[6]: Copied! <pre>def plot_spectrogram_with_gamma_bursts(\n    lfp_data,                             # Raw LFP signal\n    lfp_sxx,                              # Spectrogram data\n    gamma_freq=55,                        # Which frequency to monitor for bursts\n    gamma_bandwidth=1.0,                  # Bandwidth of wavelet filter\n    power_threshold_percentile=85,        # Percentile threshold (higher = stricter)\n    min_burst_gap=0.05,                   # Min gap between bursts (seconds)\n    fooof_params=None,                    # Parameters for 1/f removal\n    log_power='dB',                       # Power scaling for visualization\n    plt_range=None,                       # Frequency range to plot\n    clr_freq_range=None,                  # Color scale range\n    figsize=(10, 6),                      # Figure size\n    channel_id=0                          # Which channel to analyze (always 0)\n):\n    \"\"\"\n    Detect gamma bursts and plot them on a spectrogram.\n    \n    This function identifies periods of elevated gamma power and marks them\n    on the spectrogram visualization.\n    \n    Returns\n    -------\n    fig, ax : matplotlib objects\n        The figure and axis for the plot\n    burst_groups : list of tuples\n        Each tuple is (start_time, end_time) of a detected burst\n    power_threshold : float\n        The actual power threshold value that was used\n    \"\"\"\n    \n    # ===== STEP 1: CALCULATE GAMMA POWER =====\n    lfp_channel = lfp_data.sel(channel_id=0)  # Select channel 0\n    \n    # Use wavelet filtering to get power at the gamma frequency\n    gamma_power = get_lfp_power(\n        lfp_channel,\n        freq_of_interest=gamma_freq,\n        fs=lfp_data.attrs['fs'],\n        filter_method='wavelet',\n        bandwidth=gamma_bandwidth\n    )\n    \n    # ===== STEP 2: FIND HIGH-POWER TIMES =====\n    # Set threshold as a percentile of all power values\n    power_threshold = np.percentile(gamma_power.values, power_threshold_percentile)\n    \n    # Find all time points where power exceeds threshold\n    high_power_mask = gamma_power.values &gt; power_threshold\n    burst_times = lfp_channel.time.values[high_power_mask]\n    \n    # ===== STEP 3: GROUP CONSECUTIVE DETECTIONS =====\n    # Nearby detections are part of the same burst event\n    burst_groups = []\n    if len(burst_times) &gt; 0:\n        burst_start = burst_times[0]\n        burst_end = burst_times[0]\n        \n        for i in range(1, len(burst_times)):\n            time_gap = burst_times[i] - burst_end\n            \n            if time_gap &lt;= min_burst_gap:\n                # Close enough - extend this burst\n                burst_end = burst_times[i]\n            else:\n                # Too far apart - start a new burst\n                burst_groups.append((burst_start, burst_end))\n                burst_start = burst_times[i]\n                burst_end = burst_times[i]\n        \n        # Don't forget the last burst\n        burst_groups.append((burst_start, burst_end))\n    \n    # Calculate center of each burst for marker placement\n    burst_centers = [(start + end) / 2 for start, end in burst_groups]\n    \n    # ===== STEP 4: CREATE THE PLOT =====\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    sxx = lfp_sxx.sel(channel_id=0)\n    sxx_tot = sxx.PSD.mean(dim='time')  # Average power spectrum\n    \n    # Fit and remove the aperiodic (1/f) background for clearer visualization\n    fooof_results, _ = fit_fooof(\n        sxx_tot.frequency.values, \n        sxx_tot.values, \n        **fooof_params, \n        plot=False\n    )\n    \n    # Plot the spectrogram with aperiodic component removed\n    _ = plot_spectrogram(\n        sxx, \n        remove_aperiodic=fooof_results,  # Remove 1/f background\n        log_power=log_power,\n        plt_range=plt_range, \n        clr_freq_range=clr_freq_range, \n        pad=0.01, \n        ax=ax\n    )\n    \n    # Mark each detected burst with a red triangle\n    for burst_center in burst_centers:\n        ax.plot(\n            burst_center, gamma_freq, \n            'r^',                           # Red triangle marker\n            markersize=8, \n            markerfacecolor='none',         # Hollow marker\n            markeredgewidth=1.5\n        )\n    \n    # Add informative title\n    ax.set_title(\n        f'Spectrogram with Gamma Bursts Marked\\n'\n        f'(threshold: {power_threshold_percentile}th percentile, '\n        f'gamma freq: {gamma_freq} Hz, bursts: {len(burst_groups)})'\n    )\n    plt.tight_layout()\n    \n    return fig, ax, burst_groups, power_threshold\n\n\n# ===== RUN THE ANALYSIS =====\nfig, ax, burst_groups, power_threshold = plot_spectrogram_with_gamma_bursts(\n    lfp_data=lfp,\n    lfp_sxx=lfp_sxx,\n    fooof_params=fooof_params,\n    plt_range=plt_range,\n    gamma_freq=55,                        # Look for bursts at 55 Hz\n    gamma_bandwidth=1.0,                  # Wavelet bandwidth\n    power_threshold_percentile=95,        # Only detect top 5% of power\n    min_burst_gap=0.05,                   # Group bursts within 50 ms\n    figsize=(10, 6)\n)\n\n# ===== PRINT RESULTS =====\nprint(f\"Number of burst groups detected: {len(burst_groups)}\")\nprint(f\"Power threshold used: {power_threshold:.2f}\")\n\nif len(burst_groups) &gt; 0:\n    print(f\"\\nBurst durations (seconds):\")\n    for i, (start, end) in enumerate(burst_groups):\n        duration = end - start\n        print(f\"  Burst {i+1}: {duration:.3f}s (from {start:.2f}s to {end:.2f}s)\")\n</pre> def plot_spectrogram_with_gamma_bursts(     lfp_data,                             # Raw LFP signal     lfp_sxx,                              # Spectrogram data     gamma_freq=55,                        # Which frequency to monitor for bursts     gamma_bandwidth=1.0,                  # Bandwidth of wavelet filter     power_threshold_percentile=85,        # Percentile threshold (higher = stricter)     min_burst_gap=0.05,                   # Min gap between bursts (seconds)     fooof_params=None,                    # Parameters for 1/f removal     log_power='dB',                       # Power scaling for visualization     plt_range=None,                       # Frequency range to plot     clr_freq_range=None,                  # Color scale range     figsize=(10, 6),                      # Figure size     channel_id=0                          # Which channel to analyze (always 0) ):     \"\"\"     Detect gamma bursts and plot them on a spectrogram.          This function identifies periods of elevated gamma power and marks them     on the spectrogram visualization.          Returns     -------     fig, ax : matplotlib objects         The figure and axis for the plot     burst_groups : list of tuples         Each tuple is (start_time, end_time) of a detected burst     power_threshold : float         The actual power threshold value that was used     \"\"\"          # ===== STEP 1: CALCULATE GAMMA POWER =====     lfp_channel = lfp_data.sel(channel_id=0)  # Select channel 0          # Use wavelet filtering to get power at the gamma frequency     gamma_power = get_lfp_power(         lfp_channel,         freq_of_interest=gamma_freq,         fs=lfp_data.attrs['fs'],         filter_method='wavelet',         bandwidth=gamma_bandwidth     )          # ===== STEP 2: FIND HIGH-POWER TIMES =====     # Set threshold as a percentile of all power values     power_threshold = np.percentile(gamma_power.values, power_threshold_percentile)          # Find all time points where power exceeds threshold     high_power_mask = gamma_power.values &gt; power_threshold     burst_times = lfp_channel.time.values[high_power_mask]          # ===== STEP 3: GROUP CONSECUTIVE DETECTIONS =====     # Nearby detections are part of the same burst event     burst_groups = []     if len(burst_times) &gt; 0:         burst_start = burst_times[0]         burst_end = burst_times[0]                  for i in range(1, len(burst_times)):             time_gap = burst_times[i] - burst_end                          if time_gap &lt;= min_burst_gap:                 # Close enough - extend this burst                 burst_end = burst_times[i]             else:                 # Too far apart - start a new burst                 burst_groups.append((burst_start, burst_end))                 burst_start = burst_times[i]                 burst_end = burst_times[i]                  # Don't forget the last burst         burst_groups.append((burst_start, burst_end))          # Calculate center of each burst for marker placement     burst_centers = [(start + end) / 2 for start, end in burst_groups]          # ===== STEP 4: CREATE THE PLOT =====     fig, ax = plt.subplots(1, 1, figsize=figsize)     sxx = lfp_sxx.sel(channel_id=0)     sxx_tot = sxx.PSD.mean(dim='time')  # Average power spectrum          # Fit and remove the aperiodic (1/f) background for clearer visualization     fooof_results, _ = fit_fooof(         sxx_tot.frequency.values,          sxx_tot.values,          **fooof_params,          plot=False     )          # Plot the spectrogram with aperiodic component removed     _ = plot_spectrogram(         sxx,          remove_aperiodic=fooof_results,  # Remove 1/f background         log_power=log_power,         plt_range=plt_range,          clr_freq_range=clr_freq_range,          pad=0.01,          ax=ax     )          # Mark each detected burst with a red triangle     for burst_center in burst_centers:         ax.plot(             burst_center, gamma_freq,              'r^',                           # Red triangle marker             markersize=8,              markerfacecolor='none',         # Hollow marker             markeredgewidth=1.5         )          # Add informative title     ax.set_title(         f'Spectrogram with Gamma Bursts Marked\\n'         f'(threshold: {power_threshold_percentile}th percentile, '         f'gamma freq: {gamma_freq} Hz, bursts: {len(burst_groups)})'     )     plt.tight_layout()          return fig, ax, burst_groups, power_threshold   # ===== RUN THE ANALYSIS ===== fig, ax, burst_groups, power_threshold = plot_spectrogram_with_gamma_bursts(     lfp_data=lfp,     lfp_sxx=lfp_sxx,     fooof_params=fooof_params,     plt_range=plt_range,     gamma_freq=55,                        # Look for bursts at 55 Hz     gamma_bandwidth=1.0,                  # Wavelet bandwidth     power_threshold_percentile=95,        # Only detect top 5% of power     min_burst_gap=0.05,                   # Group bursts within 50 ms     figsize=(10, 6) )  # ===== PRINT RESULTS ===== print(f\"Number of burst groups detected: {len(burst_groups)}\") print(f\"Power threshold used: {power_threshold:.2f}\")  if len(burst_groups) &gt; 0:     print(f\"\\nBurst durations (seconds):\")     for i, (start, end) in enumerate(burst_groups):         duration = end - start         print(f\"  Burst {i+1}: {duration:.3f}s (from {start:.2f}s to {end:.2f}s)\")  <pre>Number of burst groups detected: 8\nPower threshold used: 1.05\n\nBurst durations (seconds):\n  Burst 1: 0.097s (from 0.59s to 0.69s)\n  Burst 2: 0.172s (from 1.52s to 1.69s)\n  Burst 3: 0.054s (from 3.92s to 3.98s)\n  Burst 4: 0.223s (from 9.30s to 9.52s)\n  Burst 5: 0.209s (from 14.71s to 14.92s)\n  Burst 6: 0.165s (from 18.02s to 18.19s)\n  Burst 7: 0.181s (from 20.43s to 20.62s)\n  Burst 8: 0.228s (from 21.27s to 21.50s)\n</pre>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool_updated/#spectrogram-analysis-with-gamma-burst-detection","title":"Spectrogram Analysis with Gamma Burst Detection\u00b6","text":"<p>By Gregory Glickert</p> <p>This notebook demonstrates how to:</p> <ol> <li>Generate a synthetic LFP (Local Field Potential) signal with gamma bursts</li> <li>Calculate a spectrogram using continuous wavelet transform</li> <li>Detect and mark gamma burst events on the spectrogram</li> </ol> <p>What is a spectrogram? A spectrogram shows how the frequency content of a signal changes over time. The x-axis is time, the y-axis is frequency, and the color shows the power at each frequency.</p>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool_updated/#step-1-import-libraries","title":"Step 1: Import Libraries\u00b6","text":"<p>First, we import the necessary Python libraries for signal processing and visualization.</p>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool_updated/#step-2-create-synthetic-lfp-signal","title":"Step 2: Create Synthetic LFP Signal\u00b6","text":"<p>This function generates a realistic LFP-like signal with random gamma bursts. You can adjust the parameters to control:</p> <ul> <li>How long the signal is (<code>duration</code>)</li> <li>How many gamma bursts appear (<code>n_bursts</code>)</li> <li>The frequency range of gamma activity (<code>gamma_freq_range</code>)</li> <li>The amplitude and noise levels</li> </ul>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool_updated/#step-3-calculate-spectrogram","title":"Step 3: Calculate Spectrogram\u00b6","text":"<p>Now we compute the spectrogram using continuous wavelet transform (CWT). This method is good for analyzing signals with time-varying frequency content.</p> <p>Key parameters:</p> <ul> <li><code>downsample_freq</code>: Reduces data size for faster processing (200 Hz is sufficient for viewing)</li> <li><code>freq_range</code>: Only analyze frequencies above 2 Hz (filters out very slow drifts)</li> </ul>"},{"location":"examples/notebooks/analysis/spectrogram/spectrogram_with_bmtool_updated/#step-4-detect-and-visualize-gamma-bursts","title":"Step 4: Detect and Visualize Gamma Bursts\u00b6","text":"<p>This function does three things:</p> <ol> <li>Detects gamma bursts by calculating power in the gamma band and finding times when it exceeds a threshold</li> <li>Groups nearby detections into distinct burst events (so we don't count the same burst multiple times)</li> <li>Plots the spectrogram with markers showing where bursts were detected</li> </ol> <p>Key parameters to adjust:</p> <ul> <li><code>gamma_freq</code>: Center frequency to look for bursts (default: 55 Hz)</li> <li><code>power_threshold_percentile</code>: Higher values = only detect strongest bursts (default: 95%)</li> <li><code>min_burst_gap</code>: How close bursts must be to group them together (default: 0.05 seconds)</li> </ul>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/","title":"Spikes Module","text":"In\u00a0[41]: Copied! <pre>import matplotlib.pyplot as plt\n\nfrom bmtool.analysis.spikes import (\n    average_spike_rate_over_windows,\n    compute_firing_rate_stats,\n    get_population_spike_rate,\n    load_spikes_to_df,\n)\nfrom bmtool.bmplot.spikes import plot_firing_rate_distribution, plot_firing_rate_pop_stats, raster\n\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt  from bmtool.analysis.spikes import (     average_spike_rate_over_windows,     compute_firing_rate_stats,     get_population_spike_rate,     load_spikes_to_df, ) from bmtool.bmplot.spikes import plot_firing_rate_distribution, plot_firing_rate_pop_stats, raster  %matplotlib inline In\u00a0[42]: Copied! <pre>config_path = '/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json'\noutput_path = '/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/final_result_2/block1/long/spikes.h5'\n\ndf = load_spikes_to_df(spike_file=output_path,network_name='cortex',config=config_path)\ndf\n</pre> config_path = '/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json' output_path = '/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/final_result_2/block1/long/spikes.h5'  df = load_spikes_to_df(spike_file=output_path,network_name='cortex',config=config_path) df Out[42]: node_ids timestamps pop_name 0 8622 7.1 FSI 1 9477 7.3 LTS 2 9903 7.7 LTS 3 9300 8.4 FSI 4 3791 9.0 CP ... ... ... ... 464167 9864 14499.6 LTS 464168 9681 14499.6 LTS 464169 1890 14499.6 CP 464170 6613 14499.7 CS 464171 1647 14499.9 CP <p>464172 rows \u00d7 3 columns</p> <p>You can actually have the function label the spikes with any attribute that is there when you build the nodes. By default it is pop_name but you could do anything. So if your build looks like this.</p> <pre>net.add_nodes(N=n_I, pop_name='PV',     # N = number of inhibitory cells\n        model_type='biophysical',\n        model_template='hoc:WBInhCell',  # WBInhCell hoc definition\n        morphology='blank.swc',\n        positions = positions_cuboid(N=n_I,center=[0.,0.,0.],xside_length=10.,yside_length=10.,height=10.,min_dist=2))\n</pre> <p>You could use, N,pop_name,model_type,model_template,morphology,and position to label the cells. For this example N and positions would be poor choices to label due to being unique for every cell.</p> In\u00a0[43]: Copied! <pre>df = load_spikes_to_df(output_path,network_name='cortex',config=config_path,groupby=['model_template','pop_name'])\ndf\n</pre> df = load_spikes_to_df(output_path,network_name='cortex',config=config_path,groupby=['model_template','pop_name']) df Out[43]: node_ids timestamps model_template pop_name 0 8622 7.1 hoc:FSI_Cell FSI 1 9477 7.3 hoc:LTS_Cell LTS 2 9903 7.7 hoc:LTS_Cell LTS 3 9300 8.4 hoc:FSI_Cell FSI 4 3791 9.0 hoc:CP_Cell CP ... ... ... ... ... 464167 9864 14499.6 hoc:LTS_Cell LTS 464168 9681 14499.6 hoc:LTS_Cell LTS 464169 1890 14499.6 hoc:CP_Cell CP 464170 6613 14499.7 hoc:CS_Cell CS 464171 1647 14499.9 hoc:CP_Cell CP <p>464172 rows \u00d7 4 columns</p> <p>This can be helpful if you have nodes with the same pop_name, but different layers so you may want more than one label to tell which cells are spiking.</p> In\u00a0[44]: Copied! <pre>raster(spikes_df=df)\nplt.show()\n</pre> raster(spikes_df=df) plt.show() <p>You can also play with some settings in the raster function if you want to view the data better or in a different way.</p> In\u00a0[45]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n\ncolor_map = {\n    'CS': 'red',\n    'CP': 'red', \n    'FSI': 'blue', \n    'LTS': 'green'\n}\n\nraster(spikes_df=df,ax=ax,\n       groupby='pop_name',\n       color_map=color_map,\n       tstart=0,tstop=1000)\nplt.show()\n</pre> fig, ax = plt.subplots(1, 1, figsize=(8, 5))  color_map = {     'CS': 'red',     'CP': 'red',      'FSI': 'blue',      'LTS': 'green' }  raster(spikes_df=df,ax=ax,        groupby='pop_name',        color_map=color_map,        tstart=0,tstop=1000) plt.show() In\u00a0[46]: Copied! <pre>df = load_spikes_to_df(output_path,network_name='cortex',config=config_path)\nspike_rate = get_population_spike_rate(df,config=config_path,network_name='cortex')\ndisplay(spike_rate)\n</pre> df = load_spikes_to_df(output_path,network_name='cortex',config=config_path) spike_rate = get_population_spike_rate(df,config=config_path,network_name='cortex') display(spike_rate) <pre>&lt;xarray.DataArray (time: 5800, population: 4, type: 2)&gt;\narray([[[0.        , 1.4118588 ],\n        [0.        , 0.88750121],\n        [0.        , 0.78587889],\n        [0.        , 0.72381333]],\n\n       [[0.        , 1.42681938],\n        [0.        , 0.8934122 ],\n        [0.        , 0.79454548],\n        [0.        , 0.73308296]],\n\n       [[0.47058824, 1.45641973],\n        [0.61538462, 0.90502934],\n        [0.        , 0.81154425],\n        [0.        , 0.75131488]],\n\n       ...,\n\n       [[2.35294118, 3.74999671],\n        [4.30769231, 3.12395504],\n        [1.03529412, 1.54630566],\n        [1.78823529, 1.6247682 ]],\n\n       [[2.82352941, 3.73754173],\n        [3.69230769, 3.14184906],\n        [1.69411765, 1.54911239],\n        [1.69411765, 1.62555177]],\n\n       [[4.23529412, 3.73125568],\n        [6.15384615, 3.15105916],\n        [1.97647059, 1.55052619],\n        [1.31764706, 1.62596401]]])\nCoordinates:\n  * time        (time) float64 0.0 2.5 5.0 7.5 ... 1.449e+04 1.45e+04 1.45e+04\n  * population  (population) object 'FSI' 'LTS' 'CP' 'CS'\n  * type        (type) &lt;U8 'raw' 'smoothed'\nAttributes:\n    fs:             400.0\n    normalized:     False\n    smooth_method:  gaussian\n    smooth_window:  50</pre>xarray.DataArray<ul><li>time: 5800</li><li>population: 4</li><li>type: 2</li></ul><ul><li>0.0 1.412 0.0 0.8875 0.0 0.7859 ... 3.151 1.976 1.551 1.318 1.626<pre>array([[[0.        , 1.4118588 ],\n        [0.        , 0.88750121],\n        [0.        , 0.78587889],\n        [0.        , 0.72381333]],\n\n       [[0.        , 1.42681938],\n        [0.        , 0.8934122 ],\n        [0.        , 0.79454548],\n        [0.        , 0.73308296]],\n\n       [[0.47058824, 1.45641973],\n        [0.61538462, 0.90502934],\n        [0.        , 0.81154425],\n        [0.        , 0.75131488]],\n\n       ...,\n\n       [[2.35294118, 3.74999671],\n        [4.30769231, 3.12395504],\n        [1.03529412, 1.54630566],\n        [1.78823529, 1.6247682 ]],\n\n       [[2.82352941, 3.73754173],\n        [3.69230769, 3.14184906],\n        [1.69411765, 1.54911239],\n        [1.69411765, 1.62555177]],\n\n       [[4.23529412, 3.73125568],\n        [6.15384615, 3.15105916],\n        [1.97647059, 1.55052619],\n        [1.31764706, 1.62596401]]])</pre></li><li>Coordinates: (3)<ul><li>time(time)float640.0 2.5 5.0 ... 1.45e+04 1.45e+04<pre>array([0.00000e+00, 2.50000e+00, 5.00000e+00, ..., 1.44925e+04, 1.44950e+04,\n       1.44975e+04])</pre></li><li>population(population)object'FSI' 'LTS' 'CP' 'CS'<pre>array(['FSI', 'LTS', 'CP', 'CS'], dtype=object)</pre></li><li>type(type)&lt;U8'raw' 'smoothed'<pre>array(['raw', 'smoothed'], dtype='&lt;U8')</pre></li></ul></li><li>Indexes: (3)<ul><li>timePandasIndex<pre>PandasIndex(Float64Index([    0.0,     2.5,     5.0,     7.5,    10.0,    12.5,    15.0,\n                 17.5,    20.0,    22.5,\n              ...\n              14475.0, 14477.5, 14480.0, 14482.5, 14485.0, 14487.5, 14490.0,\n              14492.5, 14495.0, 14497.5],\n             dtype='float64', name='time', length=5800))</pre></li><li>populationPandasIndex<pre>PandasIndex(Index(['FSI', 'LTS', 'CP', 'CS'], dtype='object', name='population'))</pre></li><li>typePandasIndex<pre>PandasIndex(Index(['raw', 'smoothed'], dtype='object', name='type'))</pre></li></ul></li><li>Attributes: (4)fs :400.0normalized :Falsesmooth_method :gaussiansmooth_window :50</li></ul> <p>You can plot the data as well. This example just uses the simple plotting built into an xarray, but you ca also plot it with matplotlib.</p> In\u00a0[47]: Copied! <pre>spike_rate.sel(type='raw').plot(hue='population')\nspike_rate.sel(type='smoothed').plot(hue='population')\nplt.title('Population Spike Rate')\nplt.xlim(1000, 5500)\nplt.show()\n</pre> spike_rate.sel(type='raw').plot(hue='population') spike_rate.sel(type='smoothed').plot(hue='population') plt.title('Population Spike Rate') plt.xlim(1000, 5500) plt.show() In\u00a0[48]: Copied! <pre>pulse_times = [(1000,2000), (2500,3500), (4000,5000)]\nspike_rate_avg = average_spike_rate_over_windows(spike_rate, pulse_times)\nspike_rate_avg.sel(type='raw').plot(hue='population')\nspike_rate_avg.sel(type='smoothed').plot(hue='population')\nplt.title('Population Spike Rate avgeraged over windows')\nplt.show()\n</pre> pulse_times = [(1000,2000), (2500,3500), (4000,5000)] spike_rate_avg = average_spike_rate_over_windows(spike_rate, pulse_times) spike_rate_avg.sel(type='raw').plot(hue='population') spike_rate_avg.sel(type='smoothed').plot(hue='population') plt.title('Population Spike Rate avgeraged over windows') plt.show() In\u00a0[49]: Copied! <pre>df = load_spikes_to_df(output_path, network_name=\"cortex\", config=config_path,groupby=['model_template','pop_name'])\npop_stats,indivdual_stats = compute_firing_rate_stats(df, groupby=['model_template','pop_name'])\ndisplay(indivdual_stats)\ndisplay(pop_stats)\n</pre> df = load_spikes_to_df(output_path, network_name=\"cortex\", config=config_path,groupby=['model_template','pop_name']) pop_stats,indivdual_stats = compute_firing_rate_stats(df, groupby=['model_template','pop_name']) display(indivdual_stats) display(pop_stats) node_ids spike_count model_template pop_name firing_rate 0 9848 852 hoc:LTS_Cell LTS 58.787812 1 8541 707 hoc:FSI_Cell FSI 48.782844 2 9430 617 hoc:LTS_Cell LTS 42.572864 3 8527 617 hoc:FSI_Cell FSI 42.572864 4 9214 558 hoc:FSI_Cell FSI 38.501877 ... ... ... ... ... ... 9986 7224 1 hoc:CS_Cell CS 0.069000 9987 8315 1 hoc:CS_Cell CS 0.069000 9988 8980 1 hoc:FSI_Cell FSI 0.069000 9989 9341 1 hoc:FSI_Cell FSI 0.069000 9990 9218 1 hoc:FSI_Cell FSI 0.069000 <p>9991 rows \u00d7 5 columns</p> model_template pop_name firing_rate_mean firing_rate_std 0 hoc:CP_Cell CP 2.210251 1.516544 1 hoc:CS_Cell CS 2.173785 1.428758 2 hoc:FSI_Cell FSI 4.494283 5.867914 3 hoc:LTS_Cell LTS 14.776037 7.553584 In\u00a0[50]: Copied! <pre>plot_firing_rate_distribution(indivdual_stats,groupby='pop_name',plot_type='violin') # type can be box,swarm,violin or a combo \nplt.show()\n</pre> plot_firing_rate_distribution(indivdual_stats,groupby='pop_name',plot_type='violin') # type can be box,swarm,violin or a combo  plt.show() In\u00a0[51]: Copied! <pre>plot_firing_rate_pop_stats(pop_stats, groupby=\"pop_name\")\nplt.show()\n</pre> plot_firing_rate_pop_stats(pop_stats, groupby=\"pop_name\") plt.show() <p>Like the raster function you can also make some changes to the appearance of the plot if you want. Note both plotting functions have the ability only one is shown</p> In\u00a0[52]: Copied! <pre>df = load_spikes_to_df(output_path, network_name=\"cortex\", config=config_path,groupby='pop_name')\npop_stats,indivdual_stats = compute_firing_rate_stats(df, groupby=\"pop_name\")\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5))\nplot_firing_rate_pop_stats(pop_stats, groupby='pop_name',color_map=color_map,ax=ax)\n\nplt.show()\n</pre> df = load_spikes_to_df(output_path, network_name=\"cortex\", config=config_path,groupby='pop_name') pop_stats,indivdual_stats = compute_firing_rate_stats(df, groupby=\"pop_name\")  fig, ax = plt.subplots(1, 1, figsize=(8, 5)) plot_firing_rate_pop_stats(pop_stats, groupby='pop_name',color_map=color_map,ax=ax)  plt.show() In\u00a0[53]: Copied! <pre>from bmtool.analysis.spikes import compare_firing_over_times\n\nspike_df = load_spikes_to_df(\"/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/final_result_2/block1/long/spikes.h5\", \"cortex\",config='/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json')\ncompare_firing_over_times(spike_df,group_by='pop_name',time_window_1=(0,1000),time_window_2=(1000,2000))\n</pre> from bmtool.analysis.spikes import compare_firing_over_times  spike_df = load_spikes_to_df(\"/home/gjgpb9/cortex_modeling/V1_Layer5/Run-Storage/final_result_2/block1/long/spikes.h5\", \"cortex\",config='/home/gjgpb9/cortex_modeling/V1_Layer5/Model-Parameters/simulation_config_baseline.json') compare_firing_over_times(spike_df,group_by='pop_name',time_window_1=(0,1000),time_window_2=(1000,2000)) <pre>Population: FSI\n    Average firing rate in window 1: 4.60 Hz\n    Average firing rate in window 2: 4.69 Hz\n    U-statistic: 366210.50\n    p-value: 0.40101877211121495\n    Significant difference (p&lt;0.05): No\nPopulation: LTS\n    Average firing rate in window 1: 2.14 Hz\n    Average firing rate in window 2: 20.67 Hz\n    U-statistic: 22456.00\n    p-value: 6.613772768519174e-176\n    Significant difference (p&lt;0.05): Yes\nPopulation: CP\n    Average firing rate in window 1: 1.57 Hz\n    Average firing rate in window 2: 2.68 Hz\n    U-statistic: 9692980.50\n    p-value: 1.629101314611588e-10\n    Significant difference (p&lt;0.05): Yes\nPopulation: CS\n    Average firing rate in window 1: 1.57 Hz\n    Average firing rate in window 2: 2.44 Hz\n    U-statistic: 9945849.00\n    p-value: 1.0708612377729946e-18\n    Significant difference (p&lt;0.05): Yes\n</pre>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#using-the-bmtool-spike-analysis-module","title":"Using the bmtool spike analysis module\u00b6","text":"<p>By Gregory Glickert</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#load_spikes_df","title":"load_spikes_df\u00b6","text":"<p>We can use the load_spikes_df to load our network output into a dataframe. It can also be useful to provide the config for the network so we can see which population the cells come from.</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#raster-plot","title":"raster plot\u00b6","text":"<p>You may want a way to look at the spikes to see when they are firing. One way to do this is by a raster plot</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#spike-rate","title":"Spike Rate\u00b6","text":"<p>There is also a population spike rate function. This can turn your dataframe into a time series, which can then be futher analyzed.</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#averaging-the-data","title":"Averaging the data\u00b6","text":"<p>You can also average the data across several windows of time.</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#firing-rate-stats","title":"Firing rate stats\u00b6","text":"<p>You can easily get some firing rate stats and plot them using the below functions</p>"},{"location":"examples/notebooks/analysis/spiking/using_spikes/#compare_firing_over_times","title":"compare_firing_over_times\u00b6","text":"<p>You may want to compare the firing rates during different times during your simulation and compute if the firing is different. This can be useful if you are giving a changing input and want to see if you cells are responding differently.</p>"},{"location":"examples/notebooks/bmplot/bmplot/","title":"BMPlot Tutorial","text":"In\u00a0[1]: Copied! <pre>import os\n\nimport numpy as np\nfrom bmtk.builder import NetworkBuilder\nfrom bmtk.builder.auxi.node_params import positions_cuboid, positions_list, xiter_random\n\nfrom bmtool.connectors import (\n    GapJunction,\n    OneToOneSequentialConnector,\n    ReciprocalConnector,\n    UnidirectionConnector,\n)\n\nif os.path.isfile(\"conn.csv\"):\n    os.remove(\"conn.csv\")\n\n# cell count\nnum_of_PN = 250\nnum_of_FSI = 50\n# generate rand postions\npostions = positions_cuboid(num_of_PN+num_of_FSI,min_dist=10)\n\n# select locations \ninds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_PN, replace=False)\npos = postions[inds, :]\nnet = NetworkBuilder('bio_net')\nnet.add_nodes(N=num_of_PN, \n              pop_name='PN',\n              positions=positions_list(pos),\n              model_type='biophysical',\n              rotation_angle_xaxis = xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),\n              rotation_angle_yaxis = xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),\n              rotation_angle_zaxis= xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),\n              morphology=None)\n# Get rid of coordinates already used\npostions = np.delete(postions, inds, 0)\n\ninds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_FSI, replace=False)\npos = postions[inds, :]\nnet.add_nodes(N=num_of_FSI, \n              pop_name='FSI',\n              positions=positions_list(pos),\n              model_type='biophysical',\n              rotation_angle_xaxis = xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),\n              rotation_angle_yaxis = xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),\n              rotation_angle_zaxis= xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),\n              morphology=None)\n\nbackground = NetworkBuilder('background')\nbackground.add_nodes(N=300,\n                   pop_name='background_nodes',\n                   potential='exc',\n                   model_type='virtual')\n</pre> import os  import numpy as np from bmtk.builder import NetworkBuilder from bmtk.builder.auxi.node_params import positions_cuboid, positions_list, xiter_random  from bmtool.connectors import (     GapJunction,     OneToOneSequentialConnector,     ReciprocalConnector,     UnidirectionConnector, )  if os.path.isfile(\"conn.csv\"):     os.remove(\"conn.csv\")  # cell count num_of_PN = 250 num_of_FSI = 50 # generate rand postions postions = positions_cuboid(num_of_PN+num_of_FSI,min_dist=10)  # select locations  inds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_PN, replace=False) pos = postions[inds, :] net = NetworkBuilder('bio_net') net.add_nodes(N=num_of_PN,                pop_name='PN',               positions=positions_list(pos),               model_type='biophysical',               rotation_angle_xaxis = xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),               rotation_angle_yaxis = xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),               rotation_angle_zaxis= xiter_random(N=num_of_PN, min_x=-np.pi, max_x=np.pi),               morphology=None) # Get rid of coordinates already used postions = np.delete(postions, inds, 0)  inds = np.random.choice(np.arange(0, np.size(postions, 0)), num_of_FSI, replace=False) pos = postions[inds, :] net.add_nodes(N=num_of_FSI,                pop_name='FSI',               positions=positions_list(pos),               model_type='biophysical',               rotation_angle_xaxis = xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),               rotation_angle_yaxis = xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),               rotation_angle_zaxis= xiter_random(N=num_of_FSI, min_x=-np.pi, max_x=np.pi),               morphology=None)  background = NetworkBuilder('background') background.add_nodes(N=300,                    pop_name='background_nodes',                    potential='exc',                    model_type='virtual')  <p>Now lets build som example connections using bmtool's connection module</p> In\u00a0[2]: Copied! <pre>connector = UnidirectionConnector(p=0.10,verbose=False,save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\n\nconnector = ReciprocalConnector(p0=0.22,p1=0.36,pr=0.17,verbose=False,save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['FSI']))\nnet.add_edges(**connector.edge_params())\n\nconnector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\n\nconnector = UnidirectionConnector(p=0.30,verbose=False,save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI']))\nnet.add_edges(**connector.edge_params())\n\nconnector = GapJunction(p=0.10,verbose=False,save_report=True)\nconnector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI']))\nnet.add_edges(is_gap_junction=True,**connector.edge_params())\n\nconnector = OneToOneSequentialConnector(verbose=False)\nconnector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\nconnector.setup_nodes(target=net.nodes(pop_name = 'FSI'))\nnet.add_edges(**connector.edge_params())\n\nnet.save('network/')\nbackground.save('network/')\n\n\nfrom bmtk.utils.sim_setup import build_env_bionet\n\nbuild_env_bionet(base_dir='./',      \n                network_dir='network/',\n                tstop=3000.0, dt=0.1,\n                spikes_inputs=[('background', \n                                'background.h5')], \n                include_examples=False,    \n                compile_mechanisms=False,   \n                config_file='config.json',\n                overwrite_config=True\n                )\n</pre> connector = UnidirectionConnector(p=0.10,verbose=False,save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params())  connector = ReciprocalConnector(p0=0.22,p1=0.36,pr=0.17,verbose=False,save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['FSI'])) net.add_edges(**connector.edge_params())  connector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params())  connector = UnidirectionConnector(p=0.30,verbose=False,save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI'])) net.add_edges(**connector.edge_params())  connector = GapJunction(p=0.10,verbose=False,save_report=True) connector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI'])) net.add_edges(is_gap_junction=True,**connector.edge_params())  connector = OneToOneSequentialConnector(verbose=False) connector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params()) connector.setup_nodes(target=net.nodes(pop_name = 'FSI')) net.add_edges(**connector.edge_params())  net.save('network/') background.save('network/')   from bmtk.utils.sim_setup import build_env_bionet  build_env_bionet(base_dir='./',                       network_dir='network/',                 tstop=3000.0, dt=0.1,                 spikes_inputs=[('background',                                  'background.h5')],                  include_examples=False,                     compile_mechanisms=False,                    config_file='config.json',                 overwrite_config=True                 )   <pre>WARNING:root:No edges have been made for this network, skipping saving of edges file.\n</pre> In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\n\nfrom bmtool.bmplot import connections as bp\n\n%matplotlib inline\nbp.total_connection_matrix(config='config.json',\n                           sources='bio_net', \n                           targets='bio_net', \n                           tids='pop_name', \n                           sids='pop_name', \n                           no_prepend_pop=True,# if the network source or target should be added to the name or not\n                           include_gap=True # if there are gap junctions in the network should they be included or not in the connection number\n                           )\nplt.show()\n</pre> import matplotlib.pyplot as plt  from bmtool.bmplot import connections as bp  %matplotlib inline bp.total_connection_matrix(config='config.json',                            sources='bio_net',                             targets='bio_net',                             tids='pop_name',                             sids='pop_name',                             no_prepend_pop=True,# if the network source or target should be added to the name or not                            include_gap=True # if there are gap junctions in the network should they be included or not in the connection number                            ) plt.show() <pre>numprocs=1\n</pre> In\u00a0[4]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.convergence_connection_matrix(**kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.convergence_connection_matrix(**kwargs) plt.show() <p>There are also different methods that can be used. The current methods are 'mean','min','max','stdev' or 'mean+std'. By default the function will use 'mean+std' like above, but you can specify a method like below. In the example below min means display the convergence value that is the smallest in the whole network for those sid and tid pairs.</p> In\u00a0[5]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.convergence_connection_matrix(method='min',\n                                 **kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.convergence_connection_matrix(method='min',                                  **kwargs) plt.show() In\u00a0[6]: Copied! <pre>import pandas as pd\n\nbio_net_convergence = bp.convergence_connection_matrix(return_dict=True,**kwargs)\nplt.show()\n\ndef gen_edge_df(dict):\n    # Get the unique labels (source and target)\n    source_labels = list(dict.keys())\n    target_labels = list(next(iter(dict.values())).keys())\n\n    # Initialize a matrix to store mean and std values\n    matrix_mean = np.full((len(source_labels), len(target_labels)), np.nan)\n    matrix_std = np.full((len(source_labels), len(target_labels)), np.nan)\n\n    # Create a mapping from label to index for source and target\n    source_index = {source_labels[i]: i for i in range(len(source_labels))}\n    target_index = {target_labels[i]: i for i in range(len(target_labels))}\n\n    # Fill the matrices with mean and std values\n    for source in source_labels:\n        for target in target_labels:\n            edge_info = dict.get(source, {}).get(target, 'nan\\nnan')\n            \n            # Check if the edge_info is 'nan\\nnan' or valid\n            if edge_info != 'nan\\nnan':\n                mean, std = map(float, edge_info.split('\\n'))\n                matrix_mean[source_index[source], target_index[target]] = mean\n                matrix_std[source_index[source], target_index[target]] = std\n\n    # Convert the matrices to pandas DataFrame for better visualization\n    df_mean = pd.DataFrame(matrix_mean, index=source_labels, columns=target_labels)\n    df_std = pd.DataFrame(matrix_std, index=source_labels, columns=target_labels)\n\n    return df_mean, df_std\n\nmean_df, std_df = gen_edge_df(bio_net_convergence)\ndisplay(mean_df)\n</pre> import pandas as pd  bio_net_convergence = bp.convergence_connection_matrix(return_dict=True,**kwargs) plt.show()  def gen_edge_df(dict):     # Get the unique labels (source and target)     source_labels = list(dict.keys())     target_labels = list(next(iter(dict.values())).keys())      # Initialize a matrix to store mean and std values     matrix_mean = np.full((len(source_labels), len(target_labels)), np.nan)     matrix_std = np.full((len(source_labels), len(target_labels)), np.nan)      # Create a mapping from label to index for source and target     source_index = {source_labels[i]: i for i in range(len(source_labels))}     target_index = {target_labels[i]: i for i in range(len(target_labels))}      # Fill the matrices with mean and std values     for source in source_labels:         for target in target_labels:             edge_info = dict.get(source, {}).get(target, 'nan\\nnan')                          # Check if the edge_info is 'nan\\nnan' or valid             if edge_info != 'nan\\nnan':                 mean, std = map(float, edge_info.split('\\n'))                 matrix_mean[source_index[source], target_index[target]] = mean                 matrix_std[source_index[source], target_index[target]] = std      # Convert the matrices to pandas DataFrame for better visualization     df_mean = pd.DataFrame(matrix_mean, index=source_labels, columns=target_labels)     df_std = pd.DataFrame(matrix_std, index=source_labels, columns=target_labels)      return df_mean, df_std  mean_df, std_df = gen_edge_df(bio_net_convergence) display(mean_df) PN FSI PN 25.2 54.2 FSI 17.6 16.8 In\u00a0[7]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.divergence_connection_matrix(**kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.divergence_connection_matrix(**kwargs) plt.show() In\u00a0[8]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.gap_junction_matrix(method='convergence',\n                       **kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.gap_junction_matrix(method='convergence',                        **kwargs) plt.show() In\u00a0[9]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.connection_histogram(source_cell='PN', # must be one of the Node attribute\n                        target_cell='FSI',\n                        **kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.connection_histogram(source_cell='PN', # must be one of the Node attribute                         target_cell='FSI',                         **kwargs) plt.show() In\u00a0[10]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.percent_connection_matrix(method='total', # should match the p in the build network above roughly\n                             include_gap=False,\n                             **kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.percent_connection_matrix(method='total', # should match the p in the build network above roughly                              include_gap=False,                              **kwargs) plt.show() In\u00a0[11]: Copied! <pre>bp.connector_percent_matrix(csv_path=\"conn.csv\") # path to csv generated by connectors\nplt.show()\n                            \n</pre> bp.connector_percent_matrix(csv_path=\"conn.csv\") # path to csv generated by connectors plt.show()                              <p>We can see that plot does not look the same as the other percent connectivity or the other plots we have made with the order of the ticks. This can all be adjusted</p> In\u00a0[12]: Copied! <pre>bp.connector_percent_matrix(csv_path=\"conn.csv\", #\n                            exclude_strings=['Gap'], # ignores gap junction can be used to ignore any string \n                            pop_order=['PN','FSI']) # order of x and y ticks\nplt.show()\n</pre> bp.connector_percent_matrix(csv_path=\"conn.csv\", #                             exclude_strings=['Gap'], # ignores gap junction can be used to ignore any string                              pop_order=['PN','FSI']) # order of x and y ticks plt.show() In\u00a0[13]: Copied! <pre>bp.connection_distance(config='config.json',\n                       sources='bio_net',\n                       targets='bio_net',\n                       source_cell_id=1, # node_id \n                       target_id_type='FSI', # pop_name of cells can be left empty for all cells\n                       ignore_z=False) # some connection rules can ignore the z axis\nplt.show()\n</pre> bp.connection_distance(config='config.json',                        sources='bio_net',                        targets='bio_net',                        source_cell_id=1, # node_id                         target_id_type='FSI', # pop_name of cells can be left empty for all cells                        ignore_z=False) # some connection rules can ignore the z axis plt.show() In\u00a0[14]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'background', # note the difference here\n          'targets': 'bio_net',\n          'sids': 'pop_name',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.convergence_connection_matrix(**kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'background', # note the difference here           'targets': 'bio_net',           'sids': 'pop_name',           'tids': 'pop_name',           'no_prepend_pop':True} bp.convergence_connection_matrix(**kwargs) plt.show() In\u00a0[15]: Copied! <pre>kwargs = {'config':'config.json',\n          'sources': 'bio_net', \n          'targets': 'bio_net',\n          'sids': 'model_type',\n          'tids': 'pop_name',\n          'no_prepend_pop':True}\nbp.convergence_connection_matrix(**kwargs)\nplt.show()\n</pre> kwargs = {'config':'config.json',           'sources': 'bio_net',            'targets': 'bio_net',           'sids': 'model_type',           'tids': 'pop_name',           'no_prepend_pop':True} bp.convergence_connection_matrix(**kwargs) plt.show() In\u00a0[16]: Copied! <pre>kwargs['sids'] = 'pop_name'\nkwargs['sources'] = 'all'\nkwargs['targets'] = 'all'\nkwargs['no_prepend_pop'] = False\nbp.convergence_connection_matrix(**kwargs)\nplt.show()\n</pre> kwargs['sids'] = 'pop_name' kwargs['sources'] = 'all' kwargs['targets'] = 'all' kwargs['no_prepend_pop'] = False bp.convergence_connection_matrix(**kwargs) plt.show() In\u00a0[17]: Copied! <pre>bp.plot_3d_positions(config='config.json', \n                     sources='bio_net', \n                     sid='pop_name', \n                     subset=5 # how many nodes in each pop you want to plot so 5 means take every 5th row and plot it good for extremely large networks\n                     )\nplt.show()\n</pre> bp.plot_3d_positions(config='config.json',                       sources='bio_net',                       sid='pop_name',                       subset=5 # how many nodes in each pop you want to plot so 5 means take every 5th row and plot it good for extremely large networks                      ) plt.show() In\u00a0[18]: Copied! <pre>bp.plot_3d_cell_rotation(config='config.json',\n                         sources='bio_net',\n                         sids='pop_name',\n                         quiver_length=20,\n                         arrow_length_ratio=0.25,\n                         subset=5)\nplt.show()\n</pre> bp.plot_3d_cell_rotation(config='config.json',                          sources='bio_net',                          sids='pop_name',                          quiver_length=20,                          arrow_length_ratio=0.25,                          subset=5) plt.show()"},{"location":"examples/notebooks/bmplot/bmplot/#tutorial-of-how-to-use-bmplot-module","title":"Tutorial of how to use bmplot module\u00b6","text":"<p>By Gregory Glickert</p>"},{"location":"examples/notebooks/bmplot/bmplot/#generate-example-bmtk-network","title":"Generate example bmtk network\u00b6","text":"<p>First we will generate an example BMTK network to then use the functions on</p>"},{"location":"examples/notebooks/bmplot/bmplot/#how-bmplot-works","title":"How bmplot Works\u00b6","text":"<p>In general, bmplot functions take the following inputs:</p> <ul> <li><code>config</code>: Path to a BMTK config file</li> <li><code>sources</code>: Name of the source network</li> <li><code>targets</code>: Name of the target network</li> <li><code>sids</code>: Source IDs</li> <li><code>tids</code>: Target IDs</li> </ul> <p>These inputs are used to generate plots. Additional optional inputs can be provided to refine and clean up the output.</p> <p><code>config</code></p> <ul> <li>A path to the BMTK configuration file.</li> </ul> <p><code>sources</code> and <code>targets</code></p> <ul> <li><p>The name of the source and target network you'd like to analyze.</p> </li> <li><p>In this example generated above a network called <code>bio_net</code> and a network called <code>background</code></p> </li> </ul> <p><code>sids</code>and <code>tids</code></p> <ul> <li><p>The node attribute you want the nodes separted by when generating the plot</p> </li> <li><p>In this example above valid options are <code>pop_name</code>, <code>positions</code>, <code>model_type</code>, <code>rotation_angle_xaxis</code>, <code>rotation_angle_yaxis</code>, <code>rotation_angle_zaxis</code>, and <code>morphology</code>.</p> </li> </ul>"},{"location":"examples/notebooks/bmplot/bmplot/#total_connection_matrix","title":"Total_connection_matrix\u00b6","text":"<p>generates a matrix with the number of total connections between sid and tid pairs.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#convergence_connection_matrix","title":"convergence_connection_matrix\u00b6","text":"<p>generate a matrix with the average and standard deviation of the convergence. Convergence is how many of sid cells connection to a single tid cell.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#connection-dictionary","title":"connection dictionary\u00b6","text":"<p>You can also have the some of the bmplot functions return a dictionary with the connection data. This can be helpful making tables and graphs out of the data. Below is an example function that generates a dataframe from the dictionary and then plots the data. This feature could be helpful for generating custom graphs.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#divergence_connection_matrix","title":"divergence_connection_matrix\u00b6","text":"<p>works the same as the convergence only calculates a different metric. Divergence is a count of how many tids the a given sid cell will go and connect into.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#gap_junction_matrix","title":"gap_junction_matrix\u00b6","text":"<p>generate a matrix only showing the connectivity of gap junctions formed in the network. Method can be either convergence or percent</p>"},{"location":"examples/notebooks/bmplot/bmplot/#connection_histogram","title":"connection_histogram\u00b6","text":"<p>plots the number of connections individual cells in a population receive from another population. The source_cell and target_cell must both be valid strings in sid and tid. In the example below pop_name is used for the sids and tids and \"PN\" and \"FSI\" are valid pop_names in our network</p>"},{"location":"examples/notebooks/bmplot/bmplot/#percent_connection_matrix","title":"percent_connection_matrix\u00b6","text":"<p>generates a matrix of percent connectivity in the network. This percent connectivity does not factor in distance and instead looks at the whole percent connectivity of the population. There are three methods either 'uni', 'bi' or 'total'. 'uni' means unidirectional percent connectivity,'bi' means the bidirectional connectivity and 'total' is both added together.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#connector_percent_matrix","title":"connector_percent_matrix\u00b6","text":"<p>uses the report generated by the bmtool.connectors module to plot the percent connectivity. However this report will show the percent connectivity from possible connections within the connection rule. This makes it the only way to look at percent connectivity of a network while factoring in distance</p>"},{"location":"examples/notebooks/bmplot/bmplot/#connection_distance","title":"connection_distance\u00b6","text":"<p>shows the location of the source and target cells along with the distance between the cells.</p>"},{"location":"examples/notebooks/bmplot/bmplot/#between-network-analysis","title":"Between network analysis\u00b6","text":"<p>Currently the demo has only shown the biophysical connectivity, but bmplot can display any combo of networks connectivity by switching the sources,targets,sids, and tids</p>"},{"location":"examples/notebooks/bmplot/bmplot/#plot_3d_positions","title":"plot_3d_positions\u00b6","text":"<p>takes similar arguments to the previously shown bmplot functions and will generate a 3d plot of where the nodes are located in the network</p>"},{"location":"examples/notebooks/bmplot/bmplot/#plot_3d_cell_rotation","title":"plot_3d_cell_rotation\u00b6","text":"<p>will plot the rotation of the neuron in 3d space.</p>"},{"location":"examples/notebooks/bmplot/run_bionet/","title":"Run bionet","text":"In\u00a0[\u00a0]: Copied! <pre>import sys\n</pre> import sys In\u00a0[\u00a0]: Copied! <pre>from bmtk.simulator import bionet\n</pre> from bmtk.simulator import bionet In\u00a0[\u00a0]: Copied! <pre>def run(config_file):\n    conf = bionet.Config.from_json(config_file, validate=True)\n    conf.build_env()\n\n    graph = bionet.BioNetwork.from_config(conf)\n    sim = bionet.BioSimulator.from_config(conf, network=graph)\n    sim.run()\n    bionet.nrn.quit_execution()\n</pre> def run(config_file):     conf = bionet.Config.from_json(config_file, validate=True)     conf.build_env()      graph = bionet.BioNetwork.from_config(conf)     sim = bionet.BioSimulator.from_config(conf, network=graph)     sim.run()     bionet.nrn.quit_execution() In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n    if __file__ != sys.argv[-1]:\n        run(sys.argv[-1])\n    else:\n        run('${CONFIG}')\n</pre> if __name__ == '__main__':     if __file__ != sys.argv[-1]:         run(sys.argv[-1])     else:         run('${CONFIG}')"},{"location":"examples/notebooks/connectors/GaussianDropoff_tutorial/","title":"Gaussian Dropoff Tutorial","text":"In\u00a0[1]: Copied! <pre># Imports\n%matplotlib inline\n\nimport sys\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom bmtool.connectors import GaussianDropoff\n</pre> # Imports %matplotlib inline  import sys import numpy as np import matplotlib import matplotlib.pyplot as plt  from bmtool.connectors import GaussianDropoff In\u00a0[2]: Copied! <pre># Method 1: Specify pmax directly\ndropoff_pmax = GaussianDropoff(\n    stdev=80.0,\n    min_dist=0,\n    max_dist=300,\n    pmax=0.25,\n    dist_type='spherical'\n)\n\n# Method 2: Specify ptotal - class calculates pmax\ndropoff_ptotal = GaussianDropoff(\n    stdev=80.0,\n    min_dist=0,\n    max_dist=300,\n    ptotal=0.25,\n    ptotal_dist_range=(0, 150),\n    dist_type='spherical'\n)\n\ndistances = np.linspace(0, 400, 1000)\nprob_pmax = dropoff_pmax.probability(distances)\nprob_ptotal = dropoff_ptotal.probability(distances)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nax1.plot(distances, prob_pmax, 'b-', linewidth=2)\nax1.axhline(y=0.25, color='r', linestyle='--', alpha=0.5, label=f'pmax = {dropoff_pmax.pmax:.3f}')\nax1.set_xlabel('Distance (\u03bcm)', fontsize=11)\nax1.set_ylabel('Connection Probability', fontsize=11)\nax1.set_title('Method 1: Specified pmax=0.25', fontsize=12, fontweight='bold')\nax1.grid(True, alpha=0.3)\nax1.legend()\nax1.set_ylim([0, 0.4])\n\nax2.plot(distances, prob_ptotal, 'g-', linewidth=2)\nax2.axhline(y=dropoff_ptotal.pmax, color='r', linestyle='--', alpha=0.5, \n            label=f'pmax = {dropoff_ptotal.pmax:.3f} (calculated)')\nax2.axvspan(0, 150, alpha=0.1, color='orange', label='ptotal_dist_range')\nax2.set_xlabel('Distance (\u03bcm)', fontsize=11)\nax2.set_ylabel('Connection Probability', fontsize=11)\nax2.set_title(f'Method 2: Specified ptotal=0.25 within 0-150 \u03bcm', fontsize=12, fontweight='bold')\nax2.grid(True, alpha=0.3)\nax2.legend()\nax2.set_ylim([0, 0.4])\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"pmax method: peak = {dropoff_pmax.pmax:.4f}\")\nprint(f\"ptotal method: peak = {dropoff_ptotal.pmax:.4f} (calculated)\")\n</pre> # Method 1: Specify pmax directly dropoff_pmax = GaussianDropoff(     stdev=80.0,     min_dist=0,     max_dist=300,     pmax=0.25,     dist_type='spherical' )  # Method 2: Specify ptotal - class calculates pmax dropoff_ptotal = GaussianDropoff(     stdev=80.0,     min_dist=0,     max_dist=300,     ptotal=0.25,     ptotal_dist_range=(0, 150),     dist_type='spherical' )  distances = np.linspace(0, 400, 1000) prob_pmax = dropoff_pmax.probability(distances) prob_ptotal = dropoff_ptotal.probability(distances)  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))  ax1.plot(distances, prob_pmax, 'b-', linewidth=2) ax1.axhline(y=0.25, color='r', linestyle='--', alpha=0.5, label=f'pmax = {dropoff_pmax.pmax:.3f}') ax1.set_xlabel('Distance (\u03bcm)', fontsize=11) ax1.set_ylabel('Connection Probability', fontsize=11) ax1.set_title('Method 1: Specified pmax=0.25', fontsize=12, fontweight='bold') ax1.grid(True, alpha=0.3) ax1.legend() ax1.set_ylim([0, 0.4])  ax2.plot(distances, prob_ptotal, 'g-', linewidth=2) ax2.axhline(y=dropoff_ptotal.pmax, color='r', linestyle='--', alpha=0.5,              label=f'pmax = {dropoff_ptotal.pmax:.3f} (calculated)') ax2.axvspan(0, 150, alpha=0.1, color='orange', label='ptotal_dist_range') ax2.set_xlabel('Distance (\u03bcm)', fontsize=11) ax2.set_ylabel('Connection Probability', fontsize=11) ax2.set_title(f'Method 2: Specified ptotal=0.25 within 0-150 \u03bcm', fontsize=12, fontweight='bold') ax2.grid(True, alpha=0.3) ax2.legend() ax2.set_ylim([0, 0.4])  plt.tight_layout() plt.show()  print(f\"pmax method: peak = {dropoff_pmax.pmax:.4f}\") print(f\"ptotal method: peak = {dropoff_ptotal.pmax:.4f} (calculated)\") <pre>pmax method: peak = 0.2500\nptotal method: peak = 0.6434 (calculated)\n</pre> In\u00a0[3]: Copied! <pre>ptotal_value = 0.25\nstdev_value = 80.0\n\ndropoff_small = GaussianDropoff(\n    stdev=stdev_value, min_dist=0, max_dist=300,\n    ptotal=ptotal_value, ptotal_dist_range=(0, 50),\n    dist_type='spherical'\n)\n\ndropoff_medium = GaussianDropoff(\n    stdev=stdev_value, min_dist=0, max_dist=300,\n    ptotal=ptotal_value, ptotal_dist_range=(0, 100),\n    dist_type='spherical'\n)\n\ndropoff_large = GaussianDropoff(\n    stdev=stdev_value, min_dist=0, max_dist=300,\n    ptotal=ptotal_value, ptotal_dist_range=(0, 200),\n    dist_type='spherical'\n)\n\ndistances = np.linspace(0, 400, 1000)\n\nplt.figure(figsize=(12, 6))\nplt.plot(distances, dropoff_small.probability(distances), 'r-', linewidth=2.5,\n         label=f'Range: 0-50 \u03bcm \u2192 pmax={dropoff_small.pmax:.3f}')\nplt.plot(distances, dropoff_medium.probability(distances), 'g-', linewidth=2.5,\n         label=f'Range: 0-100 \u03bcm \u2192 pmax={dropoff_medium.pmax:.3f}')\nplt.plot(distances, dropoff_large.probability(distances), 'b-', linewidth=2.5,\n         label=f'Range: 0-200 \u03bcm \u2192 pmax={dropoff_large.pmax:.3f}')\n\nplt.axvspan(0, 50, alpha=0.1, color='red')\nplt.axvspan(0, 100, alpha=0.05, color='green')\nplt.axvspan(0, 200, alpha=0.05, color='blue')\n\nplt.xlabel('Distance (\u03bcm)', fontsize=12)\nplt.ylabel('Connection Probability', fontsize=12)\nplt.title(f'Same ptotal={ptotal_value}, Different Integration Ranges (\u03c3={stdev_value} \u03bcm)', \n          fontsize=13, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.xlim([0, 350])\nplt.tight_layout()\nplt.show()\n\nprint(\"Same ptotal (25%), different ranges:\")\nprint(f\"0-50 \u03bcm: pmax = {dropoff_small.pmax:.4f}\")\nprint(f\"0-100 \u03bcm: pmax = {dropoff_medium.pmax:.4f}\")\nprint(f\"0-200 \u03bcm: pmax = {dropoff_large.pmax:.4f}\")\n</pre> ptotal_value = 0.25 stdev_value = 80.0  dropoff_small = GaussianDropoff(     stdev=stdev_value, min_dist=0, max_dist=300,     ptotal=ptotal_value, ptotal_dist_range=(0, 50),     dist_type='spherical' )  dropoff_medium = GaussianDropoff(     stdev=stdev_value, min_dist=0, max_dist=300,     ptotal=ptotal_value, ptotal_dist_range=(0, 100),     dist_type='spherical' )  dropoff_large = GaussianDropoff(     stdev=stdev_value, min_dist=0, max_dist=300,     ptotal=ptotal_value, ptotal_dist_range=(0, 200),     dist_type='spherical' )  distances = np.linspace(0, 400, 1000)  plt.figure(figsize=(12, 6)) plt.plot(distances, dropoff_small.probability(distances), 'r-', linewidth=2.5,          label=f'Range: 0-50 \u03bcm \u2192 pmax={dropoff_small.pmax:.3f}') plt.plot(distances, dropoff_medium.probability(distances), 'g-', linewidth=2.5,          label=f'Range: 0-100 \u03bcm \u2192 pmax={dropoff_medium.pmax:.3f}') plt.plot(distances, dropoff_large.probability(distances), 'b-', linewidth=2.5,          label=f'Range: 0-200 \u03bcm \u2192 pmax={dropoff_large.pmax:.3f}')  plt.axvspan(0, 50, alpha=0.1, color='red') plt.axvspan(0, 100, alpha=0.05, color='green') plt.axvspan(0, 200, alpha=0.05, color='blue')  plt.xlabel('Distance (\u03bcm)', fontsize=12) plt.ylabel('Connection Probability', fontsize=12) plt.title(f'Same ptotal={ptotal_value}, Different Integration Ranges (\u03c3={stdev_value} \u03bcm)',            fontsize=13, fontweight='bold') plt.legend(fontsize=11) plt.grid(True, alpha=0.3) plt.xlim([0, 350]) plt.tight_layout() plt.show()  print(\"Same ptotal (25%), different ranges:\") print(f\"0-50 \u03bcm: pmax = {dropoff_small.pmax:.4f}\") print(f\"0-100 \u03bcm: pmax = {dropoff_medium.pmax:.4f}\") print(f\"0-200 \u03bcm: pmax = {dropoff_large.pmax:.4f}\") <pre>\nWarning: Maximum probability=1.154 is greater than 1. Probability crosses 1 at distance 42.9.\n ptotal may not be reached.\n</pre> <pre>Same ptotal (25%), different ranges:\n0-50 \u03bcm: pmax = 0.2807\n0-100 \u03bcm: pmax = 0.3911\n0-200 \u03bcm: pmax = 1.1544\n</pre> In\u00a0[4]: Copied! <pre>my_dropoff = GaussianDropoff(\n    stdev=80.0,\n    min_dist=0,\n    max_dist=300,\n    pmax=0.25,\n    dist_type='spherical'\n)\n\nranges_to_check = [(0, 50), (0, 100), (0, 150), (0, 200), (0, 300)]\n\ndistances = np.linspace(0, 350, 1000)\nprobs = my_dropoff.probability(distances)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nax1.plot(distances, probs, 'b-', linewidth=2.5)\ncolors = ['red', 'orange', 'yellow', 'green', 'cyan']\nfor i, (min_d, max_d) in enumerate(ranges_to_check):\n    ax1.axvspan(min_d, max_d, alpha=0.1, color=colors[i])\nax1.set_xlabel('Distance (\u03bcm)', fontsize=11)\nax1.set_ylabel('Connection Probability', fontsize=11)\nax1.set_title(f'Gaussian Connection Probability (pmax={my_dropoff.pmax})', fontsize=12, fontweight='bold')\nax1.grid(True, alpha=0.3)\nax1.set_xlim([0, 350])\n\nptotal_values = []\nfor min_d, max_d in ranges_to_check:\n    my_dropoff.ptotal_dist_range = (min_d, max_d)\n    computed_ptotal = my_dropoff.compute_ptotal_integral(num_points=2000)\n    ptotal_values.append(computed_ptotal)\n\nrange_widths = [max_d for _, max_d in ranges_to_check]\nax2.plot(range_widths, ptotal_values, 'o-', linewidth=2, markersize=8, color='darkblue')\nax2.set_xlabel('Integration Range Upper Bound (\u03bcm)', fontsize=11)\nax2.set_ylabel('Computed ptotal', fontsize=11)\nax2.set_title('How Integration Range Affects ptotal', fontsize=12, fontweight='bold')\nax2.grid(True, alpha=0.3)\nax2.set_ylim([0, max(ptotal_values) * 1.1])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"ptotal for different ranges (pmax=0.25, \u03c3=80 \u03bcm):\")\nfor min_d, max_d in ranges_to_check:\n    my_dropoff.ptotal_dist_range = (min_d, max_d)\n    computed_ptotal = my_dropoff.compute_ptotal_integral(num_points=2000)\n    print(f\"{min_d}-{max_d} \u03bcm: {computed_ptotal:.3f}\")\n</pre> my_dropoff = GaussianDropoff(     stdev=80.0,     min_dist=0,     max_dist=300,     pmax=0.25,     dist_type='spherical' )  ranges_to_check = [(0, 50), (0, 100), (0, 150), (0, 200), (0, 300)]  distances = np.linspace(0, 350, 1000) probs = my_dropoff.probability(distances)  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))  ax1.plot(distances, probs, 'b-', linewidth=2.5) colors = ['red', 'orange', 'yellow', 'green', 'cyan'] for i, (min_d, max_d) in enumerate(ranges_to_check):     ax1.axvspan(min_d, max_d, alpha=0.1, color=colors[i]) ax1.set_xlabel('Distance (\u03bcm)', fontsize=11) ax1.set_ylabel('Connection Probability', fontsize=11) ax1.set_title(f'Gaussian Connection Probability (pmax={my_dropoff.pmax})', fontsize=12, fontweight='bold') ax1.grid(True, alpha=0.3) ax1.set_xlim([0, 350])  ptotal_values = [] for min_d, max_d in ranges_to_check:     my_dropoff.ptotal_dist_range = (min_d, max_d)     computed_ptotal = my_dropoff.compute_ptotal_integral(num_points=2000)     ptotal_values.append(computed_ptotal)  range_widths = [max_d for _, max_d in ranges_to_check] ax2.plot(range_widths, ptotal_values, 'o-', linewidth=2, markersize=8, color='darkblue') ax2.set_xlabel('Integration Range Upper Bound (\u03bcm)', fontsize=11) ax2.set_ylabel('Computed ptotal', fontsize=11) ax2.set_title('How Integration Range Affects ptotal', fontsize=12, fontweight='bold') ax2.grid(True, alpha=0.3) ax2.set_ylim([0, max(ptotal_values) * 1.1])  plt.tight_layout() plt.show()  print(\"ptotal for different ranges (pmax=0.25, \u03c3=80 \u03bcm):\") for min_d, max_d in ranges_to_check:     my_dropoff.ptotal_dist_range = (min_d, max_d)     computed_ptotal = my_dropoff.compute_ptotal_integral(num_points=2000)     print(f\"{min_d}-{max_d} \u03bcm: {computed_ptotal:.3f}\") <pre>ptotal for different ranges (pmax=0.25, \u03c3=80 \u03bcm):\n0-50 \u03bcm: 0.223\n0-100 \u03bcm: 0.160\n0-150 \u03bcm: 0.097\n0-200 \u03bcm: 0.054\n0-300 \u03bcm: 0.018\n</pre> In\u00a0[5]: Copied! <pre>ptotal_val = 0.25\nsigma_val = 80.0\ndist_range = (0, 150)\n\ndropoff_spherical = GaussianDropoff(\n    stdev=sigma_val,\n    min_dist=0,\n    max_dist=300,\n    ptotal=ptotal_val,\n    ptotal_dist_range=dist_range,\n    dist_type='spherical'\n)\n\ndropoff_cylindrical = GaussianDropoff(\n    stdev=sigma_val,\n    min_dist=0,\n    max_dist=300,\n    ptotal=ptotal_val,\n    ptotal_dist_range=dist_range,\n    dist_type='cylindrical'\n)\n\ndistances = np.linspace(0, 350, 1000)\nprob_spherical = dropoff_spherical.probability(distances)\nprob_cylindrical = dropoff_cylindrical.probability(distances)\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\naxes[0].plot(distances, prob_spherical, 'b-', linewidth=2.5)\naxes[0].axhline(y=dropoff_spherical.pmax, color='r', linestyle='--', alpha=0.5,\n                label=f'pmax = {dropoff_spherical.pmax:.4f}')\naxes[0].axvspan(dist_range[0], dist_range[1], alpha=0.1, color='blue')\naxes[0].set_xlabel('Distance (\u03bcm)', fontsize=11)\naxes[0].set_ylabel('Connection Probability', fontsize=11)\naxes[0].set_title(\"Spherical (3D)\", fontsize=12, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\naxes[0].set_xlim([0, 350])\n\naxes[1].plot(distances, prob_cylindrical, 'g-', linewidth=2.5)\naxes[1].axhline(y=dropoff_cylindrical.pmax, color='r', linestyle='--', alpha=0.5,\n                label=f'pmax = {dropoff_cylindrical.pmax:.4f}')\naxes[1].axvspan(dist_range[0], dist_range[1], alpha=0.1, color='green')\naxes[1].set_xlabel('Distance (\u03bcm)', fontsize=11)\naxes[1].set_ylabel('Connection Probability', fontsize=11)\naxes[1].set_title(\"Cylindrical (2D)\", fontsize=12, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].set_xlim([0, 350])\n\naxes[2].plot(distances, prob_spherical, 'b-', linewidth=2.5, label=f'Spherical (pmax={dropoff_spherical.pmax:.4f})')\naxes[2].plot(distances, prob_cylindrical, 'g-', linewidth=2.5, label=f'Cylindrical (pmax={dropoff_cylindrical.pmax:.4f})')\naxes[2].axvspan(dist_range[0], dist_range[1], alpha=0.05, color='gray')\naxes[2].set_xlabel('Distance (\u03bcm)', fontsize=11)\naxes[2].set_ylabel('Connection Probability', fontsize=11)\naxes[2].set_title('Comparison', fontsize=12, fontweight='bold')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\naxes[2].set_xlim([0, 350])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Spherical: pmax =\", f\"{dropoff_spherical.pmax:.4f}\")\nprint(\"Cylindrical: pmax =\", f\"{dropoff_cylindrical.pmax:.4f}\")\nprint(\"Different because of spatial weighting.\")\n</pre> ptotal_val = 0.25 sigma_val = 80.0 dist_range = (0, 150)  dropoff_spherical = GaussianDropoff(     stdev=sigma_val,     min_dist=0,     max_dist=300,     ptotal=ptotal_val,     ptotal_dist_range=dist_range,     dist_type='spherical' )  dropoff_cylindrical = GaussianDropoff(     stdev=sigma_val,     min_dist=0,     max_dist=300,     ptotal=ptotal_val,     ptotal_dist_range=dist_range,     dist_type='cylindrical' )  distances = np.linspace(0, 350, 1000) prob_spherical = dropoff_spherical.probability(distances) prob_cylindrical = dropoff_cylindrical.probability(distances)  fig, axes = plt.subplots(1, 3, figsize=(16, 5))  axes[0].plot(distances, prob_spherical, 'b-', linewidth=2.5) axes[0].axhline(y=dropoff_spherical.pmax, color='r', linestyle='--', alpha=0.5,                 label=f'pmax = {dropoff_spherical.pmax:.4f}') axes[0].axvspan(dist_range[0], dist_range[1], alpha=0.1, color='blue') axes[0].set_xlabel('Distance (\u03bcm)', fontsize=11) axes[0].set_ylabel('Connection Probability', fontsize=11) axes[0].set_title(\"Spherical (3D)\", fontsize=12, fontweight='bold') axes[0].legend() axes[0].grid(True, alpha=0.3) axes[0].set_xlim([0, 350])  axes[1].plot(distances, prob_cylindrical, 'g-', linewidth=2.5) axes[1].axhline(y=dropoff_cylindrical.pmax, color='r', linestyle='--', alpha=0.5,                 label=f'pmax = {dropoff_cylindrical.pmax:.4f}') axes[1].axvspan(dist_range[0], dist_range[1], alpha=0.1, color='green') axes[1].set_xlabel('Distance (\u03bcm)', fontsize=11) axes[1].set_ylabel('Connection Probability', fontsize=11) axes[1].set_title(\"Cylindrical (2D)\", fontsize=12, fontweight='bold') axes[1].legend() axes[1].grid(True, alpha=0.3) axes[1].set_xlim([0, 350])  axes[2].plot(distances, prob_spherical, 'b-', linewidth=2.5, label=f'Spherical (pmax={dropoff_spherical.pmax:.4f})') axes[2].plot(distances, prob_cylindrical, 'g-', linewidth=2.5, label=f'Cylindrical (pmax={dropoff_cylindrical.pmax:.4f})') axes[2].axvspan(dist_range[0], dist_range[1], alpha=0.05, color='gray') axes[2].set_xlabel('Distance (\u03bcm)', fontsize=11) axes[2].set_ylabel('Connection Probability', fontsize=11) axes[2].set_title('Comparison', fontsize=12, fontweight='bold') axes[2].legend() axes[2].grid(True, alpha=0.3) axes[2].set_xlim([0, 350])  plt.tight_layout() plt.show()  print(\"Spherical: pmax =\", f\"{dropoff_spherical.pmax:.4f}\") print(\"Cylindrical: pmax =\", f\"{dropoff_cylindrical.pmax:.4f}\") print(\"Different because of spatial weighting.\") <pre>Spherical: pmax = 0.6434\nCylindrical: pmax = 0.5310\nDifferent because of spatial weighting.\n</pre> In\u00a0[6]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(14, 5))\ndistances = np.linspace(0, 400, 1000)\n\n# ============== Panel 1: Effect of stdev (\u03c3) ==============\nax = axes[0]\nstdev_values = [50, 100, 150]\nfor sigma in stdev_values:\n    drop = GaussianDropoff(stdev=sigma, min_dist=0, max_dist=300, pmax=0.25, dist_type='spherical')\n    ax.plot(distances, drop.probability(distances), linewidth=2.5, label=f'\u03c3 = {sigma} \u03bcm')\n\nax.set_xlabel('Distance (\u03bcm)', fontsize=11)\nax.set_ylabel('Connection Probability', fontsize=11)\nax.set_title('Effect of Standard Deviation (\u03c3)', fontsize=12, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nax.set_xlim([0, 400])\n\n# ============== Panel 2: Effect of mean (\u03bc) ==============\nax = axes[1]\nmean_values = [0, 30, 60]\nfor mu in mean_values:\n    drop = GaussianDropoff(mean=mu, stdev=80, min_dist=0, max_dist=300, pmax=0.25, dist_type='spherical')\n    ax.plot(distances, drop.probability(distances), linewidth=2.5, label=f'\u03bc = {mu} \u03bcm')\n\nax.set_xlabel('Distance (\u03bcm)', fontsize=11)\nax.set_ylabel('Connection Probability', fontsize=11)\nax.set_title('Effect of Mean (\u03bc)', fontsize=12, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nax.set_xlim([0, 400])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Parameter Effects:\")\nprint(\"stdev (\u03c3): Controls how spread out connections are. Smaller \u03c3 means connections drop off faster.\")\nprint(\"mean (\u03bc): Shifts the peak. Usually 0, but can model connections peaking at specific distances.\")\n</pre> fig, axes = plt.subplots(1, 2, figsize=(14, 5)) distances = np.linspace(0, 400, 1000)  # ============== Panel 1: Effect of stdev (\u03c3) ============== ax = axes[0] stdev_values = [50, 100, 150] for sigma in stdev_values:     drop = GaussianDropoff(stdev=sigma, min_dist=0, max_dist=300, pmax=0.25, dist_type='spherical')     ax.plot(distances, drop.probability(distances), linewidth=2.5, label=f'\u03c3 = {sigma} \u03bcm')  ax.set_xlabel('Distance (\u03bcm)', fontsize=11) ax.set_ylabel('Connection Probability', fontsize=11) ax.set_title('Effect of Standard Deviation (\u03c3)', fontsize=12, fontweight='bold') ax.legend(fontsize=10) ax.grid(True, alpha=0.3) ax.set_xlim([0, 400])  # ============== Panel 2: Effect of mean (\u03bc) ============== ax = axes[1] mean_values = [0, 30, 60] for mu in mean_values:     drop = GaussianDropoff(mean=mu, stdev=80, min_dist=0, max_dist=300, pmax=0.25, dist_type='spherical')     ax.plot(distances, drop.probability(distances), linewidth=2.5, label=f'\u03bc = {mu} \u03bcm')  ax.set_xlabel('Distance (\u03bcm)', fontsize=11) ax.set_ylabel('Connection Probability', fontsize=11) ax.set_title('Effect of Mean (\u03bc)', fontsize=12, fontweight='bold') ax.legend(fontsize=10) ax.grid(True, alpha=0.3) ax.set_xlim([0, 400])  plt.tight_layout() plt.show()  print(\"Parameter Effects:\") print(\"stdev (\u03c3): Controls how spread out connections are. Smaller \u03c3 means connections drop off faster.\") print(\"mean (\u03bc): Shifts the peak. Usually 0, but can model connections peaking at specific distances.\") <pre>Parameter Effects:\nstdev (\u03c3): Controls how spread out connections are. Smaller \u03c3 means connections drop off faster.\nmean (\u03bc): Shifts the peak. Usually 0, but can model connections peaking at specific distances.\n</pre>"},{"location":"examples/notebooks/connectors/GaussianDropoff_tutorial/#gaussiandropoff-tutorial","title":"GaussianDropoff Tutorial\u00b6","text":"<p>This notebook shows how to use <code>bmtool.connectors.GaussianDropoff</code> for distance-dependent connection probabilities. It covers:</p> <ul> <li>What <code>pmax</code> vs <code>ptotal</code> mean and when to use each</li> <li>How parameters affect the connection curve</li> <li>Examples with plots</li> </ul> <p>Run cells in order.</p>"},{"location":"examples/notebooks/connectors/GaussianDropoff_tutorial/#quick-parameter-guide","title":"Quick parameter guide\u00b6","text":"<ul> <li><code>mean</code> (\u03bc): Center of the Gaussian (usually 0)</li> <li><code>stdev</code> (\u03c3): How spread out the curve is</li> <li><code>pmax</code>: Peak probability at the center</li> <li><code>ptotal</code>: Total connection probability within a distance range (class calculates <code>pmax</code> from this)</li> <li><code>ptotal_dist_range</code>: The range for <code>ptotal</code> calculation</li> <li><code>min_dist</code>, <code>max_dist</code>: Hard cutoffs for connections</li> <li><code>dist_type</code>: 'spherical' (3D) or 'cylindrical' (2D)</li> </ul> <p>Use <code>pmax</code> if you know the peak connection rate. Use <code>ptotal</code> if you have data on overall connection rates within a range.</p>"},{"location":"examples/notebooks/connectors/GaussianDropoff_tutorial/#example-1-pmax-vs-ptotal","title":"Example 1: pmax vs ptotal\u00b6","text":"<p>Create two Gaussians: one with <code>pmax</code> set directly, one with <code>ptotal</code> (class calculates <code>pmax</code>). Plot both.</p>"},{"location":"examples/notebooks/connectors/GaussianDropoff_tutorial/#example-2-how-ptotal_dist_range-affects-the-curve","title":"Example 2: How ptotal_dist_range affects the curve\u00b6","text":"<p>Same <code>ptotal</code> (25%), different ranges. See how <code>pmax</code> changes.</p>"},{"location":"examples/notebooks/connectors/GaussianDropoff_tutorial/#example-3-validate-pmax-with-compute_ptotal_integral","title":"Example 3: Validate pmax with compute_ptotal_integral()\u00b6","text":"<p>If you set <code>pmax</code>, use this method to check what <code>ptotal</code> it gives over different ranges.</p>"},{"location":"examples/notebooks/connectors/GaussianDropoff_tutorial/#example-4-spherical-vs-cylindrical-distance","title":"Example 4: Spherical vs cylindrical distance\u00b6","text":"<p>Same parameters, different <code>dist_type</code>. See how it affects <code>pmax</code>.</p>"},{"location":"examples/notebooks/connectors/GaussianDropoff_tutorial/#example-5-parameter-effects","title":"Example 5: Parameter effects\u00b6","text":"<p>How <code>stdev</code> and <code>mean</code> change the curve.</p>"},{"location":"examples/notebooks/connectors/connectors/","title":"Connectors Tutorial","text":"In\u00a0[12]: Copied! <pre>import os\n\nimport numpy as np\nfrom bmtk.builder import NetworkBuilder\nfrom bmtk.builder.auxi.node_params import positions_cuboid, positions_list\n\n# cell count\nnum_of_PN = 400\nnum_of_FSI = 100\n# generate random positions\npositions = positions_cuboid(num_of_PN+num_of_FSI,min_dist=10)\n\n# select locations \ninds = np.random.choice(np.arange(0, np.size(positions, 0)), num_of_PN, replace=False)\npos = positions[inds, :]\nnet = NetworkBuilder('bio_net')\nnet.add_nodes(N=num_of_PN, \n              pop_name='PN',\n              positions=positions_list(pos),\n              model_type='biophysical',\n              morphology=None)\n# Get rid of coordinates already used\npositions = np.delete(positions, inds, 0)\n\ninds = np.random.choice(np.arange(0, np.size(positions, 0)), num_of_FSI, replace=False)\npos = positions[inds, :]\nnet.add_nodes(N=num_of_FSI, \n              pop_name='FSI',\n              positions=positions_list(pos),\n              model_type='biophysical',\n              morphology=None)\n\nbackground = NetworkBuilder('background')\nbackground.add_nodes(N=num_of_FSI+num_of_PN,\n                   pop_name='background_nodes',\n                   potential='exc',\n                   model_type='virtual')\n</pre> import os  import numpy as np from bmtk.builder import NetworkBuilder from bmtk.builder.auxi.node_params import positions_cuboid, positions_list  # cell count num_of_PN = 400 num_of_FSI = 100 # generate random positions positions = positions_cuboid(num_of_PN+num_of_FSI,min_dist=10)  # select locations  inds = np.random.choice(np.arange(0, np.size(positions, 0)), num_of_PN, replace=False) pos = positions[inds, :] net = NetworkBuilder('bio_net') net.add_nodes(N=num_of_PN,                pop_name='PN',               positions=positions_list(pos),               model_type='biophysical',               morphology=None) # Get rid of coordinates already used positions = np.delete(positions, inds, 0)  inds = np.random.choice(np.arange(0, np.size(positions, 0)), num_of_FSI, replace=False) pos = positions[inds, :] net.add_nodes(N=num_of_FSI,                pop_name='FSI',               positions=positions_list(pos),               model_type='biophysical',               morphology=None)  background = NetworkBuilder('background') background.add_nodes(N=num_of_FSI+num_of_PN,                    pop_name='background_nodes',                    potential='exc',                    model_type='virtual')  <p>Now that we have our nodes we can start building some edges with the bmtool.connectors module. There are two main types of network models either homogenous or distance depended. A homogenous network will form connections using a constant probability, while a distance depended model will vary its probability based off how far away the cell pairs are from each other. The connector functions are able to use either type of probability.</p> In\u00a0[13]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom bmtool.connectors import GaussianDropoff\n\n%matplotlib inline\n\ndrop_off = GaussianDropoff(stdev=126.77, min_dist=0, max_dist=300,\n                           ptotal=0.1538, ptotal_dist_range=(0, 50.),\n                           dist_type='spherical')\n\ndistances = np.linspace(0, 500, 1000)\nprobabilities_for_dropoff = drop_off.probability(distances)\nplt.plot(distances, probabilities_for_dropoff,label=f'Pmax={drop_off.pmax:.2f} Sigma={drop_off.stdev}')\nplt.title(\"Distance depended connector function\")\nplt.xlabel(\"Distance (um)\")\nplt.ylabel(\"probability of connection\")\nplt.grid(True)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from bmtool.connectors import GaussianDropoff  %matplotlib inline  drop_off = GaussianDropoff(stdev=126.77, min_dist=0, max_dist=300,                            ptotal=0.1538, ptotal_dist_range=(0, 50.),                            dist_type='spherical')  distances = np.linspace(0, 500, 1000) probabilities_for_dropoff = drop_off.probability(distances) plt.plot(distances, probabilities_for_dropoff,label=f'Pmax={drop_off.pmax:.2f} Sigma={drop_off.stdev}') plt.title(\"Distance depended connector function\") plt.xlabel(\"Distance (um)\") plt.ylabel(\"probability of connection\") plt.grid(True) plt.show() In\u00a0[14]: Copied! <pre>from bmtool.connectors import UnidirectionConnector, spherical_dist\n\n# distance depeneded example \nconnector = UnidirectionConnector(p=drop_off, # function from above cell\n                                  p_arg=spherical_dist, # spherical distance meaning check x,y,z for distance calculation\n                                  verbose=False,\n                                  save_report=False)\nconnector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\n\n# constant p example\nconnector = UnidirectionConnector(p=0.10,verbose=True,save_report=False)\nconnector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\n</pre> from bmtool.connectors import UnidirectionConnector, spherical_dist  # distance depeneded example  connector = UnidirectionConnector(p=drop_off, # function from above cell                                   p_arg=spherical_dist, # spherical distance meaning check x,y,z for distance calculation                                   verbose=False,                                   save_report=False) connector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params())  # constant p example connector = UnidirectionConnector(p=0.10,verbose=True,save_report=False) connector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params()) Out[14]: <pre>&lt;bmtk.builder.connection_map.ConnectionMap at 0x7fe94a51f250&gt;</pre> <p>The <code>ReciprocalConnector</code> uses three key probabilities to model bidirectional connections between neurons:</p> <ul> <li>P0: The probability of a forward connection (from source population to target population).</li> <li>P1: The probability of a backward connection (from target population to source population).</li> <li>Pr: The probability of a reciprocal connection (both directions exist).</li> </ul> <p>These probabilities determine the likelihood of different connection types for each pair of neurons:</p> <ul> <li>Unidirectional forward connection (source \u2192 target only): <code>P0 - Pr</code></li> <li>Unidirectional backward connection (target \u2192 source only): <code>P1 - Pr</code></li> <li>Bidirectional (reciprocal) connection (both directions): <code>Pr</code></li> <li>No connection: <code>1 - P0 - P1 + Pr</code></li> </ul> <p>When <code>Pr = P0 \u00d7 P1</code>, the forward and backward connections are statistically independent. Values of <code>Pr</code> higher than <code>P0 \u00d7 P1</code> indicate positive correlation between the directions, while lower values indicate negative correlation.</p> <p>The correlation coefficient \u03c1 can be calculated as:</p> <pre><code>\u03c1 = (Pr - P0 \u00d7 P1) / \u221a(P0 \u00d7 (1 - P0) \u00d7 P1 \u00d7 (1 - P1))\n</code></pre> <p>This framework allows modeling realistic neural connectivity where bidirectional connections are more or less likely than expected by chance.</p> In\u00a0[15]: Copied! <pre>from bmtool.connectors import ReciprocalConnector\n\n# sets up a homogeneous network with forward prob of 22%, backward prob of 36%, and reciprocal prob of 17% for both connections \nconnector = ReciprocalConnector(p0=0.22,p1=0.36,pr=0.17,verbose=True,save_report=False)\nconnector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['FSI']))\nnet.add_edges(**connector.edge_params())\n# have to run setup_nodes for both directions since we have forwards and backwards probs.\nconnector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\n</pre> from bmtool.connectors import ReciprocalConnector  # sets up a homogeneous network with forward prob of 22%, backward prob of 36%, and reciprocal prob of 17% for both connections  connector = ReciprocalConnector(p0=0.22,p1=0.36,pr=0.17,verbose=True,save_report=False) connector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['FSI'])) net.add_edges(**connector.edge_params()) # have to run setup_nodes for both directions since we have forwards and backwards probs. connector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params()) Out[15]: <pre>&lt;bmtk.builder.connection_map.ConnectionMap at 0x7fe94a51f460&gt;</pre> In\u00a0[16]: Copied! <pre>from bmtool.connectors import GapJunction\n\nconnector = GapJunction(p=0.10,verbose=False,save_report=False)\nconnector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI']))\nnet.add_edges(is_gap_junction=True,**connector.edge_params())\n</pre> from bmtool.connectors import GapJunction  connector = GapJunction(p=0.10,verbose=False,save_report=False) connector.setup_nodes(source=net.nodes(pop_name = ['FSI']), target=net.nodes(pop_name = ['FSI'])) net.add_edges(is_gap_junction=True,**connector.edge_params()) Out[16]: <pre>&lt;bmtk.builder.connection_map.ConnectionMap at 0x7fe94a60acd0&gt;</pre> In\u00a0[17]: Copied! <pre>from bmtool.connectors import CorrelatedGapJunction\n\n# First, create chemical synapses in FSI population\nchemical_connector = ReciprocalConnector(p0=0.10, p1=0.10, pr=0.05, verbose=False, save_report=False)\nchemical_connector.setup_nodes(source=net.nodes(pop_name=['FSI']), target=net.nodes(pop_name=['FSI']))\nnet.add_edges(**chemical_connector.edge_params())\n\n# Then create correlated gap junctions based on chemical connections\ngap_connector = CorrelatedGapJunction(\n    p_non=0.02,    # Low probability for pairs with no chemical synapses\n    p_uni=0.08,    # Higher probability for unidirectional chemical pairs\n    p_rec=0.15,    # Highest probability for reciprocal chemical pairs\n    connector=chemical_connector,  # Reference to chemical connector\n    verbose=False, \n    save_report=False\n)\ngap_connector.setup_nodes(source=net.nodes(pop_name=['FSI']), target=net.nodes(pop_name=['FSI']))\nnet.add_edges(is_gap_junction=True, **gap_connector.edge_params())\n</pre> from bmtool.connectors import CorrelatedGapJunction  # First, create chemical synapses in FSI population chemical_connector = ReciprocalConnector(p0=0.10, p1=0.10, pr=0.05, verbose=False, save_report=False) chemical_connector.setup_nodes(source=net.nodes(pop_name=['FSI']), target=net.nodes(pop_name=['FSI'])) net.add_edges(**chemical_connector.edge_params())  # Then create correlated gap junctions based on chemical connections gap_connector = CorrelatedGapJunction(     p_non=0.02,    # Low probability for pairs with no chemical synapses     p_uni=0.08,    # Higher probability for unidirectional chemical pairs     p_rec=0.15,    # Highest probability for reciprocal chemical pairs     connector=chemical_connector,  # Reference to chemical connector     verbose=False,      save_report=False ) gap_connector.setup_nodes(source=net.nodes(pop_name=['FSI']), target=net.nodes(pop_name=['FSI'])) net.add_edges(is_gap_junction=True, **gap_connector.edge_params()) Out[17]: <pre>&lt;bmtk.builder.connection_map.ConnectionMap at 0x7fe94a60a670&gt;</pre> In\u00a0[18]: Copied! <pre>from bmtool.connectors import GapJunctionConditionalReciprocalConnector\n\n# Example: Conditional chemical synapses based on gap junctions in FSI population\n# Note: This example assumes no prior connections in FSI\n\n# First, create gap junctions\ngap_connector_fsi = GapJunction(p=0.08, verbose=True, save_report=False)\ngap_connector_fsi.setup_nodes(source=net.nodes(pop_name=['FSI']), target=net.nodes(pop_name=['FSI']))\nnet.add_edges(is_gap_junction=True, **gap_connector_fsi.edge_params())\n\n# Then create conditional chemical synapses\nconditional_chemical_connector = GapJunctionConditionalReciprocalConnector(\n    gap_connector=gap_connector_fsi,\n    p0_elec=0.50, p1_elec=0.50, pr_elec=0.25,        # For electrically coupled pairs\n    p0_nonelec=0.125, p1_nonelec=0.125, pr_nonelec=0.03,  # For non-electrically coupled pairs\n    verbose=True, save_report=False\n)\nconditional_chemical_connector.setup_nodes(source=net.nodes(pop_name=['FSI']), target=net.nodes(pop_name=['FSI']))\nnet.add_edges(**conditional_chemical_connector.edge_params())\n</pre> from bmtool.connectors import GapJunctionConditionalReciprocalConnector  # Example: Conditional chemical synapses based on gap junctions in FSI population # Note: This example assumes no prior connections in FSI  # First, create gap junctions gap_connector_fsi = GapJunction(p=0.08, verbose=True, save_report=False) gap_connector_fsi.setup_nodes(source=net.nodes(pop_name=['FSI']), target=net.nodes(pop_name=['FSI'])) net.add_edges(is_gap_junction=True, **gap_connector_fsi.edge_params())  # Then create conditional chemical synapses conditional_chemical_connector = GapJunctionConditionalReciprocalConnector(     gap_connector=gap_connector_fsi,     p0_elec=0.50, p1_elec=0.50, pr_elec=0.25,        # For electrically coupled pairs     p0_nonelec=0.125, p1_nonelec=0.125, pr_nonelec=0.03,  # For non-electrically coupled pairs     verbose=True, save_report=False ) conditional_chemical_connector.setup_nodes(source=net.nodes(pop_name=['FSI']), target=net.nodes(pop_name=['FSI'])) net.add_edges(**conditional_chemical_connector.edge_params()) Out[18]: <pre>&lt;bmtk.builder.connection_map.ConnectionMap at 0x7fe94a60a400&gt;</pre> In\u00a0[19]: Copied! <pre>from bmtool.connectors import OneToOneSequentialConnector\n\nconnector = OneToOneSequentialConnector(verbose=True)\nconnector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name = ['PN']))\nnet.add_edges(**connector.edge_params())\nconnector.setup_nodes(target=net.nodes(pop_name = 'FSI'))\nnet.add_edges(**connector.edge_params())\n</pre> from bmtool.connectors import OneToOneSequentialConnector  connector = OneToOneSequentialConnector(verbose=True) connector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name = ['PN'])) net.add_edges(**connector.edge_params()) connector.setup_nodes(target=net.nodes(pop_name = 'FSI')) net.add_edges(**connector.edge_params()) <pre>All target population partitions are filled.\n</pre> Out[19]: <pre>&lt;bmtk.builder.connection_map.ConnectionMap at 0x7fe94a413a60&gt;</pre> In\u00a0[20]: Copied! <pre>net.build()\n</pre> net.build() <pre>\nStart building connection \n  from bio_net: pop_name=='['PN']'\n  to bio_net: pop_name=='['PN']'\nNumber of connected pairs: 16018\nNumber of possible connections: 160000\nFraction of connected pairs in possible ones: 10.01%\nNumber of total pairs: 160000\nFraction of connected pairs in all pairs: 10.01%\n\nDone! \nTime for building connections: 0.321 sec\n\nStart building connection between: \n  bio_net: pop_name=='['PN']'\n  bio_net: pop_name=='['FSI']'\nEstimated value of rho=0.457\nTime for estimating rho: 0.046 sec\nTotal time for creating connection matrix: 0.407 sec\nNumbers of (forward, backward, reciprocal) connections:\nNumber of connected pairs: (8871, 14422, 6871)\nNumber of possible connections: (40000, 40000, 40000)\nFraction of connected pairs in possible ones: (22.18%, 36.05%, 17.18%)\nNumber of total pairs: 40000\nFraction of connected pairs in all pairs: (22.18%, 36.05%, 17.18%)\n\nAssigning forward connections.\nAssigning backward connections.\nDone! \nTime for building connections: 0.082 sec\n\nStart building gap junction \n  in bio_net: pop_name=='['FSI']'\nNumber of connected pairs: 384\nNumber of possible connections: 4950\nFraction of connected pairs in possible ones: 7.76%\nNumber of total pairs: 4950\nFraction of connected pairs in all pairs: 7.76%\n\nDone! \nTime for building connections: 0.017 sec\nTime for creating connection matrix: 0.028 sec\nDetailed connection statistics by electrical coupling:\n============================================================\nElectrically coupled pairs (384 pairs, 7.8% of total):\n  Unidirectional: 196 (51.0% of elec pairs)\n  Bidirectional:  90 (23.4% of elec pairs)\n  No connection:  98 (25.5% of elec pairs)\n\nNon-electrically coupled pairs (4566 pairs, 92.2% of total):\n  Unidirectional: 830 (18.2% of nonelec pairs)\n  Bidirectional:  141 (3.1% of nonelec pairs)\n  No connection:  3595 (78.7% of nonelec pairs)\n\nOverall chemical connectivity:\n  Numbers of connections: unidirectional, reciprocal\n  Number of connected pairs: (1026, 231)\n  Fraction of connected pairs: (20.73%, 4.67%)\n  Total chemical connectivity: 25.39%\n\nStart building connection from background: *\n  0. to bio_net: pop_name=='['PN']'\n    Time for this partition: 0.082 sec\n  1. to bio_net: pop_name=='FSI'\n    Time for this partition: 0.020 sec\nDone! \nTime for building connections: 0.106 sec\n</pre> In\u00a0[21]: Copied! <pre>from bmtool.connectors import syn_dist_delay_feng_section_PN\nfrom functools import partial\n\nedge_add_properties = {\n    'syn_dist_delay_E2E': {\n        'names': ['delay', 'afferent_section_id', 'afferent_section_pos'],\n        'rule': syn_dist_delay_feng_section_PN,\n        'rule_params': {\n            'p': 0.8, 'sec_id': (2, 3), 'sec_x': (0.8, 0.8),\n            'min_delay': 1.6, 'delay_bound': (0.2, 2.4)\n        },\n        'dtypes': [float, np.uint16, float]\n    }}\n\nconnector = UnidirectionConnector(p=0.10,verbose=False,save_report=False)\nconnector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN']))\nconn = net.add_edges(**connector.edge_params())\n\nedge_properties_val = edge_add_properties['syn_dist_delay_E2E'].copy()\nedge_properties_val['rule'] = partial(edge_properties_val['rule'], connector=connector)\nconn.add_properties(**edge_properties_val)\n</pre> from bmtool.connectors import syn_dist_delay_feng_section_PN from functools import partial  edge_add_properties = {     'syn_dist_delay_E2E': {         'names': ['delay', 'afferent_section_id', 'afferent_section_pos'],         'rule': syn_dist_delay_feng_section_PN,         'rule_params': {             'p': 0.8, 'sec_id': (2, 3), 'sec_x': (0.8, 0.8),             'min_delay': 1.6, 'delay_bound': (0.2, 2.4)         },         'dtypes': [float, np.uint16, float]     }}  connector = UnidirectionConnector(p=0.10,verbose=False,save_report=False) connector.setup_nodes(source=net.nodes(pop_name = ['PN']), target=net.nodes(pop_name = ['PN'])) conn = net.add_edges(**connector.edge_params())  edge_properties_val = edge_add_properties['syn_dist_delay_E2E'].copy() edge_properties_val['rule'] = partial(edge_properties_val['rule'], connector=connector) conn.add_properties(**edge_properties_val) In\u00a0[22]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot illustrating distance-dependent synaptic delay\ndistances = np.linspace(0, 500, 100)  # distances in microns\nvelocity = 1000.0  # um/ms\nmin_delay = 0.8  # ms\nfluc_stdev = 0.1  # ms, for illustration\n\n# Mean delay\nmean_delays = distances / velocity + min_delay\n\n# Sample delays with fluctuation\nrng = np.random.default_rng(42)\nsample_delays = []\nfor d in distances[::10]:  # every 10th point\n    delay = d / velocity + min_delay + fluc_stdev * rng.normal()\n    delay = np.clip(delay, 0.2, 2.4)  # bounds\n    sample_delays.append(delay)\n\nplt.figure(figsize=(8, 5))\nplt.plot(distances, mean_delays, 'b-', label='Mean delay', linewidth=2)\nplt.scatter(distances[::10], sample_delays, color='red', alpha=0.7, label='Sample delays with fluctuation')\nplt.axhline(y=0.8, color='gray', linestyle='--', alpha=0.5, label=f'Minimum delay = {min_delay} ms')\nplt.xlabel('Distance (\u03bcm)')\nplt.ylabel('Synaptic delay (ms)')\nplt.title('Distance-Dependent Synaptic Delay')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Delay formula: delay = distance / {velocity} + {min_delay} + random_fluctuation\")\nprint(f\"At 0 \u03bcm: {min_delay} ms\")\nprint(f\"At 500 \u03bcm: {500/velocity + min_delay:.1f} ms\")\n</pre> import matplotlib.pyplot as plt import numpy as np  # Plot illustrating distance-dependent synaptic delay distances = np.linspace(0, 500, 100)  # distances in microns velocity = 1000.0  # um/ms min_delay = 0.8  # ms fluc_stdev = 0.1  # ms, for illustration  # Mean delay mean_delays = distances / velocity + min_delay  # Sample delays with fluctuation rng = np.random.default_rng(42) sample_delays = [] for d in distances[::10]:  # every 10th point     delay = d / velocity + min_delay + fluc_stdev * rng.normal()     delay = np.clip(delay, 0.2, 2.4)  # bounds     sample_delays.append(delay)  plt.figure(figsize=(8, 5)) plt.plot(distances, mean_delays, 'b-', label='Mean delay', linewidth=2) plt.scatter(distances[::10], sample_delays, color='red', alpha=0.7, label='Sample delays with fluctuation') plt.axhline(y=0.8, color='gray', linestyle='--', alpha=0.5, label=f'Minimum delay = {min_delay} ms') plt.xlabel('Distance (\u03bcm)') plt.ylabel('Synaptic delay (ms)') plt.title('Distance-Dependent Synaptic Delay') plt.legend() plt.grid(True, alpha=0.3) plt.tight_layout() plt.show()  print(f\"Delay formula: delay = distance / {velocity} + {min_delay} + random_fluctuation\") print(f\"At 0 \u03bcm: {min_delay} ms\") print(f\"At 500 \u03bcm: {500/velocity + min_delay:.1f} ms\") <pre>Delay formula: delay = distance / 1000.0 + 0.8 + random_fluctuation\nAt 0 \u03bcm: 0.8 ms\nAt 500 \u03bcm: 1.3 ms\n</pre>"},{"location":"examples/notebooks/connectors/connectors/#example-use-case-for-using-connectors-module","title":"Example use case for using connectors module\u00b6","text":"<p>By Gregory Glickert</p> <p>First we will start with an example network</p>"},{"location":"examples/notebooks/connectors/connectors/#unidirectionalconnector","title":"UnidirectionalConnector\u00b6","text":"<p>The UnidirectionalConnector will make one way or forward connection from the source population to the target population. p is the probability of a connection forming. P can be constant or a deterministic function whose value must be within the range 0 to 1. When p is a constant this will form a homogenous network and the deterministic function can be used to make a distance depended network. There is an optional argument p_arg which are input argument(s) for the p function when using a deterministic function. The example below will use the built in GaussianDropoff.</p>"},{"location":"examples/notebooks/connectors/connectors/#reciprocalconnector","title":"ReciprocalConnector\u00b6","text":"<p>The ReciprocalConnector takes in similar arguments but this time we have p0,p1 and pr. p0 is the probability of a forward connections, p1 is the probability of a backwards connection or a connection from target to source. pr is the probability of reciprocal connection. This connections can get very complex and the Pydocs here has more details on how to use the function.</p> <p>An important thing to note here is how the ReciprocalConnector is called twice. Once for the forward connection and once for the backwards connection. The connector will factor in the reciprocal connections while doing the forward and backwards connections.</p>"},{"location":"examples/notebooks/connectors/connectors/#gapjunction","title":"GapJunction\u00b6","text":"<p>The GapJunction connector is similar to the UnidirectionConnector, but will only form electrical synapses between pairs. So in this example we would connect 10% of the FSI cells to each other with gap junction.</p>"},{"location":"examples/notebooks/connectors/connectors/#correlatedgapjunction","title":"CorrelatedGapJunction\u00b6","text":"<p>The CorrelatedGapJunction connector creates gap junctions where the probability depends on the presence of chemical synapses between the same pairs. It uses conditional probabilities:</p> <ul> <li>p_non: Probability of gap junction for pairs with no chemical synapses</li> <li>p_uni: Probability of gap junction for pairs with unidirectional chemical synapses</li> <li>p_rec: Probability of gap junction for pairs with reciprocal chemical synapses</li> <li>connector: The chemical synapse connector object (must be set up first)</li> </ul> <p>This models the experimental observation that electrical and chemical coupling are often correlated.</p>"},{"location":"examples/notebooks/connectors/connectors/#gapjunctionconditionalreciprocalconnector","title":"GapJunctionConditionalReciprocalConnector\u00b6","text":"<p>The GapJunctionConditionalReciprocalConnector creates chemical synapses where the connection probabilities depend on the presence of gap junctions between cell pairs. This models the experimentally observed correlation between electrical and chemical coupling in neural populations.</p> <p>It uses different bivariate Bernoulli probability distributions for chemical synapses based on electrical coupling status:</p> <ul> <li>Electrically coupled pairs: Use p0_elec, p1_elec, pr_elec probabilities</li> <li>Non-electrically coupled pairs: Use p0_nonelec, p1_nonelec, pr_nonelec probabilities</li> </ul>"},{"location":"examples/notebooks/connectors/connectors/#onetoonesequentialconnector","title":"OneToOneSequentialConnector\u00b6","text":"<p>The OneToOneSequentialConnector is a way to connect two populations one to one. Meaning that only one node from pop A will form a synapse onto one node from pop B. This is normally how background synapses can be modeled. In the example below, one background node is connected to one PN cell in our network.</p>"},{"location":"examples/notebooks/connectors/connectors/#checking-connections-made","title":"Checking connections made\u00b6","text":"<p>There are several ways in bmtool to check the connections made and to make sure everything was set up correctly. One way to do this is by analyzing the output of <code>net.build()</code>. When calling <code>net.build</code> any connection that has verbose=True will have a print out showing some connection details. Of course another way to check the connections made is by using the bmplot.connections module part of bmtool.</p>"},{"location":"examples/notebooks/connectors/connectors/#additional-edge-properties","title":"Additional Edge Properties\u00b6","text":"<p>Beyond connection probabilities, you can add properties to edges such as synaptic delays, weights, and synapse locations. The bmtool.connectors module provides utility functions for common properties.</p> <p>For example, you can add distance-dependent delays or constant delays. These functions can access the connector's connection properties (like distance) if the connector is passed as an argument. Another important parameter is the synaptic location, which can be varied probabilistically along different sections of the dendrite.</p> <p>Synaptic delays are often modeled as distance-dependent to account for conduction time along axons. The delay typically increases linearly with distance, with some minimum delay and possible random fluctuations. Below is an example of how to use these property rules, followed by a plot illustrating how distance-dependent delay works.</p> <p>The example below creates connections between ~10% of PN pairs. For each synapse, 80% are placed at section ID 2, position 0.8 (e.g., basal dendrites), and 20% at section ID 3, position 0.8 (e.g., apical dendrites). It also adds distance-dependent delays with specified minimum delay and bounds.</p>"},{"location":"examples/notebooks/connectors/connectors/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrated how to use various connectors from the bmtool.connectors module to build neural networks with different connectivity patterns:</p> <ul> <li>UnidirectionalConnector: For one-way connections with constant or distance-dependent probabilities.</li> <li>ReciprocalConnector: For bidirectional connections with controlled reciprocal probabilities.</li> <li>GapJunction: For electrical synapses.</li> <li>CorrelatedGapJunction: For correlated electrical and chemical synapses.</li> <li>GapJunctionConditionalReciprocalConnector: For chemical synapses with probabilities conditional on gap junction presence.</li> <li>OneToOneSequentialConnector: For one-to-one mappings, useful for background inputs.</li> </ul> <p>Each connector can be configured with probability functions, and additional edge properties like delays and synaptic locations can be added. The notebook also includes examples of how to add these properties, visualizations showing how distance-dependent delays work, and methods for checking and analyzing the connections made. The network can then be saved and used for simulations in BMTK.</p> <p>For more advanced usage, refer to the full documentation and the connector classes' docstrings.</p>"},{"location":"examples/notebooks/single_cell/Simple_cell_tuning/","title":"Simple Cell Tuning","text":"In\u00a0[1]: Copied! <pre>import os\nimport glob\nimport logging\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom neuron import h\n\nfrom bmtool.singlecell import (CurrentClamp,Profiler, run_and_plot)\n\n%matplotlib inline\n\n# Disable warnings for cleaner output\nlogging.disable(logging.WARNING)\n</pre> import os import glob import logging import numpy as np import matplotlib.pyplot as plt from neuron import h  from bmtool.singlecell import (CurrentClamp,Profiler, run_and_plot)  %matplotlib inline  # Disable warnings for cleaner output logging.disable(logging.WARNING) <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> In\u00a0[2]: Copied! <pre># if already compiled then lets delete the folder and force a recompile\nif os.path.isdir('modfiles/x86_64'):\n    os.system(\"rm -rf modfiles/x86_64\")\nif os.path.isdir('modfiles/arm64'):\n    os.system(\"rm -rf modfiles/arm64\")\n\n# compile the mod files\nif not os.path.isdir(\"modfiles/x86_64\"):\n    os.chdir('modfiles')\n    os.system(\"nrnivmodl\")\n    os.chdir(\"..\")\n</pre> # if already compiled then lets delete the folder and force a recompile if os.path.isdir('modfiles/x86_64'):     os.system(\"rm -rf modfiles/x86_64\") if os.path.isdir('modfiles/arm64'):     os.system(\"rm -rf modfiles/arm64\")  # compile the mod files if not os.path.isdir(\"modfiles/x86_64\"):     os.chdir('modfiles')     os.system(\"nrnivmodl\")     os.chdir(\"..\") <pre>/Users/gregglickert/miniconda3/envs/bmtk/bin/nrnivmodl:10: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import working_set\n</pre> <pre>/usr/bin/xcrun\n/Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles\nMod files: \"./AMPA_NMDA_STP.mod\" \"./CaDynamics.mod\" \"./Ca_HVA.mod\" \"./Ca_LVA.mod\" \"./GABA_A_STP.mod\" \"./Gfluct.mod\" \"./Ih.mod\" \"./Im.mod\" \"./Im_v2.mod\" \"./K_P.mod\" \"./K_T.mod\" \"./Kd.mod\" \"./Kv2like.mod\" \"./Kv3_1.mod\" \"./NaTa.mod\" \"./NaTs.mod\" \"./NaV.mod\" \"./Nap.mod\" \"./SK.mod\" \"./cadad.mod\" \"./cal2.mod\" \"./can_mig.mod\" \"./exp2syn_stp.mod\" \"./gap.mod\" \"./h_kole.mod\" \"./imCA3.mod\" \"./k.mod\" \"./kBK.mod\" \"./kap_BS.mod\" \"./kdmc_BS.mod\" \"./kdrCA3.mod\" \"./kdr_BS.mod\" \"./kdrinter.mod\" \"./leak.mod\" \"./na.mod\" \"./nainter.mod\" \"./napCA3.mod\" \"./natCA3.mod\" \"./nax_BS.mod\" \"./vecevent_coreneuron.mod\"\n\nCreating 'arm64' directory for .o files.\n\n -&gt; NMODL ../Ca_HVA.mod\n -&gt; NMODL ../AMPA_NMDA_STP.mod\n -&gt; Compiling mod_func.cpp\n -&gt; NMODL ../CaDynamics.mod\n -&gt; NMODL ../GABA_A_STP.mod\n -&gt; NMODL ../Ca_LVA.mod\n -&gt; NMODL ../Gfluct.mod\n -&gt; NMODL ../Ih.mod\n -&gt; NMODL ../Im.mod\n -&gt; NMODL ../Im_v2.mod\n -&gt; NMODL ../K_P.mod\n -&gt; NMODL ../K_T.mod\n -&gt; NMODL ../Kd.mod\n -&gt; NMODL ../NaTa.mod\n -&gt; NMODL ../Kv2like.mod\n -&gt; NMODL ../Kv3_1.mod\n -&gt; NMODL ../NaTs.mod\n -&gt; NMODL ../NaV.mod\n -&gt; NMODL ../Nap.mod\n -&gt; NMODL ../SK.mod\n -&gt; NMODL ../cadad.mod\n -&gt; NMODL ../cal2.mod\n -&gt; NMODL ../can_mig.mod\n -&gt; NMODL ../exp2syn_stp.mod\nWarning: Default 2 of PARAMETER cao will be ignored and set by NEURON.\nWarning: Default 5e-05 of PARAMETER cai will be ignored and set by NEURON.\nWarning: Default 2 of PARAMETER cao will be ignored and set by NEURON.\nWarning: Default 5e-05 of PARAMETER cai will be ignored and set by NEURON.\n -&gt; NMODL ../gap.mod\n -&gt; NMODL ../h_kole.mod\n -&gt; NMODL ../imCA3.mod\n -&gt; NMODL ../k.mod\nWarning: Default 6.3 of PARAMETER celsius will be ignored and set by NEURON.\n -&gt; NMODL ../kBK.mod\n -&gt; NMODL ../kap_BS.mod\n -&gt; NMODL ../kdmc_BS.mod\n -&gt; NMODL ../kdrCA3.mod\n -&gt; NMODL ../kdr_BS.mod\n -&gt; NMODL ../kdrinter.mod\n -&gt; NMODL ../leak.mod\n -&gt; NMODL ../na.mod\nWarning: Default -80 of PARAMETER ek will be ignored and set by NEURON.\nWarning: Default 6.3 of PARAMETER celsius will be ignored and set by NEURON.\n -&gt; NMODL ../nainter.mod\n -&gt; NMODL ../napCA3.mod\n -&gt; NMODL ../natCA3.mod\n -&gt; NMODL ../nax_BS.mod\nWarning: Default 45 of PARAMETER ena will be ignored and set by NEURON.\n -&gt; NMODL ../vecevent_coreneuron.mod\n -&gt; Compiling AMPA_NMDA_STP.c\n -&gt; Compiling CaDynamics.c\n -&gt; Compiling Ca_HVA.c\nNotice: ARTIFICIAL_CELL is a synonym for POINT_PROCESS which hints that it\nonly affects and is affected by discrete events. As such it is not\nlocated in a section and is not associated with an integrator\n -&gt; Compiling Ca_LVA.c\n -&gt; Compiling GABA_A_STP.c\n -&gt; Compiling Gfluct.c\n -&gt; Compiling Ih.c\n -&gt; Compiling Im.c\n -&gt; Compiling Im_v2.c\n -&gt; Compiling K_P.c\n -&gt; Compiling K_T.c\n -&gt; Compiling Kd.c\n</pre> <pre>Translating AMPA_NMDA_STP.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/AMPA_NMDA_STP.c\nTranslating Ca_HVA.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/Ca_HVA.c\nTranslating CaDynamics.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/CaDynamics.c\nThread Safe\nThread Safe\nThread Safe\nTranslating Ca_LVA.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/Ca_LVA.c\nTranslating Gfluct.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/Gfluct.c\nTranslating GABA_A_STP.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/GABA_A_STP.c\nThread Safe\nThread Safe\nNotice: This mechanism cannot be used with CVODE\nThread Safe\nTranslating Ih.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/Ih.c\nTranslating Im.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/Im.c\nTranslating Im_v2.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/Im_v2.c\nThread Safe\nThread Safe\nThread Safe\nTranslating K_P.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/K_P.c\nTranslating K_T.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/K_T.c\nTranslating Kd.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/Kd.c\nThread Safe\nThread Safe\nThread Safe\nTranslating Kv2like.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/Kv2like.c\nTranslating Kv3_1.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/Kv3_1.c\nTranslating NaTa.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/NaTa.c\nThread Safe\nThread Safe\nThread Safe\nTranslating NaTs.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/NaTs.c\nTranslating NaV.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/NaV.c\nTranslating Nap.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/Nap.c\nThread Safe\nTranslating SK.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/SK.c\nNEURON's CVode method ignores conservation\nNotice: LINEAR is not thread safe.\nThread Safe\nThread Safe\nTranslating cal2.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/cal2.c\nTranslating can_mig.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/can_mig.c\nTranslating exp2syn_stp.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/exp2syn_stp.c\nTranslating cadad.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/cadad.c\nThread Safe\nThread Safe\nThread Safe\nThread Safe\nTranslating gap.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/gap.c\nTranslating h_kole.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/h_kole.c\nTranslating imCA3.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/imCA3.c\nThread Safe\nTranslating k.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/k.c\nThread Safe\nThread Safe\nThread Safe\nTranslating kBK.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/kBK.c\nTranslating kdmc_BS.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/kdmc_BS.c\nTranslating kap_BS.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/kap_BS.c\nTranslating kdrCA3.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/kdrCA3.c\nThread Safe\nThread Safe\nThread Safe\nThread Safe\nTranslating leak.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/leak.c\nTranslating kdrinter.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/kdrinter.c\nTranslating na.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/na.c\nTranslating kdr_BS.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/kdr_BS.c\nThread Safe\nThread Safe\nThread Safe\nThread Safe\nTranslating nainter.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/nainter.c\nTranslating napCA3.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/napCA3.c\nThread Safe\nTranslating nax_BS.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/nax_BS.c\nThread Safe\nTranslating natCA3.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/natCA3.c\nThread Safe\nThread Safe\nTranslating vecevent_coreneuron.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/modfiles/arm64/vecevent_coreneuron.c\nThread Safe\nCa_HVA.cAMPA_NMDA_STP.c::4342::18CaDynamics.c:42:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]   42 |          exter\n18n:  dwarning: oa function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]u\nble *getarg();\n      |                         ^   \n42 |          extern double   43  | *g/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.he         :ext15eta:r17g:( )note: ;expanded from macro 'getarg'\n\n      |                         ^\nr   n /Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.hd:o15u15b:l17 | #def: note: expanded from macro 'getarg'\n   e15ine ge t*getarg();\n      | a                        ^r\n | g  hoc_#g/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17d: note: expanded from macro 'getarg'\n   15 | #define getarg  eefinheo targc\ng      _| ge                ^teatarg  hoc_rg\nge\nt      a| r                ^\ng\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: /Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.hconflicting prototype is here:49:16   :49  | note: econflicting prototype is herex\nt\nern do   u49    | ext49 | belxerent*ern do  gdeutargo(uibnbt)l;el\ne      *|                ^ *g geett\naarrg/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h(gint()i;n\nt:15:      17: note: expanded from macro 'getarg'\n|                ^)   ;\n15 | #define getarg  ho\nc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg       |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h: 15:17h:o cnote: _getargexpanded from macro 'getarg'\n   15 | #d\ne      fine getarg  hoc_getarg\n      | |                 ^                ^\n\nAMPA_NMDA_STP.c:414:32: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  414 |   if (_tsav &gt; t){ extern char* hoc_object_name(); hoc_execerror(hoc_object_name(_pnt-&gt;ob), \":Event arrived out of order. Must call ParallelContext.set_maxstep AFTER assigning minimum NetCon.delay\");}\n      |                                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:46:14: note: conflicting prototype is here\n   46 | extern char* hoc_object_name(Object*);\n      |              ^\nCa_LVA.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n2 warnings generated.\nGfluct.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nGABA_A_STP.c:42:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   42 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nGABA_A_STP.c:356:32: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  356 |   if (_tsav &gt; t){ extern char* hoc_object_name(); hoc_execerror(hoc_object_name(_pnt-&gt;ob), \":Event arrived out of order. Must call ParallelContext.set_maxstep AFTER assigning minimum NetCon.delay\");}\n      |                                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:46:14: note: conflicting prototype is here\n   46 | extern char* hoc_object_name(Object*);\n      |              ^\nIh.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\nIm.c:43:18:    warning: 43a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype] |          extern doubl\ne *getarg();   \n43       | |                                 ^ \nextern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_ge/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.ht:a15r:g17\n:       | note:                 ^expanded from macro 'getarg'\n   15\n | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern dou/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16b:le* getar note: conflicting prototype is here\n   49 | extern double* getarg(int)g;\n      | (int);\n      |                ^\n               ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17:/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h :note: 15expanded from macro 'getarg':\n17: note: expanded from macro 'getarg'\n   15 |    #15d | e#fdienfei ngee tgaertga r gh o ch_ogce_tgaertga\nr      g| \n                ^      \n|                 ^\n1 warning generated.\n2 warnings generated.\n1 warning generated.\n1 warning generated.\nIm_v2.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nK_P.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nK_T.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nKd.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\nKv3_1.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getargKv2like.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n\n      |                 ^\n   43 |          extern double *geta/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.hr:g49(:)16;:\n       note: | conflicting prototype is here                        ^\n\n   49 | extern double* getarg(/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.hi:n15t:)17;:\n       note: | expanded from macro 'getarg'               ^\n\n   /Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n15    | 15# | d#edfeifnien eg egteatragr g  h ohco_cg_egteatragr\ng      \n|                       ^| \n                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nNaTs.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nNaTa.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\nNap.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nNaV.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nNaV.c:404:17: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  404 |  extern double *_getelm();\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/nrniv_mf.h:26:16: note: conflicting prototype is here\n   26 | extern double* _getelm(int, int);\n      |                ^\nSK.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\ncadad.c:42:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   42 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\ncal2.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\ncan_mig.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nexp2syn_stp.c:42:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   42 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nexp2syn_stp.c:343:32: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  343 |   if (_tsav &gt; t){ extern char* hoc_object_name(); hoc_execerror(hoc_object_name(_pnt-&gt;ob), \":Event arrived out of order. Must call ParallelContext.set_maxstep AFTER assigning minimum NetCon.delay\");}\n      |                                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:46:14: note: conflicting prototype is here\n   46 | extern char* hoc_object_name(Object*);\n      |              ^\n2 warnings generated.\n1 warning generated.\n1 warning generated.\n2 warnings generated.\nh_kole.c:42:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   42 |          exgap.ct:e41r:n18 :d owarning: ua function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]b\nle *getarg()   ;41\n |          extern do      u| b                        ^l\ne *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #de/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.hf:i15n:e17 :g enote: texpanded from macro 'getarg'a\nrg     15h | o#cd_egfeitnaer gg\ne      t| a                ^r\ng  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49    | 49e | xetxetrenr nd oduobulbel*e *g egteatragr(gi(nitn)t;)\n;      \n|                      ^| \n               ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h15::1517::17 :note:  expanded from macro 'getarg'note: \nexpanded from macro 'getarg'\n   15    | 15# | d#edfeifnien eg egteatragr g  h ohco_cg_egteatragr\ng      \n|                       ^| \n                ^\n</pre> <pre> -&gt; Compiling Kv2like.c\n -&gt; Compiling Kv3_1.c\n -&gt; Compiling NaTa.c\n -&gt; Compiling NaTs.c\n -&gt; Compiling Nap.c\n -&gt; Compiling NaV.c\n -&gt; Compiling SK.c\n -&gt; Compiling cadad.c\n -&gt; Compiling cal2.c\n -&gt; Compiling can_mig.c\n -&gt; Compiling exp2syn_stp.c\n -&gt; Compiling gap.c\n -&gt; Compiling h_kole.c\n -&gt; Compiling imCA3.c\n -&gt; Compiling k.c\n -&gt; Compiling kBK.c\n -&gt; Compiling kap_BS.c\n</pre> <pre>imCA3.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nk.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\nkBK.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nkap_BS.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nkdmc_BS.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nkdrCA3.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\nkdr_BS.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nkdrinter.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nleak.c:41:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   41 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nna.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\nnainter.cnapCA3.c::43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern d44ouble *getarg();\n      :18| :                        ^ \nwarning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #def   ine getarg  h44o | c_getarg\n      |                 ^         \nextern double *ge/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.ht:a49r:g();\n      |                         ^\n16: note: conflicting prototype is here\n   49 | extern double/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h*: 15g:e17t: argnote: (expanded from macro 'getarg'int)\n;\n      |                ^   \n15 | #define get/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.ha:r15g: 17: note: expanded from macro 'getarg'\n   15 | #define geta horg  hocc_geta_rgge\nt      a| r                ^g\n\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nnatCA3.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nnax_BS.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n</pre> <pre> -&gt; Compiling kdmc_BS.c\n -&gt; Compiling kdrCA3.c\n -&gt; Compiling kdr_BS.c\n -&gt; Compiling kdrinter.c\n -&gt; Compiling leak.c\n -&gt; Compiling na.c\n -&gt; Compiling nainter.c\n -&gt; Compiling napCA3.c\n -&gt; Compiling natCA3.c\n -&gt; Compiling nax_BS.c\n -&gt; Compiling vecevent_coreneuron.c\n =&gt; LINKING shared library ./libnrnmech.dylib\n</pre> <pre>vecevent_coreneuron.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nvecevent_coreneuron.c:242:32: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  242 |   if (_tsav &gt; t){ extern char* hoc_object_name(); hoc_execerror(hoc_object_name(_pnt-&gt;ob), \":Event arrived out of order. Must call ParallelContext.set_maxstep AFTER assigning minimum NetCon.delay\");}\n      |                                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:46:14: note: conflicting prototype is here\n   46 | extern char* hoc_object_name(Object*);\n      |              ^\n2 warnings generated.\n</pre> <pre> =&gt; LINKING executable ./special LDFLAGS are:    \nSuccessfully created arm64/special\n</pre> <pre>ld: warning: ignoring duplicate libraries: '-lnrnmech'\n</pre> In\u00a0[3]: Copied! <pre>profiler = Profiler(template_dir='.', mechanism_dir='modfiles', dt=0.05)\n</pre> profiler = Profiler(template_dir='.', mechanism_dir='modfiles', dt=0.05) In\u00a0[4]: Copied! <pre>h.load_file(\"stdrun.hoc\")\n\nclass SimpleSoma(object):\n    \"\"\"Single-section cell: A soma with leak, sodium, and potassium channels.\"\"\"\n    def __init__(self):\n        self.create_sections()\n        self.build_topology()\n        self.build_subsets()\n        self.define_geometry()\n        self.define_biophysics()\n    #\n    def create_sections(self):\n        \"\"\"Create the sections of the cell.\"\"\"\n        # NOTE: cell=self is required to tell NEURON of this object.\n        self.soma = h.Section(name='soma', cell=self)\n    #\n    def build_topology(self):\n        \"\"\"Connect the sections of the cell to build a tree.\"\"\"\n        # Single section - no connections needed\n        pass\n    #\n    def define_geometry(self):\n        \"\"\"Set the 3D geometry of the cell.\"\"\"\n        self.soma.L = 500     # soma length um\n        self.soma.diam = 500  # soma diameter um\n        h.define_shape()      # Translate into 3D points.\n    #\n    def define_biophysics(self):\n        \"\"\"Assign the membrane properties across the cell.\"\"\"\n        for sec in self.all: # 'all' defined in build_subsets\n            sec.cm = 1       # membrane capacitance uF/cm2\n            sec.Ra = 30      # ohm-cm\n        \n        # Insert leak channel\n        self.soma.insert('leak')  # Leak channel\n        self.soma.glbar_leak = 0.0003  # S/cm2\n        self.soma.el_leak = -60  # mV\n        \n        # Insert sodium channel\n        self.soma.insert('na')  # Sodium channel\n        self.soma.gnabar_na = 0.12  # S/cm2\n        self.soma.nao = 79.8  # extracellular Na concentration\n        \n        # Insert potassium channel\n        self.soma.insert('k')  # Potassium channel\n        self.soma.gkbar_k = 0.036  # S/cm2\n        self.soma.ki = 69.35  # intracellular K concentration\n    #\n    def build_subsets(self):\n        \"\"\"Build subset lists. For now we define 'all'.\"\"\"\n        self.all = h.SectionList()\n        self.all.wholetree(sec=self.soma)\n\n\ncell = SimpleSoma()\n</pre> h.load_file(\"stdrun.hoc\")  class SimpleSoma(object):     \"\"\"Single-section cell: A soma with leak, sodium, and potassium channels.\"\"\"     def __init__(self):         self.create_sections()         self.build_topology()         self.build_subsets()         self.define_geometry()         self.define_biophysics()     #     def create_sections(self):         \"\"\"Create the sections of the cell.\"\"\"         # NOTE: cell=self is required to tell NEURON of this object.         self.soma = h.Section(name='soma', cell=self)     #     def build_topology(self):         \"\"\"Connect the sections of the cell to build a tree.\"\"\"         # Single section - no connections needed         pass     #     def define_geometry(self):         \"\"\"Set the 3D geometry of the cell.\"\"\"         self.soma.L = 500     # soma length um         self.soma.diam = 500  # soma diameter um         h.define_shape()      # Translate into 3D points.     #     def define_biophysics(self):         \"\"\"Assign the membrane properties across the cell.\"\"\"         for sec in self.all: # 'all' defined in build_subsets             sec.cm = 1       # membrane capacitance uF/cm2             sec.Ra = 30      # ohm-cm                  # Insert leak channel         self.soma.insert('leak')  # Leak channel         self.soma.glbar_leak = 0.0003  # S/cm2         self.soma.el_leak = -60  # mV                  # Insert sodium channel         self.soma.insert('na')  # Sodium channel         self.soma.gnabar_na = 0.12  # S/cm2         self.soma.nao = 79.8  # extracellular Na concentration                  # Insert potassium channel         self.soma.insert('k')  # Potassium channel         self.soma.gkbar_k = 0.036  # S/cm2         self.soma.ki = 69.35  # intracellular K concentration     #     def build_subsets(self):         \"\"\"Build subset lists. For now we define 'all'.\"\"\"         self.all = h.SectionList()         self.all.wholetree(sec=self.soma)   cell = SimpleSoma() In\u00a0[5]: Copied! <pre>analysis_settings = {\n    'CurrentClamp': {\n        'celsius': 34.0,\n        'kwargs': {\n            'inj_amp': 100,   \n            'inj_delay': 1500.0,\n            'inj_dur': 1000.0,\n            'tstop': 3000.0,\n            'threshold': -15.0\n        }\n    },\n}\n</pre> analysis_settings = {     'CurrentClamp': {         'celsius': 34.0,         'kwargs': {             'inj_amp': 100,                'inj_delay': 1500.0,             'inj_dur': 1000.0,             'tstop': 3000.0,             'threshold': -15.0         }     }, } In\u00a0[6]: Copied! <pre>proc_settings = analysis_settings['CurrentClamp']\nh.celsius = proc_settings['celsius']\n\n# Create ReusableCurrentClamp instance\nsim_cc = CurrentClamp(cell, **proc_settings['kwargs'])\n\n# First run\nt1, v1 = sim_cc.run()\n\nplt.figure(figsize=(10, 5))\nplt.plot(t1, v1)\nplt.xlabel('Time (ms)')\nplt.ylabel('Membrane Potential (mV)')\nplt.title('Current Clamp Response')\nplt.tight_layout()\nplt.show()\n</pre> proc_settings = analysis_settings['CurrentClamp'] h.celsius = proc_settings['celsius']  # Create ReusableCurrentClamp instance sim_cc = CurrentClamp(cell, **proc_settings['kwargs'])  # First run t1, v1 = sim_cc.run()  plt.figure(figsize=(10, 5)) plt.plot(t1, v1) plt.xlabel('Time (ms)') plt.ylabel('Membrane Potential (mV)') plt.title('Current Clamp Response') plt.tight_layout() plt.show() <pre>Injection location: &lt;__main__.SimpleSoma object at 0x318013b90&gt;.soma(0.5)\nRecording: &lt;__main__.SimpleSoma object at 0x318013b90&gt;.soma(0.5)._ref_v\nCurrent clamp simulation running...\n\nNumber of spikes: 0\n\n</pre> In\u00a0[7]: Copied! <pre># Modify cell parameter\ncell.soma.gnabar_na = 0.22  # S/cm2\n</pre> # Modify cell parameter cell.soma.gnabar_na = 0.22  # S/cm2 In\u00a0[8]: Copied! <pre># Reset NEURON state and re-run with modified parameter (no cell rebuild!)\nsim_cc.reset_state()\nt2, v2 = sim_cc.run()\n\nplt.figure(figsize=(10, 5))\nplt.plot(t2, v2)\nplt.xlabel('Time (ms)')\nplt.ylabel('Membrane Potential (mV)')\nplt.title('Current Clamp Response')\nplt.tight_layout()\nplt.show()\n</pre> # Reset NEURON state and re-run with modified parameter (no cell rebuild!) sim_cc.reset_state() t2, v2 = sim_cc.run()  plt.figure(figsize=(10, 5)) plt.plot(t2, v2) plt.xlabel('Time (ms)') plt.ylabel('Membrane Potential (mV)') plt.title('Current Clamp Response') plt.tight_layout() plt.show() <pre>Current clamp simulation running...\n\nNumber of spikes: 179\n\n</pre> In\u00a0[9]: Copied! <pre># Automatically adjust gnabar_na to achieve target number of spikes\ntarget_spikes = 5  # Set desired number of spikes\ngnabar_na_start = 0.12  # Starting value\ngnabar_na_step = 0.01  # Increment step\nmax_iterations = 100  # Safety limit\n\ngnabar_na = gnabar_na_start\niteration = 0\n\n# Clear any previous spike detections\nsim_cc.nspks = 0\n\nwhile sim_cc.nspks &lt; target_spikes and iteration &lt; max_iterations:\n    cell.soma.gnabar_na = gnabar_na\n    sim_cc.reset_state()\n    t, v = sim_cc.run()\n    current_spikes = sim_cc.nspks\n    print(f\"Iteration {iteration}: gnabar_na = {gnabar_na:.3f}, spikes = {current_spikes}\")\n    if current_spikes &gt;= target_spikes:\n        break\n    gnabar_na += gnabar_na_step\n    iteration += 1\n\nif iteration &gt;= max_iterations:\n    print(\"Warning: Maximum iterations reached without achieving target spikes\")\nelse:\n    print(f\"Target achieved with gnabar_na = {gnabar_na:.3f}\")\n\n# Plot the final result\nplt.figure(figsize=(10, 5))\nplt.plot(t, v)\nplt.xlabel('Time (ms)')\nplt.ylabel('Membrane Potential (mV)')\nplt.title(f'Current Clamp Response (gnabar_na = {gnabar_na:.3f}, spikes = {sim_cc.nspks})')\nplt.tight_layout()\nplt.show()\n</pre> # Automatically adjust gnabar_na to achieve target number of spikes target_spikes = 5  # Set desired number of spikes gnabar_na_start = 0.12  # Starting value gnabar_na_step = 0.01  # Increment step max_iterations = 100  # Safety limit  gnabar_na = gnabar_na_start iteration = 0  # Clear any previous spike detections sim_cc.nspks = 0  while sim_cc.nspks &lt; target_spikes and iteration &lt; max_iterations:     cell.soma.gnabar_na = gnabar_na     sim_cc.reset_state()     t, v = sim_cc.run()     current_spikes = sim_cc.nspks     print(f\"Iteration {iteration}: gnabar_na = {gnabar_na:.3f}, spikes = {current_spikes}\")     if current_spikes &gt;= target_spikes:         break     gnabar_na += gnabar_na_step     iteration += 1  if iteration &gt;= max_iterations:     print(\"Warning: Maximum iterations reached without achieving target spikes\") else:     print(f\"Target achieved with gnabar_na = {gnabar_na:.3f}\")  # Plot the final result plt.figure(figsize=(10, 5)) plt.plot(t, v) plt.xlabel('Time (ms)') plt.ylabel('Membrane Potential (mV)') plt.title(f'Current Clamp Response (gnabar_na = {gnabar_na:.3f}, spikes = {sim_cc.nspks})') plt.tight_layout() plt.show() <pre>Current clamp simulation running...\n\nNumber of spikes: 0\n\nIteration 0: gnabar_na = 0.120, spikes = 0\nCurrent clamp simulation running...\n\nNumber of spikes: 0\n\nIteration 1: gnabar_na = 0.130, spikes = 0\nCurrent clamp simulation running...\n\nNumber of spikes: 0\n\nIteration 2: gnabar_na = 0.140, spikes = 0\nCurrent clamp simulation running...\n\nNumber of spikes: 0\n\nIteration 3: gnabar_na = 0.150, spikes = 0\nCurrent clamp simulation running...\n\nNumber of spikes: 0\n\nIteration 4: gnabar_na = 0.160, spikes = 0\nCurrent clamp simulation running...\n\nNumber of spikes: 0\n\nIteration 5: gnabar_na = 0.170, spikes = 0\nCurrent clamp simulation running...\n\nNumber of spikes: 172\n\nIteration 6: gnabar_na = 0.180, spikes = 172\nTarget achieved with gnabar_na = 0.180\n</pre>"},{"location":"examples/notebooks/single_cell/Simple_cell_tuning/#simple-cell-tuning-with-bmtool","title":"Simple Cell Tuning with bmtool\u00b6","text":"<p>This tutorial demonstrates how to perform parameter tuning on a simple cell model using bmtool's single cell analysis tools. We'll cover:</p> <ol> <li>Setup and Compilation: Importing libraries and compiling NEURON mechanisms</li> <li>Cell Model Definition: Creating a Python class-based cell model</li> <li>Parameter Modification: Changing cell parameters on-the-fly</li> <li>Simulation Re-running: Resetting and re-executing simulations efficiently</li> <li>Automated Tuning: Iteratively optimizing parameters to achieve target behavior</li> </ol>"},{"location":"examples/notebooks/single_cell/Simple_cell_tuning/#part-1-setup-and-compilation","title":"Part 1: Setup and Compilation\u00b6","text":"<p>First, we import the necessary libraries and compile the NEURON MOD files. This section sets up the environment for single cell analysis.</p>"},{"location":"examples/notebooks/single_cell/Simple_cell_tuning/#part-2-creating-the-profiler","title":"Part 2: Creating the Profiler\u00b6","text":"<p>The Profiler manages the NEURON environment and handles cell loading. We'll use it to work with our custom Python cell class.</p>"},{"location":"examples/notebooks/single_cell/Simple_cell_tuning/#part-3-defining-a-python-class-cell-model","title":"Part 3: Defining a Python Class Cell Model\u00b6","text":"<p>Here we define a simple soma cell model as a Python class. This approach allows us to easily modify parameters and re-run simulations without rebuilding the entire cell structure.</p>"},{"location":"examples/notebooks/single_cell/Simple_cell_tuning/#part-4-running-the-first-simulation","title":"Part 4: Running the First Simulation\u00b6","text":"<p>We run the current clamp simulation with the initial parameters and visualize the membrane potential response.</p>"},{"location":"examples/notebooks/single_cell/Simple_cell_tuning/#part-5-modifying-cell-parameters","title":"Part 5: Modifying Cell Parameters\u00b6","text":"<p>Since our cell is a Python class, we can directly modify its parameters. Here we increase the sodium conductance to see how it affects spiking behavior.</p>"},{"location":"examples/notebooks/single_cell/Simple_cell_tuning/#part-6-automated-parameter-tuning","title":"Part 6: Automated Parameter Tuning\u00b6","text":"<p>Finally, we demonstrate automated tuning: iteratively adjusting the sodium conductance until the cell produces the desired number of spikes. This shows how Python class-based models enable efficient parameter optimization. This simple example could be expanded and you could tune other parameters such as passive properties and FI curves.</p>"},{"location":"examples/notebooks/single_cell/bmtk_example/","title":"Single Cell with BMTK","text":"In\u00a0[1]: Copied! <pre>from bmtool.singlecell import Profiler\n\nprofiler = Profiler(config='simulation_config.json')\nprofiler.interactive_runner()\n</pre> from bmtool.singlecell import Profiler  profiler = Profiler(config='simulation_config.json') profiler.interactive_runner() <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> <pre>loading /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/single_cell/components/templates/templates.hoc\n</pre> <pre>VBox(children=(HBox(children=(Dropdown(description='Template:', layout=Layout(width='300px'), options=('CP_Cel\u2026</pre> <pre>Output(layout=Layout(border_bottom='1px solid #ccc', border_left='1px solid #ccc', border_right='1px solid #cc\u2026</pre>"},{"location":"examples/notebooks/single_cell/bmtk_example/#how-the-interactive-runner-works-with-bmtk-configs","title":"How the Interactive Runner Works with BMTK Configs\u00b6","text":"<p>The <code>Profiler.interactive_runner()</code> method provides an interactive GUI for single-cell electrophysiological profiling using BMTK (Brain Modeling Toolkit) configuration files. This section describes how it reads and processes BMTK configs to populate the template dropdown with available cell models along with the asumptions made by the profiler interactive runner.</p>"},{"location":"examples/notebooks/single_cell/bmtk_example/#initialization-and-config-loading","title":"Initialization and Config Loading\u00b6","text":"<p>When initializing the <code>Profiler</code> with a <code>config</code> parameter:</p> <pre>profiler = Profiler(config='simulation_config.json')\n</pre> <p>The profiler loads the BMTK simulation configuration file and extracts key paths for mechanisms and templates.</p>"},{"location":"examples/notebooks/single_cell/bmtk_example/#bmtk-config-structure","title":"BMTK Config Structure\u00b6","text":""},{"location":"examples/notebooks/single_cell/bmtk_example/#simulation_configjson","title":"simulation_config.json\u00b6","text":"<p>The main simulation config contains a <code>\"network\"</code> key pointing to the circuit configuration:</p> <pre>{\n  \"network\": \"circuit_config.json\",\n  \"manifest\": {\n    \"$BASE_DIR\": \"${configdir}\"\n  }\n}\n</pre>"},{"location":"examples/notebooks/single_cell/bmtk_example/#circuit_configjson","title":"circuit_config.json\u00b6","text":"<p>The circuit config defines the network structure and component directories:</p> <pre>{\n  \"manifest\": {\n    \"$BASE_DIR\": \"${configdir}\",\n    \"$COMPONENTS_DIR\": \"$BASE_DIR/components\"\n  },\n  \"components\": {\n    \"mechanisms_dir\": \"$COMPONENTS_DIR/mechanisms/modfiles\",\n    \"templates_dir\": \"$COMPONENTS_DIR/templates\"\n  },\n  \"networks\": {\n    \"nodes\": [\n      {\n        \"node_types_file\": \"$NETWORK_DIR/bio_net_node_types.csv\",\n        \"nodes_file\": \"$NETWORK_DIR/bio_net_nodes.h5\"\n      }\n    ]\n  }\n}\n</pre>"},{"location":"examples/notebooks/single_cell/bmtk_example/#path-resolution","title":"Path Resolution\u00b6","text":"<ol> <li><p>Mechanisms Directory: <code>$COMPONENTS_DIR/mechanisms/modfiles</code></p> <ul> <li>Contains NEURON mechanism files (.mod) compiled into shared libraries</li> <li>Loaded via <code>neuron.load_mechanisms(mechanisms_dir)</code></li> </ul> </li> <li><p>Templates Directory: <code>$COMPONENTS_DIR/templates</code></p> <ul> <li>Contains HOC files defining cell templates (e.g., <code>templates.hoc</code>)</li> <li>All <code>.hoc</code> files in this directory are loaded into NEURON via <code>h.load_file()</code></li> </ul> </li> <li><p>Node Types File: <code>$NETWORK_DIR/bio_net_node_types.csv</code></p> <ul> <li>CSV file with cell type properties</li> <li>Must include a <code>model_template</code> column specifying NEURON template names</li> </ul> </li> </ol>"},{"location":"examples/notebooks/single_cell/bmtk_example/#template-discovery-process","title":"Template Discovery Process\u00b6","text":"<ol> <li><p>Load HOC Templates: All <code>.hoc</code> files in <code>templates_dir</code> are loaded into NEURON, defining template classes like <code>CP_Cell</code>, <code>FSI_Cell</code></p> </li> <li><p>Extract Template Names:</p> <ul> <li>Reads node types from the CSV files specified in <code>circuit_config.json</code></li> <li>Looks for the <code>model_template</code> column in the node type data</li> <li>Collects unique template names (e.g., <code>CP_Cell</code>, <code>FSI_Cell</code>)</li> </ul> </li> <li><p>Populate Dropdown: The interactive runner uses these template names to populate the \"Template\" dropdown menu</p> </li> </ol>"},{"location":"examples/notebooks/single_cell/bmtk_example/#node-types-csv-format","title":"Node Types CSV Format\u00b6","text":"<p>The node types CSV must be space-separated with this structure:</p> <pre><code>node_type_id model_type morphology pop_name model_template\n100 biophysical PN  CP_Cell\n101 biophysical FSI  FSI_Cell\n</code></pre> <ul> <li><code>node_type_id</code>: Unique identifier for the cell type</li> <li><code>model_type</code>: \"biophysical\" for NEURON-based cells</li> <li><code>morphology</code>: Path to morphology file (can be relative)</li> <li><code>pop_name</code>: Population name (optional)</li> <li><code>model_template</code>: Required - NEURON template class name defined in the HOC files</li> </ul>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/","title":"Single Cell Analysis","text":"In\u00a0[15]: Copied! <pre>import os\nimport glob\nimport logging\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom neuron import h\n\nfrom bmtool.singlecell import (\n    Passive, CurrentClamp, ZAP, FI, Profiler, run_and_plot,\n    load_allen_database_cells\n)\n\n%matplotlib inline\n\n# Disable warnings for cleaner output\nlogging.disable(logging.WARNING)\n</pre> import os import glob import logging import numpy as np import matplotlib.pyplot as plt from neuron import h  from bmtool.singlecell import (     Passive, CurrentClamp, ZAP, FI, Profiler, run_and_plot,     load_allen_database_cells )  %matplotlib inline  # Disable warnings for cleaner output logging.disable(logging.WARNING) In\u00a0[16]: Copied! <pre># if already compiled then lets delete the folder and force a recompile\nif os.path.isdir('modfiles/x86_64'):\n    os.system(\"rm -rf modfiles/x86_64\")\nif os.path.isdir('modfiles/arm64'):\n    os.system(\"rm -rf modfiles/arm64\")\n\n# compile the mod files\nif not os.path.isdir(\"modfiles/x86_64\"):\n    os.chdir('modfiles')\n    os.system(\"nrnivmodl\")\n    os.chdir(\"..\")\n</pre> # if already compiled then lets delete the folder and force a recompile if os.path.isdir('modfiles/x86_64'):     os.system(\"rm -rf modfiles/x86_64\") if os.path.isdir('modfiles/arm64'):     os.system(\"rm -rf modfiles/arm64\")  # compile the mod files if not os.path.isdir(\"modfiles/x86_64\"):     os.chdir('modfiles')     os.system(\"nrnivmodl\")     os.chdir(\"..\") <pre>/Users/gregglickert/miniconda3/envs/bmtk/bin/nrnivmodl:10: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import working_set\n</pre> <pre>/usr/bin/xcrun\n/Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles\nMod files: \"./AMPA_NMDA_STP.mod\" \"./CaDynamics.mod\" \"./Ca_HVA.mod\" \"./Ca_LVA.mod\" \"./GABA_A_STP.mod\" \"./Gfluct.mod\" \"./Ih.mod\" \"./Im.mod\" \"./Im_v2.mod\" \"./K_P.mod\" \"./K_T.mod\" \"./Kd.mod\" \"./Kv2like.mod\" \"./Kv3_1.mod\" \"./NaTa.mod\" \"./NaTs.mod\" \"./NaV.mod\" \"./Nap.mod\" \"./SK.mod\" \"./cadad.mod\" \"./cal2.mod\" \"./can_mig.mod\" \"./exp2syn_stp.mod\" \"./gap.mod\" \"./h_kole.mod\" \"./imCA3.mod\" \"./k.mod\" \"./kBK.mod\" \"./kap_BS.mod\" \"./kdmc_BS.mod\" \"./kdrCA3.mod\" \"./kdr_BS.mod\" \"./kdrinter.mod\" \"./leak.mod\" \"./na.mod\" \"./nainter.mod\" \"./napCA3.mod\" \"./natCA3.mod\" \"./nax_BS.mod\" \"./vecevent_coreneuron.mod\"\n\nCreating 'arm64' directory for .o files.\n\n -&gt; Compiling mod_func.cpp\n -&gt; NMODL ../CaDynamics.mod\n -&gt; NMODL ../AMPA_NMDA_STP.mod\n -&gt; NMODL ../Ca_HVA.mod\n -&gt; NMODL ../Ca_LVA.mod\n -&gt; NMODL ../GABA_A_STP.mod\n -&gt; NMODL ../Gfluct.mod\n -&gt; NMODL ../Ih.mod\n -&gt; NMODL ../Im.mod\n -&gt; NMODL ../Im_v2.mod\n -&gt; NMODL ../K_T.mod\n -&gt; NMODL ../Kd.mod\n -&gt; NMODL ../K_P.mod\n -&gt; NMODL ../Kv2like.mod\n -&gt; NMODL ../Kv3_1.mod\n -&gt; NMODL ../NaTa.mod\n -&gt; NMODL ../NaTs.mod\n -&gt; NMODL ../NaV.mod\n -&gt; NMODL ../Nap.mod\n -&gt; NMODL ../SK.mod\n -&gt; NMODL ../cadad.mod\n -&gt; NMODL ../cal2.mod\nWarning: Default 2 of PARAMETER cao will be ignored and set by NEURON.\nWarning: Default 5e-05 of PARAMETER cai will be ignored and set by NEURON.\n -&gt; NMODL ../can_mig.mod\n -&gt; NMODL ../exp2syn_stp.mod\n -&gt; NMODL ../gap.mod\nWarning: Default 2 of PARAMETER cao will be ignored and set by NEURON.\nWarning: Default 5e-05 of PARAMETER cai will be ignored and set by NEURON.\n -&gt; NMODL ../h_kole.mod\n -&gt; NMODL ../imCA3.mod\n -&gt; NMODL ../k.mod\nWarning: Default 6.3 of PARAMETER celsius will be ignored and set by NEURON.\n -&gt; NMODL ../kBK.mod\n -&gt; NMODL ../kap_BS.mod\n -&gt; NMODL ../kdmc_BS.mod\n -&gt; NMODL ../kdrCA3.mod\n -&gt; NMODL ../kdr_BS.mod\n -&gt; NMODL ../kdrinter.mod\nWarning: Default -80 of PARAMETER ek will be ignored and set by NEURON.\n -&gt; NMODL ../na.mod\n -&gt; NMODL ../leak.mod\n -&gt; NMODL ../nainter.mod\nWarning: Default 45 of PARAMETER ena will be ignored and set by NEURON.\nWarning: Default 6.3 of PARAMETER celsius will be ignored and set by NEURON.\n -&gt; NMODL ../natCA3.mod\n -&gt; NMODL ../napCA3.mod\n -&gt; NMODL ../nax_BS.mod\n -&gt; NMODL ../vecevent_coreneuron.mod\n -&gt; Compiling CaDynamics.c\n -&gt; Compiling AMPA_NMDA_STP.c\nNotice: ARTIFICIAL_CELL is a synonym for POINT_PROCESS which hints that it\nonly affects and is affected by discrete events. As such it is not\nlocated in a section and is not associated with an integrator\n -&gt; Compiling Ca_HVA.c\n -&gt; Compiling Ca_LVA.c\n -&gt; Compiling GABA_A_STP.c\n -&gt; Compiling Gfluct.c\n -&gt; Compiling Ih.c\n</pre> <pre>Translating AMPA_NMDA_STP.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/AMPA_NMDA_STP.c\nTranslating Ca_HVA.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/Ca_HVA.c\nTranslating CaDynamics.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/CaDynamics.c\nThread Safe\nThread Safe\nThread Safe\nTranslating Ca_LVA.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/Ca_LVA.c\nTranslating GABA_A_STP.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/GABA_A_STP.c\nTranslating Gfluct.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/Gfluct.c\nThread Safe\nThread Safe\nNotice: This mechanism cannot be used with CVODE\nThread Safe\nTranslating Ih.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/Ih.c\nTranslating Im.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/Im.c\nTranslating Im_v2.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/Im_v2.c\nThread Safe\nThread Safe\nThread Safe\nTranslating K_T.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/K_T.c\nTranslating K_P.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/K_P.c\nTranslating Kd.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/Kd.c\nThread Safe\nThread Safe\nThread Safe\nTranslating Kv2like.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/Kv2like.c\nTranslating Kv3_1.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/Kv3_1.c\nTranslating NaTa.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/NaTa.c\nThread Safe\nThread Safe\nThread Safe\nTranslating NaTs.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/NaTs.c\nTranslating NaV.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/NaV.c\nTranslating Nap.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/Nap.c\nNEURON's CVode method ignores conservation\nThread Safe\nNotice: LINEAR is not thread safe.\nThread Safe\nTranslating cadad.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/cadad.c\nTranslating SK.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/SK.c\nTranslating cal2.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/cal2.c\nThread Safe\nThread Safe\nThread Safe\nTranslating exp2syn_stp.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/exp2syn_stp.c\nTranslating can_mig.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/can_mig.c\nTranslating gap.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/gap.c\nThread Safe\nThread Safe\nThread Safe\nTranslating h_kole.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/h_kole.c\nTranslating imCA3.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/imCA3.c\nTranslating k.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/k.c\nThread Safe\nThread Safe\nThread Safe\nTranslating kBK.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/kBK.c\nTranslating kdmc_BS.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/kdmc_BS.c\nTranslating kap_BS.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/kap_BS.c\nThread Safe\nThread Safe\nThread Safe\nTranslating kdrCA3.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/kdrCA3.c\nTranslating kdr_BS.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/kdr_BS.c\nTranslating kdrinter.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/kdrinter.c\nThread Safe\nThread Safe\nThread Safe\nTranslating leak.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/leak.c\nTranslating nainter.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/nainter.c\nTranslating na.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/na.c\nThread Safe\nThread Safe\nThread Safe\nTranslating nax_BS.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/nax_BS.c\nTranslating napCA3.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/napCA3.c\nTranslating natCA3.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/natCA3.c\nThread Safe\nThread Safe\nThread Safe\nTranslating vecevent_coreneuron.mod into /Users/gregglickert/.Trash/single_cell 16-27-02-218/modfiles/arm64/vecevent_coreneuron.c\nThread Safe\nAMPA_NMDA_STP.cCaDynamics.c::4242::1818::  warning: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n\n   42 |             42e | x        t eerxnt edronu bdloeu b*lgee t*agregt(a)r;g\n(      )| ;                        ^\n\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h::1715:: 17note: :expanded from macro 'getarg' \nnote: expanded from macro 'getarg'\n   15 |    #15d | e#fdienfei ngee tgaertga r gh o ch_ogce_tgaertga\nr      g| \n                ^      \n|                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h49::4916::16 :note:  conflicting prototype is herenote: \nconflicting prototype is here   \n49 | ex   t49e | rextern double*n getarg(int) ;d\no      u|                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15ble* getarg(int);\n      :17: | note: expanded from macro 'getarg'\n               ^\n   /Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h15: | 15#:d17efine ge:t anote: rexpanded from macro 'getarg'g\n  hoc_   g15e | t#adregf\ni      n| e                ^ \ngetarg  hoc_getarg\n      |                 ^\nAMPA_NMDA_STP.c:414:32: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  414 |   if (_tsav &gt; t){ extern char* hoc_object_name(); hoc_execerror(hoc_object_name(_pnt-&gt;ob), \":Event arrived out of order. Must call ParallelContext.set_maxstep AFTER assigning minimum NetCon.delay\");}\n      |                                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:46:14: note: conflicting prototype is here\n   46 | extern char* hoc_object_name(Object*);\n      |              ^\nCa_HVA.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n2 warnings generated.\nCa_LVA.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nGfluct.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nGABA_A_STP.c:42:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   42 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nGABA_A_STP.c:356:32: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  356 |   if (_tsav &gt; t){ extern char* hoc_object_name(); hoc_execerror(hoc_object_name(_pnt-&gt;ob), \":Event arrived out of order. Must call ParallelContext.set_maxstep AFTER assigning minimum NetCon.delay\");}\n      |                                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:46:14: note: conflicting prototype is here\n   46 | extern char* hoc_object_name(Object*);\n      |              ^\nIh.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n2 warnings generated.\nK_T.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nIm.c:/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:43:1849:: 16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\nwarning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          /Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'extern d\noub   le *getarg();\n      | 15 | #defi                        ^\nne getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: Im_v2.c:note: 43expanded from macro 'getarg'\n:18:    15 | #define getwarning: ara function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]g\n  hoc_getarg\n      |                 ^\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nK_P.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\nKv3_1.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nKd.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nNaTa.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #defineKv2like.c:43:18: warning:  a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]g\netarg  hoc_getarg\n      |                 ^\n   43 | /Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49         :e16xte:r nnote: conflicting prototype is here\n   49 | extern do uble* getargd(iont);\nuble *getarg();      |                ^\n\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getar/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.hg:15:17: note: expanded from macro 'getarg'\n   15  hoc_getarg\n |       #| d                ^e\nfine getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\nNaTs.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extNaV.cern doub:l43e: 18*:g ewarning: ta function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]a\nrg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n      4315 |  |         # deexftienren double  ge*tgetarg(a)r;g\n        | h                        ^o\nc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h   :1549 | :16: note: conflicting prototype is here\n#define ge   t49a | externr gd  hoc_goetuarble* getag\n      r| g                ^\n(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getar/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: g  note: hconflicting prototype is hereo\nc_getarg\n      |                 ^\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nNap.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nSK.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nNaV.c:404:17: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  404 |  extern double *_getelm();\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/nrniv_mf.h:26:16: note: conflicting prototype is here\n   26 | extern double* _getelm(int, int);\n      |                ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\ncal2.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\ncan_mig.c:43:18: /Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.hwarning: :a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]15\n:17: note: expanded from macro 'getarg'\n   cadad.c:42:1815: |  #warning: da function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]e\n   43 |             extefrinn ed o42u | b        l eegxettear n* gdeotragr g (u)b;l\ne       | *                        ^g\nhoc_geteatragr\ng      (| )                ^;\n\n      | /Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17:                        ^ \nnote: expanded from macro 'getarg'\n   15 | #de/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h\n:15:   1749: |  enote: xexpanded from macro 'getarg't\nern    d15o | u#bdleeff*i ngeetarg(i ngte)t;a\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: inote: expanded from macro 'getarg'\nne   r getg15a   | h#rdefine go c _hoc_ggetarg\n      | etargge                 ^ thaoc\nrg\n      |                 ^\n_/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.hg:etarg\n      | 49/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:                ^:49:16: \n16: note: conflicting prototype is here\nnote: conflicting prototype is here\n   49 | exte   49 | exrtern nd oduble*o ugbetargl(ei*n tgetarg);\n      |                ^\n(int/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h);:\n15      | :               ^17:\n note: expanded from macro 'getarg'\n   15 | #de/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.hf:15i:n17e:  note: gexpanded from macro 'getarg'\netarg    15 | #de fhine geotc_geatarg\n      |                 ^\nrg  hoc_getarg\n      |                 ^\n1 warning generated.\n</pre> <pre> -&gt; Compiling Im.c\n -&gt; Compiling Im_v2.c\n -&gt; Compiling K_P.c\n -&gt; Compiling K_T.c\n -&gt; Compiling Kv2like.c\n -&gt; Compiling Kd.c\n -&gt; Compiling Kv3_1.c\n -&gt; Compiling NaTa.c\n -&gt; Compiling NaV.c\n -&gt; Compiling NaTs.c\n -&gt; Compiling Nap.c\n -&gt; Compiling SK.c\n -&gt; Compiling can_mig.c\n -&gt; Compiling cadad.c\n -&gt; Compiling cal2.c\n -&gt; Compiling exp2syn_stp.c\n -&gt; Compiling gap.c\n -&gt; Compiling h_kole.c\n -&gt; Compiling imCA3.c\n</pre> <pre>1 warning generated.\n1 warning generated.\n2 warnings generated.\nexp2syn_stp.c:42:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   42 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nexp2syn_stp.c:343:32: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  343 |   if (_tsav &gt; t){ extern char* hoc_object_name(); hoc_execerror(hoc_object_name(_pnt-&gt;ob), \":Event arrived out of order. Must call ParallelContext.set_maxstep AFTER assigning minimum NetCon.delay\");}\n      |                                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:46:14: note: conflicting prototype is here\n   46 | extern char* hoc_object_name(Object*);\n      |              ^\nh_kole.c:42:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   42 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\ngap.c:41:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   41 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nimCA3.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n2 warnings generated.\n1 warning generated.\n1 warning generated.\nkBK.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();k.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:\n49:16      :|                         ^ \nnote: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nkap_BS.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nkdmc_BS.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\nkdrCA3.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nkdr_BS.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nkdrinter.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nleak.c:41:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   41 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\nnatCA3.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\nnainter.c/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | :#def43ine get:a18r:g   warning: ha function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]na.c\n:oc_getar43g:\n18      :|                  ^warning: \na function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |         /Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h :e49x:t16e:r nnote:  conflicting prototype is hered\nouble       4943 |  |         *extergne double* getargt(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: argnote: expanded from macro 'getarg'\n   () ;\nnapCA3.c15      |                         ^e | :#xd44\n:t18:/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h :warning: er15n: 17d:o unote: blea function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype] *g\nexpanded from macro 'getarg'e\ne   t15a | r#gd(efine get   fine gearg  44 | )h;\notca_rgge t ahro              cg_\ngetarg      \n|                         ^\n extern|  d                ^o\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.hu:bl49e:16 :* genote: tconflicting prototype is here\n   49 | extern douablrg();/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: \n      |                         ^\ne* getarg(int);\n      note: |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15expanded from macro 'getarg':17/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:: note: expanded from macro 'getarg'\n   1515 | :#17d:e fnote: iexpanded from macro 'getarg'n\ne\n ge      15 | #d15tefine a | rg#  getargdef i hhoc_nogce_targ\n      |                 ^\ngetarg\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49e      |                 ^ \ngetarg  ho | ex/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.hterc:49:16: note: conflicting prototype is here\n   49 | e_getanr double* ggetarg(int);\n      | x               ^t\nern do/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.hu:\nb      15:17: note: expanded from macro 'getarg'l\n   15 | #define getarg  hoc_getarg\n      |                 ^\ne* ge| targ(in                ^t\n);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n</pre> <pre> -&gt; Compiling k.c\n -&gt; Compiling kBK.c\n -&gt; Compiling kap_BS.c\n -&gt; Compiling kdmc_BS.c\n -&gt; Compiling kdrCA3.c\n -&gt; Compiling kdr_BS.c\n -&gt; Compiling kdrinter.c\n -&gt; Compiling leak.c\n -&gt; Compiling na.c\n -&gt; Compiling nainter.c\n -&gt; Compiling napCA3.c\n -&gt; Compiling natCA3.c\n -&gt; Compiling nax_BS.c\n -&gt; Compiling vecevent_coreneuron.c\n =&gt; LINKING shared library ./libnrnmech.dylib\n</pre> <pre>nax_BS.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\nvecevent_coreneuron.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n\n   15 | #def   i43n | e          geexttaerrgn   dhooucb_lgee t*agregt\na      r| g                ^(\n);\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   /Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h49: | 15e:x17t:e rnote: nexpanded from macro 'getarg' \ndou   15 | #deble* gfeitnaer gg(eitnatr)g; \n       h| o               ^c\n_ge/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17t:a rnote: gexpanded from macro 'getarg'\n\n      |                 ^   \n15 | #define getarg  hoc_ge/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.ht:a49r:g16\n:       | note:                 ^conflicting prototype is here\n\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nvecevent_coreneuron.c:242:32: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  242 |   if (_tsav &gt; t){ extern char* hoc_object_name(); hoc_execerror(hoc_object_name(_pnt-&gt;ob), \":Event arrived out of order. Must call ParallelContext.set_maxstep AFTER assigning minimum NetCon.delay\");}\n      |                                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:46:14: note: conflicting prototype is here\n   46 | extern char* hoc_object_name(Object*);\n      |              ^\n2 warnings generated.\n1 warning generated.\n</pre> <pre> =&gt; LINKING executable ./special LDFLAGS are:    \nSuccessfully created arm64/special\n</pre> <pre>ld: warning: ignoring duplicate libraries: '-lnrnmech'\n</pre> In\u00a0[17]: Copied! <pre># Method 1: Allen Database Cell\n\n# this downloads from allen database which is already done\n# from allensdk.api.queries.biophysical_api import BiophysicalApi\n\n# # Download cell model from Allen\n# bp = BiophysicalApi()\n# bp.cache_stimulus = False\n# neuronal_model_id = 472451419  # Example: Layer 5 Pyramidal Cell\n# bp.cache_data(neuronal_model_id)\n    \ndynamic_params = \"386049446_fit.json\"\nmorphology = \"Nr5a1-Cre_Ai14-177334.05.01.01_491459171_m.swc\"\n\n# Create Allen cell loader\ncell1 = load_allen_database_cells(\n    morphology=morphology,\n    dynamic_params=dynamic_params,\n    model_processing='aibs_allactive'\n)\n</pre> # Method 1: Allen Database Cell  # this downloads from allen database which is already done # from allensdk.api.queries.biophysical_api import BiophysicalApi  # # Download cell model from Allen # bp = BiophysicalApi() # bp.cache_stimulus = False # neuronal_model_id = 472451419  # Example: Layer 5 Pyramidal Cell # bp.cache_data(neuronal_model_id)      dynamic_params = \"386049446_fit.json\" morphology = \"Nr5a1-Cre_Ai14-177334.05.01.01_491459171_m.swc\"  # Create Allen cell loader cell1 = load_allen_database_cells(     morphology=morphology,     dynamic_params=dynamic_params,     model_processing='aibs_allactive' )  In\u00a0[18]: Copied! <pre>profiler = Profiler(template_dir='.', mechanism_dir='modfiles', dt=0.05)\n</pre> profiler = Profiler(template_dir='.', mechanism_dir='modfiles', dt=0.05) <pre>Mechanisms already loaded from path: modfiles.  Aborting.\n</pre> <p>It is important to note the order in which we are doing things for loading the Allen database cell. You need to first load the cell, but do not call the class. Then load the mod files(here done with the profiler to also get our template setup for the cell2) and then initalize cell1.</p> In\u00a0[19]: Copied! <pre>cell1 = cell1()\n</pre> cell1 = cell1() In\u00a0[20]: Copied! <pre># Method 2: NEURON HOC Template\n# The template is loaded by passing the template name as a string\ncell2 = 'CP_Cell'  # Change this to match your template name\n</pre> # Method 2: NEURON HOC Template # The template is loaded by passing the template name as a string cell2 = 'CP_Cell'  # Change this to match your template name In\u00a0[21]: Copied! <pre># Method 3: Python Class-Based Cell\n\nh.load_file(\"stdrun.hoc\")\n\nclass SimpleSoma(object):\n    \"\"\"Single-section cell: A soma with leak, sodium, and potassium channels.\"\"\"\n    def __init__(self):\n        self.create_sections()\n        self.build_topology()\n        self.build_subsets()\n        self.define_geometry()\n        self.define_biophysics()\n    #\n    def create_sections(self):\n        \"\"\"Create the sections of the cell.\"\"\"\n        # NOTE: cell=self is required to tell NEURON of this object.\n        self.soma = h.Section(name='soma', cell=self)\n    #\n    def build_topology(self):\n        \"\"\"Connect the sections of the cell to build a tree.\"\"\"\n        # Single section - no connections needed\n        pass\n    #\n    def define_geometry(self):\n        \"\"\"Set the 3D geometry of the cell.\"\"\"\n        self.soma.L = 500     # soma length um\n        self.soma.diam = 500  # soma diameter um\n        h.define_shape()      # Translate into 3D points.\n    #\n    def define_biophysics(self):\n        \"\"\"Assign the membrane properties across the cell.\"\"\"\n        for sec in self.all: # 'all' defined in build_subsets\n            sec.cm = 1       # membrane capacitance uF/cm2\n            sec.Ra = 30      # ohm-cm\n        \n        # Insert leak channel\n        self.soma.insert('leak')  # Leak channel\n        self.soma.glbar_leak = 0.0003  # S/cm2\n        self.soma.el_leak = -60  # mV\n        \n        # Insert sodium channel\n        self.soma.insert('na')  # Sodium channel\n        self.soma.gnabar_na = 0.22  # S/cm2\n        self.soma.nao = 79.8  # extracellular Na concentration\n        \n        # Insert potassium channel\n        self.soma.insert('k')  # Potassium channel\n        self.soma.gkbar_k = 0.036  # S/cm2\n        self.soma.ki = 69.35  # intracellular K concentration\n    #\n    def build_subsets(self):\n        \"\"\"Build subset lists. For now we define 'all'.\"\"\"\n        self.all = h.SectionList()\n        self.all.wholetree(sec=self.soma)\n\n\ncell3 = SimpleSoma()\n</pre> # Method 3: Python Class-Based Cell  h.load_file(\"stdrun.hoc\")  class SimpleSoma(object):     \"\"\"Single-section cell: A soma with leak, sodium, and potassium channels.\"\"\"     def __init__(self):         self.create_sections()         self.build_topology()         self.build_subsets()         self.define_geometry()         self.define_biophysics()     #     def create_sections(self):         \"\"\"Create the sections of the cell.\"\"\"         # NOTE: cell=self is required to tell NEURON of this object.         self.soma = h.Section(name='soma', cell=self)     #     def build_topology(self):         \"\"\"Connect the sections of the cell to build a tree.\"\"\"         # Single section - no connections needed         pass     #     def define_geometry(self):         \"\"\"Set the 3D geometry of the cell.\"\"\"         self.soma.L = 500     # soma length um         self.soma.diam = 500  # soma diameter um         h.define_shape()      # Translate into 3D points.     #     def define_biophysics(self):         \"\"\"Assign the membrane properties across the cell.\"\"\"         for sec in self.all: # 'all' defined in build_subsets             sec.cm = 1       # membrane capacitance uF/cm2             sec.Ra = 30      # ohm-cm                  # Insert leak channel         self.soma.insert('leak')  # Leak channel         self.soma.glbar_leak = 0.0003  # S/cm2         self.soma.el_leak = -60  # mV                  # Insert sodium channel         self.soma.insert('na')  # Sodium channel         self.soma.gnabar_na = 0.22  # S/cm2         self.soma.nao = 79.8  # extracellular Na concentration                  # Insert potassium channel         self.soma.insert('k')  # Potassium channel         self.soma.gkbar_k = 0.036  # S/cm2         self.soma.ki = 69.35  # intracellular K concentration     #     def build_subsets(self):         \"\"\"Build subset lists. For now we define 'all'.\"\"\"         self.all = h.SectionList()         self.all.wholetree(sec=self.soma)   cell3 = SimpleSoma() In\u00a0[22]: Copied! <pre># Analysis settings dictionary\n# These parameters can be tuned based on your specific cell type and experimental constraints\n\nanalysis_settings = {\n    'Passive': {\n        'celsius': 26.0,\n        'kwargs': {\n            'inj_amp': -50.0,\n            'inj_delay': 1500.0,\n            'inj_dur': 1000.0,\n            'tstop': 2500.0,\n            'method': 'exp2'  # options: 'exp2', 'exp', 'simple'\n        }\n    },\n    'CurrentClamp': {\n        'celsius': 34.0,\n        'kwargs': {\n            'inj_amp': 130.0,\n            'inj_delay': 1500.0,\n            'inj_dur': 1000.0,\n            'tstop': 3000.0,\n            'threshold': -15.0\n        }\n    },\n    'ZAP': {\n        'celsius': 34.0,\n        'kwargs': {\n            'inj_amp': 100.0,\n            'inj_delay': 1000.0,\n            'inj_dur': 15000.0,\n            'tstop': 15500.0,\n            'fstart': 0.0,\n            'fend': 15.0,\n            'chirp_type': 'linear'\n        }\n    },\n    'FI': {\n        'celsius': 34.0,\n        'kwargs': {\n            'i_start': 0.0,\n            'i_stop': 2000.0,\n            'i_increment': 100.0,\n            'tstart': 1500.0\n        }\n    }\n}\n</pre> # Analysis settings dictionary # These parameters can be tuned based on your specific cell type and experimental constraints  analysis_settings = {     'Passive': {         'celsius': 26.0,         'kwargs': {             'inj_amp': -50.0,             'inj_delay': 1500.0,             'inj_dur': 1000.0,             'tstop': 2500.0,             'method': 'exp2'  # options: 'exp2', 'exp', 'simple'         }     },     'CurrentClamp': {         'celsius': 34.0,         'kwargs': {             'inj_amp': 130.0,             'inj_delay': 1500.0,             'inj_dur': 1000.0,             'tstop': 3000.0,             'threshold': -15.0         }     },     'ZAP': {         'celsius': 34.0,         'kwargs': {             'inj_amp': 100.0,             'inj_delay': 1000.0,             'inj_dur': 15000.0,             'tstop': 15500.0,             'fstart': 0.0,             'fend': 15.0,             'chirp_type': 'linear'         }     },     'FI': {         'celsius': 34.0,         'kwargs': {             'i_start': 0.0,             'i_stop': 2000.0,             'i_increment': 100.0,             'tstart': 1500.0         }     } } In\u00a0[23]: Copied! <pre>def find_slope(X, Y):\n    \"\"\"Find the slope and intercept using least squares regression.\"\"\"\n    A = np.vstack([X, np.ones_like(X)]).T\n    m, c = np.linalg.lstsq(A, Y, rcond=None)[0]\n    return m, c\n\n\ndef fit_slope_to_max_y(X, Y):\n    \"\"\"Fit a line from first spike to maximum spike frequency.\n    \n    This captures the linear region of the FI curve where the relationship\n    is most pronounced.\n    \"\"\"\n    # Find where Y &gt; 0\n    positive_data = Y &gt; 0\n    X_positive = X[positive_data]\n    Y_positive = Y[positive_data]\n    \n    if len(Y_positive) == 0:\n        return None, None, X, Y\n    \n    # Find the index of the maximum Y value\n    max_idx = np.argmax(Y_positive)\n    \n    # Only use data from first spike to max\n    X_fit = X_positive[:max_idx + 1]\n    Y_fit = Y_positive[:max_idx + 1]\n    \n    m, c = find_slope(X_fit, Y_fit)\n    return m, c, X_fit, Y_fit\n\n\ndef find_rheobase(X, Y):\n    \"\"\"Find the rheobase: minimum current required to evoke a spike.\"\"\"\n    non_zero_indices = np.nonzero(Y)[0]\n    if len(non_zero_indices) &gt; 0:\n        rheobase = X[non_zero_indices[0]]\n        return rheobase\n    return None\n</pre> def find_slope(X, Y):     \"\"\"Find the slope and intercept using least squares regression.\"\"\"     A = np.vstack([X, np.ones_like(X)]).T     m, c = np.linalg.lstsq(A, Y, rcond=None)[0]     return m, c   def fit_slope_to_max_y(X, Y):     \"\"\"Fit a line from first spike to maximum spike frequency.          This captures the linear region of the FI curve where the relationship     is most pronounced.     \"\"\"     # Find where Y &gt; 0     positive_data = Y &gt; 0     X_positive = X[positive_data]     Y_positive = Y[positive_data]          if len(Y_positive) == 0:         return None, None, X, Y          # Find the index of the maximum Y value     max_idx = np.argmax(Y_positive)          # Only use data from first spike to max     X_fit = X_positive[:max_idx + 1]     Y_fit = Y_positive[:max_idx + 1]          m, c = find_slope(X_fit, Y_fit)     return m, c, X_fit, Y_fit   def find_rheobase(X, Y):     \"\"\"Find the rheobase: minimum current required to evoke a spike.\"\"\"     non_zero_indices = np.nonzero(Y)[0]     if len(non_zero_indices) &gt; 0:         rheobase = X[non_zero_indices[0]]         return rheobase     return None  In\u00a0[24]: Copied! <pre>proc_settings = analysis_settings['Passive']\nh.celsius = proc_settings['celsius']\n\nsim_passive = Passive(cell1, **proc_settings['kwargs'])\n\nX_passive, Y_passive = run_and_plot(\n    sim_passive,\n    title='Passive Membrane Properties',\n    xlabel='Time (ms)',\n    ylabel='Membrane Potential (mV)',\n    plot_injection_only=True\n)\n\n# Add exponential fit\nif sim_passive.method == 'exp2':\n    plt.gca().plot(*sim_passive.double_exponential_fit(), 'r:', linewidth=2, label='Double exponential fit')\n    plt.legend()\nelif sim_passive.method == 'exp':\n    plt.gca().plot(*sim_passive.single_exponential_fit(), 'r:', linewidth=2, label='Single exponential fit')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n</pre> proc_settings = analysis_settings['Passive'] h.celsius = proc_settings['celsius']  sim_passive = Passive(cell1, **proc_settings['kwargs'])  X_passive, Y_passive = run_and_plot(     sim_passive,     title='Passive Membrane Properties',     xlabel='Time (ms)',     ylabel='Membrane Potential (mV)',     plot_injection_only=True )  # Add exponential fit if sim_passive.method == 'exp2':     plt.gca().plot(*sim_passive.double_exponential_fit(), 'r:', linewidth=2, label='Double exponential fit')     plt.legend() elif sim_passive.method == 'exp':     plt.gca().plot(*sim_passive.single_exponential_fit(), 'r:', linewidth=2, label='Single exponential fit')     plt.legend()  plt.tight_layout() plt.show() <pre>Injection location: Biophys1[1].soma[0](0.5)\nRecording: Biophys1[1].soma[0](0.5)._ref_v\nRunning simulation for passive properties...\n\nV Rest: -99.70 (mV)\nResistance: 362.19 (MOhms)\nMembrane time constant: 54.19 (ms)\n\nV_rest Calculation: Voltage taken at time 1500.0 (ms) is\n-99.70 (mV)\n\nR_in Calculation: dV/dI = (v_final-v_rest)/(i_final-i_start)\n(-117.81 - (-99.70)) / (-0.05 - 0)\n18.11 (mV) / 0.05 (nA) = 362.19 (MOhms)\n\nTau Calculation: Fit a double exponential curve to the membrane potential response\nf(t) = a0 + a1*exp(-t/tau1) + a2*exp(-t/tau2)\nConstrained by initial value: f(0) = a0 + a1 + a2 = v_rest\nFit parameters: (a0, a1, a2, tau1, tau2) = (-117.80, 14.49, 3.61, 54.19, 2.77)\nMembrane time constant is determined from the slowest exponential term: 54.19 (ms)\n\nSag potential: v_sag = v_peak - v_final = 0.00 (mV)\nNormalized sag potential: v_sag / (v_peak - v_rest) = -0.000\n\n</pre> In\u00a0[25]: Copied! <pre>proc_settings = analysis_settings['CurrentClamp']\nh.celsius = proc_settings['celsius']\n\nsim_cc = CurrentClamp(cell3, **proc_settings['kwargs'])\n\nX_cc, Y_cc = run_and_plot(\n    sim_cc,\n    title='Current Clamp Response',\n    xlabel='Time (ms)',\n    ylabel='Membrane Potential (mV)',\n    plot_injection_only=True\n)\n\nplt.tight_layout()\nplt.show()\n</pre> proc_settings = analysis_settings['CurrentClamp'] h.celsius = proc_settings['celsius']  sim_cc = CurrentClamp(cell3, **proc_settings['kwargs'])  X_cc, Y_cc = run_and_plot(     sim_cc,     title='Current Clamp Response',     xlabel='Time (ms)',     ylabel='Membrane Potential (mV)',     plot_injection_only=True )  plt.tight_layout() plt.show() <pre>Injection location: &lt;__main__.SimpleSoma object at 0x14d5cc510&gt;.soma(0.5)\nRecording: &lt;__main__.SimpleSoma object at 0x14d5cc510&gt;.soma(0.5)._ref_v\nCurrent clamp simulation running...\n\nNumber of spikes: 179\n\n</pre> In\u00a0[26]: Copied! <pre>proc_settings = analysis_settings['ZAP']\nh.celsius = proc_settings['celsius']\n\nsim_zap = ZAP(cell2, **proc_settings['kwargs'])\n\n# Plot voltage response\nX_zap, Y_zap = run_and_plot(\n    sim_zap,\n    title='ZAP Voltage Response',\n    xlabel='Time (ms)',\n    ylabel='Membrane Potential (mV)',\n    plot_injection_only=True\n)\nplt.tight_layout()\nplt.show()\n\n# Plot ZAP current stimulus\nplt.figure(figsize=(10, 4))\nplt.plot(X_zap, sim_zap.zap_vec)\nplt.title('ZAP Current Stimulus (Linear Chirp)')\nplt.xlabel('Time (ms)')\nplt.ylabel('Current Injection (nA)')\nplt.tight_layout()\nplt.show()\n\n# Plot impedance profile\nplt.figure(figsize=(10, 4))\nfreq, imp = sim_zap.get_impedance(smooth=9)\nplt.plot(freq, imp, linewidth=2)\nplt.title('Impedance Amplitude Profile')\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Impedance (MOhms)')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Find resonant frequency\nif len(freq) &gt; 0:\n    max_imp_idx = np.argmax(imp)\n    resonant_freq = freq[max_imp_idx]\n    max_imp = imp[max_imp_idx]\n    print(f\"Resonant frequency: {resonant_freq:.2f} Hz\")\n    print(f\"Maximum impedance: {max_imp:.2f} MOhms\")\n</pre> proc_settings = analysis_settings['ZAP'] h.celsius = proc_settings['celsius']  sim_zap = ZAP(cell2, **proc_settings['kwargs'])  # Plot voltage response X_zap, Y_zap = run_and_plot(     sim_zap,     title='ZAP Voltage Response',     xlabel='Time (ms)',     ylabel='Membrane Potential (mV)',     plot_injection_only=True ) plt.tight_layout() plt.show()  # Plot ZAP current stimulus plt.figure(figsize=(10, 4)) plt.plot(X_zap, sim_zap.zap_vec) plt.title('ZAP Current Stimulus (Linear Chirp)') plt.xlabel('Time (ms)') plt.ylabel('Current Injection (nA)') plt.tight_layout() plt.show()  # Plot impedance profile plt.figure(figsize=(10, 4)) freq, imp = sim_zap.get_impedance(smooth=9) plt.plot(freq, imp, linewidth=2) plt.title('Impedance Amplitude Profile') plt.xlabel('Frequency (Hz)') plt.ylabel('Impedance (MOhms)') plt.grid(True, alpha=0.3) plt.tight_layout() plt.show()  # Find resonant frequency if len(freq) &gt; 0:     max_imp_idx = np.argmax(imp)     resonant_freq = freq[max_imp_idx]     max_imp = imp[max_imp_idx]     print(f\"Resonant frequency: {resonant_freq:.2f} Hz\")     print(f\"Maximum impedance: {max_imp:.2f} MOhms\") <pre>Injection location: CP_Cell[2].soma[0](0.5)\nRecording: CP_Cell[2].soma[0](0.5)._ref_v\nZAP current simulation running...\n\nChirp current injection with frequency changing from 0 to 15 Hz over 15 seconds\nImpedance is calculated as the ratio of FFT amplitude of membrane voltage to FFT amplitude of chirp current\n\n</pre> <pre>Resonant Peak Frequency: 0.333 (Hz)\n</pre> <pre>Resonant frequency: 0.33 Hz\nMaximum impedance: 84.45 MOhms\n</pre> In\u00a0[27]: Copied! <pre>proc_settings = analysis_settings['FI']\nh.celsius = proc_settings['celsius']\n\nsim_fi = FI(cell2, **proc_settings['kwargs'])\n\nX_fi, Y_fi = run_and_plot(\n    sim_fi,\n    title='Frequency-Intensity Curve',\n    xlabel='Injection Current (nA)',\n    ylabel='Spike Frequency (Hz)'\n)\nplt.tight_layout()\nplt.show()\n</pre> proc_settings = analysis_settings['FI'] h.celsius = proc_settings['celsius']  sim_fi = FI(cell2, **proc_settings['kwargs'])  X_fi, Y_fi = run_and_plot(     sim_fi,     title='Frequency-Intensity Curve',     xlabel='Injection Current (nA)',     ylabel='Spike Frequency (Hz)' ) plt.tight_layout() plt.show() <pre>Injection location: CP_Cell[3].soma[0](0.5)\nRecording: CP_Cell[3].soma[0](0.5)._ref_v\nRunning simulations for FI curve...\n\nResults\n    Injection (pA):  number of spikes\n0               0.0                 0\n1             100.0                 0\n2             200.0                15\n3             300.0                23\n4             400.0                31\n5             500.0                40\n6             600.0                49\n7             700.0                59\n8             800.0                70\n9             900.0                 4\n10           1000.0                 2\n11           1100.0                 2\n12           1200.0                 2\n13           1300.0                 2\n14           1400.0                 2\n15           1500.0                 1\n16           1600.0                 1\n17           1700.0                 1\n18           1800.0                 1\n19           1900.0                 1\n\n</pre> In\u00a0[28]: Copied! <pre># Fit linear portion of FI curve\nm, c, X_fit, Y_fit = fit_slope_to_max_y(X_fi, Y_fi)\n\n# Find rheobase\nrheobase = find_rheobase(X_fi, Y_fi)\nprint(f\"  Rheobase (minimum current for firing): {rheobase:.1f} pA\")\n</pre> # Fit linear portion of FI curve m, c, X_fit, Y_fit = fit_slope_to_max_y(X_fi, Y_fi)  # Find rheobase rheobase = find_rheobase(X_fi, Y_fi) print(f\"  Rheobase (minimum current for firing): {rheobase:.1f} pA\") <pre>  Rheobase (minimum current for firing): 200.0 pA\n</pre>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#comprehensive-single-cell-analysis-with-bmtool","title":"Comprehensive Single Cell Analysis with bmtool\u00b6","text":"<p>By Gregory Glickert</p> <p>This tutorial demonstrates multiple methods to load neurons into bmtool's single cell analysis module, and shows how to apply various electrophysiological analysis techniques to any loaded cell model. We'll cover:</p> <ol> <li>Cell Loading Methods: Allen Database, NEURON HOC templates, and Python class-based models</li> <li>Analysis Techniques: Passive properties, Current clamp, Impedance (ZAP), and Frequency-Intensity (FI) curves</li> </ol> <p>This notebook demonstrates that all analysis methods work consistently regardless of how the cell was loaded.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#part-1-setup-and-compilation","title":"Part 1: Setup and Compilation\u00b6","text":"<p>First, we compile the MOD files and set up the analysis environment. This section applies to all cell loading methods.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#part-2-cell-loading-methods","title":"Part 2: Cell Loading Methods\u00b6","text":"<p>bmtool supports three different approaches to load neuron models. Each method returns a cell creator that can be used with any analysis function.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#21-loading-allen-database-cells","title":"2.1: Loading Allen Database Cells\u00b6","text":"<p>Download and load a realistic neuron model from the Allen Institute Brain Observatory.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#22-loading-neuron-hoc-template-cells","title":"2.2: Loading NEURON HOC Template Cells\u00b6","text":"<p>Use pre-defined HOC template files for standard neuronal models.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#23-loading-python-class-based-cells","title":"2.3: Loading Python Class-Based Cells\u00b6","text":"<p>Define and use custom neuron models programmatically using Python classes.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#part-3-analysis-methods","title":"Part 3: Analysis Methods\u00b6","text":"<p>Now we demonstrate all available analysis techniques. These methods work with any of the cell loading approaches above.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#setup-analysis-parameters","title":"Setup Analysis Parameters\u00b6","text":"<p>Define comprehensive settings for each analysis method.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#helper-functions-for-fi-curve-analysis","title":"Helper Functions for FI Curve Analysis\u00b6","text":"<p>Define utility functions for analyzing frequency-current relationships.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#31-passive-properties-analysis","title":"3.1: Passive Properties Analysis\u00b6","text":"<p>Analyze membrane properties under hyperpolarizing current injection.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#32-current-clamp-analysis","title":"3.2: Current Clamp Analysis\u00b6","text":"<p>Measure voltage response to current injection - fundamental for understanding cell excitability.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#33-impedance-amplitude-profile-zap","title":"3.3: Impedance Amplitude Profile (ZAP)\u00b6","text":"<p>Analyze frequency response using a chirp stimulus to measure impedance at different frequencies.</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#34-frequency-intensity-fi-curve-analysis","title":"3.4: Frequency-Intensity (FI) Curve Analysis\u00b6","text":"<p>Measure the relationship between injected current and spike frequency</p>"},{"location":"examples/notebooks/single_cell/single_cell_analysis/#fi-curve-post-processing-and-analysis","title":"FI Curve Post-Processing and Analysis\u00b6","text":""},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/","title":"Gap Junction Tuner","text":"<p>First we will compile the modfiles</p> In\u00a0[1]: Copied! <pre>import os\n\n# if already compiled then lets delete the folder and force a recompile\nif os.path.isdir('modfiles/x86_64'):\n    os.system(\"rm -rf modfiles/x86_64 \")\n# compile the mod files\nif not os.path.isdir(\"modfiles/x86_64\"):\n    os.chdir('modfiles')\n    os.system(\"nrnivmodl\")\n    os.chdir(\"..\")\n</pre> import os  # if already compiled then lets delete the folder and force a recompile if os.path.isdir('modfiles/x86_64'):     os.system(\"rm -rf modfiles/x86_64 \") # compile the mod files if not os.path.isdir(\"modfiles/x86_64\"):     os.chdir('modfiles')     os.system(\"nrnivmodl\")     os.chdir(\"..\") <pre>/Users/gregglickert/miniconda3/envs/bmtk/bin/nrnivmodl:10: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import working_set\n</pre> <pre>/usr/bin/xcrun\n/Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/gap_junction_tuner/modfiles\nMod files: \"./AMPA_NMDA_STP.mod\" \"./GABA_A_STP.mod\" \"./Gfluct.mod\" \"./cadad.mod\" \"./cal2.mod\" \"./can_mig.mod\" \"./exp2syn_stp.mod\" \"./gap.mod\" \"./h_kole.mod\" \"./imCA3.mod\" \"./kBK.mod\" \"./kap_BS.mod\" \"./kdmc_BS.mod\" \"./kdrCA3.mod\" \"./kdr_BS.mod\" \"./kdrinter.mod\" \"./leak.mod\" \"./nainter.mod\" \"./napCA3.mod\" \"./natCA3.mod\" \"./nax_BS.mod\" \"./vecevent_coreneuron.mod\"\n\n -&gt; Compiling mod_func.cpp\n =&gt; LINKING shared library ./libnrnmech.dylib\nSuccessfully created arm64/special\n</pre> In\u00a0[2]: Copied! <pre>from bmtool.synapses import GapJunctionTuner\n\n%matplotlib inline\n\nmechanisms_dir = 'modfiles'\ntemplates_file = 'templates.hoc'\n\ngeneral_settings = {\n    'tstart': 500., # when the current clamp starts\n    'tdur': 500.,    # Dur of the current clamp\n    'dt': 0.025, # simulation dt\n    'celsius': 20 # temp of sim\n}\n\nconn_type_settings = {\n    'FSI_gap': {\n        'cell': 'FSI_Cell', #template name \n        'iclamp_amp' : -0.01, # nA what amp to stim cell at\n        'sec_x': 0.5, # location of syn\n        'sec_id': 0,  # location of syn\n        \"level_of_detail\": \"gap\", # which synaptic mech to use\n    }\n}\n\n# Pass the full conn_type_settings dictionary instead of just the FSI_gap part\ntuner = GapJunctionTuner(mechanisms_dir=mechanisms_dir,templates_dir=templates_file,\n                         general_settings=general_settings,conn_type_settings=conn_type_settings)\n</pre> from bmtool.synapses import GapJunctionTuner  %matplotlib inline  mechanisms_dir = 'modfiles' templates_file = 'templates.hoc'  general_settings = {     'tstart': 500., # when the current clamp starts     'tdur': 500.,    # Dur of the current clamp     'dt': 0.025, # simulation dt     'celsius': 20 # temp of sim }  conn_type_settings = {     'FSI_gap': {         'cell': 'FSI_Cell', #template name          'iclamp_amp' : -0.01, # nA what amp to stim cell at         'sec_x': 0.5, # location of syn         'sec_id': 0,  # location of syn         \"level_of_detail\": \"gap\", # which synaptic mech to use     } }  # Pass the full conn_type_settings dictionary instead of just the FSI_gap part tuner = GapJunctionTuner(mechanisms_dir=mechanisms_dir,templates_dir=templates_file,                          general_settings=general_settings,conn_type_settings=conn_type_settings) <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> <pre>{'cell': 'FSI_Cell', 'iclamp_amp': -0.01, 'sec_x': 0.5, 'sec_id': 0, 'level_of_detail': 'gap'}\n</pre> <p>We can then manually tune using the run.model method</p> In\u00a0[\u00a0]: Copied! <pre>tuner.InteractiveTuner()\n</pre> tuner.InteractiveTuner() <pre>VBox(children=(HBox(children=(Dropdown(description='Connection:', options=('FSI_gap',), style=DescriptionStyle\u2026</pre> <pre>Output()</pre> In\u00a0[4]: Copied! <pre>from bmtool.synapses import GapJunctionOptimizer\n\n# Create optimizer\noptimizer = GapJunctionOptimizer(tuner)\n\n# Optimize for a target coupling coefficient\ntarget_cc = 0.37  # Example target coupling coefficient\nresult = optimizer.optimize_resistance(\n    target_cc=target_cc,\n    resistance_bounds=(1e-4, 1e-2)  # Min and max resistance values to try\n)\n\n# Plot optimization results\noptimizer.plot_optimization_results(result)\n</pre> from bmtool.synapses import GapJunctionOptimizer  # Create optimizer optimizer = GapJunctionOptimizer(tuner)  # Optimize for a target coupling coefficient target_cc = 0.37  # Example target coupling coefficient result = optimizer.optimize_resistance(     target_cc=target_cc,     resistance_bounds=(1e-4, 1e-2)  # Min and max resistance values to try )  # Plot optimization results optimizer.plot_optimization_results(result) In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nresistance_range = np.logspace(-4, -2, 20)  # 20 points between 1e-4 and 1e-2\nsweep_results = optimizer.parameter_sweep(resistance_range)\n\n# Plot sweep results\nplt.figure(figsize=(8, 6))\nplt.semilogx(sweep_results['resistance'], sweep_results['coupling_coefficient'], 'o-')\nplt.xlabel('Resistance')\nplt.ylabel('Coupling Coefficient')\nplt.title('Resistance vs Coupling Coefficient')\nplt.grid(True)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  resistance_range = np.logspace(-4, -2, 20)  # 20 points between 1e-4 and 1e-2 sweep_results = optimizer.parameter_sweep(resistance_range)  # Plot sweep results plt.figure(figsize=(8, 6)) plt.semilogx(sweep_results['resistance'], sweep_results['coupling_coefficient'], 'o-') plt.xlabel('Resistance') plt.ylabel('Coupling Coefficient') plt.title('Resistance vs Coupling Coefficient') plt.grid(True) plt.show() <pre>Sweeping resistance values:   0%|          | 0/20 [00:00&lt;?, ?it/s]</pre>"},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/#example-of-how-to-use-bmtool-gap-junction-tuner","title":"Example of how to use BMTOOL gap junction tuner.\u00b6","text":"<p>By Gregory Glickert</p> <p>We first need to set up the tuner. We have general settings and connection specific parameters. We also need to say where our mechanisms and cell templates are located</p>"},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/#initialize-tuner","title":"initialize tuner\u00b6","text":""},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/#interactivetuner","title":"InteractiveTuner()\u00b6","text":""},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/#gapjunctionoptimizer","title":"GapJunctionOptimizer\u00b6","text":"<p>While we can manually tune the coupling coefficient since there is only one parameter that we are tuning we can optimize and find the best resistance!</p>"},{"location":"examples/notebooks/synapses/gap_junction_tuner/gap_junction_tuner/#parameter-sweep","title":"parameter sweep\u00b6","text":"<p>We can also perform a parameter sweep to see the relationship between resistance and coupling coefficient</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/bmtk_chem_syn_tuner/","title":"BMTK Chemical Synapse Tuner","text":"In\u00a0[1]: Copied! <pre># like normal lets first compile mod files\nimport os\n\n# if already compiled then lets delete the folder and force a recompile\nif os.path.isdir('bmtk_files/components/mechanisms/x86_64'):\n    os.system(\"rm -rf bmtk_files/components/mechanisms/x86_64\")\nif os.path.isdir('bmtk_files/components/mechanisms/arm64'):\n    os.system(\"rm -rf bmtk_files/components/mechanisms/arm64\")\n# compile the mod files\nif not os.path.isdir(\"bmtk_files/components/mechanisms/x86_64\"):\n    os.chdir('bmtk_files/components/mechanisms')\n    os.system(\"nrnivmodl\")\n    os.chdir(\"../../..\")\n</pre> # like normal lets first compile mod files import os  # if already compiled then lets delete the folder and force a recompile if os.path.isdir('bmtk_files/components/mechanisms/x86_64'):     os.system(\"rm -rf bmtk_files/components/mechanisms/x86_64\") if os.path.isdir('bmtk_files/components/mechanisms/arm64'):     os.system(\"rm -rf bmtk_files/components/mechanisms/arm64\") # compile the mod files if not os.path.isdir(\"bmtk_files/components/mechanisms/x86_64\"):     os.chdir('bmtk_files/components/mechanisms')     os.system(\"nrnivmodl\")     os.chdir(\"../../..\") <pre>/usr/bin/xcrun\n</pre> <pre>/Users/gregglickert/miniconda3/envs/bmtk/bin/nrnivmodl:10: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import working_set\nTranslating k_rtm.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/mechanisms/arm64/k_rtm.c\nTranslating exp2syn_stsp.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/mechanisms/arm64/exp2syn_stsp.c\nTranslating exp2syn_stp.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/mechanisms/arm64/exp2syn_stp.c\nThread Safe\nThread Safe\nThread Safe\nTranslating k_wb.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/mechanisms/arm64/k_wb.c\nTranslating na_rtm.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/mechanisms/arm64/na_rtm.c\nTranslating leak.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/mechanisms/arm64/leak.c\nThread Safe\n</pre> <pre>/Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/mechanisms\nMod files: \"./exp2syn_stp.mod\" \"./exp2syn_stsp.mod\" \"./k_rtm.mod\" \"./k_wb.mod\" \"./leak.mod\" \"./na_rtm.mod\" \"./na_wb.mod\" \"./vecevent.mod\"\n\nCreating 'arm64' directory for .o files.\n\n -&gt; Compiling mod_func.cpp\n -&gt; NMODL ../exp2syn_stp.mod\n -&gt; NMODL ../exp2syn_stsp.mod\n -&gt; NMODL ../k_rtm.mod\n -&gt; NMODL ../k_wb.mod\n -&gt; NMODL ../na_rtm.mod\n -&gt; NMODL ../leak.mod\n -&gt; NMODL ../na_wb.mod\n -&gt; Compiling exp2syn_stp.c\n -&gt; NMODL ../vecevent.mod\nNotice: ARTIFICIAL_CELL is a synonym for POINT_PROCESS which hints that it\nonly affects and is affected by discrete events. As such it is not\nlocated in a section and is not associated with an integrator\n -&gt; Compiling exp2syn_stsp.c\n -&gt; Compiling k_rtm.c\n -&gt; Compiling k_wb.c\n</pre> <pre>Thread Safe\nThread Safe\nTranslating na_wb.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/mechanisms/arm64/na_wb.c\nTranslating vecevent.mod into /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/mechanisms/arm64/vecevent.c\nThread Safe\nThread Safe\nexp2syn_stp.c:42:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   42 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nexp2syn_stp.c:343:32: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  343 |   if (_tsav &gt; t){ extern char* hoc_object_name(); hoc_execerror(hoc_object_name(_pnt-&gt;ob), \":Event arrived out of order. Must call ParallelContext.set_maxstep AFTER assigning minimum NetCon.delay\");}\n      |                                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:46:14: note: conflicting prototype is here\n   46 | extern char* hoc_object_name(Object*);\n      |              ^\nexp2syn_stsp.c:42:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   42 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getark_rtm.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]g\n  hoc_getarg\n      |                    ^43\n |          extern double *getarg()/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h;:\n49      :| 16                        ^:\n note: conflicting prototype is here\n   49 | extern double* getar/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:g17: note: expanded from macro 'getarg'\n(   i15n | t#defi)n;e\n       g| e               ^t\narg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define geta/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16:rg  hoc _note: gconflicting prototype is heree\ntarg\n      |                    ^49\n | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nexp2syn_stsp.c:335:32: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  335 |   if (_tsav &gt; t){ extern char* hoc_object_name(); hoc_execerror(hoc_object_name(_pnt-&gt;ob), \":Event arrived out of order. Must call ParallelContext.set_maxstep AFTER assigning minimum NetCon.delay\");}\n      |                                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:46:14: note: conflicting prototype is here\n   46 | extern char* hoc_object_name(Object*);\n      |              ^\n2 warnings generated.\nk_wb.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n2 warnings generated.\n1 warning generated.\n1 warning generated.\nleak.c:41:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   41 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nna_rtm.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\nna_wb.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n1 warning generated.\n1 warning generated.\nvecevent.c:43:18: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n   43 |          extern double *getarg();\n      |                         ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg  hoc_getarg\n      |                 ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:49:16: note: conflicting prototype is here\n   49 | extern double* getarg(int);\n      |                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/hocdec.h:15:17: note: expanded from macro 'getarg'\n   15 | #define getarg1 warning generated.\n  hoc_getarg\n      |                 ^\nvecevent.c:242:32: warning: a function declaration without a prototype is deprecated in all versions of C and is treated as a zero-parameter prototype in C23, conflicting with a previous declaration [-Wdeprecated-non-prototype]\n  242 |   if (_tsav &gt; t){ extern char* hoc_object_name(); hoc_execerror(hoc_object_name(_pnt-&gt;ob), \":Event arrived out of order. Must call ParallelContext.set_maxstep AFTER assigning minimum NetCon.delay\");}\n      |                                ^\n/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/neuron/.data/include/oc_ansi.h:46:14: note: conflicting prototype is here\n   46 | extern char* hoc_object_name(Object*);\n      |              ^\n2 warnings generated.\n</pre> <pre> -&gt; Compiling leak.c\n -&gt; Compiling na_rtm.c\n -&gt; Compiling na_wb.c\n -&gt; Compiling vecevent.c\n =&gt; LINKING shared library ./libnrnmech.dylib\n =&gt; LINKING executable ./special LDFLAGS are:    \nSuccessfully created arm64/special\n</pre> <pre>ld: warning: ignoring duplicate libraries: '-lnrnmech'\n</pre> In\u00a0[2]: Copied! <pre>from bmtool.synapses import SynapseTuner\n\ntuner = SynapseTuner(\n    config='bmtk_files/simulation_config.json',  # Path to BMTK config\n    current_name='i',                            # Synaptic current to record\n    connection = 'Exc2SOM', \n    slider_vars=['initW','Dep','Fac','Use','tau1','tau2']  # Parameters for sliders\n)\n</pre> from bmtool.synapses import SynapseTuner  tuner = SynapseTuner(     config='bmtk_files/simulation_config.json',  # Path to BMTK config     current_name='i',                            # Synaptic current to record     connection = 'Exc2SOM',      slider_vars=['initW','Dep','Fac','Use','tau1','tau2']  # Parameters for sliders )  <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> <pre>loading /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/templates/README.md\nloading /Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/bmtk_files/components/templates/cell_templates.hoc\nBuilding conn_type_settings from BMTK config files...\nFound 8 connection types: ['Exc2PV', 'SOM2PV', 'PV2PV', 'Exc2SOM', 'Exc2Exc', 'PV2Exc', 'SOM2Exc', 'background_syn']\n</pre> <p>When using the tuner with multiple networks you can switch between your different bmtk networks to test every single connection in your network. Note the synaptic weight (sometimes called initW) is the bmtk netconn weight. If you are tuning a synapse for its strength make sure to keep this in mind. You should be able to convert that to a scale factor for you weight inside the mod file if needed</p> In\u00a0[3]: Copied! <pre>tuner.InteractiveTuner()\n</pre> tuner.InteractiveTuner() <pre>VBox(children=(HBox(children=(Dropdown(description='Network:', options=('network_to_network', 'extnet_to_netwo\u2026</pre> <pre>Output()</pre> <p>If using the optimizer with a bmtk network just make sure you specifiy which connection you want to use. You can switch it in the Interactive tuner or using the switch_connection method</p> In\u00a0[4]: Copied! <pre>tuner._switch_connection('PV2Exc')\n</pre> tuner._switch_connection('PV2Exc') <pre>Successfully switched to connection: PV2Exc\n</pre> <p>Now we tune that PV2Exc synapse</p> In\u00a0[\u00a0]: Copied! <pre>from bmtool.synapses import SynapseOptimizer\n\n# Create the optimizer\noptimizer = SynapseOptimizer(tuner)\n\n# Define parameter bounds these can be any range variable you wish to tune\nparam_bounds = {\n    'Dep': (0, 200.0),\n    'Fac': (0, 400.0),\n    'Use': (0.1, 1.0),\n    'tau1': (1,4), # tau r needs to be less than tau d so be careful\n    'tau2': (5,20)\n}\n\n# Define target metrics these are the metrics that the tuner will try to automatic get the synapse to respond with\n# max amps is an absolute value\ntarget_metrics = {\n    'induction': -0.75,\n    'simple_ppr': 0.8,\n    'recovery': 0.0,\n    'rise_time': 2,          # This wont always be the case, but for this synapse Use controls STP and max amps so it can sometimes struggle to fit.\n    'decay_time': 9\n}\n\n# currently the only metrics in the SynapseOptimizer are \n#            - induction: measure of synaptic facilitation/depression\n#            - ppr: paired-pulse ratio\n#            - recovery: recovery from facilitation/depression\n#            - max_amplitude: maximum synaptic response amplitude\n#            - rise_time: time for synaptic response to rise from 20% to 80% of peak\n#            - decay_time: time constant of synaptic response decay\n#            - latency: synaptic response latency\n#            - half_width: synaptic response half-width\n#            - baseline: baseline current\n#            - amp: peak amplitude from syn_props\n\ndef custom_cost(metrics, targets):\n    # equal zero unless using train input\n    induction_error = (metrics['induction'] - targets['induction']) ** 2\n    ppr_error = (metrics['simple_ppr'] - targets['simple_ppr']) ** 2\n    recovery_error = (metrics['recovery'] - targets['recovery']) ** 2\n    # equal zero unless using SingleEvent\n    rise_time_error = (metrics['rise_time'] - targets['rise_time']) ** 2\n    decay_time_error = (metrics['decay_time'] - targets['decay_time']) ** 2 \n\n    #return rise_time_error + decay_time_error\n    return induction_error + 3 * ppr_error + recovery_error + rise_time_error + decay_time_error #+ 0.5*max_amp_errror\n\n# Run optimization with custom cost function\nresult = optimizer.optimize_parameters(\n    target_metrics=target_metrics,\n    param_bounds=param_bounds,\n    run_single_event=True,  # Run and use parameters from SingleEvent\n    run_train_input=True,   # Run and use parameters from train input \n    train_frequency=50,     # Freq in Hz of train input\n    train_delay=250,        # delay in ms of second train\n    init_guess='random',    # either random or middle_guess. Random will start the synapse witha random value in the param_bound. Middle guess will pick the middle value in the param_bounds\n    cost_function=custom_cost,\n    method='SLSQP'          # I believe this will be the fastest method, but you may try others check out https://docs.scipy.org/doc/scipy-1.15.0/reference/generated/scipy.optimize.minimize.html\n                            # SLSQP is a gradient based method while nelder-mead is simplex (whatever that means)\n)\n\n# Plot results\noptimizer.plot_optimization_results(result)\n</pre> from bmtool.synapses import SynapseOptimizer  # Create the optimizer optimizer = SynapseOptimizer(tuner)  # Define parameter bounds these can be any range variable you wish to tune param_bounds = {     'Dep': (0, 200.0),     'Fac': (0, 400.0),     'Use': (0.1, 1.0),     'tau1': (1,4), # tau r needs to be less than tau d so be careful     'tau2': (5,20) }  # Define target metrics these are the metrics that the tuner will try to automatic get the synapse to respond with # max amps is an absolute value target_metrics = {     'induction': -0.75,     'simple_ppr': 0.8,     'recovery': 0.0,     'rise_time': 2,          # This wont always be the case, but for this synapse Use controls STP and max amps so it can sometimes struggle to fit.     'decay_time': 9 }  # currently the only metrics in the SynapseOptimizer are  #            - induction: measure of synaptic facilitation/depression #            - ppr: paired-pulse ratio #            - recovery: recovery from facilitation/depression #            - max_amplitude: maximum synaptic response amplitude #            - rise_time: time for synaptic response to rise from 20% to 80% of peak #            - decay_time: time constant of synaptic response decay #            - latency: synaptic response latency #            - half_width: synaptic response half-width #            - baseline: baseline current #            - amp: peak amplitude from syn_props  def custom_cost(metrics, targets):     # equal zero unless using train input     induction_error = (metrics['induction'] - targets['induction']) ** 2     ppr_error = (metrics['simple_ppr'] - targets['simple_ppr']) ** 2     recovery_error = (metrics['recovery'] - targets['recovery']) ** 2     # equal zero unless using SingleEvent     rise_time_error = (metrics['rise_time'] - targets['rise_time']) ** 2     decay_time_error = (metrics['decay_time'] - targets['decay_time']) ** 2       #return rise_time_error + decay_time_error     return induction_error + 3 * ppr_error + recovery_error + rise_time_error + decay_time_error #+ 0.5*max_amp_errror  # Run optimization with custom cost function result = optimizer.optimize_parameters(     target_metrics=target_metrics,     param_bounds=param_bounds,     run_single_event=True,  # Run and use parameters from SingleEvent     run_train_input=True,   # Run and use parameters from train input      train_frequency=50,     # Freq in Hz of train input     train_delay=250,        # delay in ms of second train     init_guess='random',    # either random or middle_guess. Random will start the synapse witha random value in the param_bound. Middle guess will pick the middle value in the param_bounds     cost_function=custom_cost,     method='SLSQP'          # I believe this will be the fastest method, but you may try others check out https://docs.scipy.org/doc/scipy-1.15.0/reference/generated/scipy.optimize.minimize.html                             # SLSQP is a gradient based method while nelder-mead is simplex (whatever that means) )  # Plot results optimizer.plot_optimization_results(result) <pre>/Users/gregglickert/miniconda3/envs/bmtk/lib/python3.11/site-packages/scipy/optimize/_optimize.py:353: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n  warnings.warn(\"Values in x were outside bounds during a \"\n</pre> <pre>Optimization Results:\nFinal Error: 2.13e-02\n\nTarget Metrics:\ninduction: -0.747 (target: -0.750)\nsimple_ppr: 0.826 (target: 0.800)\nrecovery: -0.059 (target: 0.000)\nrise_time: 2.125 (target: 2.000)\ndecay_time: 8.993 (target: 9.000)\n\nOptimal Parameters:\nDep: 137.814\nFac: 302.192\nUse: 0.566\ntau1: 2.163\ntau2: 7.792\n</pre> <pre>\n========================================\nShort Term Plasticity Results for 50Hz with 250 Delay\n========================================\nSimple PPR: Above 1 is facilitating, below 1 is depressing\nPPR:        Above 0 is facilitating, below 0 is depressing.\nInduction:  Above 0 is facilitating, below 0 is depressing.\nRecovery:   A measure of how fast STP decays.\n\nSimple Paired Pulse Ratio (PPR)\n    Calculation: Avg 2nd pulse / Avg 1st pulse\n    Values: 0.058 / 0.071 = 0.826\n\nPaired Pulse Response (PPR)\n    Calculation: (Avg 2nd pulse - Avg 1st pulse) / 90th percentile amplitude\n    Values: (0.058 - 0.071) / 0.070 = -0.177\n\nInduction\n    Calculation: (Avg(6th, 7th, 8th pulses) - Avg 1st pulse) / 90th percentile amplitude\n    Values: 0.019 - 0.071 / 0.070 = -0.747\n\nRecovery\n    Calculation: (Avg(9th, 10th, 11th, 12th pulses) - Avg(1st to 4th pulses)) / 90th percentile amplitude\n    Values: 0.041 - 0.045 / 0.070 = -0.059\n\n========================================\n\n</pre> <pre>('baseline', 0.0)\n('sign', 1.0)\n('latency', 1.35)\n('amp', 7.073698623809886e-05)\n('rise_time', 2.125)\n('decay_time', 8.993371099564527)\n('half_width', 12.100000000000001)\nCurrent Integral in pA*ms: 0.90\n</pre>"},{"location":"examples/notebooks/synapses/synaptic_tuner/bmtk_chem_syn_tuner/#demo-of-bmtool-chemical-synaptic-tuner-using-bmtk-network","title":"Demo of bmtool chemical synaptic tuner using BMTK network\u00b6","text":"<p>by Gregory Glickert</p> <p>For a bit more context in using the tuner you can check out the notebook called neuron_chem_syn_tuner which goes over more details about using the tuner with pure NEURON models.</p> <p>When running this tuner you will see some warnings. That is on purpose, some of the synapses in this bmtk testing network do not have the Dep, Fac or Use parameters. That is ok and the tuner will still work for those synapses, but you can't tune those parameteres since they dont exist in the mod file.</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/bmtk_chem_syn_tuner/#load-the-synaptic-tuner-using-a-bmtk-config","title":"Load the synaptic tuner using a bmtk config\u00b6","text":""},{"location":"examples/notebooks/synapses/synaptic_tuner/bmtk_chem_syn_tuner/#switch-a-connection-for-the-opimizer","title":"Switch a connection for the Opimizer\u00b6","text":""},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/","title":"Neuron Chemical Synapse Tuner","text":"In\u00a0[1]: Copied! <pre>general_settings = {\n    'vclamp': True, # if vclamp should start on or off used mostly for singleEventv\n    'rise_interval': (0.1, 0.9), #10-90%\n    'tstart': 500., # when the singleEvent should start\n    'tdur': 100.,    # Dur of sim after single synaptic event has occured\n    'threshold': -15., #threshold for spike in mV\n    'delay': 1.3, # netcon delay\n    'weight': 1., # netcon weight\n    'dt': 0.025, # simulation dt\n    'celsius': 20 # temp of sim\n}\n\nconn_type_settings = {\n    'Fac2FSI': { # facilitating synapse \n        'spec_settings': {\n            'post_cell': 'FSI_Cell', \n            'vclamp_amp' : -70., # voltage clamp amps\n            'sec_x': 0.5, # location of synapse\n            'sec_id': 1,  # location of synapse \n            \"level_of_detail\": \"AMPA_NMDA_STP\", # name of mechanism from the modfile\n        },\n        'spec_syn_param': { # synaptic parameters from modfile \n            'initW': 0.76,\n            'tau_r_AMPA': 0.45,\n            'tau_d_AMPA': 7.5,\n            'Use': 0.13,\n            'Dep': 0.,\n            'Fac': 200.\n        },\n    },\n    'Dep2FSI': { # depressing synapse\n        'spec_settings': {\n            'post_cell': 'FSI_Cell',\n            'vclamp_amp': -55,\n            'sec_x': 0.5,\n            'sec_id':0,\n            \"level_of_detail\": \"GABA_A_STP\",\n        },\n        'spec_syn_param': {\n            'initW': 20,\n            'tau_r_GABAA': 0.9,\n            'tau_d_GABAA': 15,\n            'e_GABAA':-75,\n            'Use': 0.4,\n            'Dep': 190.,\n            'Fac': 0.\n        },\n    },\n\n}\n</pre> general_settings = {     'vclamp': True, # if vclamp should start on or off used mostly for singleEventv     'rise_interval': (0.1, 0.9), #10-90%     'tstart': 500., # when the singleEvent should start     'tdur': 100.,    # Dur of sim after single synaptic event has occured     'threshold': -15., #threshold for spike in mV     'delay': 1.3, # netcon delay     'weight': 1., # netcon weight     'dt': 0.025, # simulation dt     'celsius': 20 # temp of sim }  conn_type_settings = {     'Fac2FSI': { # facilitating synapse          'spec_settings': {             'post_cell': 'FSI_Cell',              'vclamp_amp' : -70., # voltage clamp amps             'sec_x': 0.5, # location of synapse             'sec_id': 1,  # location of synapse              \"level_of_detail\": \"AMPA_NMDA_STP\", # name of mechanism from the modfile         },         'spec_syn_param': { # synaptic parameters from modfile              'initW': 0.76,             'tau_r_AMPA': 0.45,             'tau_d_AMPA': 7.5,             'Use': 0.13,             'Dep': 0.,             'Fac': 200.         },     },     'Dep2FSI': { # depressing synapse         'spec_settings': {             'post_cell': 'FSI_Cell',             'vclamp_amp': -55,             'sec_x': 0.5,             'sec_id':0,             \"level_of_detail\": \"GABA_A_STP\",         },         'spec_syn_param': {             'initW': 20,             'tau_r_GABAA': 0.9,             'tau_d_GABAA': 15,             'e_GABAA':-75,             'Use': 0.4,             'Dep': 190.,             'Fac': 0.         },     },  } <p>Then the modfiles must be compiled in order for the tuner to work properly</p> In\u00a0[2]: Copied! <pre>import os\n\n# if already compiled then lets delete the folder and force a recompile\nif os.path.isdir('modfiles/x86_64'):\n    os.system(\"rm -rf modfiles/x86_64 \")\n# compile the mod files\nif not os.path.isdir(\"modfiles/x86_64\"):\n    os.chdir('modfiles')\n    os.system(\"nrnivmodl\")\n    os.chdir(\"..\")\n</pre> import os  # if already compiled then lets delete the folder and force a recompile if os.path.isdir('modfiles/x86_64'):     os.system(\"rm -rf modfiles/x86_64 \") # compile the mod files if not os.path.isdir(\"modfiles/x86_64\"):     os.chdir('modfiles')     os.system(\"nrnivmodl\")     os.chdir(\"..\") <pre>/Users/gregglickert/miniconda3/envs/bmtk/bin/nrnivmodl:10: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import working_set\n</pre> <pre>/usr/bin/xcrun\n/Users/gregglickert/Documents/GitHub/bmtool/docs/examples/notebooks/synapses/synaptic_tuner/modfiles\nMod files: \"./AMPA_NMDA_STP.mod\" \"./GABA_A_STP.mod\" \"./Gfluct.mod\" \"./cadad.mod\" \"./cal2.mod\" \"./can_mig.mod\" \"./exp2syn_stp.mod\" \"./gap.mod\" \"./h_kole.mod\" \"./imCA3.mod\" \"./kBK.mod\" \"./kap_BS.mod\" \"./kdmc_BS.mod\" \"./kdrCA3.mod\" \"./kdr_BS.mod\" \"./kdrinter.mod\" \"./leak.mod\" \"./nainter.mod\" \"./napCA3.mod\" \"./natCA3.mod\" \"./nax_BS.mod\" \"./vecevent_coreneuron.mod\"\n\n -&gt; Compiling mod_func.cpp\n =&gt; LINKING shared library ./libnrnmech.dylib\nSuccessfully created arm64/special\n</pre> In\u00a0[3]: Copied! <pre>mechanisms_dir = 'modfiles'\ntemplates_file = 'templates.hoc'\n\n# Initialize our tuner\nfrom bmtool.synapses import SynapseTuner\n\ntuner = SynapseTuner(mechanisms_dir=mechanisms_dir, # where x86_64 is located\n                    templates_dir=templates_file, # where the neuron templates are located\n                    conn_type_settings=conn_type_settings, # dict of connection settings\n                    general_settings = general_settings, # dict of general settings\n                    connection = 'Dep2FSI', # key in connection settings for which connection you want to tune\n                    current_name = 'i', # name of current variable in synapase\n                    other_vars_to_record = ['record_Pr', 'record_use'], # Other synaptic variables you wish to record besides the normal ones\n                    slider_vars=['initW','Dep','Fac','Use']) # Range variables you want to tune to adjust synaptic response.\n</pre> mechanisms_dir = 'modfiles' templates_file = 'templates.hoc'  # Initialize our tuner from bmtool.synapses import SynapseTuner  tuner = SynapseTuner(mechanisms_dir=mechanisms_dir, # where x86_64 is located                     templates_dir=templates_file, # where the neuron templates are located                     conn_type_settings=conn_type_settings, # dict of connection settings                     general_settings = general_settings, # dict of general settings                     connection = 'Dep2FSI', # key in connection settings for which connection you want to tune                     current_name = 'i', # name of current variable in synapase                     other_vars_to_record = ['record_Pr', 'record_use'], # Other synaptic variables you wish to record besides the normal ones                     slider_vars=['initW','Dep','Fac','Use']) # Range variables you want to tune to adjust synaptic response.  <pre>Warning: no DISPLAY environment variable.\n--No graphics will be displayed.\n</pre> In\u00a0[4]: Copied! <pre>tuner.SingleEvent()\n</pre> tuner.SingleEvent() <pre>('baseline', 0.03107559632675816)\n('sign', 1.0)\n('latency', 1.35)\n('amp', 0.16000855570656114)\n('rise_time', 1.35)\n('decay_time', 15.182347895738655)\n('half_width', 14.850000000000001)\nCurrent Integral in pA*ms: -15777.35\n</pre> In\u00a0[5]: Copied! <pre>tuner.InteractiveTuner()\n</pre> tuner.InteractiveTuner() <pre>VBox(children=(HBox(children=(Dropdown(description='Connection:', options=('Dep2FSI', 'Fac2FSI'), style=Descri\u2026</pre> <pre>Output()</pre> In\u00a0[6]: Copied! <pre>results = tuner.stp_frequency_response(log_plot=False)\n</pre> results = tuner.stp_frequency_response(log_plot=False) <pre>Analyzing frequencies:   0%|          | 0/16 [00:00&lt;?, ?it/s]</pre> In\u00a0[12]: Copied! <pre>from bmtool.synapses import SynapseOptimizer\n\n# Create the optimizer\noptimizer = SynapseOptimizer(tuner)\n\n# Define parameter bounds these can be any range variable you wish to tune\nparam_bounds = {\n    'Dep': (0, 800.0),\n    'Fac': (0, 100.0),\n    'Use': (0.1, 1.0),\n}\n\n# Define target metrics these are the metrics that the tuner will try to automatic get the synapse to respond with\n# max amps is an absolute value\ntarget_metrics = {\n    'induction': -0.75,\n    'simple_ppr': 0.8,\n    'recovery': 0.0,\n}\n\n# currently the metrics in the SynapseOptimizer are \n#            - induction: measure of synaptic facilitation/depression\n#            - ppr: paired-pulse ratio from allen\n#            - simple_ppr ration of first to second amplitude \n#            - recovery: recovery from facilitation/depression\n#            - max_amplitude: maximum synaptic response amplitude\n#            - rise_time: time for synaptic response to rise from 20% to 80% of peak\n#            - decay_time: time constant of synaptic response decay\n#            - latency: synaptic response latency\n#            - half_width: synaptic response half-width\n#            - baseline: baseline current\n#            - amp: peak amplitude from syn_props\ndef custom_cost(metrics, targets):\n    # equal zero unless using train input\n    induction_error = (metrics['induction'] - targets['induction']) ** 2\n    ppr_error = (metrics['simple_ppr'] - targets['simple_ppr']) ** 2\n    recovery_error = (metrics['recovery'] - targets['recovery']) ** 2\n\n    return induction_error + 3 * ppr_error + recovery_error \n\n# Run optimization with custom cost function\nresult = optimizer.optimize_parameters(\n    target_metrics=target_metrics,\n    param_bounds=param_bounds,\n    run_single_event=True,  # Run and use parameters from SingleEvent\n    run_train_input=True,   # Run and use parameters from train input \n    train_frequency=50,     # Freq in Hz of train input\n    train_delay=250,        # delay in ms of second train\n    init_guess='middle_guess',    # either random or middle_guess. Random will start the synapse witha random value in the param_bound. Middle guess will pick the middle value in the param_bounds\n    cost_function=custom_cost,\n    method='SLSQP')          # check out https://docs.scipy.org/doc/scipy-1.15.0/reference/generated/scipy.optimize.minimize.html\n                               \n\n# Plot results\noptimizer.plot_optimization_results(result)\n</pre> from bmtool.synapses import SynapseOptimizer  # Create the optimizer optimizer = SynapseOptimizer(tuner)  # Define parameter bounds these can be any range variable you wish to tune param_bounds = {     'Dep': (0, 800.0),     'Fac': (0, 100.0),     'Use': (0.1, 1.0), }  # Define target metrics these are the metrics that the tuner will try to automatic get the synapse to respond with # max amps is an absolute value target_metrics = {     'induction': -0.75,     'simple_ppr': 0.8,     'recovery': 0.0, }  # currently the metrics in the SynapseOptimizer are  #            - induction: measure of synaptic facilitation/depression #            - ppr: paired-pulse ratio from allen #            - simple_ppr ration of first to second amplitude  #            - recovery: recovery from facilitation/depression #            - max_amplitude: maximum synaptic response amplitude #            - rise_time: time for synaptic response to rise from 20% to 80% of peak #            - decay_time: time constant of synaptic response decay #            - latency: synaptic response latency #            - half_width: synaptic response half-width #            - baseline: baseline current #            - amp: peak amplitude from syn_props def custom_cost(metrics, targets):     # equal zero unless using train input     induction_error = (metrics['induction'] - targets['induction']) ** 2     ppr_error = (metrics['simple_ppr'] - targets['simple_ppr']) ** 2     recovery_error = (metrics['recovery'] - targets['recovery']) ** 2      return induction_error + 3 * ppr_error + recovery_error   # Run optimization with custom cost function result = optimizer.optimize_parameters(     target_metrics=target_metrics,     param_bounds=param_bounds,     run_single_event=True,  # Run and use parameters from SingleEvent     run_train_input=True,   # Run and use parameters from train input      train_frequency=50,     # Freq in Hz of train input     train_delay=250,        # delay in ms of second train     init_guess='middle_guess',    # either random or middle_guess. Random will start the synapse witha random value in the param_bound. Middle guess will pick the middle value in the param_bounds     cost_function=custom_cost,     method='SLSQP')          # check out https://docs.scipy.org/doc/scipy-1.15.0/reference/generated/scipy.optimize.minimize.html                                  # Plot results optimizer.plot_optimization_results(result) <pre>Optimization Results:\nFinal Error: 2.91e-07\n\nTarget Metrics:\ninduction: -0.750 (target: -0.750)\nsimple_ppr: 0.800 (target: 0.800)\nrecovery: 0.000 (target: 0.000)\n\nOptimal Parameters:\nDep: 144.855\nFac: 99.077\nUse: 0.680\n</pre> <pre>\n========================================\nShort Term Plasticity Results for 50Hz with 250 Delay\n========================================\nSimple PPR: Above 1 is facilitating, below 1 is depressing\nPPR:        Above 0 is facilitating, below 0 is depressing.\nInduction:  Above 0 is facilitating, below 0 is depressing.\nRecovery:   A measure of how fast STP decays.\n\nSimple Paired Pulse Ratio (PPR)\n    Calculation: Avg 2nd pulse / Avg 1st pulse\n    Values: 54.576 / 68.223 = 0.800\n\nPaired Pulse Response (PPR)\n    Calculation: (Avg 2nd pulse - Avg 1st pulse) / 90th percentile amplitude\n    Values: (54.576 - 68.223) / 66.858 = -0.204\n\nInduction\n    Calculation: (Avg(6th, 7th, 8th pulses) - Avg 1st pulse) / 90th percentile amplitude\n    Values: 18.053 - 68.223 / 66.858 = -0.750\n\nRecovery\n    Calculation: (Avg(9th, 10th, 11th, 12th pulses) - Avg(1st to 4th pulses)) / 90th percentile amplitude\n    Values: 44.122 - 44.122 / 66.858 = 0.000\n\n========================================\n\n</pre> <pre>('baseline', -0.0002407732493736603)\n('sign', 1.0)\n('latency', 1.35)\n('amp', 0.06798172722533025)\n('rise_time', 1.35)\n('decay_time', 15.182348785123999)\n('half_width', 14.850000000000001)\nCurrent Integral in pA*ms: 1362.98\n</pre>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#demo-of-bmtool-chemical-synaptic-tuner-using-only-neuron","title":"Demo of bmtool chemical synaptic tuner using only Neuron\u00b6","text":"<p>By Gregory Glickert</p> <p>First we must define some general settings and the settings for the connection we would like to tune. Below is an example of what this could look like for excitatory and inhibitory connections. Currently all of these settings, but the ones in the spec_syn_param are needed in order to use the tuner. If you dont include the general settting when initalizing the tuner then by default it will use these settings. The spec_settings are going to depend on your exact use case and connection type.</p> <p>If you are using the tuner for a BMTK network you can look at this notebook, but make sure to also check out the bmtk_tuner notebook in this same directory.</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#initialize-tuner","title":"initialize tuner\u00b6","text":"<p>Now we can initialize the synaptic tuner. When initializing you will have to change a few arguments depending on your use case. other_vars_to_record can be any variable in your synaptic mechanism, while slider_vars can be any range variable you wish to tune in the synapse. If the variable is not defined in the spec_syn_param than the tuner will get the value from the mechanism and try to set up some sliders to tune it.</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#singleevent","title":"SingleEvent\u00b6","text":"<p>The SingleEvent method will run a short pulse and then print out the synaptic properties for the synapse.</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#interactivetuner","title":"InteractiveTuner\u00b6","text":"<p>The InteractiveTuner will deliver an input to the cell at a desired weight and frequency. The frequency by default will be 8 spikes then a 250ms delay and then 4 more spikes.</p> <p>Paired-pulse ratio is (Avg 2nd pulse - Avg 1st pulse) \u00f7 90th percentile amplitude.</p> <p>Induction is (Avg (6th, 7th, 8th pulses) - Avg 1st pulse) \u00f7 90th percentile amplitude.</p> <p>Recovery is (Avg (9th, 10th, 11th, 12th pulses) - Avg (1st, 2nd, 3rd, 4th pulses)) \u00f7 90th percentile amplitude</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#frequency-reponse","title":"Frequency reponse\u00b6","text":"<p>We can also see how the STP parameters vary with different train frequencies</p>"},{"location":"examples/notebooks/synapses/synaptic_tuner/neuron_chem_syn_tuner/#synapseoptimizer","title":"SynapseOptimizer\u00b6","text":"<p>If we don't feel like tuner by hand we can also try to optimize an output of our model. In this example we will optimize and find the best STP parameters that give the induction and paired pulse response we want. Something to note is that the optimizer does not know what the trace should look like and only knows the features. So it might get some wild trace that happens to work. Also if you are using the random init_guess and don't like the voltage trace then run it again. The seed is different each time so the optimization will be different and can result in a better fit.</p>"},{"location":"modules/analysis/","title":"Analysis Module","text":"<p>The Analysis module provides a comprehensive suite of tools for processing and analyzing simulation results from BMTK models. It is organized into several submodules, each focusing on specific aspects of neural data analysis.</p>"},{"location":"modules/analysis/#overview","title":"Overview","text":""},{"location":"modules/analysis/#spike-analysis","title":"Spike Analysis","text":"<ul> <li>Loading and processing spike data</li> <li>Computing firing rate statistics</li> <li>Population spike rate analysis</li> <li>Spike train analysis tools</li> </ul>"},{"location":"modules/analysis/#lfpecp-analysis","title":"LFP/ECP Analysis","text":"<ul> <li>Loading and processing LFP/ECP data</li> <li>Spectral analysis and filtering</li> <li>Time-frequency analysis</li> <li>Signal quality metrics</li> </ul>"},{"location":"modules/analysis/#entrainment-analysis","title":"Entrainment Analysis","text":"<ul> <li>Phase-locking analysis</li> <li>Spike-field coherence</li> <li>Population entrainment metrics</li> <li>Cross-frequency coupling</li> </ul>"},{"location":"modules/analysis/#network-connectivity","title":"Network Connectivity","text":"<ul> <li>Connection statistics</li> <li>Network topology analysis</li> <li>Synaptic weight distributions</li> <li>Connectivity visualization</li> </ul>"},{"location":"modules/analysis/#spike-analysis_1","title":"Spike Analysis","text":"<p>Load and analyze spike data from simulation output:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom bmtool.analysis.spikes import load_spikes_to_df, compute_firing_rate_stats, get_population_spike_rate\nfrom bmtool.bmplot import raster, plot_firing_rate_pop_stats, plot_firing_rate_distribution\n\n# Load spike data from a simulation\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network',\n    config='config.json',  # Optional: to label cell types\n    groupby='pop_name'\n)\n\n# Get basic spike statistics by population\npop_stats, individual_stats = compute_firing_rate_stats(\n    df=spikes_df,\n    groupby='pop_name',\n    start_time=500,\n    stop_time=1500\n)\n\nprint(\"Population firing rate statistics:\")\nprint(pop_stats)\n\n# Calculate population spike rates over time\npopulation_rates = get_population_spike_rate(\n    spikes=spikes_df,\n    fs=400.0,               # Sampling frequency in Hz\n    t_start=0,\n    t_stop=2000,\n    config='config.json',   # Optional\n    network_name='network'  # Optional\n)\n\n# Plot population rates\nfor pop_name, rates in population_rates.items():\n    plt.plot(rates, label=pop_name)\nplt.xlabel('Time (ms)')\nplt.ylabel('Firing Rate (Hz)')\nplt.legend()\nplt.title('Population Firing Rates')\nplt.show()\n</code></pre>"},{"location":"modules/analysis/#raster-plots","title":"Raster Plots","text":"<p>Create raster plots to visualize spike patterns using the BMPlot module:</p> <pre><code>import matplotlib.pyplot as plt\nfrom bmtool.analysis.spikes import load_spikes_to_df\nfrom bmtool.bmplot import raster\n\n# Load spike data\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network',\n    config='config.json'\n)\n\n# Create a basic raster plot\nfig, ax = plt.subplots(figsize=(10, 6))\nraster(\n    spikes_df=spikes_df,\n    groupby='pop_name',\n    time_range=(0, 2000),\n    ax=ax\n)\nplt.show()\n\n# Plot firing rate statistics\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_pop_stats(\n    firing_stats=pop_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n\n# Plot firing rate distributions\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_distribution(\n    individual_stats=individual_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/analysis/#lfpecp-analysis_1","title":"LFP/ECP Analysis","text":"<p>Analyze LFP (Local Field Potential) and ECP (Extracellular Potential) data:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport xarray as xr\nfrom bmtool.analysis.lfp import (\n    load_ecp_to_xarray,\n    ecp_to_lfp,\n    slice_time_series,\n    cwt_spectrogram_xarray,\n    plot_spectrogram,\n    butter_bandpass_filter,\n    fit_fooof\n)\n\n# Load ECP data\necp_data = load_ecp_to_xarray('output/ecp.h5', demean=True)\n\n# Convert ECP to LFP with filtering\nlfp_data = ecp_to_lfp(\n    ecp_data=ecp_data,\n    cutoff=250,        # Cutoff frequency in Hz\n    fs=10000           # Sampling frequency in Hz\n)\n\n# Slice data to specific time range\nlfp_slice = slice_time_series(lfp_data, time_ranges=(500, 1500))\n\n# Calculate spectrogram\nspectrogram = cwt_spectrogram_xarray(\n    x=lfp_slice.sel(channel=0).data,\n    fs=10000,\n    freq_range=(1, 100),\n    nNotes=8\n)\n\n# Plot spectrogram\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_spectrogram(\n    sxx_xarray=spectrogram,\n    log_power=True,\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/analysis/#frequency-analysis-and-phase-locking","title":"Frequency Analysis and Phase Locking","text":"<p>Analyze frequency content and phase locking between signals:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom bmtool.analysis.lfp import (\n    butter_bandpass_filter,\n    calculate_spike_lfp_plv,\n    calculate_signal_signal_plv,\n    calculate_ppc\n)\nfrom bmtool.analysis.spikes import load_spikes_to_df\n\n# Load spike data and LFP data\nspikes_df = load_spikes_to_df('output/spikes.h5', network_name='network')\nlfp_data = load_ecp_to_xarray('output/ecp.h5', demean=True)\n\n# Filter LFP to specific frequency band (e.g., theta: 4-8 Hz)\nlfp_signal = lfp_data.sel(channel=0).data\nfs = 10000  # Hz\nfiltered_lfp = butter_bandpass_filter(\n    data=lfp_signal,\n    lowcut=4,\n    highcut=8,\n    fs=fs\n)\n\n# Extract spike times for a specific population\npopulation_spikes = spikes_df[spikes_df['pop_name'] == 'Pyramidal']\nspike_times = population_spikes['timestamps'].to_numpy()\n\n# Calculate phase-locking value between spikes and LFP\nplv = calculate_spike_lfp_plv(\n    spike_times=spike_times,\n    lfp_signal=filtered_lfp,\n    spike_fs=1000,  # Spike time unit in milliseconds\n    lfp_fs=fs,\n    fmin=4,\n    fmax=8\n)\nprint(f\"Phase-locking value: {plv}\")\n\n# Calculate pairwise phase consistency\nppc = calculate_ppc(\n    spike_times=spike_times,\n    lfp_signal=filtered_lfp,\n    spike_fs=1000,\n    lfp_fs=fs,\n    fmin=4,\n    fmax=8\n)\nprint(f\"Pairwise phase consistency: {ppc}\")\n</code></pre>"},{"location":"modules/analysis/#signal-processing","title":"Signal Processing","text":"<p>Apply filters and transformations to time series data:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom bmtool.analysis.lfp import (\n    butter_bandpass_filter,\n    wavelet_filter,\n    fit_fooof,\n    generate_resd_from_fooof,\n    calculate_SNR\n)\n\n# Apply band-pass filter\nfiltered_signal = butter_bandpass_filter(\n    data=lfp_signal,\n    lowcut=30,\n    highcut=80,\n    fs=10000\n)\n\n# Apply wavelet filter centered at a specific frequency\ngamma_filtered = wavelet_filter(\n    x=lfp_signal,\n    freq=40,        # Center frequency in Hz\n    fs=10000,       # Sampling rate\n    bandwidth=10    # Bandwidth in Hz\n)\n\n# Calculate power spectrum and fit FOOOF model\nfrom scipy import signal\n\n# Calculate power spectrum\nfreqs, pxx = signal.welch(lfp_signal, fs=10000, nperseg=4096)\n\n# Fit FOOOF model to extract oscillatory and aperiodic components\nfooof_model = fit_fooof(\n    f=freqs,\n    pxx=pxx,\n    freq_range=[1, 100],\n    peak_width_limits=[1, 8],\n    max_n_peaks=6\n)\n\n# Get the residuals between the original spectrum and the aperiodic fit\nresid_spectra, idx_freqs = generate_resd_from_fooof(fooof_model)\n\n# Calculate signal-to-noise ratio in a specific frequency band\nsnr = calculate_SNR(fooof_model, freq_band=(30, 80))\nprint(f\"Signal-to-noise ratio in gamma band: {snr}\")\n</code></pre>"},{"location":"modules/bmplot/","title":"BMPlot Module","text":"<p>The BMPlot module provides visualization tools for BMTK networks, allowing you to analyze and plot connectivity patterns, cell positions, and network properties.</p>"},{"location":"modules/bmplot/#features","title":"Features","text":"<ul> <li>Connection Matrices: Generate matrices showing connectivity between populations</li> <li>Position Plots: Visualize 3D positions of cells in the network</li> <li>Rotation Plots: Visualize cell orientation in 3D space</li> <li>Connection Analysis: Analyze connection properties and distributions</li> <li>Raster Plots: Visualize spike data from simulations</li> </ul>"},{"location":"modules/bmplot/#connection-matrices","title":"Connection Matrices","text":""},{"location":"modules/bmplot/#total-connection-matrix","title":"Total Connection Matrix","text":"<pre><code>import bmtool.bmplot.connections as connections\n\n# Default - all connections\nconnections.total_connection_matrix(\n    config='config.json',\n    title='Total Connection Matrix',\n    sources=None,  # Use all sources (default)\n    targets=None,  # Use all targets (default)\n    save_file=None  # Optional path to save the figure\n)\n\n# Specific source/target populations\nconnections.total_connection_matrix(\n    config='config.json',\n    sources='LA',\n    targets='LA'\n)\n</code></pre>"},{"location":"modules/bmplot/#percent-connection-matrix","title":"Percent Connection Matrix","text":"<p>Generate a matrix showing the percent connectivity between neuron populations:</p> <pre><code># Default - all connections\nconnections.percent_connection_matrix(\n    config='config.json',\n    method='total'  # Default method\n)\n\n# Only unidirectional connections\nconnections.percent_connection_matrix(\n    config='config.json',\n    method='unidirectional'\n)\n\n# Only bidirectional connections\nconnections.percent_connection_matrix(\n    config='config.json',\n    method='bidirectional'\n)\n</code></pre>"},{"location":"modules/bmplot/#convergence-connection-matrix","title":"Convergence Connection Matrix","text":"<p>Generate a matrix showing the convergence of connections between neuron populations:</p> <pre><code># Mean convergence (default)\nconnections.convergence_connection_matrix(\n    config='config.json',\n    method='mean+std'  # Default method\n)\n\n# Maximum convergence\nconnections.convergence_connection_matrix(\n    config='config.json',\n    method='max'\n)\n\n# Minimum convergence\nconnections.convergence_connection_matrix(\n    config='config.json',\n    method='min'\n)\n\n# Standard deviation of convergence\nconnections.convergence_connection_matrix(\n    config='config.json',\n    method='std'\n)\n</code></pre>"},{"location":"modules/bmplot/#divergence-connection-matrix","title":"Divergence Connection Matrix","text":"<p>Generate a matrix showing the divergence of connections between neuron populations:</p> <pre><code># Mean divergence (default)\nconnections.divergence_connection_matrix(\n    config='config.json',\n    method='mean+std'  # Default method\n)\n\n# Maximum divergence\nconnections.divergence_connection_matrix(\n    config='config.json',\n    method='max'\n)\n\n# Minimum divergence\nconnections.divergence_connection_matrix(\n    config='config.json',\n    method='min'\n)\n\n# Standard deviation of divergence\nconnections.divergence_connection_matrix(\n    config='config.json',\n    method='std'\n)\n</code></pre>"},{"location":"modules/bmplot/#gap-junction-matrix","title":"Gap Junction Matrix","text":"<p>Generate a matrix specifically for gap junctions:</p> <pre><code>connections.gap_junction_matrix(config='config.json', method='percent')\n</code></pre>"},{"location":"modules/bmplot/#connector-percent-matrix","title":"Connector Percent Matrix","text":"<p>Generate a percentage connectivity matrix from a CSV file produced by BMTool connectors:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.connector_percent_matrix(\n    csv_path='connections.csv',\n    title='Percent Connection Matrix',\n    exclude_strings=None  # Optional strings to exclude\n)\n</code></pre>"},{"location":"modules/bmplot/#connection-distance","title":"Connection Distance","text":"<p>Generate a 3D plot with source and target cell locations and connection distance analysis:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.connection_distance(\n    config='config.json',\n    sources='PopA',\n    targets='PopB',\n    source_cell_id=1,  # Node ID of source cell\n    target_id_type='PopB',  # Target population to analyze\n    ignore_z=False  # Whether to ignore z-axis in distance calculations\n)\n</code></pre>"},{"location":"modules/bmplot/#connection-histogram","title":"Connection Histogram","text":"<p>Generate a histogram showing the distribution of connections:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.connection_histogram(\n    config='config.json',\n    sources='PopA',\n    targets='PopB',\n    source_cell='PopA',  # Source cell type\n    target_cell='PopB'   # Target cell type\n)\n</code></pre>"},{"location":"modules/bmplot/#3d-visualization","title":"3D Visualization","text":""},{"location":"modules/bmplot/#3d-cell-positions","title":"3D Cell Positions","text":"<p>Generate a plot of cell positions in 3D space:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.plot_3d_positions(\n    config='config.json',\n    sources=['PopA', 'PopB'],\n    title='3D Cell Positions',\n    save_file=None  # Optional path to save the figure\n)\n</code></pre>"},{"location":"modules/bmplot/#3d-cell-orientation","title":"3D Cell Orientation","text":"<p>Generate a plot showing cell locations and orientation in 3D space:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.plot_3d_cell_rotation(\n    config='config.json',\n    sources=['PopA'],\n    title='3D Cell Orientation',\n    save_file=None  # Optional path to save the figure\n)\n</code></pre>"},{"location":"modules/bmplot/#network-visualization","title":"Network Visualization","text":""},{"location":"modules/bmplot/#network-graph","title":"Network Graph","text":"<p>Plot a network connection diagram:</p> <pre><code>import bmtool.bmplot.connections as connections\n\nconnections.plot_network_graph(\n    config='config.json',\n    sources='LA',\n    targets='LA',\n    tids='pop_name',\n    sids='pop_name',\n    no_prepend_pop=True  # Whether to prepend population name to node labels\n)\n</code></pre>"},{"location":"modules/bmplot/#spike-analysis","title":"Spike Analysis","text":""},{"location":"modules/bmplot/#raster-plot","title":"Raster Plot","text":"<p>Generate a raster plot of spike times:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom bmtool.bmplot.spikes import raster\n\n# Load spike data\nspikes_df = pd.read_csv('spikes.csv')\n\n# Create raster plot\nraster(\n    spikes_df=spikes_df,\n    config='config.json',  # Optional, to load node population data\n    network_name='network',  # Optional, specific network to use\n    groupby='pop_name'  # Column to group spikes by\n)\n\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/#firing-rate-statistics","title":"Firing Rate Statistics","text":"<p>Plot firing rate statistics for different populations:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom bmtool.bmplot.spikes import plot_firing_rate_pop_stats, plot_firing_rate_distribution\n\n# Assuming you already have firing rate statistics dataframes\n# from bmtool.analysis.spikes.compute_firing_rate_stats()\n\n# Plot mean firing rates with error bars\nplot_firing_rate_pop_stats(\n    firing_stats=firing_stats_df,\n    groupby='pop_name'\n)\n\n# Plot distribution of individual firing rates\nplot_firing_rate_distribution(\n    individual_stats=individual_stats_df,\n    groupby='pop_name',\n    plot_type=['box', 'swarm']  # Can use 'box', 'violin', 'swarm' or combinations\n)\n\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/#entrainment-analysis","title":"Entrainment Analysis","text":""},{"location":"modules/bmplot/#spike-power-correlation","title":"Spike-Power Correlation","text":"<p>Plot the correlation between population spike rates and LFP power:</p> <pre><code>import bmtool.bmplot.entrainment as entrainment\n\n# Assuming you have correlation results from bmtool.analysis.entrainment\nentrainment.plot_spike_power_correlation(\n    correlation_results=correlation_results,\n    frequencies=frequencies,\n    pop_names=pop_names\n)\n</code></pre>"},{"location":"modules/bmplot/#lfp-analysis","title":"LFP Analysis","text":""},{"location":"modules/bmplot/#spectrogram","title":"Spectrogram","text":"<p>Plot a spectrogram from LFP data:</p> <p>```python import bmtool.bmplot.lfp as lfp</p>"},{"location":"modules/bmplot/#assuming-you-have-an-xarray-dataset-with-spectrogram-data","title":"Assuming you have an xarray dataset with spectrogram data","text":"<p>lfp.plot_spectrogram(     sxx_xarray=spectrogram_data,     remove_aperiodic=None,  # Optional aperiodic component to remove     log_power=True,  # Whether to use log scale for power     plt_range=[0, 100]  # Frequency range to plot )</p>"},{"location":"modules/connectors/","title":"Connectors Module","text":"<p>The Connectors module provides helper functions and classes that work with BMTK's NetworkBuilder module to facilitate building complex network connectivity patterns. It supports creating reciprocal connections, distance-dependent connections, gap junctions, and more.</p>"},{"location":"modules/connectors/#features","title":"Features","text":"<ul> <li>Unidirectional Connector: Build connections with given probability between populations</li> <li>Reciprocal Connector: Build connections with reciprocal probability between populations</li> <li>Correlated Gap Junction: Create gap junctions correlated with chemical synapses</li> <li>One-to-One Sequential Connector: Create one-to-one mappings between populations</li> </ul>"},{"location":"modules/connectors/#basic-setup","title":"Basic Setup","text":"<p>All connector examples use the following network node structure:</p> <pre><code>from bmtk.builder import NetworkBuilder\n\n# Create main network\nnet = NetworkBuilder('example_net')\nnet.add_nodes(N=100, pop_name='PopA', model_type='biophysical')\nnet.add_nodes(N=100, pop_name='PopB', model_type='biophysical')\n\n# Create background inputs\nbackground = NetworkBuilder('background')\nbackground.add_nodes(N=300, pop_name='tON', potential='exc', model_type='virtual')\n</code></pre>"},{"location":"modules/connectors/#unidirectional-connector","title":"Unidirectional Connector","text":"<p>Build unidirectional connections in a BMTK network model with a given probability within a single population or between two populations.</p> <pre><code>from bmtool.connectors import UnidirectionConnector\n\n# Create connector with 15% connection probability and 1 synapse per connection\nconnector = UnidirectionConnector(p=0.15, n_syn=1)\n\n# Set up source and target nodes\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopB'))\n\n# Add the edges to the network\nnet.add_edges(**connector.edge_params())\n</code></pre>"},{"location":"modules/connectors/#reciprocal-connector","title":"Reciprocal Connector","text":"<p>Build connections with reciprocal probability within a single population or between two populations.</p> <pre><code>from bmtool.connectors import ReciprocalConnector\n\n# Create connector with 15% base probability and 6.7% reciprocal probability\nconnector = ReciprocalConnector(p0=0.15, pr=0.06767705087, n_syn0=1, n_syn1=1, estimate_rho=False)\n\n# Setup for recurrent connections within PopA\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\n\n# Add the edges to the network\nnet.add_edges(**connector.edge_params())\n</code></pre>"},{"location":"modules/connectors/#correlated-gap-junction","title":"Correlated Gap Junction","text":"<p>Build gap junction connections that can be correlated with chemical synapses.</p> <pre><code>from bmtool.connectors import ReciprocalConnector, CorrelatedGapJunction\n\n# First create a chemical synapse connectivity pattern\nconnector = ReciprocalConnector(p0=0.15, pr=0.06, n_syn0=1, n_syn1=1, estimate_rho=False)\nconnector.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\nnet.add_edges(**connector.edge_params())\n\n# Then create gap junctions that are correlated with chemical synapses\ngap_junc = CorrelatedGapJunction(p_non=0.1228, p_uni=0.56, p_rec=1, connector=connector)\ngap_junc.setup_nodes(source=net.nodes(pop_name='PopA'), target=net.nodes(pop_name='PopA'))\n\n# Add gap junction edges\nconn = net.add_edges(\n    is_gap_junction=True,\n    syn_weight=0.0000495,\n    target_sections=None,\n    afferent_section_id=0,\n    afferent_section_pos=0.5,\n    **gap_junc.edge_params()\n)\n</code></pre>"},{"location":"modules/connectors/#one-to-one-sequential-connector","title":"One-to-One Sequential Connector","text":"<p>Build one-to-one correspondence connections between two populations.</p> <pre><code>from bmtool.connectors import OneToOneSequentialConnector\n\n# Create the connector\nconnector = OneToOneSequentialConnector()\n\n# Connect background to PopA\nconnector.setup_nodes(source=background.nodes(), target=net.nodes(pop_name='PopA'))\nnet.add_edges(**connector.edge_params())\n\n# Connect background to PopB\nconnector.setup_nodes(target=net.nodes(pop_name='PopB'))\nnet.add_edges(**connector.edge_params())\n</code></pre>"},{"location":"modules/graphs/","title":"Graphs Module","text":"<p>The Graphs module provides functions for analyzing BMTK network connectivity as graph structures using the NetworkX library. It allows you to convert BMTK networks into graph representations for analysis and visualization.</p>"},{"location":"modules/graphs/#features","title":"Features","text":"<ul> <li>Graph Generation: Convert BMTK network connectivity to NetworkX graphs</li> <li>Data Export: Export connection data for further analysis with other tools</li> </ul>"},{"location":"modules/graphs/#generate-graph","title":"Generate Graph","text":"<p>Convert a BMTK network into a NetworkX graph for analysis:</p> <pre><code>import networkx as nx\nfrom bmtool.graphs import generate_graph\n\n# Generate a graph from a BMTK network model\ngraph = generate_graph(config='config.json', source='LA', target='LA')\n\n# Get basic graph statistics\nprint(\"Number of nodes:\", graph.number_of_nodes())\nprint(\"Number of edges:\", graph.number_of_edges())\n\n# Examine node attributes\nnode_attrs = graph.nodes(data=True)\nprint(\"Sample node:\", list(node_attrs)[0])\n\n# Examine edge attributes\nedge_attrs = graph.edges(data=True)\nprint(\"Sample edge:\", list(edge_attrs)[0])\n</code></pre>"},{"location":"modules/graphs/#export-node-connections","title":"Export Node Connections","text":"<p>Export connection data to CSV format for analysis in other tools:</p> <pre><code>from bmtool.graphs import generate_graph, export_node_connections_to_csv\nimport pandas as pd\n\n# Generate a graph from a BMTK network model\ngraph = generate_graph(config='config.json', source='LA', target='LA')\n\n# Export connection data to CSV\nexport_node_connections_to_csv(graph, 'node_connections.csv')\n\n# Load and view the connection data\ndf = pd.read_csv('node_connections.csv')\nprint(df.head())\n</code></pre>"},{"location":"modules/graphs/#advanced-analysis-with-networkx","title":"Advanced Analysis with NetworkX","text":"<p>Use NetworkX's built-in functions for graph analysis:</p> <pre><code>import networkx as nx\nimport matplotlib.pyplot as plt\nfrom bmtool.graphs import generate_graph\n\n# Generate a graph from a BMTK network model\ngraph = generate_graph(config='config.json', source='LA', target='LA')\n\n# Calculate node degree distribution\ndegrees = [d for n, d in graph.degree()]\nplt.figure(figsize=(8, 6))\nplt.hist(degrees, bins=20)\nplt.xlabel('Degree')\nplt.ylabel('Count')\nplt.title('Node Degree Distribution')\nplt.show()\n\n# Find connected components\nif nx.is_directed(graph):\n    components = list(nx.weakly_connected_components(graph))\nelse:\n    components = list(nx.connected_components(graph))\nprint(f\"Number of connected components: {len(components)}\")\nprint(f\"Size of largest component: {len(max(components, key=len))}\")\n\n# Calculate centrality measures\ncentrality = nx.degree_centrality(graph)\nsorted_centrality = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 nodes by degree centrality:\")\nfor node, cent in sorted_centrality[:5]:\n    print(f\"Node {node}: {cent:.4f}\")\n</code></pre>"},{"location":"modules/graphs/#working-with-networkx-attributes","title":"Working with NetworkX Attributes","text":"<p>Access and manipulate node and edge attributes:</p> <pre><code>import networkx as nx\nfrom bmtool.graphs import generate_graph\n\n# Generate a graph from a BMTK network model\ngraph = generate_graph(config='config.json', source='LA', target='LA')\n\n# Get all unique node labels (e.g., cell types)\nnode_labels = set(nx.get_node_attributes(graph, 'label').values())\nprint(\"Node labels:\", node_labels)\n\n# Count nodes by label\nlabel_counts = {}\nfor node, attrs in graph.nodes(data=True):\n    label = attrs.get('label', 'unknown')\n    label_counts[label] = label_counts.get(label, 0) + 1\nprint(\"Nodes per label:\", label_counts)\n\n# Find all edges with a specific property\nedge_types = {}\nfor u, v, attrs in graph.edges(data=True):\n    edge_type = attrs.get('edge_type', 'unknown')\n    edge_types[edge_type] = edge_types.get(edge_type, 0) + 1\nprint(\"Edge types:\", edge_types)\n</code></pre>"},{"location":"modules/singlecell/","title":"Single Cell Module","text":"<p>The Single Cell module provides tools for analyzing and tuning biophysical cell models. It works with any neuron HOC object and can also turn Allen Institute database SWC and JSON files into HOC objects for analysis.</p>"},{"location":"modules/singlecell/#features","title":"Features","text":"<ul> <li>Passive Properties: Calculate resting membrane potential, input resistance, and membrane time constant</li> <li>Current Injection: Run current clamp simulations to observe spiking behavior</li> <li>FI Curves: Generate frequency-current curves to characterize neuronal excitability</li> <li>ZAP Protocol: Analyze frequency response characteristics using chirp current injections</li> <li>Cell Tuning: Interactive interface for tuning cell parameters</li> <li>VHalf Segregation: Simplify channel tuning by separating channel activation based on Alturki et al. (2016)</li> </ul>"},{"location":"modules/singlecell/#getting-started","title":"Getting Started","text":"<p>First, initialize the Profiler with paths to your templates and mechanisms:</p> <pre><code>from bmtool.singlecell import Profiler\nprofiler = Profiler(template_dir='templates', mechanism_dir='mechanisms', dt=0.1)\n</code></pre> <p>For Allen Institute cell models, load them using:</p> <pre><code>from bmtool.singlecell import load_allen_database_cells\ncell = load_allen_database_cells(path_to_SWC_file, path_to_json_file)\n</code></pre>"},{"location":"modules/singlecell/#passive-properties","title":"Passive Properties","text":"<p>Calculate passive membrane properties (V-rest, input resistance, and time constant):</p> <pre><code>from bmtool.singlecell import Passive, run_and_plot\nimport matplotlib.pyplot as plt\n\nsim = Passive('Cell_Cf', inj_amp=-100., inj_delay=1500., inj_dur=1000.,\n              tstop=2500., method='exp2')\ntitle = 'Passive Cell Current Injection'\nxlabel = 'Time (ms)'\nylabel = 'Membrane Potential (mV)'\nX, Y = run_and_plot(sim, title, xlabel, ylabel, plot_injection_only=True)\nplt.gca().plot(*sim.double_exponential_fit(), 'r:', label='double exponential fit')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"modules/singlecell/#current-clamp","title":"Current Clamp","text":"<p>Run a current clamp simulation:</p> <pre><code>from bmtool.singlecell import CurrentClamp\nsim = CurrentClamp('Cell_Cf', inj_amp=350., inj_delay=1500., inj_dur=1000.,\n                   tstop=3000., threshold=-15.)\nX, Y = run_and_plot(sim, title='Current Injection', xlabel='Time (ms)',\n                    ylabel='Membrane Potential (mV)', plot_injection_only=True)\nplt.show()\n</code></pre>"},{"location":"modules/singlecell/#fi-curve","title":"FI Curve","text":"<p>Generate a frequency-current (FI) curve:</p> <pre><code>from bmtool.singlecell import FI\nsim = FI('Cell_Cf', i_start=0., i_stop=1000., i_increment=50.,\n          tstart=1500., threshold=-15.)\nX, Y = run_and_plot(sim, title='FI Curve', xlabel='Injection (nA)',\n                    ylabel='# Spikes')\nplt.show()\n</code></pre>"},{"location":"modules/singlecell/#zap-protocol","title":"ZAP Protocol","text":"<p>Analyze frequency response using a chirp current (ZAP):</p> <pre><code>from bmtool.singlecell import ZAP\nsim = ZAP('Cell_Cf')\nX, Y = run_and_plot(sim)\nplt.show()\n</code></pre>"},{"location":"modules/singlecell/#cell-tuning","title":"Cell Tuning","text":"<p>The cell tuning interface can be accessed via the command line:</p> <pre><code># For BMTK models with a simulation_config.json file\nbmtool util cell tune --builder\n\n# For non-BMTK cell tuning\nbmtool util cell --template TemplateFile.hoc --mod-folder ./ tune --builder\n</code></pre>"},{"location":"modules/singlecell/#vhalf-segregation","title":"VHalf Segregation","text":"<p>The VHalf Segregation module helps simplify channel tuning:</p> <pre><code># Interactive wizard mode\nbmtool util cell vhseg\n\n# Command mode\nbmtool util cell --template CA3PyramidalCell vhseg --othersec dend[0],dend[1] \\\n  --infvars inf_im --segvars gbar_im --gleak gl_ichan2CA3 --eleak el_ichan2CA3\n</code></pre> <p>For building simple models:</p> <pre><code>bmtool util cell --hoc cell_template.hoc vhsegbuild --build\nbmtool util cell --hoc segmented_template.hoc vhsegbuild\n</code></pre>"},{"location":"modules/slurm/","title":"SLURM Module","text":"<p>The SLURM module provides utilities for running BMTK simulations on SLURM-based high-performance computing clusters. It manages job submission, parameter sweeps, and workflow automation through a YAML-based configuration system.</p>"},{"location":"modules/slurm/#quick-start","title":"Quick Start","text":"<p>The simplest way to use the SLURM module is through the YAML configuration workflow:</p> <pre><code>python run_simulation_w_config.py slurm_config_setup.yaml\n</code></pre> <p>This command reads a YAML configuration file that specifies: - Which simulation configs to run - SLURM resource requirements (time, nodes, memory, partition) - Environment-specific module loading commands - Optional parameter sweeps across model parameters</p>"},{"location":"modules/slurm/#features","title":"Features","text":"<ul> <li>YAML-Based Workflow: Configure entire simulation campaigns in a single YAML file</li> <li>Parameter Sweeps: Systematically vary model parameters with value-based or percentage-based sweeps</li> <li>Multi-Environment Support: Switch between local, Expanse, Hellbender, or other HPC systems</li> <li>Component Directory Cloning: Automatically creates isolated parameter copies for parallel execution</li> <li>Job Monitoring: Tracks SLURM job status and manages dependencies</li> <li>Teams Notifications: Optional webhook notifications for job progress updates</li> <li>Flexible Submission: Submit blocks sequentially or in parallel based on resource availability</li> </ul>"},{"location":"modules/slurm/#yaml-configuration","title":"YAML Configuration","text":"<p>The YAML configuration file controls all aspects of the simulation workflow. Here's a complete example:</p> <pre><code># Simulation configuration\nsimulation:\n  output_name: \"../Run-Storage/final_runs_SST2PV_+25%\"  # Output directory\n  environment: \"local\"                     # Execution environment (local, expanse, hellbender)\n  seed_sweep: 'none'                       # Parameter sweep type (none, seed, multiseed)\n  component_path: \"components\"             # Base path for simulation components\n  parallel_submission: true                # Submit all jobs at once (true) or sequentially (false)\n  use_coreneuron: false                    # Whether to use CoreNeuron backend\n  webhook_url: \"https://...\"               # Microsoft Teams webhook URL for notifications\n\n# SLURM configurations for different environments\nslurm:\n  local:\n    time: \"08:00:00\"       # Maximum runtime (HH:MM:SS)\n    partition: \"batch\"     # SLURM partition name\n    nodes: 1               # Number of nodes\n    ntasks: 40             # Number of tasks (cores)\n    mem: 80               # Total memory in GB\n\n  expanse:\n    time: \"02:30:00\"\n    partition: \"shared\"\n    nodes: 1\n    ntasks: 120\n    mem: 240\n    account: \"umc113\"      # SLURM account (required for some HPC systems)\n\n  hellbender:\n    time: \"02:30:00\"\n    partition: \"general\"\n    nodes: 1\n    ntasks: 30\n    mem: 60\n\n# Module loading commands for different environments\nmodule_commands:\n  expanse:\n    - \"module purge\"\n    - \"module load slurm\"\n    - \"module load cpu/0.17.3b\"\n    - \"module load gcc/10.2.0/npcyll4\"\n    - \"module load openmpi/4.1.1\"\n    - \"export HDF5_USE_FILE_LOCKING=FALSE\"\n\n  hellbender:\n    - \"module load intel_mpi\"\n    - \"module load gcc/12.2.1\"\n    - \"export HDF5_USE_FILE_LOCKING=FALSE\"\n\n  local:\n    - \"module load mpich-x86_64-nopy\"\n\n# Simulation cases configuration\nsimulation_cases:\n  baseline:\n    config_file: \"simulation_config_baseline.json\"  \n  short_1s:\n    config_file: \"simulation_config_short.json\"   \n  long_1s:\n    config_file: \"simulation_config_long.json\"\n\n# Parameter sweep configuration (only used if seed_sweep is not 'none')\nsweep_config:\n  param_name: \"initW\"              # Parameter name in JSON file to modify\n  sweep_method: \"value\"            # \"value\" for explicit values, \"percentage\" for incremental changes\n\n  # Option 1: Specify exact values to use\n  param_values: [2, 3, 4, 5, 6, 7]\n\n  # Option 2: Use percentage-based changes (alternative to param_values)\n  # base_value: 5.0                # Starting parameter value\n  # percent_change: 10             # Percentage increase per iteration\n  # iterations: 2                  # Number of iterations\n\n  base_json_file: \"synaptic_models/synapses_final/SST2PV.json\"  # JSON file to modify (path after components/)\n\n  # For multiseed sweeps: specify related files that change proportionally\n  multiseed_files:\n    - path: \"synaptic_models/synapses_final/Pulse2IT.json\"\n      ratio: 1.28  # Ratio relative to base parameter\n</code></pre>"},{"location":"modules/slurm/#configuration-sections-explained","title":"Configuration Sections Explained","text":""},{"location":"modules/slurm/#simulation","title":"<code>simulation</code>","text":"<ul> <li><code>output_name</code>: Directory where results will be stored (can be relative or absolute)</li> <li><code>environment</code>: Which SLURM config to use (<code>local</code>, <code>expanse</code>, <code>hellbender</code>, or add your own)</li> <li><code>seed_sweep</code>: Type of parameter sweep</li> <li><code>'none'</code>: Run simulations without parameter variations</li> <li><code>'seed'</code>: Vary a single JSON parameter across blocks</li> <li><code>'multiseed'</code>: Vary multiple related JSON parameters with fixed ratios</li> <li><code>component_path</code>: Base directory containing simulation components (mechanisms, synaptic models, etc.)</li> <li><code>parallel_submission</code>: </li> <li><code>true</code>: Submit all blocks at once (faster but uses more resources)</li> <li><code>false</code>: Submit blocks sequentially (one completes before next starts)</li> <li><code>use_coreneuron</code>: Enable CoreNEURON for faster simulations</li> <li><code>webhook_url</code>: Microsoft Teams webhook for job status notifications</li> </ul>"},{"location":"modules/slurm/#slurm","title":"<code>slurm</code>","text":"<p>Define SLURM parameters for each environment. The active environment is selected via <code>simulation.environment</code>.</p> <ul> <li><code>time</code>: Job time limit in <code>HH:MM:SS</code> format</li> <li><code>partition</code>: SLURM partition/queue name</li> <li><code>nodes</code>: Number of compute nodes</li> <li><code>ntasks</code>: Total number of MPI tasks (typically number of cores)</li> <li><code>mem</code>: Total memory in GB (automatically divided across tasks as <code>--mem-per-cpu</code>)</li> <li><code>account</code>: SLURM account to charge (optional, required on some HPC systems)</li> </ul>"},{"location":"modules/slurm/#module_commands","title":"<code>module_commands</code>","text":"<p>Shell commands executed before running simulations. Typically used to: - Load required modules (MPI, compilers, Python) - Set environment variables - Purge conflicting modules</p>"},{"location":"modules/slurm/#simulation_cases","title":"<code>simulation_cases</code>","text":"<p>Dictionary of simulation cases to run. Each case specifies: - <code>config_file</code>: Path to BMTK simulation config JSON file</p> <p>All cases run for each parameter value in a sweep, or once if <code>seed_sweep: 'none'</code>.</p>"},{"location":"modules/slurm/#sweep_config","title":"<code>sweep_config</code>","text":"<p>Only used when <code>seed_sweep</code> is <code>'seed'</code> or <code>'multiseed'</code>.</p> <ul> <li><code>param_name</code>: JSON key to modify (e.g., <code>\"initW\"</code>, <code>\"tau1\"</code>)</li> <li><code>sweep_method</code>: How to generate parameter values</li> <li><code>\"value\"</code>: Use explicit values from <code>param_values</code></li> <li><code>\"percentage\"</code>: Calculate values from <code>base_value</code> and <code>percent_change</code></li> <li><code>param_values</code>: List of explicit parameter values (for <code>sweep_method: \"value\"</code>)</li> <li><code>base_value</code>, <code>percent_change</code>, <code>iterations</code>: For percentage-based sweeps</li> <li><code>base_json_file</code>: Path to JSON file to modify (relative to <code>component_path</code>)</li> <li><code>multiseed_files</code>: (For <code>seed_sweep: 'multiseed'</code>) List of related JSON files</li> <li><code>path</code>: Path to related JSON file</li> <li><code>ratio</code>: Scaling ratio relative to base parameter value</li> </ul>"},{"location":"modules/slurm/#core-classes","title":"Core Classes","text":"<p>The SLURM module provides several classes for direct Python usage. While most users should use the YAML workflow, these classes can be used for custom workflows.</p>"},{"location":"modules/slurm/#simulationblock","title":"SimulationBlock","text":"<p>Represents a single SLURM job block containing one or more simulation cases.</p> <pre><code>from bmtool.SLURM import SimulationBlock\n\nblock = SimulationBlock(\n    block_name=\"block1\",\n    time=\"02:00:00\",\n    partition=\"shared\",\n    nodes=1,\n    ntasks=40,\n    mem=80,  # GB total memory\n    simulation_cases={\n        \"baseline\": \"mpirun nrniv -mpi -python run_network.py simulation_config.json\",\n        \"long\": \"mpirun nrniv -mpi -python run_network.py simulation_config_long.json\"\n    },\n    output_base_dir=\"/path/to/output\",\n    account=\"myaccount\",  # Optional\n    additional_commands=[  # Optional\n        \"module load neuron\",\n        \"export HDF5_USE_FILE_LOCKING=FALSE\"\n    ],\n    status_list=[\"COMPLETED\", \"FAILED\", \"CANCELLED\"],  # Job states to wait for\n    component_path=\"components\"  # Optional\n)\n\n# Submit the block to SLURM\nblock.submit_block()\n\n# Check if all jobs in block are complete\nis_done = block.check_block_status()\n</code></pre> <p>Key Methods: - <code>submit_block()</code>: Creates batch scripts and submits all simulation cases as SLURM jobs - <code>check_block_status()</code>: Returns <code>True</code> if all jobs have reached a state in <code>status_list</code> - <code>check_block_completed()</code>: Returns <code>True</code> only if all jobs are <code>COMPLETED</code> - <code>check_block_running()</code>: Returns <code>True</code> if all jobs are currently <code>RUNNING</code> - <code>create_batch_script(case_name, command)</code>: Generates SLURM batch script for a case</p>"},{"location":"modules/slurm/#blockrunner","title":"BlockRunner","text":"<p>Manages multiple <code>SimulationBlock</code> instances and coordinates parameter sweeps.</p> <pre><code>from bmtool.SLURM import BlockRunner, seedSweep\n\n# Create multiple blocks\nblocks = [\n    SimulationBlock(\"block1\", time=\"02:00:00\", ...),\n    SimulationBlock(\"block2\", time=\"02:00:00\", ...),\n    SimulationBlock(\"block3\", time=\"02:00:00\", ...)\n]\n\n# Create runner with parameter sweep\nrunner = BlockRunner(\n    blocks=blocks,\n    json_file_path=\"synaptic_models/synapses_final/SST2PV.json\",  # Relative to component_path\n    param_name=\"initW\",\n    param_values=[2.0, 3.0, 4.0],\n    check_interval=60,  # Seconds between status checks\n    webhook=\"https://...\"  # Optional Teams webhook\n)\n\n# Submit all blocks in parallel\nrunner.submit_blocks_parallel()\n\n# Or submit sequentially (each waits for previous to complete)\n# runner.submit_blocks_sequentially()\n</code></pre> <p>Key Methods: - <code>submit_blocks_sequentially()</code>: Submits blocks one at a time, waiting for each to complete - <code>submit_blocks_parallel()</code>: Submits all blocks at once for simultaneous execution - <code>restore_component_paths()</code>: Restores original component directory paths after cloning</p> <p>Parameters: - <code>blocks</code>: List of <code>SimulationBlock</code> instances - <code>json_file_path</code>: Path to JSON file to modify (relative to <code>component_path</code>) - <code>param_name</code>: Parameter name to modify in JSON - <code>param_values</code>: List of parameter values (one per block) - <code>check_interval</code>: Seconds to wait between SLURM status checks - <code>syn_dict</code>: Dictionary for multiseed sweeps <code>{\"json_file_path\": \"...\", \"ratio\": 1.28}</code> - <code>webhook</code>: Microsoft Teams webhook URL for notifications</p>"},{"location":"modules/slurm/#seedsweep","title":"seedSweep","text":"<p>Edits a single parameter in a JSON file for parameter sweeps.</p> <pre><code>from bmtool.SLURM import seedSweep\n\neditor = seedSweep(\n    json_file_path=\"components/synaptic_models/SST2PV.json\",\n    param_name=\"initW\"\n)\n\n# Update the parameter value\neditor.edit_json(5.0)\n\n# Change to a different JSON file\neditor.change_json_file_path(\"components/synaptic_models/Pulse2IT.json\")\neditor.edit_json(3.5)\n</code></pre> <p>Methods: - <code>edit_json(new_value)</code>: Updates the JSON file with the new parameter value - <code>change_json_file_path(new_json_file_path)</code>: Points the editor to a different JSON file</p>"},{"location":"modules/slurm/#multiseedsweep","title":"multiSeedSweep","text":"<p>Extends <code>seedSweep</code> to modify multiple related JSON files with proportional scaling.</p> <pre><code>from bmtool.SLURM import multiSeedSweep\n\neditor = multiSeedSweep(\n    base_json_file_path=\"components/synaptic_models/SST2PV.json\",\n    param_name=\"initW\",\n    syn_dict={\n        \"json_file_path\": \"components/synaptic_models/Pulse2IT.json\",\n        \"ratio\": 1.28  # Pulse2IT.initW = SST2PV.initW * 1.28\n    },\n    base_ratio=1\n)\n\n# Update base JSON to 5.0 and related JSON to 5.0 * 1.28 = 6.4\neditor.edit_all_jsons(5.0)\n</code></pre> <p>Methods: - <code>edit_all_jsons(new_value)</code>: Updates base JSON and scales related JSON proportionally</p>"},{"location":"modules/slurm/#workflow-explanation","title":"Workflow Explanation","text":"<p>Understanding the complete workflow helps debug issues and customize behavior.</p>"},{"location":"modules/slurm/#1-configuration-loading","title":"1. Configuration Loading","text":"<p><code>run_simulation_w_config.py</code> parses the YAML file and extracts: - SLURM resource requirements - Environment-specific commands - Simulation cases to run - Parameter sweep settings</p>"},{"location":"modules/slurm/#2-parameter-value-generation","title":"2. Parameter Value Generation","text":"<p>Based on <code>sweep_config.sweep_method</code>: - <code>\"value\"</code>: Uses <code>param_values</code> directly - <code>\"percentage\"</code>: Calculates values: <code>[base_value, base_value * (1 + percent/100), ...]</code></p>"},{"location":"modules/slurm/#3-simulationblock-creation","title":"3. SimulationBlock Creation","text":"<p>One <code>SimulationBlock</code> is created for each parameter value (or just one if <code>seed_sweep: 'none'</code>).</p> <p>Each block contains all simulation cases from <code>simulation_cases</code>.</p>"},{"location":"modules/slurm/#4-component-directory-cloning","title":"4. Component Directory Cloning","text":"<p>For parameter sweeps, <code>BlockRunner</code> clones the component directory for each block: - <code>components</code> \u2192 <code>components1</code>, <code>components2</code>, <code>components3</code>, ... - Each clone gets its own parameter value edited in the JSON files - Prevents conflicts during parallel execution</p>"},{"location":"modules/slurm/#5-json-parameter-editing","title":"5. JSON Parameter Editing","text":"<p>For each cloned directory: - seed sweep: Edits <code>base_json_file</code> using <code>seedSweep</code> - multiseed sweep: Edits base file and related files using <code>multiSeedSweep</code> - All edits happen in the cloned directory (e.g., <code>components1/...</code>)</p>"},{"location":"modules/slurm/#6-batch-script-generation","title":"6. Batch Script Generation","text":"<p>For each simulation case in each block, a SLURM batch script is created with: - SLURM resource directives (<code>--time</code>, <code>--partition</code>, <code>--nodes</code>, <code>--ntasks</code>, <code>--mem-per-cpu</code>) - Module loading commands - Environment variables: <code>COMPONENT_PATH</code> and <code>OUTPUT_DIR</code> - Simulation command</p>"},{"location":"modules/slurm/#7-slurm-submission","title":"7. SLURM Submission","text":"<p>Scripts are submitted via <code>sbatch</code>. Depending on <code>parallel_submission</code>: - Parallel: All blocks submitted immediately - Sequential: Each block waits for previous to reach a state in <code>status_list</code></p>"},{"location":"modules/slurm/#8-job-monitoring","title":"8. Job Monitoring","text":"<p><code>BlockRunner</code> periodically checks job status using <code>scontrol show job</code>. When all jobs complete: - Original component paths are restored - Teams notification sent (if webhook configured)</p>"},{"location":"modules/slurm/#9-simulation-execution","title":"9. Simulation Execution","text":"<p>Each SLURM job runs <code>run_network.py</code> which: - Reads <code>COMPONENT_PATH</code> environment variable - Temporarily updates network config to use the cloned component directory - Runs BMTK simulation - Saves synaptic parameters report - Copies component directory to output for reproducibility - Restores original network config</p>"},{"location":"modules/slurm/#examples","title":"Examples","text":""},{"location":"modules/slurm/#example-1-single-simulation-no-parameter-sweep","title":"Example 1: Single Simulation (No Parameter Sweep)","text":"<pre><code>simulation:\n  output_name: \"../Run-Storage/single_run\"\n  environment: \"local\"\n  seed_sweep: 'none'\n  component_path: \"components\"\n  parallel_submission: false\n  use_coreneuron: false\n\nslurm:\n  local:\n    time: \"04:00:00\"\n    partition: \"batch\"\n    nodes: 1\n    ntasks: 40\n    mem: 80\n\nmodule_commands:\n  local:\n    - \"module load mpich-x86_64-nopy\"\n\nsimulation_cases:\n  baseline:\n    config_file: \"simulation_config_baseline.json\"\n</code></pre> <p>Run with: <pre><code>python run_simulation_w_config.py slurm_config_setup.yaml\n</code></pre></p>"},{"location":"modules/slurm/#example-2-value-based-parameter-sweep","title":"Example 2: Value-Based Parameter Sweep","text":"<pre><code>simulation:\n  output_name: \"../Run-Storage/sweep_initW\"\n  environment: \"expanse\"\n  seed_sweep: 'seed'\n  component_path: \"components\"\n  parallel_submission: true\n  use_coreneuron: false\n\nslurm:\n  expanse:\n    time: \"02:00:00\"\n    partition: \"shared\"\n    nodes: 1\n    ntasks: 120\n    mem: 240\n    account: \"umc113\"\n\nmodule_commands:\n  expanse:\n    - \"module purge\"\n    - \"module load cpu/0.17.3b\"\n    - \"module load gcc/10.2.0/npcyll4\"\n    - \"module load openmpi/4.1.1\"\n\nsimulation_cases:\n  baseline:\n    config_file: \"simulation_config_baseline.json\"\n\nsweep_config:\n  param_name: \"initW\"\n  sweep_method: \"value\"\n  param_values: [2.0, 3.0, 4.0, 5.0, 6.0]\n  base_json_file: \"synaptic_models/synapses_final/SST2PV.json\"\n</code></pre> <p>This creates 5 blocks, each with a different <code>initW</code> value in <code>SST2PV.json</code>.</p>"},{"location":"modules/slurm/#example-3-percentage-based-parameter-sweep","title":"Example 3: Percentage-Based Parameter Sweep","text":"<pre><code>sweep_config:\n  param_name: \"tau1\"\n  sweep_method: \"percentage\"\n  base_value: 10.0\n  percent_change: 25\n  iterations: 3\n  base_json_file: \"synaptic_models/synapses_final/SST2PV.json\"\n</code></pre> <p>This generates parameter values: - Block 1: <code>10.0</code> - Block 2: <code>10.0 * 1.25 = 12.5</code> - Block 3: <code>12.5 * 1.25 = 15.625</code> - Block 4: <code>15.625 * 1.25 = 19.53</code></p>"},{"location":"modules/slurm/#example-4-multi-seed-sweep-with-proportional-scaling","title":"Example 4: Multi-Seed Sweep with Proportional Scaling","text":"<pre><code>simulation:\n  seed_sweep: 'multiseed'\n  # ... other settings ...\n\nsweep_config:\n  param_name: \"initW\"\n  sweep_method: \"value\"\n  param_values: [2.0, 4.0, 6.0]\n  base_json_file: \"synaptic_models/synapses_final/SST2PV.json\"\n  multiseed_files:\n    - path: \"synaptic_models/synapses_final/Pulse2IT.json\"\n      ratio: 1.28\n</code></pre> <p>This modifies both files in each block: - Block 1: <code>SST2PV.initW = 2.0</code>, <code>Pulse2IT.initW = 2.0 * 1.28 = 2.56</code> - Block 2: <code>SST2PV.initW = 4.0</code>, <code>Pulse2IT.initW = 4.0 * 1.28 = 5.12</code> - Block 3: <code>SST2PV.initW = 6.0</code>, <code>Pulse2IT.initW = 6.0 * 1.28 = 7.68</code></p>"},{"location":"modules/slurm/#example-5-multiple-simulation-cases","title":"Example 5: Multiple Simulation Cases","text":"<pre><code>simulation_cases:\n  baseline:\n    config_file: \"simulation_config_baseline.json\"\n  short:\n    config_file: \"simulation_config_short.json\"\n  long:\n    config_file: \"simulation_config_long.json\"\n\nsweep_config:\n  param_values: [2.0, 4.0]\n  # ... other sweep settings ...\n</code></pre> <p>Each block runs all three cases: - Block 1 (param=2.0): runs baseline, short, and long configs - Block 2 (param=4.0): runs baseline, short, and long configs</p> <p>Total: 6 SLURM jobs (2 blocks \u00d7 3 cases)</p>"},{"location":"modules/slurm/#advanced-features","title":"Advanced Features","text":""},{"location":"modules/slurm/#microsoft-teams-notifications","title":"Microsoft Teams Notifications","text":"<p>Configure webhook notifications to track simulation progress:</p> <pre><code>simulation:\n  webhook_url: \"https://prod-XX.westus.logic.azure.com/workflows/...\"\n</code></pre> <p>Notifications are sent when: - Each block is submitted - All simulations complete</p>"},{"location":"modules/slurm/#parallel-vs-sequential-submission","title":"Parallel vs Sequential Submission","text":"<p>Parallel submission (<code>parallel_submission: true</code>): - All blocks submitted immediately to SLURM queue - Faster overall completion if resources available - Requires more disk space (all component directories cloned at once) - Higher queue usage</p> <p>Sequential submission (<code>parallel_submission: false</code>): - Each block waits for previous to reach <code>status_list</code> states before submitting - Lower resource usage at any given time - Useful when queue has strict limits - Default <code>status_list: [\"COMPLETED\", \"FAILED\", \"CANCELLED\"]</code> waits for full completion</p>"},{"location":"modules/slurm/#custom-status-lists","title":"Custom Status Lists","text":"<p>Control when the next block submits by customizing <code>status_list</code> in <code>SimulationBlock</code>:</p> <pre><code># Wait for completion before next block\nstatus_list=[\"COMPLETED\", \"FAILED\", \"CANCELLED\"]\n\n# Submit next block as soon as current starts running (aggressive)\nstatus_list=[\"RUNNING\", \"COMPLETED\", \"FAILED\", \"CANCELLED\"]\n</code></pre>"},{"location":"modules/slurm/#component-path-cloning","title":"Component Path Cloning","text":"<p>The component cloning mechanism: 1. Original directory: <code>components/</code> 2. Cloned directories: <code>components1/</code>, <code>components2/</code>, <code>components3/</code>, ... 3. Each clone has independent parameter values 4. Original directory restored after completion</p> <p>This allows parallel sweeps without file conflicts.</p>"},{"location":"modules/slurm/#globus-file-transfer","title":"Globus File Transfer","text":"<p>For transferring large result datasets between HPC systems:</p> <pre><code>from bmtool.SLURM import globus_transfer\n\nglobus_transfer(\n    source_endpoint=\"endpoint-uuid-1\",\n    dest_endpoint=\"endpoint-uuid-2\",\n    source_path=\"/path/on/source\",\n    dest_path=\"/path/on/dest\"\n)\n</code></pre>"},{"location":"modules/slurm/#utility-functions","title":"Utility Functions","text":"<pre><code>from bmtool.SLURM import check_job_status, submit_job, send_teams_message\n\n# Check SLURM job status\nstatus = check_job_status(\"12345678\")  # Returns \"RUNNING\", \"COMPLETED\", etc.\n\n# Submit a batch script\njob_id = submit_job(\"path/to/script.sh\")\n\n# Send Teams notification\nsend_teams_message(\n    webhook=\"https://...\",\n    message=\"Simulation started!\"\n)\n</code></pre>"},{"location":"modules/synapses/","title":"Synapses Module","text":"<p>The Synapses module provides tools for configuring and tuning synaptic connections in NEURON models, including both chemical synapses and electrical synapses (gap junctions).</p>"},{"location":"modules/synapses/#features","title":"Features","text":"<ul> <li>Synaptic Tuner: Interactive tuning of synaptic properties via Jupyter notebooks</li> <li>Gap Junction Tuner: Tools for adjusting gap junction properties with coupling coefficient optimization</li> </ul>"},{"location":"modules/synapses/#synaptic-tuner","title":"Synaptic Tuner","text":"<p>The SynapseTuner provides two main usage modes: one for BMTK networks and one for pure NEURON models. It offers an interactive interface with sliders in a Jupyter notebook to adjust synaptic parameters and view the effects in real-time.</p>"},{"location":"modules/synapses/#key-features","title":"Key Features","text":"<ul> <li>Interactive sliders for all synapse parameters</li> <li>Visualization of postsynaptic responses</li> <li>Support for BMTK network configurations</li> <li>Support for pure NEURON model tuning</li> <li>Parameter optimization algorithms</li> <li>Support for various synapse types (Exp2Syn, AMPA, NMDA, STP mechanisms, etc.)</li> </ul>"},{"location":"modules/synapses/#example-usage-with-bmtk","title":"Example Usage with BMTK","text":"<pre><code>from bmtool.synapses import SynapseTuner\n\n# Create a tuner for BMTK networks\ntuner = SynapseTuner(\n    config='simulation_config.json',  # Path to BMTK config\n    current_name='i',                 # Synaptic current to record\n    slider_vars=['initW','Dep','Fac','Use','tau1','tau2']  # Parameters for sliders\n)\n\n# Display the interactive tuner\ntuner.InteractiveTuner()\n\n# Switch between different connections in your network\ntuner._switch_connection('PV2Exc')\n</code></pre>"},{"location":"modules/synapses/#example-usage-with-pure-neuron","title":"Example Usage with Pure NEURON","text":"<pre><code>from bmtool.synapses import SynapseTuner\n\n# Define general and connection-specific settings\ngeneral_settings = {\n    'vclamp': True,\n    'rise_interval': (0.1, 0.9),\n    'tstart': 500.,\n    'tdur': 100.,\n    'threshold': -15.,\n    'delay': 1.3,\n    'weight': 1.,\n    'dt': 0.025,\n    'celsius': 20\n}\n\nconn_type_settings = {\n    'Exc2FSI': {\n        'spec_settings': {\n            'post_cell': 'FSI_Cell',\n            'vclamp_amp': -70.,\n            'sec_x': 0.5,\n            'sec_id': 1,\n            \"level_of_detail\": \"AMPA_NMDA_STP\",\n        },\n        'spec_syn_param': {\n            'initW': 0.76,\n            'tau_r_AMPA': 0.45,\n            'tau_d_AMPA': 7.5,\n            'Use': 0.13,\n            'Dep': 0.,\n            'Fac': 200.\n        },\n    }\n}\n\n# Create tuner with custom settings\ntuner = SynapseTuner(\n    general_settings=general_settings,\n    conn_type_settings=conn_type_settings\n)\n\n# Display the interactive tuner\ntuner.InteractiveTuner()\n</code></pre>"},{"location":"modules/synapses/#gap-junction-tuner","title":"Gap Junction Tuner","text":"<p>The GapJunctionTuner provides tools for tuning electrical synapses (gap junctions) to achieve desired coupling coefficients.</p>"},{"location":"modules/synapses/#key-features_1","title":"Key Features","text":"<ul> <li>Interactive sliders for gap junction resistance</li> <li>Calculation of coupling coefficient</li> <li>Optimization algorithm to automatically find resistance values for desired coupling coefficients</li> <li>Visualization of voltage changes in coupled cells</li> </ul>"},{"location":"modules/synapses/#example-usage","title":"Example Usage","text":"<pre><code>from bmtool.synapses import GapJunctionTuner\n\n# Create a tuner for gap junctions\ntuner = GapJunctionTuner(\n    cell1_template='Interneuron',\n    cell2_template='Interneuron',\n    template_dir='path/to/templates',\n    mod_dir='path/to/mechanisms'\n)\n\n# Display the interactive tuner\ntuner.show()\n\n# Use the optimizer to find resistance for a target coupling coefficient\noptimal_resistance = tuner.optimize(target_cc=0.05)\nprint(f\"Optimal gap junction resistance: {optimal_resistance} MOhm\")\n</code></pre>"},{"location":"modules/synapses/#advanced-features","title":"Advanced Features","text":""},{"location":"modules/synapses/#synapse-optimization","title":"Synapse Optimization","text":"<p>Use the SynapseOptimizer to automatically tune synapse parameters:</p> <pre><code>from bmtool.synapses import SynapseOptimizer\n\n# Create the optimizer\noptimizer = SynapseOptimizer(tuner)\n\n# Define parameter bounds\nparam_bounds = {\n    'Dep': (0, 200.0),\n    'Fac': (0, 400.0),\n    'Use': (0.1, 1.0),\n    'tau1': (1, 4),\n    'tau2': (5, 20)\n}\n\n# Define target metrics\ntarget_metrics = {\n    'max_amp': 5.0,  # Target maximum amplitude (mV)\n    'half_width': 10.0,  # Target half-width (ms)\n    'rise_time': 2.0  # Target rise time (ms)\n}\n\n# Run optimization\nresult = optimizer.optimize_parameters(param_bounds, target_metrics)\nprint(result)\n</code></pre>"},{"location":"modules/synapses/#short-term-plasticity-analysis","title":"Short-Term Plasticity Analysis","text":"<p>Analyze frequency response characteristics of synapses with short-term plasticity:</p> <pre><code># Analyze STP frequency response\nfrequencies, responses = tuner.stp_frequency_response(\n    frequencies=[1, 5, 10, 20, 50, 100],  # Hz\n    duration=1000  # ms\n)\n\n# Plot the results\nimport matplotlib.pyplot as plt\nplt.plot(frequencies, responses)\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Steady-state Response')\nplt.title('STP Frequency Response')\nplt.show()\n</code></pre>"},{"location":"modules/analysis/entrainment/","title":"Entrainment Analysis","text":"<p>This module provides tools for analyzing the relationship between spike timing and LFP oscillations, including phase-locking and spike-field coherence metrics.</p>"},{"location":"modules/analysis/entrainment/#phase-locking-analysis","title":"Phase-Locking Analysis","text":"<p>Calculate phase-locking between spikes and LFP:</p> <pre><code>import numpy as np\nfrom bmtool.analysis.entrainment import (\n    calculate_spike_lfp_plv,\n    calculate_signal_signal_plv,\n    calculate_ppc,\n    calculate_ppc2\n)\nfrom bmtool.analysis.spikes import load_spikes_to_df\nfrom bmtool.analysis.lfp import load_ecp_to_xarray, butter_bandpass_filter\n\n# Load data\nspikes_df = load_spikes_to_df('output/spikes.h5', network_name='network')\nlfp_data = load_ecp_to_xarray('output/ecp.h5', demean=True)\n\n# Filter LFP to specific frequency band (e.g., theta: 4-8 Hz)\nlfp_signal = lfp_data.sel(channel=0).data\nfs = 10000  # Hz\nfiltered_lfp = butter_bandpass_filter(\n    data=lfp_signal,\n    lowcut=4,\n    highcut=8,\n    fs=fs\n)\n\n# Get spike times for a specific population\npopulation_spikes = spikes_df[spikes_df['pop_name'] == 'Pyramidal']\nspike_times = population_spikes['timestamps'].values\n\n# Calculate phase-locking value (PLV)\nplv = calculate_spike_lfp_plv(\n    spike_times=spike_times,\n    lfp_data=filtered_lfp,\n    spike_fs=1000,  # Spike times in milliseconds\n    lfp_fs=fs,\n    filter_method='butter',\n    lowcut=4,\n    highcut=8\n)\nprint(f\"Phase-locking value: {plv}\")\n\n# Calculate pairwise phase consistency (PPC)\nppc = calculate_ppc(\n    spike_times=spike_times,\n    lfp_data=filtered_lfp,\n    spike_fs=1000,\n    lfp_fs=fs,\n    filter_method='butter',\n    lowcut=4,\n    highcut=8\n)\nprint(f\"Pairwise phase consistency: {ppc}\")\n\n# Calculate PPC2 (alternate method)\nppc2 = calculate_ppc2(\n    spike_times=spike_times,\n    lfp_data=filtered_lfp,\n    spike_fs=1000,\n    lfp_fs=fs,\n    filter_method='butter',\n    lowcut=4,\n    highcut=8\n)\nprint(f\"PPC2 value: {ppc2}\")\n</code></pre>"},{"location":"modules/analysis/entrainment/#signal-signal-phase-locking","title":"Signal-Signal Phase Locking","text":"<p>Calculate phase-locking between two signals (e.g., two LFP channels):</p> <pre><code>from bmtool.analysis.entrainment import calculate_signal_signal_plv\n\n# Calculate PLV between two LFP signals\nplv = calculate_signal_signal_plv(\n    signal1=lfp_channel1,\n    signal2=lfp_channel2,\n    fs=10000,\n    freq_of_interest=40,  # For wavelet method\n    filter_method='wavelet',\n    bandwidth=2.0\n)\nprint(f\"Signal-signal PLV: {plv}\")\n</code></pre>"},{"location":"modules/analysis/entrainment/#population-entrainment-analysis","title":"Population Entrainment Analysis","text":"<p>Analyze entrainment across multiple cells or populations:</p> <pre><code>from bmtool.analysis.entrainment import calculate_entrainment_per_cell\n\n# Calculate entrainment metrics for all cells\nentrainment_dict = calculate_entrainment_per_cell(\n    spike_df=spikes_df,\n    lfp_data=lfp_signal,\n    filter_method='wavelet',\n    pop_names=['Pyramidal', 'Basket'],\n    entrainment_method='plv',  # or 'ppc', 'ppc2'\n    spike_fs=1000,\n    lfp_fs=fs,\n    freqs=[4, 8, 20, 40, 80]  # Frequencies of interest\n)\n\n# Print results for each population\nfor pop, cell_dict in entrainment_dict.items():\n    print(f\"\\nPopulation: {pop}\")\n    for cell_id, freq_dict in cell_dict.items():\n        print(f\"Cell {cell_id}:\")\n        for freq, value in freq_dict.items():\n            print(f\"  {freq} Hz: {value:.3f}\")\n</code></pre>"},{"location":"modules/analysis/entrainment/#get-spikes-in-lfp-cycles","title":"Get Spikes in LFP Cycles","text":"<p>Extract spikes that occur within specific LFP oscillation cycles:</p> <pre><code>from bmtool.analysis.entrainment import get_spikes_in_cycle\n\n# Get spikes occurring within LFP cycles\nspikes_in_cycles = get_spikes_in_cycle(\n    spike_times=spike_times,\n    lfp_data=lfp_data,\n    fs=10000,\n    freq_of_interest=40\n)\n</code></pre>"},{"location":"modules/analysis/entrainment/#firing-rate-phase-amplitude-analysis","title":"Firing Rate Phase-Amplitude Analysis","text":"<p>Compute firing rate as a function of LFP phase and amplitude:</p> <pre><code>from bmtool.analysis.entrainment import compute_fr_hist_phase_amplitude\n\n# Compute phase-amplitude histogram for firing rates\nphase_amp_hist = compute_fr_hist_phase_amplitude(\n    spike_df=spikes_df,\n    lfp_data=lfp_data,\n    fs=10000,\n    pop_names=['Pyramidal', 'Basket'],\n    freq_of_interest=40\n)\n</code></pre>"},{"location":"modules/analysis/lfp/","title":"LFP/ECP Analysis","text":"<p>This module provides tools for analyzing Local Field Potentials (LFP) and Extracellular Potentials (ECP) from BMTK simulations.</p>"},{"location":"modules/analysis/lfp/#loading-and-processing-lfpecp-data","title":"Loading and Processing LFP/ECP Data","text":"<pre><code>import numpy as np\nimport xarray as xr\nfrom bmtool.analysis.lfp import load_ecp_to_xarray, ecp_to_lfp, slice_time_series\n\n# Load ECP data\necp_data = load_ecp_to_xarray('output/ecp.h5', demean=True)\n\n# Convert ECP to LFP with filtering\nlfp_data = ecp_to_lfp(\n    ecp_data=ecp_data,\n    cutoff=250,        # Cutoff frequency in Hz\n    fs=10000           # Sampling frequency in Hz\n)\n\n# Slice data to specific time range\nlfp_slice = slice_time_series(lfp_data, time_ranges=(500, 1500))\n</code></pre>"},{"location":"modules/analysis/lfp/#spectral-analysis","title":"Spectral Analysis","text":"<p>Analyze frequency content using wavelets and FOOOF:</p> <pre><code>from bmtool.analysis.lfp import (\n    cwt_spectrogram_xarray,\n    fit_fooof,\n    generate_resd_from_fooof\n)\nimport matplotlib.pyplot as plt\n\n# Calculate wavelet spectrogram\nspectrogram = cwt_spectrogram_xarray(\n    x=lfp_slice.sel(channel=0).data,\n    fs=10000,\n    freq_range=(1, 100),\n    nNotes=8\n)\n\n# Calculate power spectrum and fit FOOOF model\nfrom scipy import signal\n\n# Calculate power spectrum\nfreqs, pxx = signal.welch(lfp_data.sel(channel=0).data, fs=10000, nperseg=4096)\n\n# Fit FOOOF model\nfooof_model = fit_fooof(\n    f=freqs,\n    pxx=pxx,\n    freq_range=[1, 100],\n    peak_width_limits=[1, 8],\n    max_n_peaks=6\n)\n\n# Get residuals between original spectrum and aperiodic fit\nresid_spectra, idx_freqs = generate_resd_from_fooof(fooof_model)\n</code></pre>"},{"location":"modules/analysis/lfp/#filtering-and-signal-processing","title":"Filtering and Signal Processing","text":"<p>Apply various filters to LFP/ECP data:</p> <pre><code>from bmtool.analysis.lfp import butter_bandpass_filter, wavelet_filter, calculate_SNR\n\n# Band-pass filter\nfiltered_signal = butter_bandpass_filter(\n    data=lfp_data.sel(channel=0).data,\n    lowcut=30,\n    highcut=80,\n    fs=10000\n)\n\n# Wavelet filter centered at specific frequency\ngamma_filtered = wavelet_filter(\n    x=lfp_data.sel(channel=0).data,\n    freq=40,        # Center frequency in Hz\n    fs=10000,       # Sampling rate\n    bandwidth=10    # Bandwidth in Hz\n)\n\n# Calculate signal-to-noise ratio\nsnr = calculate_SNR(fooof_model, freq_band=(30, 80))\nprint(f\"Signal-to-noise ratio in gamma band: {snr}\")\n</code></pre>"},{"location":"modules/analysis/netcon_reports/","title":"Network Connectivity Analysis","text":"<p>This module provides tools for analyzing and visualizing network connectivity patterns in BMTK simulations.</p>"},{"location":"modules/analysis/netcon_reports/#overview","title":"Overview","text":"<p>The <code>netcon_reports</code> module helps analyze: - Connection statistics between populations - Synaptic weight distributions - Network topology metrics - Connectivity visualization</p>"},{"location":"modules/analysis/netcon_reports/#example-usage","title":"Example Usage","text":"<p>Coming soon. This module is currently under development.</p>"},{"location":"modules/analysis/spikes/","title":"Spike Analysis","text":"<p>This module provides tools for analyzing spike data from BMTK simulations.</p>"},{"location":"modules/analysis/spikes/#loading-and-processing-spike-data","title":"Loading and Processing Spike Data","text":"<pre><code>import pandas as pd\nfrom bmtool.analysis.spikes import load_spikes_to_df, compute_firing_rate_stats\n\n# Load spike data from a simulation\nspikes_df = load_spikes_to_df(\n    spike_file='output/spikes.h5',\n    network_name='network',\n    config='config.json',  # Optional: to label cell types\n    groupby='pop_name'\n)\n\n# Get basic spike statistics by population\npop_stats, individual_stats = compute_firing_rate_stats(\n    df=spikes_df,\n    groupby='pop_name',\n    start_time=500,\n    stop_time=1500\n)\n\nprint(\"Population firing rate statistics:\")\nprint(pop_stats)\n</code></pre>"},{"location":"modules/analysis/spikes/#population-spike-rates","title":"Population Spike Rates","text":"<p>Calculate and visualize population spike rates:</p> <pre><code>from bmtool.analysis.spikes import get_population_spike_rate\nimport matplotlib.pyplot as plt\n\n# Calculate population spike rates over time\npopulation_rates = get_population_spike_rate(\n    spikes=spikes_df,\n    fs=400.0,               # Sampling frequency in Hz\n    t_start=0,\n    t_stop=2000,\n    config='config.json',   # Optional\n    network_name='network'  # Optional\n)\n\n# Plot population rates\nfor pop_name, rates in population_rates.items():\n    plt.plot(rates, label=pop_name)\nplt.xlabel('Time (ms)')\nplt.ylabel('Firing Rate (Hz)')\nplt.legend()\nplt.title('Population Firing Rates')\nplt.show()\n</code></pre>"},{"location":"modules/analysis/spikes/#visualizing-spike-data","title":"Visualizing Spike Data","text":"<p>Use BMPlot module for spike data visualization:</p> <pre><code>from bmtool.bmplot import raster, plot_firing_rate_pop_stats, plot_firing_rate_distribution\n\n# Create a raster plot\nfig, ax = plt.subplots(figsize=(10, 6))\nraster(\n    spikes_df=spikes_df,\n    groupby='pop_name',\n    time_range=(0, 2000),\n    ax=ax\n)\nplt.show()\n\n# Plot firing rate statistics\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_pop_stats(\n    firing_stats=pop_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n\n# Plot firing rate distributions\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_distribution(\n    individual_stats=individual_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/connections/","title":"Network Connections Plotting","text":"<p>The <code>bmplot.connections</code> module provides functions for visualizing network connectivity patterns and connection statistics.</p>"},{"location":"modules/bmplot/connections/#connection-matrix","title":"Connection Matrix","text":"<pre><code>from bmtool.bmplot.connections import plot_connection_matrix\nimport matplotlib.pyplot as plt\n\n# Plot connection matrix\nfig, ax = plt.subplots(figsize=(8, 8))\nplot_connection_matrix(\n    connection_data=connection_matrix,\n    source_pops=source_populations,\n    target_pops=target_populations,\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/connections/#connection-statistics","title":"Connection Statistics","text":"<pre><code>from bmtool.bmplot.connections import plot_connection_statistics\n\n# Plot connection statistics\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_connection_statistics(\n    stats_df=connection_stats,\n    metric='convergence',  # or 'divergence', 'probability'\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/connections/#weight-distributions","title":"Weight Distributions","text":"<pre><code>from bmtool.bmplot.connections import plot_weight_distribution\n\n# Plot synaptic weight distributions\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_weight_distribution(\n    weights=synapse_weights,\n    by_population=True,\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/entrainment/","title":"Entrainment Plotting","text":"<p>The <code>bmplot.entrainment</code> module provides functions for visualizing phase-locking, coherence, and other entrainment metrics.</p>"},{"location":"modules/bmplot/entrainment/#spike-power-correlation","title":"Spike Power Correlation","text":"<p>Plot the correlation between population spike rates and LFP power across frequencies:</p> <pre><code>from bmtool.bmplot.entrainment import plot_spike_power_correlation\nimport matplotlib.pyplot as plt\n\n# Plot spike rate-LFP power correlation across frequencies\nfig = plot_spike_power_correlation(\n    spike_df=spike_df,\n    lfp_data=lfp,\n    fs=400,\n    pop_names=['PV', 'SST'],\n    freq_range=(10, 100),\n    freq_step=5\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/entrainment/#trial-averaged-spike-power-correlation","title":"Trial-Averaged Spike Power Correlation","text":"<p>Plot trial-averaged spike rate-LFP power correlation:</p> <pre><code>from bmtool.bmplot.entrainment import plot_trial_avg_spike_power_correlation\n\nfig = plot_trial_avg_spike_power_correlation(\n    spike_df=spike_df,\n    lfp_data=lfp,\n    fs=400,\n    pop_names=['PV', 'SST'],\n    time_range=[(0, 1000), (1000, 2000)],  # Trial time ranges\n    freq_range=(10, 100),\n    freq_step=5\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/entrainment/#population-entrainment","title":"Population Entrainment","text":"<p>Plot entrainment (PPC) metrics by population:</p> <pre><code>from bmtool.bmplot.entrainment import plot_entrainment_by_population\n\n# Plot PPC entrainment metrics for each population\nfig = plot_entrainment_by_population(\n    ppc_dict=ppc_results,\n    pop_names=['PV', 'SST'],\n    freqs=[4, 8, 20, 40, 80]\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/entrainment/#entrainment-swarm-plots","title":"Entrainment Swarm Plots","text":"<p>Visualize individual cell entrainment values at a specific frequency:</p> <pre><code>from bmtool.bmplot.entrainment import plot_entrainment_swarm_plot\n\n# Create swarm plot of entrainment at specific frequency\nplot_entrainment_swarm_plot(\n    ppc_dict=ppc_results,\n    pop_names=['PV', 'SST'],\n    freq=40  # Frequency in Hz\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/entrainment/#phase-histograms-with-spike-counts","title":"Phase Histograms with Spike Counts","text":"<p>Plot spike phase distributions across LFP cycles:</p> <pre><code>from bmtool.bmplot.entrainment import plot_cycle_with_spike_histograms\n\n# Plot spike phase distributions\nplot_cycle_with_spike_histograms(\n    phase_data=phase_data,\n    pop_names=['PV', 'SST'],\n    bins=36\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/entrainment/#trial-averaged-entrainment","title":"Trial-Averaged Entrainment","text":"<p>Plot trial-averaged entrainment metrics over time:</p> <pre><code>from bmtool.bmplot.entrainment import plot_trial_avg_entrainment\n\nfig = plot_trial_avg_entrainment(\n    spike_df=spike_df,\n    lfp_data=lfp,\n    fs=400,\n    pop_names=['PV', 'SST'],\n    time_range=[(0, 1000), (1000, 2000)],\n    freq_of_interest=40\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/entrainment/#firing-rate-phase-amplitude-histogram","title":"Firing Rate Phase-Amplitude Histogram","text":"<p>Plot firing rate as a function of LFP phase and amplitude:</p> <pre><code>from bmtool.bmplot.entrainment import plot_fr_hist_phase_amplitude\n\nfig = plot_fr_hist_phase_amplitude(\n    spike_df=spike_df,\n    lfp_data=lfp,\n    fs=400,\n    pop_names=['PV', 'SST'],\n    freq_of_interest=40\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/entrainment/#trial-averaged-spike-rate-plv","title":"Trial-Averaged Spike Rate PLV","text":"<p>Plot trial-averaged phase-locking values for spike rates:</p> <pre><code>from bmtool.bmplot.entrainment import plot_trial_avg_spike_rate_plv\n\nfig = plot_trial_avg_spike_rate_plv(\n    spike_df=spike_df,\n    lfp_data=lfp,\n    fs=400,\n    pop_names=['PV', 'SST'],\n    time_range=[(0, 1000), (1000, 2000)],\n    freq_of_interest=40\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/lfp/","title":"LFP/ECP Plotting","text":"<p>The <code>bmplot.lfp</code> module provides functions for visualizing LFP/ECP data, spectrograms, and power spectra.</p>"},{"location":"modules/bmplot/lfp/#spectrograms","title":"Spectrograms","text":"<pre><code>from bmtool.bmplot.lfp import plot_spectrogram\nimport matplotlib.pyplot as plt\n\n# Plot spectrogram\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_spectrogram(\n    sxx_xarray=spectrogram,\n    log_power=True,\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/lfp/#power-spectra","title":"Power Spectra","text":"<pre><code>from bmtool.bmplot.lfp import plot_power_spectrum\n\n# Plot power spectrum with FOOOF fit\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_power_spectrum(\n    freqs=freqs,\n    pxx=pxx,\n    fooof_model=fooof_model,\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/lfp/#lfp-time-series","title":"LFP Time Series","text":"<pre><code>from bmtool.bmplot.lfp import plot_lfp_timeseries\n\n# Plot LFP time series\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_lfp_timeseries(\n    lfp_data=lfp_data,\n    channels=[0, 1, 2],  # Optional: specify channels to plot\n    time_range=(0, 1000),  # Optional: specify time range\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/spikes/","title":"Spike Plotting","text":"<p>The <code>bmplot.spikes</code> module provides functions for visualizing spike data and firing rate statistics.</p>"},{"location":"modules/bmplot/spikes/#raster-plots","title":"Raster Plots","text":"<pre><code>from bmtool.bmplot import raster\nimport matplotlib.pyplot as plt\n\n# Create a raster plot\nfig, ax = plt.subplots(figsize=(10, 6))\nraster(\n    spikes_df=spikes_df,\n    groupby='pop_name',\n    time_range=(0, 2000),\n    ax=ax\n)\nplt.show()\n</code></pre>"},{"location":"modules/bmplot/spikes/#firing-rate-statistics","title":"Firing Rate Statistics","text":"<pre><code>from bmtool.bmplot import plot_firing_rate_pop_stats, plot_firing_rate_distribution\n\n# Plot population firing rate statistics\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_pop_stats(\n    firing_stats=pop_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n\n# Plot firing rate distributions\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_firing_rate_distribution(\n    individual_stats=individual_stats,\n    groupby='pop_name',\n    ax=ax\n)\nplt.show()\n</code></pre>"}]}